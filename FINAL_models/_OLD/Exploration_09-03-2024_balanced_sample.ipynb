{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**APPROCHE METHODOLOGIQUE**\n",
    "---\n",
    "\n",
    "Problème de classification (28 classes) avec évaluation \"sur mesure\" (composite)<br><br>. Jeu de donnée de 25.000 personnes, après voir lancé la baseline, j'ai entrainé un **adversarial NN**, qui permettait de gagner un peu d'équité (passage de 80% à 85% sur le TRP gap) mais a entrainé un éffondrement de l'accuracy (de 65% à 35%). après avoir cherché à personaliser la fonction de perte (avec difficulté pour entrainer le modele), je suis repasser à une approche plus progressive, sur la régression logistique\n",
    "\n",
    "**regression logisitique (baseline) => 3 problemes**<br>\n",
    "1. ***l'échantillonage*** : Le \"train_test_split\" ne tient pas compte de S, nous ne savons pas si l'attribut protégé (H/F) est bien réparti entre X_train et X_test, cela n'est pas optimum pour l'apprentissage car certaines classes contiennent peu de points. Il peut aussi etre plus pertinent de fixer les hyperparametre grace à X_train, et d'entrainer le modele sur tout l'échantillon (avec les memes hyperparametres) avant de prédire X_test_true et de soumettre<br>\n",
    "=> nous réaliserons des train_test_split en tenant compte de X et Y grace à Y56 = Y + 28*S <br>\n",
    "=> nous ferons des soumissions double (modele appris sur X_train et aussi X) pour le datachallenge, pour améliorer les qualités prédicitive du modèles <br><br>\n",
    "2. la fonction d'évaluation est une ***moyenne de scores calculé par classes***, chaque classe a donc le meme poids dans le score final. Or, l'apprentissage est fait pour optimser l'accuracy de l'échantillon (chaque élément à le meme poids). Comme la distribution de l'échatillon est très mal équilibrée entre les classes (93 à 8.285 personnes), le modèle ne peut apprendre correctement<br>\n",
    "=> nous essaierons pour la forme del a data augmentation, mais sans conviction compte tenu de l'embedding sémantique fait avec BERT<br>\n",
    "=> nous utiliserons une correction de la fonction de pertes pour y inclure le poids de chaque classe (Y et Y56)<br>\n",
    "=> nous essaierons du contrastive learning pour obtenir un espace de réprésentation adéquat<br><br>\n",
    "2. ***l'apprentissage n'ast pas aligné avec le score final***: l'apprentissage est fait pour optimiser l'accuracy (la précision), or la fonction d'évaluation est une moyenne entre le F1 score (moyenne harmonique de la précision et du rappel) et une métrique de fairness. Le F1 score pénalise les écarts importants entre la précision et le rappel. Le score<br>\n",
    "=> nous adapterons le fonctions de pertes et/ou l'architecture des modèles intégrer au mieux notrescore final<br> \n",
    "\n",
    "**Nous allons procéder en 2 temps** :\n",
    "\n",
    "1/ nous allons **échantilloner X par rapport à Y et S** traiter le probleme 1 et 2, en créant un label à 56 classes (2 x 28) pour distinguer chaque label i en fonction de s'il est protégé ou non. Nous augmenterons la taille de l'échantillon (avec 56 labels) pour créer X_XL et Y_XL pour avoir le meme nombre d'items pour chacune des 56 classes et un jeu avec le meme nombre d'instance par classe, permettant d'aligner les scores moyens des classes avec la moyenne du nouveau sample \"XL\" décuplé (passage de 27.749 lignes à 255.304 lignes ). Nous programmerons la régression logisitique avec pytorch pour accélérer les calculs.\n",
    "\n",
    "2/ nous **ajusterons ensuite la fonction de pertes**, pour qu'elle tienne compte au mieux de la fonction d'évaluation, qui est composée de 2 éléments : le F1 score et le 1-equal ooprtunity gap<br>\n",
    "- le 'macro f score' (F1 score) n'est pas dérivable (à valider), nous l'approcherons donc par accuracy et le recall<br>\n",
    "- true positive gap : 1 - TRP_GAP est calculé comme \n",
    "A noter que le F1 score n'est pas dérivable. However, we can approximate the F1 score in a differentiable manner by using the ***Sørensen–Dice coefficient***, which is closely related to the F1 score and is differentiable. The Sørensen–Dice coefficient is defined as \\(2 * \\frac{precision * recall}{precision + recall}\\), which is equivalent to the F1 score formula. For a differentiable approximation, we can use soft versions of precision and recall.\n",
    "<br><br>\n",
    "\n",
    "3/ Nous explorerons **différents modèles de classification** K-means, SVM, random forest tree<br><br>\n",
    "\n",
    "4/ Avant de mener cette approche j'avais essayer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nfrom tensorflow.keras.layers import Input, Dense, Dropout\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.metrics import AUC, SparseCategoricalAccuracy\\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from evaluator_ANAELE import *\n",
    "\n",
    "'''import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total2 = X[['1','label_x2','F','H']].groupby(['label_x2']).sum()\\ntotal2\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''label_56=pd.DataFrame({'Y':Y,'S':S})\n",
    "label_56['label_x2']=Y+28*S\n",
    "label_56['1']=1\n",
    "label_56['F']=S\n",
    "label_56['H']=1-S\n",
    "total_56 = label_56[['Y','1','F','H']].groupby(['Y']).sum()\n",
    "total_56'''\n",
    "\n",
    "'''total2 = X[['1','label_x2','F','H']].groupby(['label_x2']).sum()\n",
    "total2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_X_test_true(X, model,name):\n",
    "    Y_pred = model.predict(X)\n",
    "    results=pd.DataFrame(y_pred, columns= ['score'])\n",
    "    file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "    \n",
    "    return Y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 0 = BASELINE (28 classes)**\n",
    "---\n",
    "split uniquement sur Y (sans tenir compte de S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (final _ to keep split data untouched and be able to reload in file)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.49704D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.05446D-01    |proj g|=  1.61916D-02\n",
      "\n",
      "At iterate  100    f=  5.50259D-01    |proj g|=  2.34089D-02\n",
      "\n",
      "At iterate  150    f=  4.73919D-01    |proj g|=  6.63617D-03\n",
      "\n",
      "At iterate  200    f=  4.36572D-01    |proj g|=  3.26170D-03\n",
      "\n",
      "At iterate  250    f=  4.15622D-01    |proj g|=  2.62995D-03\n",
      "\n",
      "At iterate  300    f=  4.06518D-01    |proj g|=  8.17065D-03\n",
      "\n",
      "At iterate  350    f=  4.01723D-01    |proj g|=  1.90510D-03\n",
      "\n",
      "At iterate  400    f=  3.99333D-01    |proj g|=  2.77170D-03\n",
      "\n",
      "At iterate  450    f=  3.97783D-01    |proj g|=  9.01702D-04\n",
      "\n",
      "At iterate  500    f=  3.96904D-01    |proj g|=  4.49510D-04\n",
      "\n",
      "At iterate  550    f=  3.96496D-01    |proj g|=  5.93727D-04\n",
      "\n",
      "At iterate  600    f=  3.96233D-01    |proj g|=  5.38018D-04\n",
      "\n",
      "At iterate  650    f=  3.96080D-01    |proj g|=  3.29728D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    656    690      1     0     0   9.133D-05   3.961D-01\n",
      "  F =  0.39606734358358847     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy on transformed test data: 0.7633633633633634\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7633633633633634, 'macro_fscore': 0.6695276930026747, 'micro_fscore': 0.7633633633633634}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.62337659, 0.62416106, 0.85783131, 0.47500001, 0.57407405,\n",
      "       0.80952374, 0.88135589, 0.74999938, 0.7999998 , 0.78571423,\n",
      "       0.25000062, 0.64705881, 0.42424247, 0.70270259, 0.67010306,\n",
      "       0.2000012 , 0.51428571, 0.54166663, 0.83285301, 0.78620688,\n",
      "       0.59493668, 0.87262079, 0.60389609, 0.66666651, 0.49137931,\n",
      "       0.6048387 , 0.44366198, 0.5999996 ]), 'TNR': array([0.99638663, 0.98783287, 0.98312958, 0.99708846, 0.99842732,\n",
      "       0.99683329, 0.99422366, 0.99933289, 0.99888268, 0.99500794,\n",
      "       1.        , 0.98202614, 0.99552773, 0.99597135, 0.99637023,\n",
      "       0.99933333, 0.99463087, 0.9993305 , 0.98436748, 0.97100737,\n",
      "       0.99096249, 0.92258681, 0.98942772, 0.99977698, 0.98838004,\n",
      "       0.98590556, 0.98120559, 0.99977778]), 'PPV': array([0.74999992, 0.63698628, 0.83764704, 0.59374994, 0.81578931,\n",
      "       0.82926821, 0.86187841, 0.6666663 , 0.82758598, 0.77777772,\n",
      "       0.999995  , 0.64999999, 0.41176476, 0.59090905, 0.80246906,\n",
      "       0.25000125, 0.42857146, 0.81249961, 0.81638416, 0.74347825,\n",
      "       0.54022988, 0.83066202, 0.66906472, 0.93333276, 0.52777777,\n",
      "       0.71428569, 0.43448277, 0.74999875]), 'NPV': array([0.99346994, 0.98715301, 0.98553921, 0.99530516, 0.99485113,\n",
      "       0.99638254, 0.99514338, 0.99955516, 0.99865952, 0.99523377,\n",
      "       0.99866755, 0.98179696, 0.99575039, 0.99753418, 0.99276672,\n",
      "       0.99911131, 0.9961909 , 0.99754956, 0.98602746, 0.97700865,\n",
      "       0.9927569 , 0.94332247, 0.9860284 , 0.99844098, 0.98658176,\n",
      "       0.97718277, 0.98188073, 0.99955565]), 'FPR': array([3.61337173e-03, 1.21671281e-02, 1.68704181e-02, 2.91153640e-03,\n",
      "       1.57268255e-03, 3.16670663e-03, 5.77634243e-03, 6.67113633e-04,\n",
      "       1.11732067e-03, 4.99206037e-03, 2.22370470e-09, 1.79738586e-02,\n",
      "       4.47227416e-03, 4.02865041e-03, 3.62976634e-03, 6.66668890e-04,\n",
      "       5.36912977e-03, 6.69495650e-04, 1.56325181e-02, 2.89926315e-02,\n",
      "       9.03750793e-03, 7.74131923e-02, 1.05722846e-02, 2.23017396e-04,\n",
      "       1.16199613e-02, 1.40944351e-02, 1.87944099e-02, 2.22224445e-04]), 'FNR': array([0.37662341, 0.37583894, 0.14216869, 0.52499999, 0.42592595,\n",
      "       0.19047626, 0.11864411, 0.25000062, 0.2000002 , 0.21428577,\n",
      "       0.74999938, 0.35294119, 0.57575753, 0.29729741, 0.32989694,\n",
      "       0.7999988 , 0.48571429, 0.45833337, 0.16714699, 0.21379312,\n",
      "       0.40506332, 0.12737921, 0.39610391, 0.33333349, 0.50862069,\n",
      "       0.3951613 , 0.55633802, 0.4000004 ]), 'FDR': array([2.50000078e-01, 3.63013717e-01, 1.62352957e-01, 4.06250059e-01,\n",
      "       1.84210693e-01, 1.70731788e-01, 1.38121587e-01, 3.33333704e-01,\n",
      "       1.72414019e-01, 2.22222278e-01, 4.99995000e-06, 3.50000014e-01,\n",
      "       5.88235242e-01, 4.09090950e-01, 1.97530939e-01, 7.49998750e-01,\n",
      "       5.71428537e-01, 1.87500391e-01, 1.83615837e-01, 2.56521750e-01,\n",
      "       4.59770124e-01, 1.69337984e-01, 3.30935276e-01, 6.66672444e-02,\n",
      "       4.72222227e-01, 2.85714306e-01, 5.65517232e-01, 2.50001250e-01]), 'ACC': array([0.99001109, 0.97580466, 0.97158712, 0.99245283, 0.99334073,\n",
      "       0.99334073, 0.98978912, 0.99889012, 0.99755826, 0.99045505,\n",
      "       0.99866814, 0.96559378, 0.99134295, 0.9935627 , 0.98934517,\n",
      "       0.99844617, 0.990899  , 0.99689234, 0.972697  , 0.95316315,\n",
      "       0.98401775, 0.90743618, 0.97624861, 0.99822419, 0.97558268,\n",
      "       0.96492785, 0.96426193, 0.99933407]), 'PPR': array([0.01420644, 0.03240844, 0.09433963, 0.00710322, 0.00843508,\n",
      "       0.018202  , 0.04017758, 0.00199778, 0.0064373 , 0.02197559,\n",
      "       0.00044396, 0.04883463, 0.00754717, 0.00976693, 0.01798003,\n",
      "       0.00088791, 0.00932298, 0.00355161, 0.07857936, 0.10210877,\n",
      "       0.01931188, 0.31853496, 0.03085461, 0.00332964, 0.02397337,\n",
      "       0.04661488, 0.03218646, 0.00088791])}\n",
      "{'TPR': array([5.19230762e-01, 5.86956484e-01, 8.49206321e-01, 1.87500391e-01,\n",
      "       6.15384438e-01, 7.89473380e-01, 8.26086886e-01, 6.53333292e-01,\n",
      "       3.33331111e-06, 7.07316972e-01, 5.21739112e-01, 6.95238077e-01,\n",
      "       8.06451563e-01, 7.61273196e-01, 7.57142784e-01, 2.66666822e-01,\n",
      "       5.00000000e-01, 3.84615562e-01, 8.35051512e-01, 8.26839813e-01,\n",
      "       7.08333275e-01, 8.79182149e-01, 6.93877535e-01, 5.99999600e-01,\n",
      "       3.68421191e-01, 3.75000062e-01, 5.88235286e-01, 7.77777469e-01]), 'TNR': array([0.99708068, 0.99708532, 0.98206278, 0.99921135, 0.99868663,\n",
      "       0.99868455, 0.99651287, 0.99546061, 0.99973801, 0.99655993,\n",
      "       0.99815644, 0.98753462, 0.99512987, 0.97182689, 0.99573333,\n",
      "       0.99762533, 0.99737395, 0.99763593, 0.98676227, 0.97706968,\n",
      "       0.99252935, 0.93549562, 0.98013986, 0.99973787, 0.99815838,\n",
      "       0.99470899, 0.97693804, 0.99763282]), 'PPV': array([7.10526205e-01, 7.10526205e-01, 7.69784153e-01, 5.00000000e-01,\n",
      "       6.15384438e-01, 7.49999750e-01, 8.53932505e-01, 7.42424169e-01,\n",
      "       9.99980000e-06, 6.90476100e-01, 6.31578809e-01, 7.64397878e-01,\n",
      "       8.47457568e-01, 7.47395820e-01, 7.68115864e-01, 4.70588270e-01,\n",
      "       3.75000156e-01, 3.57143061e-01, 7.71428546e-01, 8.32243994e-01,\n",
      "       6.45569583e-01, 8.42386459e-01, 7.05394174e-01, 7.49998750e-01,\n",
      "       5.00000000e-01, 4.28571469e-01, 6.10328628e-01, 6.08695558e-01]), 'NPV': array([0.99338974, 0.9949762 , 0.9892716 , 0.9965915 , 0.99868663,\n",
      "       0.99894737, 0.9957116 , 0.99307405, 0.99921445, 0.99682371,\n",
      "       0.99710602, 0.98236428, 0.99351702, 0.97380675, 0.99546787,\n",
      "       0.99421509, 0.99842271, 0.99789805, 0.99113573, 0.97619756,\n",
      "       0.99438652, 0.95179829, 0.97904442, 0.99947589, 0.99684708,\n",
      "       0.99339498, 0.97477128, 0.99894653]), 'FPR': array([0.00291932, 0.00291468, 0.01793722, 0.00078865, 0.00131337,\n",
      "       0.00131545, 0.00348713, 0.00453939, 0.00026199, 0.00344007,\n",
      "       0.00184356, 0.01246538, 0.00487013, 0.02817311, 0.00426667,\n",
      "       0.00237467, 0.00262605, 0.00236407, 0.01323773, 0.02293032,\n",
      "       0.00747065, 0.06450438, 0.01986014, 0.00026213, 0.00184162,\n",
      "       0.00529101, 0.02306196, 0.00236718]), 'FNR': array([0.48076924, 0.41304352, 0.15079368, 0.81249961, 0.38461556,\n",
      "       0.21052662, 0.17391311, 0.34666671, 0.99999667, 0.29268303,\n",
      "       0.47826089, 0.30476192, 0.19354844, 0.2387268 , 0.24285722,\n",
      "       0.73333318, 0.5       , 0.61538444, 0.16494849, 0.17316019,\n",
      "       0.29166672, 0.12081785, 0.30612246, 0.4000004 , 0.63157881,\n",
      "       0.62499994, 0.41176471, 0.22222253]), 'FDR': array([0.2894738 , 0.2894738 , 0.23021585, 0.5       , 0.38461556,\n",
      "       0.25000025, 0.1460675 , 0.25757583, 0.99999   , 0.3095239 ,\n",
      "       0.36842119, 0.23560212, 0.15254243, 0.25260418, 0.23188414,\n",
      "       0.52941173, 0.62499984, 0.64285694, 0.22857145, 0.16775601,\n",
      "       0.35443042, 0.15761354, 0.29460583, 0.25000125, 0.5       ,\n",
      "       0.57142853, 0.38967137, 0.39130444]), 'ACC': array([0.99057591, 0.99214659, 0.97329842, 0.99581151, 0.99738219,\n",
      "       0.99764397, 0.99240837, 0.98874345, 0.99895287, 0.99345549,\n",
      "       0.99528795, 0.97146596, 0.98900523, 0.95104712, 0.99136125,\n",
      "       0.99188481, 0.99581151, 0.99554973, 0.97905759, 0.95890052,\n",
      "       0.98717277, 0.9196335 , 0.9617801 , 0.99921465, 0.99502617,\n",
      "       0.98821989, 0.95445026, 0.99659685]), 'PPR': array([9.94764919e-03, 9.94764919e-03, 7.27748742e-02, 1.57068586e-03,\n",
      "       3.40314659e-03, 5.23560732e-03, 2.32984345e-02, 1.72774921e-02,\n",
      "       2.61785340e-04, 1.09947696e-02, 4.97382721e-03, 5.00000051e-02,\n",
      "       3.08900575e-02, 1.00523565e-01, 1.80628324e-02, 4.45026700e-03,\n",
      "       4.18848690e-03, 3.66492669e-03, 5.49738271e-02, 1.20157073e-01,\n",
      "       2.06806335e-02, 2.93979062e-01, 6.30890103e-02, 1.04712565e-03,\n",
      "       3.66492669e-03, 9.16230888e-03, 5.57591674e-02, 6.02094763e-03])}\n",
      "final score 0.7327429522922824\n",
      "macro_fscore 0.6695276930026747\n",
      "1-eval_scores['TPR_GAP'] 0.7959582115818902\n"
     ]
    }
   ],
   "source": [
    "# training logistic model\n",
    "clf_0 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y_train)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred_0 = clf_0.predict(X_test)\n",
    "accuracy_0= accuracy_score(Y_test, Y_pred_0)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_0}\")\n",
    "eval_scores_0, confusion_matrices_eval_0 = gap_eval_scores(Y_pred_0, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_0 = (eval_scores_0['macro_fscore']+ (1-eval_scores_0['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_0)\n",
    "print('macro_fscore',eval_scores_0['macro_fscore'])\n",
    "print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores_0['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7633633633633634\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7633633633633634, 'macro_fscore': 0.6695276930026747, 'micro_fscore': 0.7633633633633634}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.62337659, 0.62416106, 0.85783131, 0.47500001, 0.57407405,\n",
      "       0.80952374, 0.88135589, 0.74999938, 0.7999998 , 0.78571423,\n",
      "       0.25000062, 0.64705881, 0.42424247, 0.70270259, 0.67010306,\n",
      "       0.2000012 , 0.51428571, 0.54166663, 0.83285301, 0.78620688,\n",
      "       0.59493668, 0.87262079, 0.60389609, 0.66666651, 0.49137931,\n",
      "       0.6048387 , 0.44366198, 0.5999996 ]), 'TNR': array([0.99638663, 0.98783287, 0.98312958, 0.99708846, 0.99842732,\n",
      "       0.99683329, 0.99422366, 0.99933289, 0.99888268, 0.99500794,\n",
      "       1.        , 0.98202614, 0.99552773, 0.99597135, 0.99637023,\n",
      "       0.99933333, 0.99463087, 0.9993305 , 0.98436748, 0.97100737,\n",
      "       0.99096249, 0.92258681, 0.98942772, 0.99977698, 0.98838004,\n",
      "       0.98590556, 0.98120559, 0.99977778]), 'PPV': array([0.74999992, 0.63698628, 0.83764704, 0.59374994, 0.81578931,\n",
      "       0.82926821, 0.86187841, 0.6666663 , 0.82758598, 0.77777772,\n",
      "       0.999995  , 0.64999999, 0.41176476, 0.59090905, 0.80246906,\n",
      "       0.25000125, 0.42857146, 0.81249961, 0.81638416, 0.74347825,\n",
      "       0.54022988, 0.83066202, 0.66906472, 0.93333276, 0.52777777,\n",
      "       0.71428569, 0.43448277, 0.74999875]), 'NPV': array([0.99346994, 0.98715301, 0.98553921, 0.99530516, 0.99485113,\n",
      "       0.99638254, 0.99514338, 0.99955516, 0.99865952, 0.99523377,\n",
      "       0.99866755, 0.98179696, 0.99575039, 0.99753418, 0.99276672,\n",
      "       0.99911131, 0.9961909 , 0.99754956, 0.98602746, 0.97700865,\n",
      "       0.9927569 , 0.94332247, 0.9860284 , 0.99844098, 0.98658176,\n",
      "       0.97718277, 0.98188073, 0.99955565]), 'FPR': array([3.61337173e-03, 1.21671281e-02, 1.68704181e-02, 2.91153640e-03,\n",
      "       1.57268255e-03, 3.16670663e-03, 5.77634243e-03, 6.67113633e-04,\n",
      "       1.11732067e-03, 4.99206037e-03, 2.22370470e-09, 1.79738586e-02,\n",
      "       4.47227416e-03, 4.02865041e-03, 3.62976634e-03, 6.66668890e-04,\n",
      "       5.36912977e-03, 6.69495650e-04, 1.56325181e-02, 2.89926315e-02,\n",
      "       9.03750793e-03, 7.74131923e-02, 1.05722846e-02, 2.23017396e-04,\n",
      "       1.16199613e-02, 1.40944351e-02, 1.87944099e-02, 2.22224445e-04]), 'FNR': array([0.37662341, 0.37583894, 0.14216869, 0.52499999, 0.42592595,\n",
      "       0.19047626, 0.11864411, 0.25000062, 0.2000002 , 0.21428577,\n",
      "       0.74999938, 0.35294119, 0.57575753, 0.29729741, 0.32989694,\n",
      "       0.7999988 , 0.48571429, 0.45833337, 0.16714699, 0.21379312,\n",
      "       0.40506332, 0.12737921, 0.39610391, 0.33333349, 0.50862069,\n",
      "       0.3951613 , 0.55633802, 0.4000004 ]), 'FDR': array([2.50000078e-01, 3.63013717e-01, 1.62352957e-01, 4.06250059e-01,\n",
      "       1.84210693e-01, 1.70731788e-01, 1.38121587e-01, 3.33333704e-01,\n",
      "       1.72414019e-01, 2.22222278e-01, 4.99995000e-06, 3.50000014e-01,\n",
      "       5.88235242e-01, 4.09090950e-01, 1.97530939e-01, 7.49998750e-01,\n",
      "       5.71428537e-01, 1.87500391e-01, 1.83615837e-01, 2.56521750e-01,\n",
      "       4.59770124e-01, 1.69337984e-01, 3.30935276e-01, 6.66672444e-02,\n",
      "       4.72222227e-01, 2.85714306e-01, 5.65517232e-01, 2.50001250e-01]), 'ACC': array([0.99001109, 0.97580466, 0.97158712, 0.99245283, 0.99334073,\n",
      "       0.99334073, 0.98978912, 0.99889012, 0.99755826, 0.99045505,\n",
      "       0.99866814, 0.96559378, 0.99134295, 0.9935627 , 0.98934517,\n",
      "       0.99844617, 0.990899  , 0.99689234, 0.972697  , 0.95316315,\n",
      "       0.98401775, 0.90743618, 0.97624861, 0.99822419, 0.97558268,\n",
      "       0.96492785, 0.96426193, 0.99933407]), 'PPR': array([0.01420644, 0.03240844, 0.09433963, 0.00710322, 0.00843508,\n",
      "       0.018202  , 0.04017758, 0.00199778, 0.0064373 , 0.02197559,\n",
      "       0.00044396, 0.04883463, 0.00754717, 0.00976693, 0.01798003,\n",
      "       0.00088791, 0.00932298, 0.00355161, 0.07857936, 0.10210877,\n",
      "       0.01931188, 0.31853496, 0.03085461, 0.00332964, 0.02397337,\n",
      "       0.04661488, 0.03218646, 0.00088791])}\n",
      "{'TPR': array([5.19230762e-01, 5.86956484e-01, 8.49206321e-01, 1.87500391e-01,\n",
      "       6.15384438e-01, 7.89473380e-01, 8.26086886e-01, 6.53333292e-01,\n",
      "       3.33331111e-06, 7.07316972e-01, 5.21739112e-01, 6.95238077e-01,\n",
      "       8.06451563e-01, 7.61273196e-01, 7.57142784e-01, 2.66666822e-01,\n",
      "       5.00000000e-01, 3.84615562e-01, 8.35051512e-01, 8.26839813e-01,\n",
      "       7.08333275e-01, 8.79182149e-01, 6.93877535e-01, 5.99999600e-01,\n",
      "       3.68421191e-01, 3.75000062e-01, 5.88235286e-01, 7.77777469e-01]), 'TNR': array([0.99708068, 0.99708532, 0.98206278, 0.99921135, 0.99868663,\n",
      "       0.99868455, 0.99651287, 0.99546061, 0.99973801, 0.99655993,\n",
      "       0.99815644, 0.98753462, 0.99512987, 0.97182689, 0.99573333,\n",
      "       0.99762533, 0.99737395, 0.99763593, 0.98676227, 0.97706968,\n",
      "       0.99252935, 0.93549562, 0.98013986, 0.99973787, 0.99815838,\n",
      "       0.99470899, 0.97693804, 0.99763282]), 'PPV': array([7.10526205e-01, 7.10526205e-01, 7.69784153e-01, 5.00000000e-01,\n",
      "       6.15384438e-01, 7.49999750e-01, 8.53932505e-01, 7.42424169e-01,\n",
      "       9.99980000e-06, 6.90476100e-01, 6.31578809e-01, 7.64397878e-01,\n",
      "       8.47457568e-01, 7.47395820e-01, 7.68115864e-01, 4.70588270e-01,\n",
      "       3.75000156e-01, 3.57143061e-01, 7.71428546e-01, 8.32243994e-01,\n",
      "       6.45569583e-01, 8.42386459e-01, 7.05394174e-01, 7.49998750e-01,\n",
      "       5.00000000e-01, 4.28571469e-01, 6.10328628e-01, 6.08695558e-01]), 'NPV': array([0.99338974, 0.9949762 , 0.9892716 , 0.9965915 , 0.99868663,\n",
      "       0.99894737, 0.9957116 , 0.99307405, 0.99921445, 0.99682371,\n",
      "       0.99710602, 0.98236428, 0.99351702, 0.97380675, 0.99546787,\n",
      "       0.99421509, 0.99842271, 0.99789805, 0.99113573, 0.97619756,\n",
      "       0.99438652, 0.95179829, 0.97904442, 0.99947589, 0.99684708,\n",
      "       0.99339498, 0.97477128, 0.99894653]), 'FPR': array([0.00291932, 0.00291468, 0.01793722, 0.00078865, 0.00131337,\n",
      "       0.00131545, 0.00348713, 0.00453939, 0.00026199, 0.00344007,\n",
      "       0.00184356, 0.01246538, 0.00487013, 0.02817311, 0.00426667,\n",
      "       0.00237467, 0.00262605, 0.00236407, 0.01323773, 0.02293032,\n",
      "       0.00747065, 0.06450438, 0.01986014, 0.00026213, 0.00184162,\n",
      "       0.00529101, 0.02306196, 0.00236718]), 'FNR': array([0.48076924, 0.41304352, 0.15079368, 0.81249961, 0.38461556,\n",
      "       0.21052662, 0.17391311, 0.34666671, 0.99999667, 0.29268303,\n",
      "       0.47826089, 0.30476192, 0.19354844, 0.2387268 , 0.24285722,\n",
      "       0.73333318, 0.5       , 0.61538444, 0.16494849, 0.17316019,\n",
      "       0.29166672, 0.12081785, 0.30612246, 0.4000004 , 0.63157881,\n",
      "       0.62499994, 0.41176471, 0.22222253]), 'FDR': array([0.2894738 , 0.2894738 , 0.23021585, 0.5       , 0.38461556,\n",
      "       0.25000025, 0.1460675 , 0.25757583, 0.99999   , 0.3095239 ,\n",
      "       0.36842119, 0.23560212, 0.15254243, 0.25260418, 0.23188414,\n",
      "       0.52941173, 0.62499984, 0.64285694, 0.22857145, 0.16775601,\n",
      "       0.35443042, 0.15761354, 0.29460583, 0.25000125, 0.5       ,\n",
      "       0.57142853, 0.38967137, 0.39130444]), 'ACC': array([0.99057591, 0.99214659, 0.97329842, 0.99581151, 0.99738219,\n",
      "       0.99764397, 0.99240837, 0.98874345, 0.99895287, 0.99345549,\n",
      "       0.99528795, 0.97146596, 0.98900523, 0.95104712, 0.99136125,\n",
      "       0.99188481, 0.99581151, 0.99554973, 0.97905759, 0.95890052,\n",
      "       0.98717277, 0.9196335 , 0.9617801 , 0.99921465, 0.99502617,\n",
      "       0.98821989, 0.95445026, 0.99659685]), 'PPR': array([9.94764919e-03, 9.94764919e-03, 7.27748742e-02, 1.57068586e-03,\n",
      "       3.40314659e-03, 5.23560732e-03, 2.32984345e-02, 1.72774921e-02,\n",
      "       2.61785340e-04, 1.09947696e-02, 4.97382721e-03, 5.00000051e-02,\n",
      "       3.08900575e-02, 1.00523565e-01, 1.80628324e-02, 4.45026700e-03,\n",
      "       4.18848690e-03, 3.66492669e-03, 5.49738271e-02, 1.20157073e-01,\n",
      "       2.06806335e-02, 2.93979062e-01, 6.30890103e-02, 1.04712565e-03,\n",
      "       3.66492669e-03, 9.16230888e-03, 5.57591674e-02, 6.02094763e-03])}\n",
      "final score 0.7327429522922824\n",
      "macro_fscore 0.6695276930026747\n",
      "1-eval_scores['TPR_GAP'] 0.7959582115818902\n"
     ]
    }
   ],
   "source": [
    "accuracy, final_score, macro_fscore , equity_score , eval_scores , confusion_matrices_eval =evaluate(Y_pred_0, Y_test,S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.48160D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.30229D-01    |proj g|=  1.75925D-02\n",
      "\n",
      "At iterate  100    f=  5.85159D-01    |proj g|=  7.52089D-03\n",
      "\n",
      "At iterate  150    f=  5.21361D-01    |proj g|=  1.53492D-02\n",
      "\n",
      "At iterate  200    f=  4.82218D-01    |proj g|=  6.42231D-03\n",
      "\n",
      "At iterate  250    f=  4.60058D-01    |proj g|=  1.24479D-02\n",
      "\n",
      "At iterate  300    f=  4.49281D-01    |proj g|=  4.70459D-03\n",
      "\n",
      "At iterate  350    f=  4.42280D-01    |proj g|=  5.22195D-03\n",
      "\n",
      "At iterate  400    f=  4.38594D-01    |proj g|=  2.02481D-03\n",
      "\n",
      "At iterate  450    f=  4.36522D-01    |proj g|=  3.92893D-03\n",
      "\n",
      "At iterate  500    f=  4.35307D-01    |proj g|=  7.10307D-04\n",
      "\n",
      "At iterate  550    f=  4.34608D-01    |proj g|=  2.45725D-03\n",
      "\n",
      "At iterate  600    f=  4.34215D-01    |proj g|=  4.29856D-04\n",
      "\n",
      "At iterate  650    f=  4.33949D-01    |proj g|=  3.35770D-04\n",
      "\n",
      "At iterate  700    f=  4.33793D-01    |proj g|=  8.50265D-04\n",
      "\n",
      "At iterate  750    f=  4.33698D-01    |proj g|=  2.10926D-04\n",
      "\n",
      "At iterate  800    f=  4.33630D-01    |proj g|=  2.02935D-04\n",
      "\n",
      "At iterate  850    f=  4.33588D-01    |proj g|=  3.34186D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    894    942      1     0     0   9.465D-05   4.336D-01\n",
      "  F =  0.43356001728963478     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "# training logistic model\n",
    "clf_0_all = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.8894014198709863\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.8894014198709863, 'macro_fscore': 0.9006861250232686, 'micro_fscore': 0.8894014198709863}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.86075946, 0.83703702, 0.93994211, 0.79687495, 0.95624994,\n",
      "       0.98148145, 0.95858894, 0.81818153, 0.98913033, 0.94524493,\n",
      "       0.99999929, 0.86052998, 0.80645155, 0.74015744, 0.9335664 ,\n",
      "       0.5714285 , 0.87121206, 0.9636362 , 0.94878705, 0.87140902,\n",
      "       0.89534881, 0.92520289, 0.74089068, 0.99999988, 0.82097185,\n",
      "       0.76521738, 0.64399092, 0.95238052]), 'TNR': array([0.99891297, 0.99542175, 0.99160159, 0.99946048, 0.99972966,\n",
      "       0.99911154, 0.99664429, 1.        , 1.        , 0.99863098,\n",
      "       1.        , 0.99044877, 0.99973087, 0.99925821, 0.99809134,\n",
      "       1.        , 0.99912304, 0.99986578, 0.99422091, 0.98265896,\n",
      "       0.99727854, 0.95777628, 0.99398423, 1.        , 0.99526262,\n",
      "       0.99208536, 0.99269721, 1.        ]), 'PPV': array([0.92727269, 0.87258686, 0.91932059, 0.92727265, 0.97452223,\n",
      "       0.96072505, 0.92867755, 0.99999944, 0.99999989, 0.94252871,\n",
      "       0.99999929, 0.8193891 , 0.94936697, 0.89523802, 0.90508472,\n",
      "       0.99999917, 0.89843744, 0.9636362 , 0.92957746, 0.84482758,\n",
      "       0.8523985 , 0.9057333 , 0.80794701, 0.99999988, 0.82307691,\n",
      "       0.84615384, 0.72820512, 0.9999995 ]), 'NPV': array([0.99776059, 0.99390497, 0.99387137, 0.99824869, 0.99952699,\n",
      "       0.99958974, 0.99810964, 0.99973223, 0.99993273, 0.99869934,\n",
      "       1.        , 0.99295923, 0.99879008, 0.99777793, 0.99870404,\n",
      "       0.99939775, 0.99885352, 0.99986578, 0.99587554, 0.98602023,\n",
      "       0.99816139, 0.96688999, 0.99117424, 1.        , 0.99519429,\n",
      "       0.98671633, 0.98922147, 0.99993305]), 'FPR': array([1.08703105e-03, 4.57824709e-03, 8.39840947e-03, 5.39520502e-04,\n",
      "       2.70344012e-04, 8.88464325e-04, 3.35570540e-03, 6.69612964e-10,\n",
      "       6.72766416e-10, 1.36901978e-03, 6.69254451e-10, 9.55123324e-03,\n",
      "       2.69125345e-04, 7.41790411e-04, 1.90865781e-03, 6.69568129e-10,\n",
      "       8.76956962e-04, 1.34219851e-04, 5.77909485e-03, 1.73410412e-02,\n",
      "       2.72145938e-03, 4.22237194e-02, 6.01576615e-03, 6.72359309e-10,\n",
      "       4.73738483e-03, 7.91463572e-03, 7.30279091e-03, 6.69568129e-10]), 'FNR': array([1.39240537e-01, 1.62962975e-01, 6.00578935e-02, 2.03125046e-01,\n",
      "       4.37500570e-02, 1.85185482e-02, 4.14110570e-02, 1.81818471e-01,\n",
      "       1.08696716e-02, 5.47550689e-02, 7.14284694e-07, 1.39470024e-01,\n",
      "       1.93548453e-01, 2.59842558e-01, 6.64335968e-02, 4.28571497e-01,\n",
      "       1.28787935e-01, 3.63638050e-02, 5.12129461e-02, 1.28590976e-01,\n",
      "       1.04651193e-01, 7.47971065e-02, 2.59109321e-01, 1.20481899e-07,\n",
      "       1.79028149e-01, 2.34782615e-01, 3.56009077e-01, 4.76194785e-02]), 'FDR': array([7.27273116e-02, 1.27413142e-01, 8.06794115e-02, 7.27273504e-02,\n",
      "       2.54777675e-02, 3.92749523e-02, 7.13224496e-02, 5.55554938e-07,\n",
      "       1.09890086e-07, 5.74712898e-02, 7.14284694e-07, 1.80610898e-01,\n",
      "       5.06330252e-02, 1.04761980e-01, 9.49152817e-02, 8.33331944e-07,\n",
      "       1.01562562e-01, 3.63638050e-02, 7.04225428e-02, 1.55172418e-01,\n",
      "       1.47601502e-01, 9.42666970e-02, 1.92052994e-01, 1.20481899e-07,\n",
      "       1.76923093e-01, 1.53846163e-01, 2.71794883e-01, 4.99999500e-07]), 'ACC': array([0.99672372, 0.98970313, 0.98682803, 0.99772666, 0.99926451,\n",
      "       0.99872961, 0.99498529, 0.99973255, 0.99993314, 0.99739235,\n",
      "       1.        , 0.98422038, 0.99852902, 0.99705804, 0.99685745,\n",
      "       0.99939823, 0.99799411, 0.99973255, 0.9908398 , 0.9717839 ,\n",
      "       0.99552019, 0.94784702, 0.9856245 , 1.        , 0.99070607,\n",
      "       0.9798743 , 0.98241508, 0.99993314]), 'PPR': array([0.01470982, 0.03463493, 0.09447713, 0.00735491, 0.01049746,\n",
      "       0.02213159, 0.04499866, 0.00120353, 0.00608452, 0.02326825,\n",
      "       0.00093608, 0.05034769, 0.00528216, 0.0070206 , 0.01972453,\n",
      "       0.00080235, 0.00855844, 0.00367746, 0.07595614, 0.1008291 ,\n",
      "       0.01811982, 0.31138005, 0.03028885, 0.00554961, 0.02607649,\n",
      "       0.04867612, 0.02607649, 0.00133726])}\n",
      "{'TPR': array([0.78666663, 0.82822082, 0.92817059, 0.64285709, 0.97826066,\n",
      "       0.98387081, 0.95321635, 0.9583333 , 0.99999933, 0.89473679,\n",
      "       0.97468342, 0.84525546, 0.96983757, 0.88477712, 0.94140622,\n",
      "       0.86956515, 0.89130418, 0.92857122, 0.92026577, 0.88392857,\n",
      "       0.86991867, 0.92646269, 0.78901373, 0.99999889, 0.72368415,\n",
      "       0.6153846 , 0.75588235, 0.9893616 ]), 'TNR': array([0.99897176, 0.9989707 , 0.99302638, 0.99984298, 0.99992155,\n",
      "       0.99992145, 0.9983937 , 0.99832709, 1.        , 0.99928803,\n",
      "       0.99976404, 0.99025438, 0.99838214, 0.98733195, 0.99840472,\n",
      "       0.99976337, 0.9995293 , 0.99992157, 0.99598064, 0.98818029,\n",
      "       0.99609468, 0.95654571, 0.98974316, 0.99992178, 0.99937092,\n",
      "       0.99826087, 0.98720383, 0.99976376]), 'PPV': array([0.9007633 , 0.91216211, 0.9087912 , 0.94736819, 0.97826066,\n",
      "       0.98387081, 0.94219651, 0.91633463, 0.99999933, 0.93793097,\n",
      "       0.96249988, 0.830703  , 0.95433788, 0.87739783, 0.92337162,\n",
      "       0.97087369, 0.87234027, 0.97499976, 0.91873962, 0.90570021,\n",
      "       0.81368819, 0.8975559 , 0.83708608, 0.8999992 , 0.87301575,\n",
      "       0.79999995, 0.7683109 , 0.9687499 ]), 'NPV': array([0.99747275, 0.99778569, 0.99461415, 0.99843199, 0.99992155,\n",
      "       0.99992145, 0.99871455, 0.99920268, 1.        , 0.99873498,\n",
      "       0.99984268, 0.99123677, 0.99894779, 0.98818354, 0.99880306,\n",
      "       0.99881797, 0.99960772, 0.99976476, 0.99606235, 0.98513894,\n",
      "       0.99744613, 0.96937521, 0.98596112, 1.        , 0.99835035,\n",
      "       0.99566349, 0.98630815, 0.99992124]), 'FPR': array([1.02823776e-03, 1.02929612e-03, 6.97361873e-03, 1.57023632e-04,\n",
      "       7.84506159e-05, 7.85492107e-05, 1.60629749e-03, 1.67290767e-03,\n",
      "       7.82595086e-10, 7.11969781e-04, 2.35961145e-04, 9.74562356e-03,\n",
      "       1.61786200e-03, 1.26680464e-02, 1.59527878e-03, 2.36631172e-04,\n",
      "       4.70699773e-04, 7.84260059e-05, 4.01935937e-03, 1.18197063e-02,\n",
      "       3.90531681e-03, 4.34542859e-02, 1.02568387e-02, 7.82235608e-05,\n",
      "       6.29079972e-04, 1.73913123e-03, 1.27961702e-02, 2.36239862e-04]), 'FNR': array([2.13333372e-01, 1.71779181e-01, 7.18294148e-02, 3.57142908e-01,\n",
      "       2.17393384e-02, 1.61291883e-02, 4.67836522e-02, 4.16667049e-02,\n",
      "       6.66665778e-07, 1.05263210e-01, 2.53165759e-02, 1.54744536e-01,\n",
      "       3.01624348e-02, 1.15222883e-01, 5.85937845e-02, 1.30434847e-01,\n",
      "       1.08695822e-01, 7.14287755e-02, 7.97342332e-02, 1.16071434e-01,\n",
      "       1.30081331e-01, 7.35373077e-02, 2.10986274e-01, 1.11110864e-06,\n",
      "       2.76315848e-01, 3.84615401e-01, 2.44117655e-01, 1.06384020e-02]), 'FDR': array([9.92367024e-02, 8.78378935e-02, 9.12088002e-02, 5.26318144e-02,\n",
      "       2.17393384e-02, 1.61291883e-02, 5.78034938e-02, 8.36653718e-02,\n",
      "       6.66665778e-07, 6.20690259e-02, 3.75001156e-02, 1.69296997e-01,\n",
      "       4.56621212e-02, 1.22602175e-01, 7.66283849e-02, 2.91263050e-02,\n",
      "       1.27659733e-01, 2.50002375e-02, 8.12603787e-02, 9.42997946e-02,\n",
      "       1.86311811e-01, 1.02444100e-01, 1.62913916e-01, 1.00000800e-01,\n",
      "       1.26984245e-01, 2.00000055e-01, 2.31689096e-01, 3.12500977e-02]), 'ACC': array([0.99648245, 0.99679512, 0.98850934, 0.99828031, 0.99984366,\n",
      "       0.99984366, 0.99718596, 0.9975768 , 1.        , 0.9980458 ,\n",
      "       0.99960916, 0.98249042, 0.99742046, 0.97780036, 0.99726413,\n",
      "       0.99859298, 0.99914015, 0.99968733, 0.99241773, 0.97631517,\n",
      "       0.99366841, 0.94778394, 0.97717502, 0.99992183, 0.99773313,\n",
      "       0.99398108, 0.97490815, 0.99968733]), 'PPR': array([0.01023998, 0.01156883, 0.07113265, 0.00297038, 0.00359572,\n",
      "       0.0048464 , 0.02704604, 0.01962011, 0.00117252, 0.01133433,\n",
      "       0.00625342, 0.05448292, 0.03423748, 0.09372313, 0.02040178,\n",
      "       0.00805128, 0.00367389, 0.00312671, 0.04713515, 0.11107637,\n",
      "       0.02055812, 0.30063316, 0.05901665, 0.00078168, 0.00492457,\n",
      "       0.00859845, 0.05229422, 0.00750411])}\n",
      "final score 0.9039500510797294\n",
      "macro_fscore 0.9006861250232686\n",
      "1-eval_scores['TPR_GAP'] 0.90721397713619\n"
     ]
    }
   ],
   "source": [
    "Y_pred_0_all = clf_0_all.predict(X)\n",
    "accuracy, final_score, macro_fscore , equity_score , eval_scores , confusion_matrices_eval =evaluate(Y_pred_0_all, Y,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true_0_all = clf_0_all.predict(X_test_true)\n",
    "results_0_all=pd.DataFrame(y_test_true_0_all, columns= ['score'])\n",
    "\n",
    "results_0_all.to_csv(\"Data_Challenge_MDI_341_0_all.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.49013D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.00321D-01    |proj g|=  2.45646D-02\n",
      "\n",
      "At iterate  100    f=  5.42959D-01    |proj g|=  1.33898D-02\n",
      "\n",
      "At iterate  150    f=  4.64078D-01    |proj g|=  4.90243D-03\n",
      "\n",
      "At iterate  200    f=  4.28360D-01    |proj g|=  9.46923D-03\n",
      "\n",
      "At iterate  250    f=  4.08372D-01    |proj g|=  3.70542D-03\n",
      "\n",
      "At iterate  300    f=  3.99541D-01    |proj g|=  9.83565D-03\n",
      "\n",
      "At iterate  350    f=  3.94082D-01    |proj g|=  4.78952D-03\n",
      "\n",
      "At iterate  400    f=  3.90846D-01    |proj g|=  2.24606D-03\n",
      "\n",
      "At iterate  450    f=  3.89635D-01    |proj g|=  5.83798D-04\n",
      "\n",
      "At iterate  500    f=  3.88954D-01    |proj g|=  9.84457D-04\n",
      "\n",
      "At iterate  550    f=  3.88570D-01    |proj g|=  1.04105D-03\n",
      "\n",
      "At iterate  600    f=  3.88323D-01    |proj g|=  1.25465D-03\n",
      "\n",
      "At iterate  650    f=  3.88213D-01    |proj g|=  4.94155D-04\n",
      "\n",
      "At iterate  700    f=  3.88151D-01    |proj g|=  2.91062D-04\n",
      "\n",
      "At iterate  750    f=  3.88114D-01    |proj g|=  1.85265D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    778    817      1     0     0   8.851D-05   3.881D-01\n",
      "  F =  0.38809803787470865     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy on transformed test data: 0.8576576576576577\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.8576576576576577, 'macro_fscore': 0.8409953402608368, 'micro_fscore': 0.8576576576576577}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.73529405, 0.80689651, 0.89376442, 0.66666657, 0.87234027,\n",
      "       0.95049496, 0.9494949 , 0.87499906, 0.96551692, 0.91891884,\n",
      "       0.5999996 , 0.79741377, 0.84374979, 0.72549011, 0.84848478,\n",
      "       0.57142837, 0.82051266, 0.72222198, 0.91954021, 0.85682325,\n",
      "       0.82352932, 0.91449814, 0.69594592, 0.95652134, 0.75806447,\n",
      "       0.69333332, 0.6614173 , 0.85714184]), 'TNR': array([0.99865259, 0.99428702, 0.98923679, 0.99866221, 0.99977648,\n",
      "       0.99886878, 0.99676151, 0.99955683, 0.99955476, 0.99773242,\n",
      "       1.        , 0.98904173, 0.99844063, 0.99865772, 0.99886929,\n",
      "       0.99955693, 0.99799197, 0.99977792, 0.99209202, 0.9783996 ,\n",
      "       0.99483494, 0.94552896, 0.99382575, 1.        , 0.99363202,\n",
      "       0.99324953, 0.98907601, 0.99977846]), 'PPV': array([0.892857  , 0.82394362, 0.89791181, 0.7999998 , 0.97619025,\n",
      "       0.95049496, 0.93069303, 0.77777716, 0.93333304, 0.91071421,\n",
      "       0.99999667, 0.79741377, 0.79411747, 0.86046495, 0.94382012,\n",
      "       0.66666611, 0.78048767, 0.92857082, 0.90651556, 0.81316347,\n",
      "       0.70886071, 0.8766928 , 0.79230765, 0.99999955, 0.77049176,\n",
      "       0.84324321, 0.63636362, 0.85714184]), 'NPV': array([0.99596864, 0.99360584, 0.98875305, 0.99732799, 0.99866041,\n",
      "       0.99886878, 0.99768465, 0.99977837, 0.99977733, 0.99795872,\n",
      "       0.99955732, 0.98904173, 0.99888567, 0.9968736 , 0.99661552,\n",
      "       0.99933555, 0.9984375 , 0.99889061, 0.99328215, 0.98419753,\n",
      "       0.99729851, 0.96311738, 0.98975176, 0.99977773, 0.99318027,\n",
      "       0.98408671, 0.99020278, 0.99977846]), 'FPR': array([1.34740849e-03, 5.71298219e-03, 1.07632119e-02, 1.33779487e-03,\n",
      "       2.23515870e-04, 1.13122398e-03, 3.23849411e-03, 4.43166409e-04,\n",
      "       4.45238202e-04, 2.26757597e-03, 2.21434899e-09, 1.09582677e-02,\n",
      "       1.55936957e-03, 1.34228412e-03, 1.13071235e-03, 4.43068233e-04,\n",
      "       2.00803436e-03, 2.22076394e-04, 7.90798229e-03, 2.16003952e-02,\n",
      "       5.16505952e-03, 5.44710361e-02, 6.17425339e-03, 2.22321032e-09,\n",
      "       6.36798046e-03, 6.75046789e-03, 1.09239896e-02, 2.21535224e-04]), 'FNR': array([0.26470595, 0.19310349, 0.10623558, 0.33333343, 0.12765973,\n",
      "       0.04950504, 0.0505051 , 0.12500094, 0.03448308, 0.08108116,\n",
      "       0.4000004 , 0.20258623, 0.15625021, 0.27450989, 0.15151522,\n",
      "       0.42857163, 0.17948734, 0.27777802, 0.08045979, 0.14317675,\n",
      "       0.17647068, 0.08550186, 0.30405408, 0.04347866, 0.24193553,\n",
      "       0.30666668, 0.3385827 , 0.14285816]), 'FDR': array([1.07142997e-01, 1.76056384e-01, 1.02088186e-01, 2.00000200e-01,\n",
      "       2.38097506e-02, 4.95050397e-02, 6.93069733e-02, 2.22222840e-01,\n",
      "       6.66669556e-02, 8.92857876e-02, 3.33331111e-06, 2.02586233e-01,\n",
      "       2.05882526e-01, 1.39535051e-01, 5.61798750e-02, 3.33333889e-01,\n",
      "       2.19512332e-01, 7.14291837e-02, 9.34844423e-02, 1.86836531e-01,\n",
      "       2.91139293e-01, 1.23307204e-01, 2.07692353e-01, 4.54545041e-07,\n",
      "       2.29508241e-01, 1.56756794e-01, 3.63636384e-01, 1.42858163e-01]), 'ACC': array([0.99469144, 0.98827693, 0.9800929 , 0.99601858, 0.99845167,\n",
      "       0.9977881 , 0.99469144, 0.99933643, 0.99933643, 0.99579739,\n",
      "       0.99955762, 0.97920814, 0.99734572, 0.9955762 , 0.9955762 ,\n",
      "       0.99889405, 0.99646096, 0.99867286, 0.98650741, 0.96637912,\n",
      "       0.99225835, 0.93629728, 0.98407432, 0.99977881, 0.98717098,\n",
      "       0.97832338, 0.97987171, 0.99955762]), 'PPR': array([0.01238664, 0.03140898, 0.0953329 , 0.0066357 , 0.00928998,\n",
      "       0.02234019, 0.04468038, 0.00199071, 0.0066357 , 0.02477328,\n",
      "       0.00066357, 0.05131608, 0.00752046, 0.00951117, 0.01968591,\n",
      "       0.00132714, 0.00906879, 0.00309666, 0.07808008, 0.1041805 ,\n",
      "       0.01747401, 0.31032958, 0.0287547 , 0.00486618, 0.02698518,\n",
      "       0.04092015, 0.02919708, 0.00154833])}\n",
      "{'TPR': array([0.64705877, 0.61538456, 0.89383559, 0.81818124, 0.84615331,\n",
      "       0.94736795, 0.94623646, 0.84722213, 0.999998  , 0.96774163,\n",
      "       0.74999979, 0.8090452 , 0.93333326, 0.82248519, 0.85714276,\n",
      "       0.71428559, 0.72727231, 0.78571388, 0.86627903, 0.88409089,\n",
      "       0.89610379, 0.91221719, 0.82278478, 0.5       , 0.77777757,\n",
      "       0.63043473, 0.68468467, 0.90624975]), 'TNR': array([0.99813482, 0.99787516, 0.988041  , 0.99947271, 0.99973621,\n",
      "       0.9997358 , 0.99730531, 0.99866023, 0.99973677, 0.99840975,\n",
      "       0.9994709 , 0.99084604, 0.997557  , 0.98470859, 0.99704856,\n",
      "       0.99867339, 0.99947271, 0.99920844, 0.99449339, 0.98365041,\n",
      "       0.99409713, 0.95553908, 0.98514157, 0.99973698, 0.99894096,\n",
      "       0.99787121, 0.98464545, 0.99893955]), 'PPV': array([0.82499984, 0.74999984, 0.86138611, 0.81818124, 0.91666597,\n",
      "       0.94736795, 0.8979591 , 0.9242423 , 0.83333222, 0.83333315,\n",
      "       0.8999996 , 0.82989687, 0.92561976, 0.83987913, 0.85714276,\n",
      "       0.83333311, 0.7999994 , 0.78571388, 0.88165676, 0.87612611,\n",
      "       0.7582417 , 0.89361701, 0.7862903 , 0.5       , 0.83999973,\n",
      "       0.78378363, 0.73429949, 0.87878765]), 'NPV': array([0.99521785, 0.99602333, 0.99114538, 0.99947271, 0.99947257,\n",
      "       0.9997358 , 0.99865083, 0.99705725, 1.        , 0.9997346 ,\n",
      "       0.99841437, 0.98947368, 0.99782786, 0.98272387, 0.99704856,\n",
      "       0.99735029, 0.99920928, 0.99920844, 0.99367262, 0.98482143,\n",
      "       0.99784541, 0.96375186, 0.98818897, 0.99973698, 0.99841228,\n",
      "       0.99548712, 0.98053934, 0.99920445]), 'FPR': array([0.00186518, 0.00212484, 0.011959  , 0.00052729, 0.00026379,\n",
      "       0.0002642 , 0.00269469, 0.00133977, 0.00026323, 0.00159025,\n",
      "       0.0005291 , 0.00915396, 0.002443  , 0.01529141, 0.00295144,\n",
      "       0.00132661, 0.00052729, 0.00079156, 0.00550661, 0.01634959,\n",
      "       0.00590287, 0.04446092, 0.01485843, 0.00026302, 0.00105904,\n",
      "       0.00212879, 0.01535455, 0.00106045]), 'FNR': array([3.52941234e-01, 3.84615444e-01, 1.06164411e-01, 1.81818760e-01,\n",
      "       1.53846686e-01, 5.26320499e-02, 5.37635368e-02, 1.52777874e-01,\n",
      "       1.99999200e-06, 3.22583663e-02, 2.50000208e-01, 1.90954805e-01,\n",
      "       6.66667389e-02, 1.77514812e-01, 1.42857236e-01, 2.85714408e-01,\n",
      "       2.72727686e-01, 2.14286122e-01, 1.33720973e-01, 1.15909108e-01,\n",
      "       1.03896207e-01, 8.77828129e-02, 1.77215217e-01, 5.00000000e-01,\n",
      "       2.22222428e-01, 3.69565274e-01, 3.15315332e-01, 9.37502539e-02]), 'FDR': array([0.17500016, 0.25000016, 0.13861389, 0.18181876, 0.08333403,\n",
      "       0.05263205, 0.1020409 , 0.0757577 , 0.16666778, 0.16666685,\n",
      "       0.1000004 , 0.17010313, 0.07438024, 0.16012087, 0.14285724,\n",
      "       0.16666689, 0.2000006 , 0.21428612, 0.11834324, 0.12387389,\n",
      "       0.2417583 , 0.10638299, 0.2137097 , 0.5       , 0.16000027,\n",
      "       0.21621637, 0.26570051, 0.12121235]), 'ACC': array([0.99342797, 0.99395373, 0.98080967, 0.99894847, 0.99921135,\n",
      "       0.99947423, 0.99605678, 0.9957939 , 0.99973711, 0.99815983,\n",
      "       0.99789695, 0.98133543, 0.99553101, 0.97029442, 0.99421661,\n",
      "       0.99605678, 0.99868559, 0.99842271, 0.9886961 , 0.97213459,\n",
      "       0.99211356, 0.94295478, 0.97502628, 0.99947423, 0.99737118,\n",
      "       0.99342797, 0.96713985, 0.99815983]), 'PPR': array([0.01051525, 0.0084122 , 0.079653  , 0.0028917 , 0.00315458,\n",
      "       0.00499475, 0.02576236, 0.01735016, 0.00157729, 0.00946373,\n",
      "       0.00525763, 0.05099895, 0.03180863, 0.08701367, 0.02024186,\n",
      "       0.00788644, 0.00262882, 0.00368034, 0.04442692, 0.11671925,\n",
      "       0.02392219, 0.29652997, 0.06519454, 0.00052577, 0.00657203,\n",
      "       0.00972661, 0.05441641, 0.00867508])}\n",
      "final score 0.8617612583914522\n",
      "macro_fscore 0.8409953402608368\n",
      "1-eval_scores['TPR_GAP'] 0.8825271765220676\n"
     ]
    }
   ],
   "source": [
    "# Split the data (final _ to keep split data untouched and be able to reload in file)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=1)\n",
    "\n",
    "# training logistic model\n",
    "clf_0_1 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y_train)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred_0_1 = clf_0.predict(X_test)\n",
    "accuracy_0_1= accuracy_score(Y_test, Y_pred_0_1)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_0_1}\")\n",
    "eval_scores_0_1, confusion_matrices_eval_0_1 = gap_eval_scores(Y_pred_0_1, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_0_1 = (eval_scores_0_1['macro_fscore']+ (1-eval_scores_0_1['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_0_1)\n",
    "print('macro_fscore',eval_scores_0_1['macro_fscore'])\n",
    "print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores_0_1['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true_0_1 = clf_0_1.predict(X_test_true)\n",
    "results_0_1=pd.DataFrame(y_test_true_0_1, columns= ['score'])\n",
    "\n",
    "results_0_1.to_csv(\"Data_Challenge_MDI_341_0_1.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 1 - BASELINE + SPLIT Y x S**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (final _ to keep split data untouched and be able to reload in file)\n",
    "X_train, X_test, Y56_train, Y56_test, S_train, S_test = train_test_split(X, Y56, S, test_size=0.3, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)\n",
    "Y_test = Y56_test % 28    # reste (original Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.49704D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.05446D-01    |proj g|=  1.61916D-02\n",
      "\n",
      "At iterate  100    f=  5.50259D-01    |proj g|=  2.34089D-02\n",
      "\n",
      "At iterate  150    f=  4.73919D-01    |proj g|=  6.63617D-03\n",
      "\n",
      "At iterate  200    f=  4.36572D-01    |proj g|=  3.26170D-03\n",
      "\n",
      "At iterate  250    f=  4.15622D-01    |proj g|=  2.62995D-03\n",
      "\n",
      "At iterate  300    f=  4.06518D-01    |proj g|=  8.17065D-03\n",
      "\n",
      "At iterate  350    f=  4.01723D-01    |proj g|=  1.90510D-03\n",
      "\n",
      "At iterate  400    f=  3.99333D-01    |proj g|=  2.77170D-03\n",
      "\n",
      "At iterate  450    f=  3.97783D-01    |proj g|=  9.01702D-04\n",
      "\n",
      "At iterate  500    f=  3.96904D-01    |proj g|=  4.49510D-04\n",
      "\n",
      "At iterate  550    f=  3.96496D-01    |proj g|=  5.93727D-04\n",
      "\n",
      "At iterate  600    f=  3.96233D-01    |proj g|=  5.38018D-04\n",
      "\n",
      "At iterate  650    f=  3.96080D-01    |proj g|=  3.29728D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    656    690      1     0     0   9.133D-05   3.961D-01\n",
      "  F =  0.39606734358358847     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy on transformed test data: 0.7633633633633634\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7633633633633634, 'macro_fscore': 0.6695276930026747, 'micro_fscore': 0.7633633633633634}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.62337659, 0.62416106, 0.85783131, 0.47500001, 0.57407405,\n",
      "       0.80952374, 0.88135589, 0.74999938, 0.7999998 , 0.78571423,\n",
      "       0.25000062, 0.64705881, 0.42424247, 0.70270259, 0.67010306,\n",
      "       0.2000012 , 0.51428571, 0.54166663, 0.83285301, 0.78620688,\n",
      "       0.59493668, 0.87262079, 0.60389609, 0.66666651, 0.49137931,\n",
      "       0.6048387 , 0.44366198, 0.5999996 ]), 'TNR': array([0.99638663, 0.98783287, 0.98312958, 0.99708846, 0.99842732,\n",
      "       0.99683329, 0.99422366, 0.99933289, 0.99888268, 0.99500794,\n",
      "       1.        , 0.98202614, 0.99552773, 0.99597135, 0.99637023,\n",
      "       0.99933333, 0.99463087, 0.9993305 , 0.98436748, 0.97100737,\n",
      "       0.99096249, 0.92258681, 0.98942772, 0.99977698, 0.98838004,\n",
      "       0.98590556, 0.98120559, 0.99977778]), 'PPV': array([0.74999992, 0.63698628, 0.83764704, 0.59374994, 0.81578931,\n",
      "       0.82926821, 0.86187841, 0.6666663 , 0.82758598, 0.77777772,\n",
      "       0.999995  , 0.64999999, 0.41176476, 0.59090905, 0.80246906,\n",
      "       0.25000125, 0.42857146, 0.81249961, 0.81638416, 0.74347825,\n",
      "       0.54022988, 0.83066202, 0.66906472, 0.93333276, 0.52777777,\n",
      "       0.71428569, 0.43448277, 0.74999875]), 'NPV': array([0.99346994, 0.98715301, 0.98553921, 0.99530516, 0.99485113,\n",
      "       0.99638254, 0.99514338, 0.99955516, 0.99865952, 0.99523377,\n",
      "       0.99866755, 0.98179696, 0.99575039, 0.99753418, 0.99276672,\n",
      "       0.99911131, 0.9961909 , 0.99754956, 0.98602746, 0.97700865,\n",
      "       0.9927569 , 0.94332247, 0.9860284 , 0.99844098, 0.98658176,\n",
      "       0.97718277, 0.98188073, 0.99955565]), 'FPR': array([3.61337173e-03, 1.21671281e-02, 1.68704181e-02, 2.91153640e-03,\n",
      "       1.57268255e-03, 3.16670663e-03, 5.77634243e-03, 6.67113633e-04,\n",
      "       1.11732067e-03, 4.99206037e-03, 2.22370470e-09, 1.79738586e-02,\n",
      "       4.47227416e-03, 4.02865041e-03, 3.62976634e-03, 6.66668890e-04,\n",
      "       5.36912977e-03, 6.69495650e-04, 1.56325181e-02, 2.89926315e-02,\n",
      "       9.03750793e-03, 7.74131923e-02, 1.05722846e-02, 2.23017396e-04,\n",
      "       1.16199613e-02, 1.40944351e-02, 1.87944099e-02, 2.22224445e-04]), 'FNR': array([0.37662341, 0.37583894, 0.14216869, 0.52499999, 0.42592595,\n",
      "       0.19047626, 0.11864411, 0.25000062, 0.2000002 , 0.21428577,\n",
      "       0.74999938, 0.35294119, 0.57575753, 0.29729741, 0.32989694,\n",
      "       0.7999988 , 0.48571429, 0.45833337, 0.16714699, 0.21379312,\n",
      "       0.40506332, 0.12737921, 0.39610391, 0.33333349, 0.50862069,\n",
      "       0.3951613 , 0.55633802, 0.4000004 ]), 'FDR': array([2.50000078e-01, 3.63013717e-01, 1.62352957e-01, 4.06250059e-01,\n",
      "       1.84210693e-01, 1.70731788e-01, 1.38121587e-01, 3.33333704e-01,\n",
      "       1.72414019e-01, 2.22222278e-01, 4.99995000e-06, 3.50000014e-01,\n",
      "       5.88235242e-01, 4.09090950e-01, 1.97530939e-01, 7.49998750e-01,\n",
      "       5.71428537e-01, 1.87500391e-01, 1.83615837e-01, 2.56521750e-01,\n",
      "       4.59770124e-01, 1.69337984e-01, 3.30935276e-01, 6.66672444e-02,\n",
      "       4.72222227e-01, 2.85714306e-01, 5.65517232e-01, 2.50001250e-01]), 'ACC': array([0.99001109, 0.97580466, 0.97158712, 0.99245283, 0.99334073,\n",
      "       0.99334073, 0.98978912, 0.99889012, 0.99755826, 0.99045505,\n",
      "       0.99866814, 0.96559378, 0.99134295, 0.9935627 , 0.98934517,\n",
      "       0.99844617, 0.990899  , 0.99689234, 0.972697  , 0.95316315,\n",
      "       0.98401775, 0.90743618, 0.97624861, 0.99822419, 0.97558268,\n",
      "       0.96492785, 0.96426193, 0.99933407]), 'PPR': array([0.01420644, 0.03240844, 0.09433963, 0.00710322, 0.00843508,\n",
      "       0.018202  , 0.04017758, 0.00199778, 0.0064373 , 0.02197559,\n",
      "       0.00044396, 0.04883463, 0.00754717, 0.00976693, 0.01798003,\n",
      "       0.00088791, 0.00932298, 0.00355161, 0.07857936, 0.10210877,\n",
      "       0.01931188, 0.31853496, 0.03085461, 0.00332964, 0.02397337,\n",
      "       0.04661488, 0.03218646, 0.00088791])}\n",
      "{'TPR': array([5.19230762e-01, 5.86956484e-01, 8.49206321e-01, 1.87500391e-01,\n",
      "       6.15384438e-01, 7.89473380e-01, 8.26086886e-01, 6.53333292e-01,\n",
      "       3.33331111e-06, 7.07316972e-01, 5.21739112e-01, 6.95238077e-01,\n",
      "       8.06451563e-01, 7.61273196e-01, 7.57142784e-01, 2.66666822e-01,\n",
      "       5.00000000e-01, 3.84615562e-01, 8.35051512e-01, 8.26839813e-01,\n",
      "       7.08333275e-01, 8.79182149e-01, 6.93877535e-01, 5.99999600e-01,\n",
      "       3.68421191e-01, 3.75000062e-01, 5.88235286e-01, 7.77777469e-01]), 'TNR': array([0.99708068, 0.99708532, 0.98206278, 0.99921135, 0.99868663,\n",
      "       0.99868455, 0.99651287, 0.99546061, 0.99973801, 0.99655993,\n",
      "       0.99815644, 0.98753462, 0.99512987, 0.97182689, 0.99573333,\n",
      "       0.99762533, 0.99737395, 0.99763593, 0.98676227, 0.97706968,\n",
      "       0.99252935, 0.93549562, 0.98013986, 0.99973787, 0.99815838,\n",
      "       0.99470899, 0.97693804, 0.99763282]), 'PPV': array([7.10526205e-01, 7.10526205e-01, 7.69784153e-01, 5.00000000e-01,\n",
      "       6.15384438e-01, 7.49999750e-01, 8.53932505e-01, 7.42424169e-01,\n",
      "       9.99980000e-06, 6.90476100e-01, 6.31578809e-01, 7.64397878e-01,\n",
      "       8.47457568e-01, 7.47395820e-01, 7.68115864e-01, 4.70588270e-01,\n",
      "       3.75000156e-01, 3.57143061e-01, 7.71428546e-01, 8.32243994e-01,\n",
      "       6.45569583e-01, 8.42386459e-01, 7.05394174e-01, 7.49998750e-01,\n",
      "       5.00000000e-01, 4.28571469e-01, 6.10328628e-01, 6.08695558e-01]), 'NPV': array([0.99338974, 0.9949762 , 0.9892716 , 0.9965915 , 0.99868663,\n",
      "       0.99894737, 0.9957116 , 0.99307405, 0.99921445, 0.99682371,\n",
      "       0.99710602, 0.98236428, 0.99351702, 0.97380675, 0.99546787,\n",
      "       0.99421509, 0.99842271, 0.99789805, 0.99113573, 0.97619756,\n",
      "       0.99438652, 0.95179829, 0.97904442, 0.99947589, 0.99684708,\n",
      "       0.99339498, 0.97477128, 0.99894653]), 'FPR': array([0.00291932, 0.00291468, 0.01793722, 0.00078865, 0.00131337,\n",
      "       0.00131545, 0.00348713, 0.00453939, 0.00026199, 0.00344007,\n",
      "       0.00184356, 0.01246538, 0.00487013, 0.02817311, 0.00426667,\n",
      "       0.00237467, 0.00262605, 0.00236407, 0.01323773, 0.02293032,\n",
      "       0.00747065, 0.06450438, 0.01986014, 0.00026213, 0.00184162,\n",
      "       0.00529101, 0.02306196, 0.00236718]), 'FNR': array([0.48076924, 0.41304352, 0.15079368, 0.81249961, 0.38461556,\n",
      "       0.21052662, 0.17391311, 0.34666671, 0.99999667, 0.29268303,\n",
      "       0.47826089, 0.30476192, 0.19354844, 0.2387268 , 0.24285722,\n",
      "       0.73333318, 0.5       , 0.61538444, 0.16494849, 0.17316019,\n",
      "       0.29166672, 0.12081785, 0.30612246, 0.4000004 , 0.63157881,\n",
      "       0.62499994, 0.41176471, 0.22222253]), 'FDR': array([0.2894738 , 0.2894738 , 0.23021585, 0.5       , 0.38461556,\n",
      "       0.25000025, 0.1460675 , 0.25757583, 0.99999   , 0.3095239 ,\n",
      "       0.36842119, 0.23560212, 0.15254243, 0.25260418, 0.23188414,\n",
      "       0.52941173, 0.62499984, 0.64285694, 0.22857145, 0.16775601,\n",
      "       0.35443042, 0.15761354, 0.29460583, 0.25000125, 0.5       ,\n",
      "       0.57142853, 0.38967137, 0.39130444]), 'ACC': array([0.99057591, 0.99214659, 0.97329842, 0.99581151, 0.99738219,\n",
      "       0.99764397, 0.99240837, 0.98874345, 0.99895287, 0.99345549,\n",
      "       0.99528795, 0.97146596, 0.98900523, 0.95104712, 0.99136125,\n",
      "       0.99188481, 0.99581151, 0.99554973, 0.97905759, 0.95890052,\n",
      "       0.98717277, 0.9196335 , 0.9617801 , 0.99921465, 0.99502617,\n",
      "       0.98821989, 0.95445026, 0.99659685]), 'PPR': array([9.94764919e-03, 9.94764919e-03, 7.27748742e-02, 1.57068586e-03,\n",
      "       3.40314659e-03, 5.23560732e-03, 2.32984345e-02, 1.72774921e-02,\n",
      "       2.61785340e-04, 1.09947696e-02, 4.97382721e-03, 5.00000051e-02,\n",
      "       3.08900575e-02, 1.00523565e-01, 1.80628324e-02, 4.45026700e-03,\n",
      "       4.18848690e-03, 3.66492669e-03, 5.49738271e-02, 1.20157073e-01,\n",
      "       2.06806335e-02, 2.93979062e-01, 6.30890103e-02, 1.04712565e-03,\n",
      "       3.66492669e-03, 9.16230888e-03, 5.57591674e-02, 6.02094763e-03])}\n",
      "final score 0.7327429522922824\n",
      "macro_fscore 0.6695276930026747\n",
      "1-eval_scores_1['TPR_GAP'] 0.7959582115818902\n"
     ]
    }
   ],
   "source": [
    "# training logistic model\n",
    "clf_1 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y_train)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred_1 = clf_1.predict(X_test)\n",
    "accuracy_1= accuracy_score(Y_test, Y_pred_1)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_1}\")\n",
    "eval_scores_1, confusion_matrices_eval_1 = gap_eval_scores(Y_pred_1, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_1 = (eval_scores_1['macro_fscore']+ (1-eval_scores_1['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_1)\n",
    "print('macro_fscore',eval_scores_1['macro_fscore'])\n",
    "print('1-eval_scores_1[\\'TPR_GAP\\']',1-eval_scores_1['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 2 = BASELINE + DATA AUGMENTATION (28 classes + augmenté)**\n",
    "---\n",
    "split uniquement sur Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203728, 768) (203728,) (203728,)\n",
      "\n",
      "distribution S (original sample): 27749\n",
      "[14956 12793] [53.9 46.1]%\n",
      "\n",
      "distribution S_XL (data augmentation): 203728\n",
      "[101864 101864] [50. 50.]%\n",
      "\n",
      "distribution Y56 (56 classes): 22199  (min 0.1% , max 65.0%)\n",
      "[ 186  447 1096  101  122  267  539   16   75  282    6  573   69  104\n",
      "  222   17  107   39  866 1172  208 3638  394   70  315  646  352   19\n",
      "  120  129  719   47   36   47  277  192   14  126   65  543  345  954\n",
      "  208  100   38   32  466 1135  198 3011  637    6   63  119  542   82]\n",
      "\n",
      "distribution Y56_XL (56 classes + data augmentation): 203728 (x9)\n",
      "[3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a dataset with features X and labels Y\n",
    "\n",
    "# Split the dat using both Y and S using Y56_XL\n",
    "X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_ = train_test_split(X, Y, S, test_size=0.2, random_state=42)\n",
    "\n",
    "#refresh (if necessary)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_\n",
    "Y56_train = Y_train + 28 * S_train\n",
    "\n",
    "# Initialize SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the model to generate the data\n",
    "X_train_XL, Y56_train_XL, = sm.fit_resample(X_train, Y56_train)\n",
    "Y_train_XL = Y56_train_XL % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train_XL = Y56_train_XL//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "print(X_train_XL.shape,Y_train_XL.shape,S_train_XL.shape)\n",
    "\n",
    "print(f'\\ndistribution S (original sample): {len(S)}\\n{np.bincount(S)} {np.round(np.bincount(S)/len(S)*100,2)}%')\n",
    "print(f'\\ndistribution S_XL (data augmentation): {len(S_train_XL)}\\n{np.bincount(S_train_XL)} {np.round(np.bincount(S_train_XL)/len(S_train_XL)*100,2)}%')\n",
    "Y56_grouped = np.bincount(Y56_train)\n",
    "print(f'\\ndistribution Y56 (56 classes): {len(Y56_train)}  (min {round(min(Y56_grouped)/len(Y56_grouped),1)}% , max {round(max(Y56_grouped)/len(Y56_grouped),1)}%)\\n{Y56_grouped}')\n",
    "print(f'\\ndistribution Y56_XL (56 classes + data augmentation): {len(Y56_train_XL)} (x{round(len(Y56_train_XL)/len(Y56_train))})\\n{np.bincount(Y56_train_XL)}')\n",
    "\n",
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.93977D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  3.88901D-01    |proj g|=  1.75555D-02\n",
      "\n",
      "At iterate  100    f=  2.45014D-01    |proj g|=  1.28118D-02\n",
      "\n",
      "At iterate  150    f=  1.83725D-01    |proj g|=  3.26611D-03\n",
      "\n",
      "At iterate  200    f=  1.54198D-01    |proj g|=  2.75911D-03\n",
      "\n",
      "At iterate  250    f=  1.39105D-01    |proj g|=  1.88299D-03\n",
      "\n",
      "At iterate  300    f=  1.28366D-01    |proj g|=  3.43925D-03\n",
      "\n",
      "At iterate  350    f=  1.21936D-01    |proj g|=  1.52751D-03\n",
      "\n",
      "At iterate  400    f=  1.17522D-01    |proj g|=  8.83049D-04\n",
      "\n",
      "At iterate  450    f=  1.14932D-01    |proj g|=  5.08136D-04\n",
      "\n",
      "At iterate  500    f=  1.13282D-01    |proj g|=  9.50201D-04\n",
      "\n",
      "At iterate  550    f=  1.12090D-01    |proj g|=  2.05493D-03\n",
      "\n",
      "At iterate  600    f=  1.11323D-01    |proj g|=  4.32173D-04\n",
      "\n",
      "At iterate  650    f=  1.10835D-01    |proj g|=  7.84113D-04\n",
      "\n",
      "At iterate  700    f=  1.10492D-01    |proj g|=  1.11582D-03\n",
      "\n",
      "At iterate  750    f=  1.10261D-01    |proj g|=  2.71370D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    794    836      1     0     0   9.954D-05   1.101D-01\n",
      "  F =  0.11010590602294348     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Coefficients: (28, 768)\n",
      "Intercept: (28,)\n",
      "Accuracy on transformed test data: 0.7282882882882883\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7282882882882883, 'macro_fscore': 0.643951515882604, 'micro_fscore': 0.7282882882882883}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.64705877, 0.62365589, 0.79720278, 0.62962953, 0.55263155,\n",
      "       0.80701744, 0.86725657, 0.66666611, 0.76470557, 0.79999991,\n",
      "       0.25000062, 0.62499998, 0.41666674, 0.78260845, 0.59374997,\n",
      "       0.25000125, 0.55999995, 0.56249992, 0.77327933, 0.71034481,\n",
      "       0.59999996, 0.76221498, 0.70999996, 0.6923074 , 0.52631578,\n",
      "       0.70440249, 0.46067417, 0.5       ]), 'TNR': array([0.99053094, 0.98284734, 0.98493754, 0.99496813, 0.99730639,\n",
      "       0.99627245, 0.99447323, 0.99900066, 0.99832831, 0.99388379,\n",
      "       0.99833333, 0.97555866, 0.99329758, 0.99329983, 0.99320652,\n",
      "       0.99700399, 0.98927254, 0.99933155, 0.98623687, 0.97203826,\n",
      "       0.99019608, 0.95160517, 0.98177441, 0.99966611, 0.98533424,\n",
      "       0.97788697, 0.97499143, 0.99900199]), 'PPV': array([0.54098359, 0.53703703, 0.84758362, 0.53124998, 0.72413778,\n",
      "       0.80701744, 0.85964906, 0.57142837, 0.72222198, 0.74285707,\n",
      "       0.2857149 , 0.56249999, 0.33333344, 0.47368422, 0.65517236,\n",
      "       0.1000008 , 0.30434791, 0.81818124, 0.83406111, 0.73049644,\n",
      "       0.50847457, 0.87422166, 0.57258063, 0.8999992 , 0.48192772,\n",
      "       0.63999998, 0.35964915, 0.25000125]), 'NPV': array([0.99389209, 0.98793103, 0.97882438, 0.99663978, 0.99429338,\n",
      "       0.99627245, 0.99481686, 0.99933355, 0.9986622 , 0.99557522,\n",
      "       0.99800066, 0.98103932, 0.99529885, 0.99831649, 0.99118644,\n",
      "       0.99899933, 0.99628629, 0.99766433, 0.97984886, 0.96918562,\n",
      "       0.99321804, 0.90068027, 0.98994452, 0.99866577, 0.9876923 ,\n",
      "       0.98340981, 0.98341396, 0.99966711]), 'FPR': array([0.00946906, 0.01715266, 0.01506246, 0.00503187, 0.00269361,\n",
      "       0.00372755, 0.00552677, 0.00099934, 0.00167169, 0.00611621,\n",
      "       0.00166667, 0.02444134, 0.00670242, 0.00670017, 0.00679348,\n",
      "       0.00299601, 0.01072746, 0.00066845, 0.01376313, 0.02796174,\n",
      "       0.00980392, 0.04839483, 0.01822559, 0.00033389, 0.01466576,\n",
      "       0.02211303, 0.02500857, 0.00099801]), 'FNR': array([0.35294123, 0.37634411, 0.20279722, 0.37037047, 0.44736845,\n",
      "       0.19298256, 0.13274343, 0.33333389, 0.23529443, 0.20000009,\n",
      "       0.74999938, 0.37500002, 0.58333326, 0.21739155, 0.40625003,\n",
      "       0.74999875, 0.44000005, 0.43750008, 0.22672067, 0.28965519,\n",
      "       0.40000004, 0.23778502, 0.29000004, 0.3076926 , 0.47368422,\n",
      "       0.29559751, 0.53932583, 0.5       ]), 'FDR': array([0.45901641, 0.46296297, 0.15241638, 0.46875002, 0.27586222,\n",
      "       0.19298256, 0.14035094, 0.42857163, 0.27777802, 0.25714293,\n",
      "       0.7142851 , 0.43750001, 0.66666656, 0.52631578, 0.34482764,\n",
      "       0.8999992 , 0.69565209, 0.18181876, 0.16593889, 0.26950356,\n",
      "       0.49152543, 0.12577834, 0.42741937, 0.1000008 , 0.51807228,\n",
      "       0.36000002, 0.64035085, 0.74999875]), 'ACC': array([0.98470744, 0.97174201, 0.96708776, 0.99168882, 0.99168882,\n",
      "       0.99268616, 0.98969414, 0.99833776, 0.99700797, 0.98969414,\n",
      "       0.99634308, 0.95877659, 0.9886968 , 0.99168882, 0.98470744,\n",
      "       0.99601063, 0.98570478, 0.99700797, 0.96874999, 0.9468085 ,\n",
      "       0.9837101 , 0.89361701, 0.97273936, 0.99833776, 0.9737367 ,\n",
      "       0.96343084, 0.95977393, 0.99867021]), 'PPR': array([0.02027926, 0.03590426, 0.0894282 , 0.0106383 , 0.00964096,\n",
      "       0.01894947, 0.03789894, 0.00232713, 0.00598405, 0.02327128,\n",
      "       0.00232713, 0.0531915 , 0.00997341, 0.01263299, 0.01928192,\n",
      "       0.00332447, 0.01529256, 0.00365692, 0.07613033, 0.09375001,\n",
      "       0.01961437, 0.26695479, 0.04122341, 0.00332447, 0.02759309,\n",
      "       0.0581782 , 0.03789894, 0.00132979])}\n",
      "{'TPR': array([5.66666622e-01, 6.47058737e-01, 7.90697641e-01, 2.22222840e-01,\n",
      "       6.99999600e-01, 7.33333022e-01, 7.99999908e-01, 7.49999896e-01,\n",
      "       9.99980000e-06, 6.15384527e-01, 5.00000000e-01, 6.97183071e-01,\n",
      "       7.20930181e-01, 7.65957424e-01, 7.08333247e-01, 5.33333289e-01,\n",
      "       7.49999375e-01, 5.00000000e-01, 7.57352903e-01, 7.91277240e-01,\n",
      "       7.49999896e-01, 7.87412579e-01, 6.82926807e-01, 9.99996667e-01,\n",
      "       3.84615562e-01, 5.00000000e-01, 6.44927515e-01, 8.33332778e-01]), 'TNR': array([0.99363057, 0.99481658, 0.98101265, 0.99763126, 0.99921011,\n",
      "       0.99841709, 0.99515543, 0.99478749, 0.9992129 , 0.99483306,\n",
      "       0.99643987, 0.98666666, 0.99348534, 0.9726918 , 0.99398556,\n",
      "       0.99366838, 0.9976322 , 0.99644549, 0.9879468 , 0.97748761,\n",
      "       0.98917401, 0.96278051, 0.97434819, 0.99960614, 0.99248715,\n",
      "       0.98649722, 0.97088186, 0.99841897]), 'PPV': array([5.15151506e-01, 6.28571355e-01, 7.51381188e-01, 2.50000625e-01,\n",
      "       7.77777160e-01, 7.33333022e-01, 8.12499902e-01, 7.34693782e-01,\n",
      "       4.99995000e-06, 5.51724102e-01, 4.37500078e-01, 7.55725152e-01,\n",
      "       7.94871719e-01, 7.40740721e-01, 6.93877472e-01, 3.33333472e-01,\n",
      "       5.00000000e-01, 3.57143061e-01, 7.80302988e-01, 8.35526294e-01,\n",
      "       5.71428549e-01, 8.92234536e-01, 6.47398827e-01, 7.49998750e-01,\n",
      "       2.08333576e-01, 2.60869669e-01, 5.59748420e-01, 7.14285408e-01]), 'NPV': array([0.99481865, 0.9952134 , 0.98475222, 0.99723757, 0.99881563,\n",
      "       0.99841709, 0.99475383, 0.99518652, 0.9996063 , 0.99602069,\n",
      "       0.99722882, 0.98216507, 0.99025974, 0.97607655, 0.99438427,\n",
      "       0.99722001, 0.99920948, 0.99802215, 0.98630705, 0.97006255,\n",
      "       0.99515933, 0.92046049, 0.97804981, 1.        , 0.99682287,\n",
      "       0.9951923 , 0.97943768, 0.99920886]), 'FPR': array([0.00636943, 0.00518342, 0.01898735, 0.00236874, 0.00078989,\n",
      "       0.00158291, 0.00484457, 0.00521251, 0.0007871 , 0.00516694,\n",
      "       0.00356013, 0.01333334, 0.00651466, 0.0273082 , 0.00601444,\n",
      "       0.00633162, 0.0023678 , 0.00355451, 0.0120532 , 0.02251239,\n",
      "       0.01082599, 0.03721949, 0.02565181, 0.00039386, 0.00751285,\n",
      "       0.01350278, 0.02911814, 0.00158103]), 'FNR': array([4.33333378e-01, 3.52941263e-01, 2.09302359e-01, 7.77777160e-01,\n",
      "       3.00000400e-01, 2.66666978e-01, 2.00000092e-01, 2.50000104e-01,\n",
      "       9.99990000e-01, 3.84615473e-01, 5.00000000e-01, 3.02816929e-01,\n",
      "       2.79069819e-01, 2.34042576e-01, 2.91666753e-01, 4.66666711e-01,\n",
      "       2.50000625e-01, 5.00000000e-01, 2.42647097e-01, 2.08722760e-01,\n",
      "       2.50000104e-01, 2.12587421e-01, 3.17073193e-01, 3.33331111e-06,\n",
      "       6.15384438e-01, 5.00000000e-01, 3.55072485e-01, 1.66667222e-01]), 'FDR': array([0.48484849, 0.37142864, 0.24861881, 0.74999938, 0.22222284,\n",
      "       0.26666698, 0.1875001 , 0.26530622, 0.999995  , 0.4482759 ,\n",
      "       0.56249992, 0.24427485, 0.20512828, 0.25925928, 0.30612253,\n",
      "       0.66666653, 0.5       , 0.64285694, 0.21969701, 0.16447371,\n",
      "       0.42857145, 0.10776546, 0.35260117, 0.25000125, 0.79166642,\n",
      "       0.73913033, 0.44025158, 0.28571459]), 'ACC': array([0.98859165, 0.99016522, 0.96813532, 0.99488591, 0.99803304,\n",
      "       0.99685286, 0.99016522, 0.99016522, 0.99881982, 0.990952  ,\n",
      "       0.99370574, 0.97049566, 0.98426435, 0.95357985, 0.98859165,\n",
      "       0.990952  , 0.99685286, 0.99449252, 0.97560975, 0.95397324,\n",
      "       0.98465774, 0.91345397, 0.95554681, 0.9996066 , 0.98937843,\n",
      "       0.981904  , 0.95318646, 0.99763965]), 'PPR': array([0.01298191, 0.01376869, 0.07120378, 0.00314714, 0.00354053,\n",
      "       0.00590087, 0.02517703, 0.01927617, 0.00078679, 0.01140835,\n",
      "       0.00629426, 0.05153423, 0.03068451, 0.09559403, 0.01927617,\n",
      "       0.00944139, 0.0047207 , 0.00550748, 0.05192762, 0.11959088,\n",
      "       0.02478364, 0.24822975, 0.06805666, 0.00157357, 0.00944139,\n",
      "       0.018096  , 0.06254918, 0.00550748])}\n",
      "final score 0.7079296524557899\n",
      "macro_fscore 0.643951515882604\n",
      "1-eval_scores['TPR_GAP'] 0.7719077890289756\n"
     ]
    }
   ],
   "source": [
    "# training logistic model 2\n",
    "clf_2 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train_XL, Y_train_XL)\n",
    "\n",
    "print(\"Coefficients:\", clf_2.coef_.shape)\n",
    "print(\"Intercept:\", clf_2.intercept_.shape)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred_2 = clf_2.predict(X_test)\n",
    "accuracy_2= accuracy_score(Y_test, Y_pred_2)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_2}\")\n",
    "eval_scores_2, confusion_matrices_eval_2 = gap_eval_scores(Y_pred_2, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_2 = (eval_scores_2['macro_fscore']+ (1-eval_scores_2['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_2)\n",
    "print('macro_fscore',eval_scores_2['macro_fscore'])\n",
    "print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores_2['TPR_GAP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 3 = REGRESSION LOGISTIQUE (28 classes + YxS + augmenté)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203728, 768) (203728,) (203728,)\n",
      "distribution S (original sample): 27749\n",
      "[14956 12793] [53.9 46.1]%\n",
      "distribution S_XL (data augmentation): 203728\n",
      "[101864 101864] [50. 50.]%\n",
      "\n",
      "distribution Y56 (56 classes): 22199  (min 0.1% , max 65.0%)\n",
      "[ 186  447 1096  101  122  267  539   16   75  282    6  573   69  104\n",
      "  222   17  107   39  866 1172  208 3638  394   70  315  646  352   19\n",
      "  120  129  719   47   36   47  277  192   14  126   65  543  345  954\n",
      "  208  100   38   32  466 1135  198 3011  637    6   63  119  542   82]\n",
      "\n",
      "distribution Y56_XL (56 classes + data augmentation): 203728 (x9)\n",
      "[3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a dataset with features X and labels Y\n",
    "\n",
    "# Split the dat using both Y and S using Y56_XL\n",
    "X_train_, X_test_, Y56_train_, Y56_test_, S_train_, S_test_ = train_test_split(X, Y56, S, test_size=0.2, random_state=42)\n",
    "Y_train_ = Y56_train_ % 28  # reste (original Y)\n",
    "Y_test_ = Y56_test_ % 28    # reste (original Y)\n",
    "\n",
    "#refresh (if necessary)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_\n",
    "\n",
    "# Initialize SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the model to generate the data\n",
    "X_train_XL, Y56_train_XL = sm.fit_resample(X_train_, Y56_train_)\n",
    "Y_train_XL = Y56_train_XL % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train_XL = Y56_train_XL//28               # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "print(X_train_XL.shape,Y_train_XL.shape,S_train_XL.shape)\n",
    "\n",
    "print(f'distribution S (original sample): {len(S)}\\n{np.bincount(S)} {np.round(np.bincount(S)/len(S)*100,2)}%')\n",
    "print(f'distribution S_XL (data augmentation): {len(S_train_XL)}\\n{np.bincount(S_train_XL)} {np.round(np.bincount(S_train_XL)/len(S_train_XL)*100,2)}%')\n",
    "Y56_grouped = np.bincount(Y56_train)\n",
    "print(f'\\ndistribution Y56 (56 classes): {len(Y56_train)}  (min {round(min(Y56_grouped)/len(Y56_grouped),1)}% , max {round(max(Y56_grouped)/len(Y56_grouped),1)}%)\\n{Y56_grouped}')\n",
    "print(f'\\ndistribution Y56_XL (56 classes + data augmentation): {len(Y56_train_XL)} (x{round(len(Y56_train_XL)/len(Y56_train))})\\n{np.bincount(Y56_train_XL)}')\n",
    "\n",
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.33220D+00    |proj g|=  1.93977D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  3.88901D-01    |proj g|=  1.75555D-02\n",
      "\n",
      "At iterate  100    f=  2.45014D-01    |proj g|=  1.28118D-02\n",
      "\n",
      "At iterate  150    f=  1.83725D-01    |proj g|=  3.26611D-03\n",
      "\n",
      "At iterate  200    f=  1.54198D-01    |proj g|=  2.75911D-03\n",
      "\n",
      "At iterate  250    f=  1.39105D-01    |proj g|=  1.88299D-03\n",
      "\n",
      "At iterate  300    f=  1.28366D-01    |proj g|=  3.43925D-03\n",
      "\n",
      "At iterate  350    f=  1.21936D-01    |proj g|=  1.52751D-03\n",
      "\n",
      "At iterate  400    f=  1.17522D-01    |proj g|=  8.83049D-04\n",
      "\n",
      "At iterate  450    f=  1.14932D-01    |proj g|=  5.08136D-04\n",
      "\n",
      "At iterate  500    f=  1.13282D-01    |proj g|=  9.50201D-04\n",
      "\n",
      "At iterate  550    f=  1.12090D-01    |proj g|=  2.05493D-03\n",
      "\n",
      "At iterate  600    f=  1.11323D-01    |proj g|=  4.32173D-04\n",
      "\n",
      "At iterate  650    f=  1.10835D-01    |proj g|=  7.84113D-04\n",
      "\n",
      "At iterate  700    f=  1.10492D-01    |proj g|=  1.11582D-03\n",
      "\n",
      "At iterate  750    f=  1.10261D-01    |proj g|=  2.71370D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532    794    836      1     0     0   9.954D-05   1.101D-01\n",
      "  F =  0.11010590602294348     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Coefficients: (28, 768)\n",
      "Intercept: (28,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training logistic model\n",
    "clf_3 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train_XL, Y_train_XL)\n",
    "\n",
    "print(\"Coefficients:\", clf_3.coef_.shape)\n",
    "print(\"Intercept:\", clf_3.intercept_.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7282882882882883\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7282882882882883, 'macro_fscore': 0.643951515882604, 'micro_fscore': 0.7282882882882883}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.64705877, 0.62365589, 0.79720278, 0.62962953, 0.55263155,\n",
      "       0.80701744, 0.86725657, 0.66666611, 0.76470557, 0.79999991,\n",
      "       0.25000062, 0.62499998, 0.41666674, 0.78260845, 0.59374997,\n",
      "       0.25000125, 0.55999995, 0.56249992, 0.77327933, 0.71034481,\n",
      "       0.59999996, 0.76221498, 0.70999996, 0.6923074 , 0.52631578,\n",
      "       0.70440249, 0.46067417, 0.5       ]), 'TNR': array([0.99053094, 0.98284734, 0.98493754, 0.99496813, 0.99730639,\n",
      "       0.99627245, 0.99447323, 0.99900066, 0.99832831, 0.99388379,\n",
      "       0.99833333, 0.97555866, 0.99329758, 0.99329983, 0.99320652,\n",
      "       0.99700399, 0.98927254, 0.99933155, 0.98623687, 0.97203826,\n",
      "       0.99019608, 0.95160517, 0.98177441, 0.99966611, 0.98533424,\n",
      "       0.97788697, 0.97499143, 0.99900199]), 'PPV': array([0.54098359, 0.53703703, 0.84758362, 0.53124998, 0.72413778,\n",
      "       0.80701744, 0.85964906, 0.57142837, 0.72222198, 0.74285707,\n",
      "       0.2857149 , 0.56249999, 0.33333344, 0.47368422, 0.65517236,\n",
      "       0.1000008 , 0.30434791, 0.81818124, 0.83406111, 0.73049644,\n",
      "       0.50847457, 0.87422166, 0.57258063, 0.8999992 , 0.48192772,\n",
      "       0.63999998, 0.35964915, 0.25000125]), 'NPV': array([0.99389209, 0.98793103, 0.97882438, 0.99663978, 0.99429338,\n",
      "       0.99627245, 0.99481686, 0.99933355, 0.9986622 , 0.99557522,\n",
      "       0.99800066, 0.98103932, 0.99529885, 0.99831649, 0.99118644,\n",
      "       0.99899933, 0.99628629, 0.99766433, 0.97984886, 0.96918562,\n",
      "       0.99321804, 0.90068027, 0.98994452, 0.99866577, 0.9876923 ,\n",
      "       0.98340981, 0.98341396, 0.99966711]), 'FPR': array([0.00946906, 0.01715266, 0.01506246, 0.00503187, 0.00269361,\n",
      "       0.00372755, 0.00552677, 0.00099934, 0.00167169, 0.00611621,\n",
      "       0.00166667, 0.02444134, 0.00670242, 0.00670017, 0.00679348,\n",
      "       0.00299601, 0.01072746, 0.00066845, 0.01376313, 0.02796174,\n",
      "       0.00980392, 0.04839483, 0.01822559, 0.00033389, 0.01466576,\n",
      "       0.02211303, 0.02500857, 0.00099801]), 'FNR': array([0.35294123, 0.37634411, 0.20279722, 0.37037047, 0.44736845,\n",
      "       0.19298256, 0.13274343, 0.33333389, 0.23529443, 0.20000009,\n",
      "       0.74999938, 0.37500002, 0.58333326, 0.21739155, 0.40625003,\n",
      "       0.74999875, 0.44000005, 0.43750008, 0.22672067, 0.28965519,\n",
      "       0.40000004, 0.23778502, 0.29000004, 0.3076926 , 0.47368422,\n",
      "       0.29559751, 0.53932583, 0.5       ]), 'FDR': array([0.45901641, 0.46296297, 0.15241638, 0.46875002, 0.27586222,\n",
      "       0.19298256, 0.14035094, 0.42857163, 0.27777802, 0.25714293,\n",
      "       0.7142851 , 0.43750001, 0.66666656, 0.52631578, 0.34482764,\n",
      "       0.8999992 , 0.69565209, 0.18181876, 0.16593889, 0.26950356,\n",
      "       0.49152543, 0.12577834, 0.42741937, 0.1000008 , 0.51807228,\n",
      "       0.36000002, 0.64035085, 0.74999875]), 'ACC': array([0.98470744, 0.97174201, 0.96708776, 0.99168882, 0.99168882,\n",
      "       0.99268616, 0.98969414, 0.99833776, 0.99700797, 0.98969414,\n",
      "       0.99634308, 0.95877659, 0.9886968 , 0.99168882, 0.98470744,\n",
      "       0.99601063, 0.98570478, 0.99700797, 0.96874999, 0.9468085 ,\n",
      "       0.9837101 , 0.89361701, 0.97273936, 0.99833776, 0.9737367 ,\n",
      "       0.96343084, 0.95977393, 0.99867021]), 'PPR': array([0.02027926, 0.03590426, 0.0894282 , 0.0106383 , 0.00964096,\n",
      "       0.01894947, 0.03789894, 0.00232713, 0.00598405, 0.02327128,\n",
      "       0.00232713, 0.0531915 , 0.00997341, 0.01263299, 0.01928192,\n",
      "       0.00332447, 0.01529256, 0.00365692, 0.07613033, 0.09375001,\n",
      "       0.01961437, 0.26695479, 0.04122341, 0.00332447, 0.02759309,\n",
      "       0.0581782 , 0.03789894, 0.00132979])}\n",
      "{'TPR': array([5.66666622e-01, 6.47058737e-01, 7.90697641e-01, 2.22222840e-01,\n",
      "       6.99999600e-01, 7.33333022e-01, 7.99999908e-01, 7.49999896e-01,\n",
      "       9.99980000e-06, 6.15384527e-01, 5.00000000e-01, 6.97183071e-01,\n",
      "       7.20930181e-01, 7.65957424e-01, 7.08333247e-01, 5.33333289e-01,\n",
      "       7.49999375e-01, 5.00000000e-01, 7.57352903e-01, 7.91277240e-01,\n",
      "       7.49999896e-01, 7.87412579e-01, 6.82926807e-01, 9.99996667e-01,\n",
      "       3.84615562e-01, 5.00000000e-01, 6.44927515e-01, 8.33332778e-01]), 'TNR': array([0.99363057, 0.99481658, 0.98101265, 0.99763126, 0.99921011,\n",
      "       0.99841709, 0.99515543, 0.99478749, 0.9992129 , 0.99483306,\n",
      "       0.99643987, 0.98666666, 0.99348534, 0.9726918 , 0.99398556,\n",
      "       0.99366838, 0.9976322 , 0.99644549, 0.9879468 , 0.97748761,\n",
      "       0.98917401, 0.96278051, 0.97434819, 0.99960614, 0.99248715,\n",
      "       0.98649722, 0.97088186, 0.99841897]), 'PPV': array([5.15151506e-01, 6.28571355e-01, 7.51381188e-01, 2.50000625e-01,\n",
      "       7.77777160e-01, 7.33333022e-01, 8.12499902e-01, 7.34693782e-01,\n",
      "       4.99995000e-06, 5.51724102e-01, 4.37500078e-01, 7.55725152e-01,\n",
      "       7.94871719e-01, 7.40740721e-01, 6.93877472e-01, 3.33333472e-01,\n",
      "       5.00000000e-01, 3.57143061e-01, 7.80302988e-01, 8.35526294e-01,\n",
      "       5.71428549e-01, 8.92234536e-01, 6.47398827e-01, 7.49998750e-01,\n",
      "       2.08333576e-01, 2.60869669e-01, 5.59748420e-01, 7.14285408e-01]), 'NPV': array([0.99481865, 0.9952134 , 0.98475222, 0.99723757, 0.99881563,\n",
      "       0.99841709, 0.99475383, 0.99518652, 0.9996063 , 0.99602069,\n",
      "       0.99722882, 0.98216507, 0.99025974, 0.97607655, 0.99438427,\n",
      "       0.99722001, 0.99920948, 0.99802215, 0.98630705, 0.97006255,\n",
      "       0.99515933, 0.92046049, 0.97804981, 1.        , 0.99682287,\n",
      "       0.9951923 , 0.97943768, 0.99920886]), 'FPR': array([0.00636943, 0.00518342, 0.01898735, 0.00236874, 0.00078989,\n",
      "       0.00158291, 0.00484457, 0.00521251, 0.0007871 , 0.00516694,\n",
      "       0.00356013, 0.01333334, 0.00651466, 0.0273082 , 0.00601444,\n",
      "       0.00633162, 0.0023678 , 0.00355451, 0.0120532 , 0.02251239,\n",
      "       0.01082599, 0.03721949, 0.02565181, 0.00039386, 0.00751285,\n",
      "       0.01350278, 0.02911814, 0.00158103]), 'FNR': array([4.33333378e-01, 3.52941263e-01, 2.09302359e-01, 7.77777160e-01,\n",
      "       3.00000400e-01, 2.66666978e-01, 2.00000092e-01, 2.50000104e-01,\n",
      "       9.99990000e-01, 3.84615473e-01, 5.00000000e-01, 3.02816929e-01,\n",
      "       2.79069819e-01, 2.34042576e-01, 2.91666753e-01, 4.66666711e-01,\n",
      "       2.50000625e-01, 5.00000000e-01, 2.42647097e-01, 2.08722760e-01,\n",
      "       2.50000104e-01, 2.12587421e-01, 3.17073193e-01, 3.33331111e-06,\n",
      "       6.15384438e-01, 5.00000000e-01, 3.55072485e-01, 1.66667222e-01]), 'FDR': array([0.48484849, 0.37142864, 0.24861881, 0.74999938, 0.22222284,\n",
      "       0.26666698, 0.1875001 , 0.26530622, 0.999995  , 0.4482759 ,\n",
      "       0.56249992, 0.24427485, 0.20512828, 0.25925928, 0.30612253,\n",
      "       0.66666653, 0.5       , 0.64285694, 0.21969701, 0.16447371,\n",
      "       0.42857145, 0.10776546, 0.35260117, 0.25000125, 0.79166642,\n",
      "       0.73913033, 0.44025158, 0.28571459]), 'ACC': array([0.98859165, 0.99016522, 0.96813532, 0.99488591, 0.99803304,\n",
      "       0.99685286, 0.99016522, 0.99016522, 0.99881982, 0.990952  ,\n",
      "       0.99370574, 0.97049566, 0.98426435, 0.95357985, 0.98859165,\n",
      "       0.990952  , 0.99685286, 0.99449252, 0.97560975, 0.95397324,\n",
      "       0.98465774, 0.91345397, 0.95554681, 0.9996066 , 0.98937843,\n",
      "       0.981904  , 0.95318646, 0.99763965]), 'PPR': array([0.01298191, 0.01376869, 0.07120378, 0.00314714, 0.00354053,\n",
      "       0.00590087, 0.02517703, 0.01927617, 0.00078679, 0.01140835,\n",
      "       0.00629426, 0.05153423, 0.03068451, 0.09559403, 0.01927617,\n",
      "       0.00944139, 0.0047207 , 0.00550748, 0.05192762, 0.11959088,\n",
      "       0.02478364, 0.24822975, 0.06805666, 0.00157357, 0.00944139,\n",
      "       0.018096  , 0.06254918, 0.00550748])}\n",
      "final score 0.7079296524557899\n",
      "macro_fscore 0.643951515882604\n",
      "1-eval_scores_3['TPR_GAP'] 0.7719077890289756\n"
     ]
    }
   ],
   "source": [
    "# predicting and assessing\n",
    "Y_pred_3 = clf_3.predict(X_test)\n",
    "accuracy_3= accuracy_score(Y_test, Y_pred_3)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_3}\")\n",
    "eval_scores_3, confusion_matrices_eval_3 = gap_eval_scores(Y_pred_3, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_3 = (eval_scores_3['macro_fscore']+ (1-eval_scores_3['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_3)\n",
    "print('macro_fscore',eval_scores_3['macro_fscore'])\n",
    "print('1-eval_scores_3[\\'TPR_GAP\\']',1-eval_scores_3['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true_3 = clf_3.predict(X_test_true)\n",
    "results_3=pd.DataFrame(y_test_true_3, columns= ['score'])\n",
    "\n",
    "results_3.to_csv(\"Data_Challenge_MDI_341_3.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 4 = REGRESSION LOGISTIQUE (56 classes + YxS + augmenté)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203728, 768) (203728,) (203728,)\n",
      "distribution S (original sample): 27749\n",
      "[14956 12793] [53.9 46.1]%\n",
      "distribution S_XL (data augmentation): 203728\n",
      "[101864 101864] [50. 50.]%\n",
      "\n",
      "distribution Y56 (56 classes): 22199  (min 0.1% , max 65.0%)\n",
      "[ 186  447 1096  101  122  267  539   16   75  282    6  573   69  104\n",
      "  222   17  107   39  866 1172  208 3638  394   70  315  646  352   19\n",
      "  120  129  719   47   36   47  277  192   14  126   65  543  345  954\n",
      "  208  100   38   32  466 1135  198 3011  637    6   63  119  542   82]\n",
      "\n",
      "distribution Y56_XL (56 classes + data augmentation): 203728 (x9)\n",
      "[3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638\n",
      " 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638 3638]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a dataset with features X and labels Y\n",
    "\n",
    "# Split the dat using both Y and S using Y56_XL\n",
    "X_train_, X_test_, Y56_train_, Y56_test_, S_train_, S_test_ = train_test_split(X, Y56, S, test_size=0.2, random_state=42)\n",
    "Y_train_ = Y56_train_ % 28  # reste (original Y)\n",
    "Y_test_ = Y56_test_ % 28    # reste (original Y)\n",
    "\n",
    "#refresh (if necessary)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_\n",
    "\n",
    "# Initialize SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the model to generate the data\n",
    "X_train_XL, Y56_train_XL = sm.fit_resample(X_train_, Y56_train_)\n",
    "Y_train_XL = Y56_train_XL % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train_XL = Y56_train_XL//28               # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "print(X_train_XL.shape,Y_train_XL.shape,S_train_XL.shape)\n",
    "\n",
    "print(f'distribution S (original sample): {len(S)}\\n{np.bincount(S)} {np.round(np.bincount(S)/len(S)*100,2)}%')\n",
    "print(f'distribution S_XL (data augmentation): {len(S_train_XL)}\\n{np.bincount(S_train_XL)} {np.round(np.bincount(S_train_XL)/len(S_train_XL)*100,2)}%')\n",
    "Y56_grouped = np.bincount(Y56_train)\n",
    "print(f'\\ndistribution Y56 (56 classes): {len(Y56_train)}  (min {round(min(Y56_grouped)/len(Y56_grouped),1)}% , max {round(max(Y56_grouped)/len(Y56_grouped),1)}%)\\n{Y56_grouped}')\n",
    "print(f'\\ndistribution Y56_XL (56 classes + data augmentation): {len(Y56_train_XL)} (x{round(len(Y56_train_XL)/len(Y56_train))})\\n{np.bincount(Y56_train_XL)}')\n",
    "\n",
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.02535D+00    |proj g|=  1.11357D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  3.42850D-01    |proj g|=  5.62567D-03\n",
      "\n",
      "At iterate  100    f=  1.91289D-01    |proj g|=  3.03229D-03\n",
      "\n",
      "At iterate  150    f=  1.39478D-01    |proj g|=  3.58310D-03\n",
      "\n",
      "At iterate  200    f=  1.16121D-01    |proj g|=  1.61312D-03\n",
      "\n",
      "At iterate  250    f=  1.03063D-01    |proj g|=  4.24770D-03\n",
      "\n",
      "At iterate  300    f=  9.54912D-02    |proj g|=  1.50461D-03\n",
      "\n",
      "At iterate  350    f=  9.03062D-02    |proj g|=  7.90046D-04\n",
      "\n",
      "At iterate  400    f=  8.71011D-02    |proj g|=  4.92046D-04\n",
      "\n",
      "At iterate  450    f=  8.48705D-02    |proj g|=  3.29116D-04\n",
      "\n",
      "At iterate  500    f=  8.35554D-02    |proj g|=  5.41333D-04\n",
      "\n",
      "At iterate  550    f=  8.26793D-02    |proj g|=  2.71808D-04\n",
      "\n",
      "At iterate  600    f=  8.21284D-02    |proj g|=  2.82210D-04\n",
      "\n",
      "At iterate  650    f=  8.17938D-02    |proj g|=  2.69958D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064    673    699      1     0     0   9.067D-05   8.168D-02\n",
      "  F =   8.1678380318028057E-002\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Coefficients: (28, 768)\n",
      "Intercept: (28,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training logistic model\n",
    "clf_4 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train_XL, Y56_train_XL)\n",
    "\n",
    "print(\"Coefficients:\", clf_3.coef_.shape)\n",
    "print(\"Intercept:\", clf_3.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7317117117117117\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7317117117117117, 'macro_fscore': 0.6512454578298856, 'micro_fscore': 0.7317117117117117}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([0.62745093, 0.65591394, 0.83566431, 0.62962953, 0.55263155,\n",
      "       0.80701744, 0.87610613, 0.33333389, 0.58823519, 0.81538452,\n",
      "       0.37500031, 0.63888887, 0.41666674, 0.73913023, 0.65624995,\n",
      "       0.25000125, 0.40000008, 0.56249992, 0.77732791, 0.73103447,\n",
      "       0.59999996, 0.78718783, 0.63999997, 0.84615331, 0.51315789,\n",
      "       0.66666665, 0.31460678, 0.5       ]), 'TNR': array([0.9895164 , 0.98524871, 0.98420279, 0.99463267, 0.99764309,\n",
      "       0.99762792, 0.99309153, 0.99933377, 0.99866265, 0.99320421,\n",
      "       0.999     , 0.97486033, 0.99463807, 0.99497487, 0.99592391,\n",
      "       0.99866844, 0.99430104, 1.        , 0.98587468, 0.96946284,\n",
      "       0.98985801, 0.94633445, 0.97661623, 0.99899833, 0.98396998,\n",
      "       0.98244998, 0.9756766 , 0.99966733]), 'PPV': array([0.50793651, 0.58653844, 0.84751771, 0.51515151, 0.74999982,\n",
      "       0.86792439, 0.83193272, 0.5       , 0.71428541, 0.72602734,\n",
      "       0.5       , 0.5609756 , 0.38461547, 0.53124998, 0.77777767,\n",
      "       0.2000012 , 0.37037047, 0.99999889, 0.8311688 , 0.71864405,\n",
      "       0.5       , 0.86618876, 0.48484849, 0.78571388, 0.45348838,\n",
      "       0.67948716, 0.28282833, 0.5       ]), 'NPV': array([0.99354838, 0.98898071, 0.98275862, 0.99663865, 0.9942953 ,\n",
      "       0.99627749, 0.99515403, 0.99866844, 0.99766199, 0.99591141,\n",
      "       0.99833444, 0.98171589, 0.99530516, 0.99798387, 0.99255247,\n",
      "       0.999001  , 0.99496813, 0.99766589, 0.98019445, 0.97124954,\n",
      "       0.99321574, 0.90971902, 0.98748261, 0.99933199, 0.98733744,\n",
      "       0.98141655, 0.97903059, 0.99966733]), 'FPR': array([1.04836017e-02, 1.47512899e-02, 1.57972117e-02, 5.36732977e-03,\n",
      "       2.35690573e-03, 2.37208066e-03, 6.90846635e-03, 6.66225852e-04,\n",
      "       1.33734872e-03, 6.79579003e-03, 1.00000334e-03, 2.51396684e-02,\n",
      "       5.36193366e-03, 5.02512900e-03, 4.07609037e-03, 1.33156126e-03,\n",
      "       5.69896415e-03, 3.34224600e-09, 1.41253206e-02, 3.05371635e-02,\n",
      "       1.01419912e-02, 5.36655537e-02, 2.33837724e-02, 1.00167279e-03,\n",
      "       1.60300171e-02, 1.75500211e-02, 2.43234019e-02, 3.32671325e-04]), 'FNR': array([0.37254907, 0.34408606, 0.16433569, 0.37037047, 0.44736845,\n",
      "       0.19298256, 0.12389387, 0.66666611, 0.41176481, 0.18461548,\n",
      "       0.62499969, 0.36111113, 0.58333326, 0.26086977, 0.34375005,\n",
      "       0.74999875, 0.59999992, 0.43750008, 0.22267209, 0.26896553,\n",
      "       0.40000004, 0.21281217, 0.36000003, 0.15384669, 0.48684211,\n",
      "       0.33333335, 0.68539322, 0.5       ]), 'FDR': array([4.92063495e-01, 4.13461555e-01, 1.52482294e-01, 4.84848494e-01,\n",
      "       2.50000179e-01, 1.32075611e-01, 1.68067283e-01, 5.00000000e-01,\n",
      "       2.85714592e-01, 2.73972665e-01, 5.00000000e-01, 4.39024398e-01,\n",
      "       6.15384527e-01, 4.68750020e-01, 2.22222325e-01, 7.99998800e-01,\n",
      "       6.29629534e-01, 1.11110864e-06, 1.68831198e-01, 2.81355947e-01,\n",
      "       5.00000000e-01, 1.33811239e-01, 5.15151513e-01, 2.14286122e-01,\n",
      "       5.46511617e-01, 3.20512844e-01, 7.17171673e-01, 5.00000000e-01]), 'ACC': array([0.98337765, 0.97506648, 0.97007978, 0.99135638, 0.99202127,\n",
      "       0.99401595, 0.9886968 , 0.99800531, 0.99634308, 0.9893617 ,\n",
      "       0.99734042, 0.95877659, 0.99002659, 0.99301861, 0.9886968 ,\n",
      "       0.99767287, 0.9893617 , 0.99767287, 0.96874999, 0.94647606,\n",
      "       0.98337765, 0.89760638, 0.96542553, 0.99833776, 0.97207446,\n",
      "       0.96575797, 0.95611701, 0.9993351 ]), 'PPR': array([0.02094416, 0.03457447, 0.09375001, 0.01097075, 0.00930852,\n",
      "       0.01761969, 0.03956118, 0.00132979, 0.00465426, 0.02426862,\n",
      "       0.00199469, 0.05452128, 0.00864362, 0.0106383 , 0.01795213,\n",
      "       0.00166224, 0.00897607, 0.00299203, 0.07679522, 0.09807181,\n",
      "       0.01994682, 0.27825798, 0.04388299, 0.00465426, 0.02859043,\n",
      "       0.05186171, 0.03291224, 0.0006649 ])}\n",
      "{'TPR': array([4.66666689e-01, 5.58823495e-01, 8.02325546e-01, 1.11111975e-01,\n",
      "       6.99999600e-01, 7.99999600e-01, 7.53846076e-01, 7.91666545e-01,\n",
      "       9.99980000e-06, 6.53846036e-01, 6.42856939e-01, 6.69014061e-01,\n",
      "       7.67441798e-01, 7.91489337e-01, 6.87499922e-01, 5.99999867e-01,\n",
      "       8.74999063e-01, 5.00000000e-01, 7.57352903e-01, 8.09968828e-01,\n",
      "       7.49999896e-01, 7.87412579e-01, 6.95121927e-01, 6.66665556e-01,\n",
      "       4.61538521e-01, 4.16666736e-01, 5.86956509e-01, 7.49999583e-01]), 'TNR': array([0.99522293, 0.99441786, 0.98523206, 0.99684169, 0.99960505,\n",
      "       0.99841709, 0.994348  , 0.99478749, 0.99960645, 0.99602543,\n",
      "       0.99485759, 0.98291666, 0.99267101, 0.97225834, 0.99719326,\n",
      "       0.99643846, 0.99684293, 0.99802527, 0.9879468 , 0.97433588,\n",
      "       0.99157979, 0.9518336 , 0.97560975, 0.99921228, 0.99525504,\n",
      "       0.99046862, 0.9671381 , 0.99762845]), 'PPV': array([5.38461509e-01, 5.75757530e-01, 7.97687827e-01, 1.11111975e-01,\n",
      "       8.74999063e-01, 7.49999688e-01, 7.77777690e-01, 7.45097943e-01,\n",
      "       9.99980000e-06, 6.29629534e-01, 4.09090992e-01, 6.98529383e-01,\n",
      "       7.85714218e-01, 7.43999980e-01, 8.24999838e-01, 5.00000000e-01,\n",
      "       4.66666711e-01, 5.00000000e-01, 7.80302988e-01, 8.20189254e-01,\n",
      "       6.31578901e-01, 8.64823337e-01, 6.62790679e-01, 5.00000000e-01,\n",
      "       3.33333519e-01, 2.94117768e-01, 5.06249999e-01, 5.99999867e-01]), 'NPV': array([0.9936407 , 0.99402152, 0.98564795, 0.99684169, 0.9988161 ,\n",
      "       0.99881235, 0.99354578, 0.99598554, 0.99960645, 0.99642147,\n",
      "       0.99801587, 0.9804655 , 0.9918633 , 0.97862129, 0.99400479,\n",
      "       0.99762282, 0.99960427, 0.99802527, 0.98630705, 0.97258427,\n",
      "       0.99517102, 0.91961924, 0.97890295, 0.99960599, 0.99722662,\n",
      "       0.99441786, 0.97607052, 0.99881282]), 'FPR': array([0.00477707, 0.00558214, 0.01476794, 0.00315831, 0.00039495,\n",
      "       0.00158291, 0.005652  , 0.00521251, 0.00039355, 0.00397457,\n",
      "       0.00514241, 0.01708334, 0.00732899, 0.02774166, 0.00280674,\n",
      "       0.00356154, 0.00315707, 0.00197473, 0.0120532 , 0.02566412,\n",
      "       0.00842021, 0.0481664 , 0.02439025, 0.00078772, 0.00474496,\n",
      "       0.00953138, 0.0328619 , 0.00237155]), 'FNR': array([0.53333331, 0.44117651, 0.19767445, 0.88888802, 0.3000004 ,\n",
      "       0.2000004 , 0.24615392, 0.20833345, 0.99999   , 0.34615396,\n",
      "       0.35714306, 0.33098594, 0.2325582 , 0.20851066, 0.31250008,\n",
      "       0.40000013, 0.12500094, 0.5       , 0.2426471 , 0.19003117,\n",
      "       0.2500001 , 0.21258742, 0.30487807, 0.33333444, 0.53846148,\n",
      "       0.58333326, 0.41304349, 0.25000042]), 'FDR': array([0.46153849, 0.42424247, 0.20231217, 0.88888802, 0.12500094,\n",
      "       0.25000031, 0.22222231, 0.25490206, 0.99999   , 0.37037047,\n",
      "       0.59090901, 0.30147062, 0.21428578, 0.25600002, 0.17500016,\n",
      "       0.5       , 0.53333329, 0.5       , 0.21969701, 0.17981075,\n",
      "       0.3684211 , 0.13517666, 0.33720932, 0.5       , 0.66666648,\n",
      "       0.70588223, 0.49375   , 0.40000013]), 'ACC': array([0.98898504, 0.98859165, 0.97285601, 0.99370574, 0.99842643,\n",
      "       0.99724625, 0.98819826, 0.990952  , 0.99921321, 0.99252556,\n",
      "       0.99291895, 0.96538158, 0.98505113, 0.95554681, 0.99134539,\n",
      "       0.99409913, 0.99645947, 0.99606608, 0.97560975, 0.95357985,\n",
      "       0.98701809, 0.90558615, 0.95751376, 0.99881982, 0.99252556,\n",
      "       0.98505113, 0.94649881, 0.99645947]), 'PPR': array([0.01022817, 0.01298191, 0.06805666, 0.00354053, 0.00314714,\n",
      "       0.00629426, 0.02478364, 0.02006295, 0.0003934 , 0.01062157,\n",
      "       0.00865461, 0.05350119, 0.03304485, 0.09834777, 0.01573565,\n",
      "       0.00708105, 0.00590087, 0.00393392, 0.05192762, 0.12470496,\n",
      "       0.0224233 , 0.25609757, 0.06766326, 0.00157357, 0.00708105,\n",
      "       0.0133753 , 0.06294257, 0.00590087])}\n",
      "final score 0.7009157735737935\n",
      "macro_fscore 0.6512454578298856\n",
      "1-eval_scores_4['TPR_GAP'] 0.7505860893177014\n"
     ]
    }
   ],
   "source": [
    "# predicting and assessing\n",
    "Y56_pred_4 = clf_4.predict(X_test)\n",
    "\n",
    "Y_pred_4 = Y56_pred_4 % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "\n",
    "accuracy_4= accuracy_score(Y_test, Y_pred_4)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_4}\")\n",
    "eval_scores_4, confusion_matrices_eval_4 = gap_eval_scores(Y_pred_4, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_4 = (eval_scores_4['macro_fscore']+ (1-eval_scores_4['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_4)\n",
    "print('macro_fscore',eval_scores_4['macro_fscore'])\n",
    "print('1-eval_scores_4[\\'TPR_GAP\\']',1-eval_scores_4['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 5 - REGRESSION LOGISTIQUE (56 classes + YxS)**\n",
    "---\n",
    "no data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19424, 768) (19424,) (19424,)\n",
      "\n",
      "distribution S (original sample): 27749\n",
      "[14956 12793] [53.9 46.1]%\n",
      "\n",
      "distribution Y56 (56 classes): 27749  (min 0.1% , max 65.0%)\n",
      "[ 186  447 1096  101  122  267  539   16   75  282    6  573   69  104\n",
      "  222   17  107   39  866 1172  208 3638  394   70  315  646  352   19\n",
      "  120  129  719   47   36   47  277  192   14  126   65  543  345  954\n",
      "  208  100   38   32  466 1135  198 3011  637    6   63  119  542   82]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a dataset with features X and labels Y\n",
    "\n",
    "# Split the dat using both Y and S using Y56_XL\n",
    "X_train_, X_test_, Y56_train_, Y56_test_, S_train_, S_test_ = train_test_split(X, Y56, S, test_size=0.3, random_state=42)\n",
    "Y_train_ = Y56_train_ % 28  # reste (original Y)\n",
    "Y_test_ = Y56_test_ % 28    # reste (original Y)\n",
    "\n",
    "#refresh (if necessary)\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_\n",
    "\n",
    "'''# Initialize SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# Fit the model to generate the data\n",
    "X_train_XL, Y56_train_XL = sm.fit_resample(X_train_, Y56_train_)\n",
    "Y_train_XL = Y56_train_XL % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train_XL = Y56_train_XL//28               # facteur (original S) ex 33//28 = 1 (attribut protégé)'''\n",
    "\n",
    "print(X_train.shape,Y_train.shape,S_train.shape)\n",
    "\n",
    "print(f'\\ndistribution S (original sample): {len(S)}\\n{np.bincount(S)} {np.round(np.bincount(S)/len(S)*100,2)}%')\n",
    "print(f'\\ndistribution Y56 (56 classes): {len(Y56)}  (min {round(min(Y56_grouped)/len(Y56_grouped),1)}% , max {round(max(Y56_grouped)/len(Y56_grouped),1)}%)\\n{Y56_grouped}')\n",
    "\n",
    "\n",
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.02535D+00    |proj g|=  8.32061D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  7.21757D-01    |proj g|=  2.49188D-02\n",
      "\n",
      "At iterate  100    f=  5.11279D-01    |proj g|=  1.04124D-02\n",
      "\n",
      "At iterate  150    f=  4.21346D-01    |proj g|=  5.36256D-03\n",
      "\n",
      "At iterate  200    f=  3.82613D-01    |proj g|=  6.31434D-03\n",
      "\n",
      "At iterate  250    f=  3.65020D-01    |proj g|=  1.03230D-02\n",
      "\n",
      "At iterate  300    f=  3.56433D-01    |proj g|=  1.67590D-03\n",
      "\n",
      "At iterate  350    f=  3.52368D-01    |proj g|=  8.45603D-04\n",
      "\n",
      "At iterate  400    f=  3.50371D-01    |proj g|=  2.84153D-03\n",
      "\n",
      "At iterate  450    f=  3.49481D-01    |proj g|=  6.37972D-04\n",
      "\n",
      "At iterate  500    f=  3.49067D-01    |proj g|=  6.02019D-04\n",
      "\n",
      "At iterate  550    f=  3.48854D-01    |proj g|=  5.37719D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064    589    631      1     0     0   9.838D-05   3.488D-01\n",
      "  F =  0.34877288158775521     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy on transformed test data: 0.7514714714714714\n",
      "gap_eval_scores\n",
      "{'accuracy': 0.7514714714714714, 'macro_fscore': 0.653061916239249, 'micro_fscore': 0.7514714714714714}\n",
      "distinct_groups size 2\n",
      "{'TPR': array([5.45454534e-01, 6.04026832e-01, 8.69879500e-01, 3.25000087e-01,\n",
      "       5.37037023e-01, 7.85714218e-01, 9.03954757e-01, 3.75000312e-01,\n",
      "       6.66666556e-01, 7.95918307e-01, 1.25000937e-01, 6.42533924e-01,\n",
      "       3.93939458e-01, 6.75675581e-01, 5.56701019e-01, 1.99999200e-06,\n",
      "       4.00000057e-01, 5.41666632e-01, 8.53025916e-01, 7.60919528e-01,\n",
      "       5.44303786e-01, 8.71888721e-01, 5.71428562e-01, 6.66666508e-01,\n",
      "       5.17241376e-01, 5.92741928e-01, 4.15492970e-01, 2.00001200e-01]), 'TNR': array([0.99616079, 0.98714417, 0.9804401 , 0.99664054, 0.99797798,\n",
      "       0.99728568, 0.99329944, 1.        , 0.99910614, 0.99478103,\n",
      "       1.        , 0.97735761, 0.99709302, 0.99776186, 0.99682395,\n",
      "       1.        , 0.99485458, 0.99955367, 0.98244348, 0.97002457,\n",
      "       0.99254406, 0.91653392, 0.98621006, 0.99955397, 0.98792435,\n",
      "       0.98590556, 0.98166399, 0.99955555]), 'PPV': array([0.71186433, 0.61643834, 0.81859409, 0.46428574, 0.76315776,\n",
      "       0.84615376, 0.84656081, 0.99999667, 0.83333306, 0.77227717,\n",
      "       0.99999   , 0.59414225, 0.5       , 0.71428559, 0.79411756,\n",
      "       0.5       , 0.37837844, 0.86666618, 0.80216801, 0.73068432,\n",
      "       0.56578946, 0.81968341, 0.59459458, 0.87499953, 0.53097345,\n",
      "       0.71014491, 0.42446044, 0.33333444]), 'NPV': array([0.99212775, 0.98646478, 0.9867126 , 0.99396917, 0.9944034 ,\n",
      "       0.99593404, 0.99606117, 0.99888938, 0.99776835, 0.99545867,\n",
      "       0.99844582, 0.98148148, 0.99553472, 0.99731543, 0.99030876,\n",
      "       0.99889012, 0.99529991, 0.99755011, 0.98766924, 0.97433366,\n",
      "       0.99187175, 0.94266055, 0.98485196, 0.99844063, 0.98724954,\n",
      "       0.9765007 , 0.98098946, 0.9991115 ]), 'FPR': array([3.83920733e-03, 1.28558334e-02, 1.95599047e-02, 3.35946473e-03,\n",
      "       2.02201978e-03, 2.71432030e-03, 6.70055685e-03, 2.22370470e-09,\n",
      "       8.93856985e-04, 5.21897210e-03, 2.22370470e-09, 2.26423927e-02,\n",
      "       2.90697899e-03, 2.23814011e-03, 3.17604583e-03, 2.22222223e-09,\n",
      "       5.14541612e-03, 4.46331177e-04, 1.75565200e-02, 2.99754325e-02,\n",
      "       7.45594444e-03, 8.34660754e-02, 1.37899357e-02, 4.46032561e-04,\n",
      "       1.20756460e-02, 1.40944351e-02, 1.83360097e-02, 4.44446668e-04]), 'FNR': array([0.45454547, 0.39597317, 0.1301205 , 0.67499991, 0.46296298,\n",
      "       0.21428578, 0.09604524, 0.62499969, 0.33333344, 0.20408169,\n",
      "       0.87499906, 0.35746608, 0.60606054, 0.32432442, 0.44329898,\n",
      "       0.999998  , 0.59999994, 0.45833337, 0.14697408, 0.23908047,\n",
      "       0.45569621, 0.12811128, 0.42857144, 0.33333349, 0.48275862,\n",
      "       0.40725807, 0.58450703, 0.7999988 ]), 'FDR': array([2.88135665e-01, 3.83561660e-01, 1.81405910e-01, 5.35714260e-01,\n",
      "       2.36842244e-01, 1.53846243e-01, 1.53439190e-01, 3.33331111e-06,\n",
      "       1.66666944e-01, 2.27722826e-01, 9.99980000e-06, 4.05857748e-01,\n",
      "       5.00000000e-01, 2.85714408e-01, 2.05882439e-01, 5.00000000e-01,\n",
      "       6.21621556e-01, 1.33333822e-01, 1.97831995e-01, 2.69315683e-01,\n",
      "       4.34210544e-01, 1.80316591e-01, 4.05405418e-01, 1.25000469e-01,\n",
      "       4.69026554e-01, 2.89855093e-01, 5.75539557e-01, 6.66665556e-01]), 'ACC': array([0.98845727, 0.9744728 , 0.97025527, 0.99067702, 0.99245283,\n",
      "       0.99334073, 0.98978912, 0.99889012, 0.99689234, 0.99045505,\n",
      "       0.99844617, 0.96093229, 0.9926748 , 0.99511653, 0.98734739,\n",
      "       0.99889012, 0.99023307, 0.99711431, 0.97247502, 0.94983351,\n",
      "       0.98468368, 0.90299667, 0.97203107, 0.99800222, 0.97580466,\n",
      "       0.96426193, 0.96381798, 0.99866814]), 'PPR': array([1.30965638e-02, 3.24084394e-02, 9.78912362e-02, 6.21532074e-03,\n",
      "       8.43507656e-03, 1.73140999e-02, 4.19533895e-02, 6.65931186e-04,\n",
      "       5.32741841e-03, 2.24195382e-02, 2.21980022e-04, 5.30521686e-02,\n",
      "       5.77136958e-03, 7.76914982e-03, 1.50943440e-02, 4.43951164e-09,\n",
      "       8.21310098e-03, 3.32963817e-03, 8.19089943e-02, 1.00554943e-01,\n",
      "       1.68701487e-02, 3.22530525e-01, 3.28523906e-02, 3.55161375e-03,\n",
      "       2.50832452e-02, 4.59489500e-02, 3.08546104e-02, 6.65931186e-04])}\n",
      "{'TPR': array([4.61538476e-01, 5.86956484e-01, 8.57142829e-01, 1.87500391e-01,\n",
      "       5.38461479e-01, 6.31578809e-01, 7.82608634e-01, 7.59999931e-01,\n",
      "       3.33331111e-06, 6.82926740e-01, 5.21739112e-01, 6.61904746e-01,\n",
      "       8.22580593e-01, 7.63925715e-01, 7.28571363e-01, 3.66666756e-01,\n",
      "       5.00000000e-01, 3.07692604e-01, 7.83505125e-01, 8.24675311e-01,\n",
      "       6.80555505e-01, 8.63382893e-01, 6.77551006e-01, 4.00000400e-01,\n",
      "       5.26315762e-01, 3.25000087e-01, 5.79185513e-01, 7.22221975e-01]), 'TNR': array([0.99814225, 0.99655538, 0.98122197, 0.99894847, 0.99921198,\n",
      "       0.99973691, 0.99624463, 0.99599466, 0.99973801, 0.99735379,\n",
      "       0.99710297, 0.98310249, 0.99296537, 0.96979378, 0.99626666,\n",
      "       0.99894459, 0.99763655, 0.99947465, 0.98896856, 0.97349613,\n",
      "       0.99279616, 0.92930029, 0.97958042, 1.        , 0.99842147,\n",
      "       0.99603174, 0.97249236, 0.99763282]), 'PPV': array([7.74193371e-01, 6.74999913e-01, 7.63250865e-01, 4.28571633e-01,\n",
      "       6.99999600e-01, 9.23076272e-01, 8.37209224e-01, 7.91666586e-01,\n",
      "       9.99980000e-06, 7.36841981e-01, 5.21739112e-01, 6.94999981e-01,\n",
      "       7.96874954e-01, 7.34693866e-01, 7.84615297e-01, 7.33333022e-01,\n",
      "       4.00000133e-01, 6.66666111e-01, 7.91666636e-01, 8.10638285e-01,\n",
      "       6.44736804e-01, 8.27248436e-01, 6.94560653e-01, 9.99995000e-01,\n",
      "       6.24999844e-01, 4.64285740e-01, 5.63876646e-01, 5.90909008e-01]), 'NPV': array([0.99261018, 0.99497354, 0.98982188, 0.99659061, 0.99842519,\n",
      "       0.99816128, 0.99464381, 0.99519744, 0.99921445, 0.99656266,\n",
      "       0.99710297, 0.98038674, 0.99404117, 0.97403734, 0.99494008,\n",
      "       0.99500657, 0.99842312, 0.99764027, 0.98842337, 0.97582089,\n",
      "       0.99385683, 0.94549499, 0.97793912, 0.99921425, 0.99763407,\n",
      "       0.99287974, 0.97411633, 0.99868352]), 'FPR': array([1.85775213e-03, 3.44462375e-03, 1.87780298e-02, 1.05152734e-03,\n",
      "       7.88024693e-04, 2.63091292e-04, 3.75536750e-03, 4.00534313e-03,\n",
      "       2.61988473e-04, 2.64620535e-03, 2.89702661e-03, 1.68975097e-02,\n",
      "       7.03463476e-03, 3.02062185e-02, 3.73333601e-03, 1.05541161e-03,\n",
      "       2.36344801e-03, 5.25350671e-04, 1.10314424e-02, 2.65038744e-02,\n",
      "       7.20384474e-03, 7.06997124e-02, 2.04195833e-02, 2.62123199e-09,\n",
      "       1.57853460e-03, 3.96825662e-03, 2.75076439e-02, 2.36717781e-03]), 'FNR': array([0.53846152, 0.41304352, 0.14285717, 0.81249961, 0.46153852,\n",
      "       0.36842119, 0.21739137, 0.24000007, 0.99999667, 0.31707326,\n",
      "       0.47826089, 0.33809525, 0.17741941, 0.23607428, 0.27142864,\n",
      "       0.63333324, 0.5       , 0.6923074 , 0.21649487, 0.17532469,\n",
      "       0.31944449, 0.13661711, 0.32244899, 0.5999996 , 0.47368424,\n",
      "       0.67499991, 0.42081449, 0.27777802]), 'FDR': array([2.25806629e-01, 3.25000087e-01, 2.36749135e-01, 5.71428367e-01,\n",
      "       3.00000400e-01, 7.69237278e-02, 1.62790776e-01, 2.08333414e-01,\n",
      "       9.99990000e-01, 2.63158019e-01, 4.78260888e-01, 3.05000019e-01,\n",
      "       2.03125046e-01, 2.65306134e-01, 2.15384703e-01, 2.66666978e-01,\n",
      "       5.99999867e-01, 3.33333889e-01, 2.08333364e-01, 1.89361715e-01,\n",
      "       3.55263196e-01, 1.72751564e-01, 3.05439347e-01, 4.99995000e-06,\n",
      "       3.75000156e-01, 5.35714260e-01, 4.36123354e-01, 4.09090992e-01]), 'ACC': array([0.99083769, 0.99162303, 0.97303664, 0.99554973, 0.99764397,\n",
      "       0.99790575, 0.99109947, 0.99136125, 0.99895287, 0.99397905,\n",
      "       0.99424083, 0.96544502, 0.98743455, 0.94947643, 0.99136125,\n",
      "       0.99397905, 0.99607329, 0.99712041, 0.97853403, 0.95549738,\n",
      "       0.98691099, 0.91073298, 0.96020942, 0.99921465, 0.99607329,\n",
      "       0.98900523, 0.94973821, 0.99633507]), 'PPR': array([8.11518846e-03, 1.04712094e-02, 7.40837747e-02, 1.83246596e-03,\n",
      "       2.61780628e-03, 3.40314659e-03, 2.25130942e-02, 1.88481727e-02,\n",
      "       2.61785340e-04, 9.94764919e-03, 6.02094763e-03, 5.23560260e-02,\n",
      "       3.35078586e-02, 1.02617806e-01, 1.70157120e-02, 3.92670680e-03,\n",
      "       3.92670680e-03, 1.57068586e-03, 5.02617852e-02, 1.23036654e-01,\n",
      "       1.98952931e-02, 2.93979062e-01, 6.25654501e-02, 5.23565444e-04,\n",
      "       4.18848690e-03, 7.32984815e-03, 5.94240888e-02, 5.75916752e-03])}\n",
      "final score 0.7022642727734636\n",
      "macro_fscore 0.653061916239249\n",
      "1-eval_scores_4['TPR_GAP'] 0.7505860893177014\n"
     ]
    }
   ],
   "source": [
    "# predicting and assessing\n",
    "clf_5 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y56_train_)\n",
    "\n",
    "Y56_pred_5 = clf_5.predict(X_test)\n",
    "\n",
    "Y_pred_5 = Y56_pred_5 % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "\n",
    "accuracy_5= accuracy_score(Y_test, Y_pred_5)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_5}\")\n",
    "eval_scores_5, confusion_matrices_eval_5 = gap_eval_scores(Y_pred_5, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_5 = (eval_scores_5['macro_fscore']+ (1-eval_scores_5['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score_5)\n",
    "print('macro_fscore',eval_scores_5['macro_fscore'])\n",
    "print('1-eval_scores_4[\\'TPR_GAP\\']',1-eval_scores_4['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>final score</th>\n",
       "      <th>Fscore macro</th>\n",
       "      <th>1 - TPR gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline (Reglog28)</td>\n",
       "      <td>0.763363</td>\n",
       "      <td>0.732743</td>\n",
       "      <td>0.669528</td>\n",
       "      <td>0.795958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline seed=1 (Reglog28)</td>\n",
       "      <td>0.857658</td>\n",
       "      <td>0.861761</td>\n",
       "      <td>0.840995</td>\n",
       "      <td>0.882527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RegLog28 + YxS</td>\n",
       "      <td>0.763363</td>\n",
       "      <td>0.732743</td>\n",
       "      <td>0.669528</td>\n",
       "      <td>0.795958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RegLog56 + YxS</td>\n",
       "      <td>0.751471</td>\n",
       "      <td>0.702264</td>\n",
       "      <td>0.653062</td>\n",
       "      <td>0.751467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RegLog28 + XL</td>\n",
       "      <td>0.728288</td>\n",
       "      <td>0.707930</td>\n",
       "      <td>0.643952</td>\n",
       "      <td>0.771908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RegLog28 + YxS + XL</td>\n",
       "      <td>0.728288</td>\n",
       "      <td>0.707930</td>\n",
       "      <td>0.643952</td>\n",
       "      <td>0.771908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RegLog56 + YxS + XL</td>\n",
       "      <td>0.731712</td>\n",
       "      <td>0.700916</td>\n",
       "      <td>0.651245</td>\n",
       "      <td>0.750586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  final score  Fscore macro  \\\n",
       "0         baseline (Reglog28)  0.763363     0.732743      0.669528   \n",
       "1  baseline seed=1 (Reglog28)  0.857658     0.861761      0.840995   \n",
       "2              RegLog28 + YxS  0.763363     0.732743      0.669528   \n",
       "3              RegLog56 + YxS  0.751471     0.702264      0.653062   \n",
       "4               RegLog28 + XL  0.728288     0.707930      0.643952   \n",
       "5         RegLog28 + YxS + XL  0.728288     0.707930      0.643952   \n",
       "6         RegLog56 + YxS + XL  0.731712     0.700916      0.651245   \n",
       "\n",
       "   1 - TPR gap  \n",
       "0     0.795958  \n",
       "1     0.882527  \n",
       "2     0.795958  \n",
       "3     0.751467  \n",
       "4     0.771908  \n",
       "5     0.771908  \n",
       "6     0.750586  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS = pd.DataFrame(columns=['Model','Accuracy','final score','Fscore macro','1 - TPR gap'])\n",
    "RESULTS.loc[0]=['baseline (Reglog28)',accuracy_0,final_score_0, eval_scores_0['macro_fscore'],1-eval_scores_0['TPR_GAP']]\n",
    "RESULTS.loc[2]=['RegLog28 + YxS',accuracy_1,final_score_1, eval_scores_1['macro_fscore'],1-eval_scores_1['TPR_GAP']]\n",
    "RESULTS.loc[4]=['RegLog28 + XL',accuracy_2,final_score_2, eval_scores_2['macro_fscore'],1-eval_scores_2['TPR_GAP']]\n",
    "RESULTS.loc[5]=['RegLog28 + YxS + XL',accuracy_3,final_score_3, eval_scores_3['macro_fscore'],1-eval_scores_3['TPR_GAP']]\n",
    "RESULTS.loc[6]=['RegLog56 + YxS + XL',accuracy_4,final_score_4, eval_scores_4['macro_fscore'],1-eval_scores_4['TPR_GAP']]\n",
    "RESULTS.loc[3]=['RegLog56 + YxS',accuracy_5,final_score_5, eval_scores_5['macro_fscore'],1-eval_scores_5['TPR_GAP']]\n",
    "RESULTS.loc[1]=['baseline seed=1 (Reglog28)',accuracy_0_1,final_score_0_1, eval_scores_0_1['macro_fscore'],1-eval_scores_0_1['TPR_GAP']]\n",
    "RESULTS = RESULTS.sort_index()\n",
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RESULTS_10-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(RESULTS, f)\n",
    "\n",
    "#RESULTS =  pd.read_pickle('RESULTS_10-03-2024.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the original index for later use\n",
    "X['original_index'] = X.index\n",
    "\n",
    "# Step 1: Keep original index by resetting it if not already in a suitable format\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "Y = Y.reset_index(drop=True)\n",
    "S = S.reset_index(drop=True)\n",
    "\n",
    "# Create a combined Y and S DataFrame for stratification\n",
    "YS = pd.DataFrame({'Y': Y, 'S': S})\n",
    "\n",
    "# Split the data while stratifying based on Y and S\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=0, stratify=YS)\n",
    "print('train',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test',X_test.shape,Y_test.shape,S_test.shape)\n",
    "\n",
    "# Create XS and XnotS subsets for training and test sets\n",
    "XS_train = X_train[S == True]\n",
    "YS_train = Y_train[S == True]\n",
    "\n",
    "XnotS_train = X_train[S == False]\n",
    "YnotS_train = Y_train[S == False]\n",
    "\n",
    "XS_test = X_test[S == True]\n",
    "YS_test = Y_test[S == True]\n",
    "\n",
    "XnotS_test = X_test[S == False ]\n",
    "XnotS_test = X_test[S == False ]\n",
    "\n",
    "XS_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train[S == True].shape)\n",
    "Y_train[S == True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Logistic Regression models\n",
    "model_X = LogisticRegression(random_state=0, max_iter=5000)\n",
    "model_XS = LogisticRegression(random_state=0, max_iter=5000)\n",
    "model_XnotS = LogisticRegression(random_state=0, max_iter=5000)\n",
    "\n",
    "# Train the models\n",
    "model_X.fit(X_train, Y_train)\n",
    "model_XS.fit(XS_train, Y_train[S==1])\n",
    "model_XnotS.fit(XnotS_train, Y_train[S==0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XF=X[S==1]\n",
    "XH=X[S==0]\n",
    "print(XF.shape,XH.shape)\n",
    "\n",
    "YF=Y[S==1]\n",
    "YH=Y[S==0]\n",
    "print(YF.shape,YH.shape)\n",
    "\n",
    "XF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "evaluation = model.evaluate(X_test, {'main_output': Y_test, 'adversary_output': S_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract shared representation model\n",
    "representation_model = Model(inputs=model.input, outputs=model.get_layer('shared_representation').output)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_transformed_2 = representation_model.predict(X_train)\n",
    "X_test_transformed_2 = representation_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train a new classifier on the transformed training data\n",
    "clf_2 = LogisticRegression(max_iter=5000)  # Increase max_iter if needed for convergence\n",
    "history_new_2 = clf_2.fit(X_train_transformed_2, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_2 = clf_2.predict(X_test_transformed_2)\n",
    "accuracy_2= accuracy_score(Y_test, Y_pred_2)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_2}\")\n",
    "\n",
    "# Step 4 : Predict with gloabl score\n",
    "eval_scores_2, confusion_matrices_eval_2 = gap_eval_scores(Y_pred_2, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_2 = (eval_scores_2['macro_fscore']+ (1-eval_scores_2['TPR_GAP']))/2\n",
    "print('\\nfinal',final_score_2)\n",
    "print('macro_fscore',eval_scores_2['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores_2['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classwise_loss(Y_test, Y_pred_2)\n",
    "classwise_accuracy(Y_test, Y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def classwise_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom metric function that calculates the accuracy for each class individually\n",
    "    and then averages these accuracies.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true labels, one-hot encoded.\n",
    "        y_pred: Tensor of predicted labels, as probabilities.\n",
    "\n",
    "    Returns:\n",
    "        A scalar tensor representing the average class-wise accuracy.\n",
    "    \"\"\"\n",
    "    # Convert probabilities to predicted class (highest probability)\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    y_true_classes = tf.argmax(y_true, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = tf.cast(tf.equal(y_true_classes, y_pred_classes), tf.float32)\n",
    "    \n",
    "    # Reduce across all dimensions but the first (batch dimension)\n",
    "    # to get accuracy per class, then take the mean across classes\n",
    "    classwise_acc = tf.reduce_mean(class_accuracies, axis=0)\n",
    "    overall_acc = tf.reduce_mean(classwise_acc)\n",
    "    \n",
    "    return overall_acc\n",
    "\n",
    "# Example usage with a model compilation in TensorFlow/Keras\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[classwise_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

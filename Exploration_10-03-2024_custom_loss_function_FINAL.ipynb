{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from evaluator_ANAELE import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_X_test_true(X, model,name):\n",
    "    Y_pred = model.predict(X)\n",
    "    results=pd.DataFrame(y_pred, columns= ['score'])\n",
    "    file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the F1 score as a loss function.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    soft_f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    loss = 1 - soft_f1.mean()  # Mean to aggregate over all classes\n",
    "    return loss\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)  # Average F1 score across all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    \"\"\"\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1)\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def tpr_gap(y_true, y_pred_probs, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_indices == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = tpr_gap(y_true, y_pred_probs, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = (1 - soft_macro_f1 + (1 - tpr_gap)) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    macro_f1 = macro_f1(y_true, y_pred)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (1 - macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TPR_gap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m outputs_train \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m---> 43\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[10], line 124\u001b[0m, in \u001b[0;36mfinal_eval\u001b[0;34m(y_true, y_pred, protected_attribute)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mCombine soft macro F1 score and TPR gap to create a final evaluation metric.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m soft_macro_f1 \u001b[38;5;241m=\u001b[39m soft_macro_f1_loss(y_true, y_pred)  \u001b[38;5;66;03m# Calculate soft macro F1 score\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mTPR_gap\u001b[49m(y_true, y_pred, protected_attribute)  \u001b[38;5;66;03m# Calculate TPR gap\u001b[39;00m\n\u001b[1;32m    126\u001b[0m final_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m soft_macro_f1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m tpr_gap)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TPR_gap' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Transform DataFrames into Tensors\n",
    "# ------------------------------------\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "# 2. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 3. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_epochs = 10#00  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor)\n",
    "            final_score_train = final_score(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "            macro_f1_train = macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "            \n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "            final_score_test = final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    final_score = final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    macro_f1 = macro_f1(Y_test_tensor, outputs_test)\n",
    "    inv_macro_tpr_gap = 1 - macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.9742507934570312\n",
      "Epoch [2/10000], Loss: 0.9742189049720764\n",
      "Epoch [3/10000], Loss: 0.9741873145103455\n",
      "Epoch [4/10000], Loss: 0.9741561412811279\n",
      "Epoch [5/10000], Loss: 0.9741252064704895\n",
      "Epoch [6/10000], Loss: 0.9740946292877197\n",
      "Epoch [7/10000], Loss: 0.9740644097328186\n",
      "Epoch [8/10000], Loss: 0.9740345478057861\n",
      "Epoch [9/10000], Loss: 0.9740051031112671\n",
      "Epoch [10/10000], Loss: 0.9739761352539062\n",
      "Epoch [11/10000], Loss: 0.9739474058151245\n",
      "Epoch [12/10000], Loss: 0.9739190936088562\n",
      "Epoch [13/10000], Loss: 0.9738910794258118\n",
      "Epoch [14/10000], Loss: 0.9738634824752808\n",
      "Epoch [15/10000], Loss: 0.9738361835479736\n",
      "Epoch [16/10000], Loss: 0.9738092422485352\n",
      "Epoch [17/10000], Loss: 0.9737827181816101\n",
      "Epoch [18/10000], Loss: 0.9737566113471985\n",
      "Epoch [19/10000], Loss: 0.9737308025360107\n",
      "Epoch [20/10000], Loss: 0.9737053513526917\n",
      "Epoch [21/10000], Loss: 0.9736801385879517\n",
      "Epoch [22/10000], Loss: 0.9736553430557251\n",
      "Epoch [23/10000], Loss: 0.973630964756012\n",
      "Epoch [24/10000], Loss: 0.9736070036888123\n",
      "Epoch [25/10000], Loss: 0.9735833406448364\n",
      "Epoch [26/10000], Loss: 0.9735599160194397\n",
      "Epoch [27/10000], Loss: 0.9735367894172668\n",
      "Epoch [28/10000], Loss: 0.9735139608383179\n",
      "Epoch [29/10000], Loss: 0.9734914898872375\n",
      "Epoch [30/10000], Loss: 0.9734691977500916\n",
      "Epoch [31/10000], Loss: 0.9734470844268799\n",
      "Epoch [32/10000], Loss: 0.9734251499176025\n",
      "Epoch [33/10000], Loss: 0.9734033346176147\n",
      "Epoch [34/10000], Loss: 0.9733816385269165\n",
      "Epoch [35/10000], Loss: 0.9733600616455078\n",
      "Epoch [36/10000], Loss: 0.9733386635780334\n",
      "Epoch [37/10000], Loss: 0.9733173847198486\n",
      "Epoch [38/10000], Loss: 0.9732962250709534\n",
      "Epoch [39/10000], Loss: 0.9732751846313477\n",
      "Epoch [40/10000], Loss: 0.9732542634010315\n",
      "Epoch [41/10000], Loss: 0.9732334017753601\n",
      "Epoch [42/10000], Loss: 0.9732125401496887\n",
      "Epoch [43/10000], Loss: 0.9731917977333069\n",
      "Epoch [44/10000], Loss: 0.9731711149215698\n",
      "Epoch [45/10000], Loss: 0.9731504321098328\n",
      "Epoch [46/10000], Loss: 0.9731297492980957\n",
      "Epoch [47/10000], Loss: 0.9731091260910034\n",
      "Epoch [48/10000], Loss: 0.9730885624885559\n",
      "Epoch [49/10000], Loss: 0.9730679988861084\n",
      "Epoch [50/10000], Loss: 0.9730474352836609\n",
      "Epoch [51/10000], Loss: 0.9730268716812134\n",
      "Epoch [52/10000], Loss: 0.9730063080787659\n",
      "Epoch [53/10000], Loss: 0.9729856848716736\n",
      "Epoch [54/10000], Loss: 0.9729650616645813\n",
      "Epoch [55/10000], Loss: 0.9729443788528442\n",
      "Epoch [56/10000], Loss: 0.9729236960411072\n",
      "Epoch [57/10000], Loss: 0.9729029536247253\n",
      "Epoch [58/10000], Loss: 0.9728821516036987\n",
      "Epoch [59/10000], Loss: 0.9728612303733826\n",
      "Epoch [60/10000], Loss: 0.9728401899337769\n",
      "Epoch [61/10000], Loss: 0.9728189706802368\n",
      "Epoch [62/10000], Loss: 0.9727977514266968\n",
      "Epoch [63/10000], Loss: 0.9727762937545776\n",
      "Epoch [64/10000], Loss: 0.9727547764778137\n",
      "Epoch [65/10000], Loss: 0.9727330803871155\n",
      "Epoch [66/10000], Loss: 0.9727112054824829\n",
      "Epoch [67/10000], Loss: 0.972689151763916\n",
      "Epoch [68/10000], Loss: 0.9726669192314148\n",
      "Epoch [69/10000], Loss: 0.9726446270942688\n",
      "Epoch [70/10000], Loss: 0.9726220369338989\n",
      "Epoch [71/10000], Loss: 0.9725992679595947\n",
      "Epoch [72/10000], Loss: 0.9725763201713562\n",
      "Epoch [73/10000], Loss: 0.9725531339645386\n",
      "Epoch [74/10000], Loss: 0.9725296497344971\n",
      "Epoch [75/10000], Loss: 0.9725058674812317\n",
      "Epoch [76/10000], Loss: 0.9724816679954529\n",
      "Epoch [77/10000], Loss: 0.9724571704864502\n",
      "Epoch [78/10000], Loss: 0.9724323749542236\n",
      "Epoch [79/10000], Loss: 0.9724072217941284\n",
      "Epoch [80/10000], Loss: 0.9723818898200989\n",
      "Epoch [81/10000], Loss: 0.9723561406135559\n",
      "Epoch [82/10000], Loss: 0.9723300337791443\n",
      "Epoch [83/10000], Loss: 0.9723033905029297\n",
      "Epoch [84/10000], Loss: 0.9722763299942017\n",
      "Epoch [85/10000], Loss: 0.9722487926483154\n",
      "Epoch [86/10000], Loss: 0.9722207188606262\n",
      "Epoch [87/10000], Loss: 0.9721921682357788\n",
      "Epoch [88/10000], Loss: 0.9721631407737732\n",
      "Epoch [89/10000], Loss: 0.9721337556838989\n",
      "Epoch [90/10000], Loss: 0.9721038937568665\n",
      "Epoch [91/10000], Loss: 0.9720736145973206\n",
      "Epoch [92/10000], Loss: 0.9720429182052612\n",
      "Epoch [93/10000], Loss: 0.972011923789978\n",
      "Epoch [94/10000], Loss: 0.9719805121421814\n",
      "Epoch [95/10000], Loss: 0.9719488024711609\n",
      "Epoch [96/10000], Loss: 0.9719167947769165\n",
      "Epoch [97/10000], Loss: 0.9718844294548035\n",
      "Epoch [98/10000], Loss: 0.9718518257141113\n",
      "Epoch [99/10000], Loss: 0.9718189835548401\n",
      "Epoch [100/10000], Loss: 0.9717859029769897\n",
      "Epoch [101/10000], Loss: 0.9717525839805603\n",
      "Epoch [102/10000], Loss: 0.9717189073562622\n",
      "Epoch [103/10000], Loss: 0.9716850519180298\n",
      "Epoch [104/10000], Loss: 0.9716509580612183\n",
      "Epoch [105/10000], Loss: 0.9716166853904724\n",
      "Epoch [106/10000], Loss: 0.9715821743011475\n",
      "Epoch [107/10000], Loss: 0.9715474247932434\n",
      "Epoch [108/10000], Loss: 0.971512496471405\n",
      "Epoch [109/10000], Loss: 0.9714773297309875\n",
      "Epoch [110/10000], Loss: 0.971441924571991\n",
      "Epoch [111/10000], Loss: 0.9714063405990601\n",
      "Epoch [112/10000], Loss: 0.9713704586029053\n",
      "Epoch [113/10000], Loss: 0.9713343381881714\n",
      "Epoch [114/10000], Loss: 0.9712980389595032\n",
      "Epoch [115/10000], Loss: 0.9712614417076111\n",
      "Epoch [116/10000], Loss: 0.9712246060371399\n",
      "Epoch [117/10000], Loss: 0.9711875319480896\n",
      "Epoch [118/10000], Loss: 0.9711502194404602\n",
      "Epoch [119/10000], Loss: 0.9711126089096069\n",
      "Epoch [120/10000], Loss: 0.9710747003555298\n",
      "Epoch [121/10000], Loss: 0.9710365533828735\n",
      "Epoch [122/10000], Loss: 0.9709981679916382\n",
      "Epoch [123/10000], Loss: 0.9709594249725342\n",
      "Epoch [124/10000], Loss: 0.9709203839302063\n",
      "Epoch [125/10000], Loss: 0.9708811044692993\n",
      "Epoch [126/10000], Loss: 0.9708415269851685\n",
      "Epoch [127/10000], Loss: 0.9708016514778137\n",
      "Epoch [128/10000], Loss: 0.9707614183425903\n",
      "Epoch [129/10000], Loss: 0.9707209467887878\n",
      "Epoch [130/10000], Loss: 0.9706801772117615\n",
      "Epoch [131/10000], Loss: 0.9706390500068665\n",
      "Epoch [132/10000], Loss: 0.9705976247787476\n",
      "Epoch [133/10000], Loss: 0.97055584192276\n",
      "Epoch [134/10000], Loss: 0.9705137610435486\n",
      "Epoch [135/10000], Loss: 0.9704713225364685\n",
      "Epoch [136/10000], Loss: 0.9704285264015198\n",
      "Epoch [137/10000], Loss: 0.9703853726387024\n",
      "Epoch [138/10000], Loss: 0.9703419208526611\n",
      "Epoch [139/10000], Loss: 0.9702980518341064\n",
      "Epoch [140/10000], Loss: 0.9702538251876831\n",
      "Epoch [141/10000], Loss: 0.9702092409133911\n",
      "Epoch [142/10000], Loss: 0.9701642990112305\n",
      "Epoch [143/10000], Loss: 0.9701189994812012\n",
      "Epoch [144/10000], Loss: 0.9700733423233032\n",
      "Epoch [145/10000], Loss: 0.9700272679328918\n",
      "Epoch [146/10000], Loss: 0.9699808955192566\n",
      "Epoch [147/10000], Loss: 0.9699341058731079\n",
      "Epoch [148/10000], Loss: 0.9698869585990906\n",
      "Epoch [149/10000], Loss: 0.9698393940925598\n",
      "Epoch [150/10000], Loss: 0.9697914719581604\n",
      "Epoch [151/10000], Loss: 0.9697431325912476\n",
      "Epoch [152/10000], Loss: 0.9696943759918213\n",
      "Epoch [153/10000], Loss: 0.9696452617645264\n",
      "Epoch [154/10000], Loss: 0.9695956707000732\n",
      "Epoch [155/10000], Loss: 0.9695457220077515\n",
      "Epoch [156/10000], Loss: 0.969495415687561\n",
      "Epoch [157/10000], Loss: 0.9694446325302124\n",
      "Epoch [158/10000], Loss: 0.9693934917449951\n",
      "Epoch [159/10000], Loss: 0.9693419337272644\n",
      "Epoch [160/10000], Loss: 0.9692899584770203\n",
      "Epoch [161/10000], Loss: 0.9692376255989075\n",
      "Epoch [162/10000], Loss: 0.9691848158836365\n",
      "Epoch [163/10000], Loss: 0.9691316485404968\n",
      "Epoch [164/10000], Loss: 0.9690780639648438\n",
      "Epoch [165/10000], Loss: 0.969024121761322\n",
      "Epoch [166/10000], Loss: 0.9689697623252869\n",
      "Epoch [167/10000], Loss: 0.9689149856567383\n",
      "Epoch [168/10000], Loss: 0.9688599109649658\n",
      "Epoch [169/10000], Loss: 0.9688044786453247\n",
      "Epoch [170/10000], Loss: 0.9687486290931702\n",
      "Epoch [171/10000], Loss: 0.968692421913147\n",
      "Epoch [172/10000], Loss: 0.9686359167098999\n",
      "Epoch [173/10000], Loss: 0.9685789942741394\n",
      "Epoch [174/10000], Loss: 0.968521773815155\n",
      "Epoch [175/10000], Loss: 0.9684641361236572\n",
      "Epoch [176/10000], Loss: 0.9684062004089355\n",
      "Epoch [177/10000], Loss: 0.9683478474617004\n",
      "Epoch [178/10000], Loss: 0.9682891964912415\n",
      "Epoch [179/10000], Loss: 0.9682301878929138\n",
      "Epoch [180/10000], Loss: 0.9681708812713623\n",
      "Epoch [181/10000], Loss: 0.9681112170219421\n",
      "Epoch [182/10000], Loss: 0.9680512547492981\n",
      "Epoch [183/10000], Loss: 0.9679909944534302\n",
      "Epoch [184/10000], Loss: 0.9679304957389832\n",
      "Epoch [185/10000], Loss: 0.9678696990013123\n",
      "Epoch [186/10000], Loss: 0.9678086638450623\n",
      "Epoch [187/10000], Loss: 0.9677473306655884\n",
      "Epoch [188/10000], Loss: 0.9676856994628906\n",
      "Epoch [189/10000], Loss: 0.9676238298416138\n",
      "Epoch [190/10000], Loss: 0.9675617218017578\n",
      "Epoch [191/10000], Loss: 0.9674994349479675\n",
      "Epoch [192/10000], Loss: 0.9674370884895325\n",
      "Epoch [193/10000], Loss: 0.9673745632171631\n",
      "Epoch [194/10000], Loss: 0.9673118591308594\n",
      "Epoch [195/10000], Loss: 0.9672490954399109\n",
      "Epoch [196/10000], Loss: 0.9671862721443176\n",
      "Epoch [197/10000], Loss: 0.9671233892440796\n",
      "Epoch [198/10000], Loss: 0.9670605063438416\n",
      "Epoch [199/10000], Loss: 0.9669976234436035\n",
      "Epoch [200/10000], Loss: 0.9669347405433655\n",
      "Epoch [201/10000], Loss: 0.9668719172477722\n",
      "Epoch [202/10000], Loss: 0.966809093952179\n",
      "Epoch [203/10000], Loss: 0.9667463898658752\n",
      "Epoch [204/10000], Loss: 0.9666837453842163\n",
      "Epoch [205/10000], Loss: 0.9666212797164917\n",
      "Epoch [206/10000], Loss: 0.9665588736534119\n",
      "Epoch [207/10000], Loss: 0.9664966464042664\n",
      "Epoch [208/10000], Loss: 0.9664345383644104\n",
      "Epoch [209/10000], Loss: 0.9663726687431335\n",
      "Epoch [210/10000], Loss: 0.9663109183311462\n",
      "Epoch [211/10000], Loss: 0.966249406337738\n",
      "Epoch [212/10000], Loss: 0.9661881327629089\n",
      "Epoch [213/10000], Loss: 0.9661270380020142\n",
      "Epoch [214/10000], Loss: 0.9660663604736328\n",
      "Epoch [215/10000], Loss: 0.9660059213638306\n",
      "Epoch [216/10000], Loss: 0.9659457802772522\n",
      "Epoch [217/10000], Loss: 0.9658859372138977\n",
      "Epoch [218/10000], Loss: 0.9658263921737671\n",
      "Epoch [219/10000], Loss: 0.9657671451568604\n",
      "Epoch [220/10000], Loss: 0.9657081961631775\n",
      "Epoch [221/10000], Loss: 0.9656495451927185\n",
      "Epoch [222/10000], Loss: 0.9655913710594177\n",
      "Epoch [223/10000], Loss: 0.965533435344696\n",
      "Epoch [224/10000], Loss: 0.9654757976531982\n",
      "Epoch [225/10000], Loss: 0.9654186367988586\n",
      "Epoch [226/10000], Loss: 0.9653617739677429\n",
      "Epoch [227/10000], Loss: 0.9653052687644958\n",
      "Epoch [228/10000], Loss: 0.9652491807937622\n",
      "Epoch [229/10000], Loss: 0.965193510055542\n",
      "Epoch [230/10000], Loss: 0.9651381373405457\n",
      "Epoch [231/10000], Loss: 0.9650831818580627\n",
      "Epoch [232/10000], Loss: 0.9650286436080933\n",
      "Epoch [233/10000], Loss: 0.9649744629859924\n",
      "Epoch [234/10000], Loss: 0.964920699596405\n",
      "Epoch [235/10000], Loss: 0.9648672938346863\n",
      "Epoch [236/10000], Loss: 0.9648142457008362\n",
      "Epoch [237/10000], Loss: 0.9647615551948547\n",
      "Epoch [238/10000], Loss: 0.9647091627120972\n",
      "Epoch [239/10000], Loss: 0.964657187461853\n",
      "Epoch [240/10000], Loss: 0.9646055102348328\n",
      "Epoch [241/10000], Loss: 0.9645541906356812\n",
      "Epoch [242/10000], Loss: 0.9645031690597534\n",
      "Epoch [243/10000], Loss: 0.9644525647163391\n",
      "Epoch [244/10000], Loss: 0.9644022583961487\n",
      "Epoch [245/10000], Loss: 0.9643522500991821\n",
      "Epoch [246/10000], Loss: 0.9643025994300842\n",
      "Epoch [247/10000], Loss: 0.9642532467842102\n",
      "Epoch [248/10000], Loss: 0.9642041921615601\n",
      "Epoch [249/10000], Loss: 0.9641554355621338\n",
      "Epoch [250/10000], Loss: 0.9641070365905762\n",
      "Epoch [251/10000], Loss: 0.9640589356422424\n",
      "Epoch [252/10000], Loss: 0.9640111327171326\n",
      "Epoch [253/10000], Loss: 0.9639636278152466\n",
      "Epoch [254/10000], Loss: 0.9639164209365845\n",
      "Epoch [255/10000], Loss: 0.9638694524765015\n",
      "Epoch [256/10000], Loss: 0.9638227224349976\n",
      "Epoch [257/10000], Loss: 0.9637762308120728\n",
      "Epoch [258/10000], Loss: 0.9637300372123718\n",
      "Epoch [259/10000], Loss: 0.9636839628219604\n",
      "Epoch [260/10000], Loss: 0.963638186454773\n",
      "Epoch [261/10000], Loss: 0.963592529296875\n",
      "Epoch [262/10000], Loss: 0.9635471105575562\n",
      "Epoch [263/10000], Loss: 0.9635018110275269\n",
      "Epoch [264/10000], Loss: 0.9634567499160767\n",
      "Epoch [265/10000], Loss: 0.963411808013916\n",
      "Epoch [266/10000], Loss: 0.9633670449256897\n",
      "Epoch [267/10000], Loss: 0.9633224010467529\n",
      "Epoch [268/10000], Loss: 0.9632779359817505\n",
      "Epoch [269/10000], Loss: 0.9632335305213928\n",
      "Epoch [270/10000], Loss: 0.9631893038749695\n",
      "Epoch [271/10000], Loss: 0.9631451368331909\n",
      "Epoch [272/10000], Loss: 0.9631010890007019\n",
      "Epoch [273/10000], Loss: 0.9630571007728577\n",
      "Epoch [274/10000], Loss: 0.9630132913589478\n",
      "Epoch [275/10000], Loss: 0.9629695415496826\n",
      "Epoch [276/10000], Loss: 0.962925910949707\n",
      "Epoch [277/10000], Loss: 0.9628823399543762\n",
      "Epoch [278/10000], Loss: 0.962838888168335\n",
      "Epoch [279/10000], Loss: 0.9627954363822937\n",
      "Epoch [280/10000], Loss: 0.962752103805542\n",
      "Epoch [281/10000], Loss: 0.9627088308334351\n",
      "Epoch [282/10000], Loss: 0.9626655578613281\n",
      "Epoch [283/10000], Loss: 0.962622344493866\n",
      "Epoch [284/10000], Loss: 0.9625791907310486\n",
      "Epoch [285/10000], Loss: 0.9625360369682312\n",
      "Epoch [286/10000], Loss: 0.9624929428100586\n",
      "Epoch [287/10000], Loss: 0.9624499082565308\n",
      "Epoch [288/10000], Loss: 0.9624068737030029\n",
      "Epoch [289/10000], Loss: 0.9623638391494751\n",
      "Epoch [290/10000], Loss: 0.9623208045959473\n",
      "Epoch [291/10000], Loss: 0.9622778296470642\n",
      "Epoch [292/10000], Loss: 0.9622347950935364\n",
      "Epoch [293/10000], Loss: 0.9621917605400085\n",
      "Epoch [294/10000], Loss: 0.9621487259864807\n",
      "Epoch [295/10000], Loss: 0.9621056318283081\n",
      "Epoch [296/10000], Loss: 0.9620625972747803\n",
      "Epoch [297/10000], Loss: 0.9620195031166077\n",
      "Epoch [298/10000], Loss: 0.9619764089584351\n",
      "Epoch [299/10000], Loss: 0.9619333148002625\n",
      "Epoch [300/10000], Loss: 0.9618901610374451\n",
      "Epoch [301/10000], Loss: 0.9618469476699829\n",
      "Epoch [302/10000], Loss: 0.961803674697876\n",
      "Epoch [303/10000], Loss: 0.961760401725769\n",
      "Epoch [304/10000], Loss: 0.9617171287536621\n",
      "Epoch [305/10000], Loss: 0.9616737961769104\n",
      "Epoch [306/10000], Loss: 0.9616304039955139\n",
      "Epoch [307/10000], Loss: 0.9615870118141174\n",
      "Epoch [308/10000], Loss: 0.961543619632721\n",
      "Epoch [309/10000], Loss: 0.9615001678466797\n",
      "Epoch [310/10000], Loss: 0.9614566564559937\n",
      "Epoch [311/10000], Loss: 0.9614131450653076\n",
      "Epoch [312/10000], Loss: 0.961369514465332\n",
      "Epoch [313/10000], Loss: 0.9613258838653564\n",
      "Epoch [314/10000], Loss: 0.9612821340560913\n",
      "Epoch [315/10000], Loss: 0.9612383246421814\n",
      "Epoch [316/10000], Loss: 0.9611944556236267\n",
      "Epoch [317/10000], Loss: 0.9611505270004272\n",
      "Epoch [318/10000], Loss: 0.961106538772583\n",
      "Epoch [319/10000], Loss: 0.961062490940094\n",
      "Epoch [320/10000], Loss: 0.9610183835029602\n",
      "Epoch [321/10000], Loss: 0.9609742164611816\n",
      "Epoch [322/10000], Loss: 0.9609299898147583\n",
      "Epoch [323/10000], Loss: 0.9608857035636902\n",
      "Epoch [324/10000], Loss: 0.9608413577079773\n",
      "Epoch [325/10000], Loss: 0.9607969522476196\n",
      "Epoch [326/10000], Loss: 0.9607524275779724\n",
      "Epoch [327/10000], Loss: 0.9607078433036804\n",
      "Epoch [328/10000], Loss: 0.9606631994247437\n",
      "Epoch [329/10000], Loss: 0.9606184959411621\n",
      "Epoch [330/10000], Loss: 0.960573673248291\n",
      "Epoch [331/10000], Loss: 0.9605288505554199\n",
      "Epoch [332/10000], Loss: 0.9604839086532593\n",
      "Epoch [333/10000], Loss: 0.9604388475418091\n",
      "Epoch [334/10000], Loss: 0.9603937864303589\n",
      "Epoch [335/10000], Loss: 0.9603486061096191\n",
      "Epoch [336/10000], Loss: 0.9603033661842346\n",
      "Epoch [337/10000], Loss: 0.9602580070495605\n",
      "Epoch [338/10000], Loss: 0.9602125883102417\n",
      "Epoch [339/10000], Loss: 0.9601671099662781\n",
      "Epoch [340/10000], Loss: 0.9601215124130249\n",
      "Epoch [341/10000], Loss: 0.960075855255127\n",
      "Epoch [342/10000], Loss: 0.9600300788879395\n",
      "Epoch [343/10000], Loss: 0.9599842429161072\n",
      "Epoch [344/10000], Loss: 0.9599382877349854\n",
      "Epoch [345/10000], Loss: 0.959892213344574\n",
      "Epoch [346/10000], Loss: 0.9598460793495178\n",
      "Epoch [347/10000], Loss: 0.9597998857498169\n",
      "Epoch [348/10000], Loss: 0.9597535133361816\n",
      "Epoch [349/10000], Loss: 0.9597070813179016\n",
      "Epoch [350/10000], Loss: 0.9596605896949768\n",
      "Epoch [351/10000], Loss: 0.9596139192581177\n",
      "Epoch [352/10000], Loss: 0.9595671892166138\n",
      "Epoch [353/10000], Loss: 0.9595203995704651\n",
      "Epoch [354/10000], Loss: 0.9594734311103821\n",
      "Epoch [355/10000], Loss: 0.9594264030456543\n",
      "Epoch [356/10000], Loss: 0.9593793153762817\n",
      "Epoch [357/10000], Loss: 0.9593321084976196\n",
      "Epoch [358/10000], Loss: 0.959284782409668\n",
      "Epoch [359/10000], Loss: 0.9592373371124268\n",
      "Epoch [360/10000], Loss: 0.959189772605896\n",
      "Epoch [361/10000], Loss: 0.9591421484947205\n",
      "Epoch [362/10000], Loss: 0.9590943455696106\n",
      "Epoch [363/10000], Loss: 0.959046483039856\n",
      "Epoch [364/10000], Loss: 0.9589985013008118\n",
      "Epoch [365/10000], Loss: 0.958950400352478\n",
      "Epoch [366/10000], Loss: 0.9589022397994995\n",
      "Epoch [367/10000], Loss: 0.9588539600372314\n",
      "Epoch [368/10000], Loss: 0.958805501461029\n",
      "Epoch [369/10000], Loss: 0.9587569832801819\n",
      "Epoch [370/10000], Loss: 0.9587083458900452\n",
      "Epoch [371/10000], Loss: 0.9586595892906189\n",
      "Epoch [372/10000], Loss: 0.9586107134819031\n",
      "Epoch [373/10000], Loss: 0.9585617184638977\n",
      "Epoch [374/10000], Loss: 0.9585126638412476\n",
      "Epoch [375/10000], Loss: 0.9584634304046631\n",
      "Epoch [376/10000], Loss: 0.9584140777587891\n",
      "Epoch [377/10000], Loss: 0.9583646059036255\n",
      "Epoch [378/10000], Loss: 0.9583150744438171\n",
      "Epoch [379/10000], Loss: 0.9582653641700745\n",
      "Epoch [380/10000], Loss: 0.9582155346870422\n",
      "Epoch [381/10000], Loss: 0.9581656455993652\n",
      "Epoch [382/10000], Loss: 0.9581155776977539\n",
      "Epoch [383/10000], Loss: 0.958065390586853\n",
      "Epoch [384/10000], Loss: 0.9580151438713074\n",
      "Epoch [385/10000], Loss: 0.9579647183418274\n",
      "Epoch [386/10000], Loss: 0.9579142332077026\n",
      "Epoch [387/10000], Loss: 0.9578635692596436\n",
      "Epoch [388/10000], Loss: 0.9578127861022949\n",
      "Epoch [389/10000], Loss: 0.9577618837356567\n",
      "Epoch [390/10000], Loss: 0.957710862159729\n",
      "Epoch [391/10000], Loss: 0.9576597809791565\n",
      "Epoch [392/10000], Loss: 0.9576085209846497\n",
      "Epoch [393/10000], Loss: 0.9575571417808533\n",
      "Epoch [394/10000], Loss: 0.9575056433677673\n",
      "Epoch [395/10000], Loss: 0.9574539661407471\n",
      "Epoch [396/10000], Loss: 0.957402229309082\n",
      "Epoch [397/10000], Loss: 0.9573503732681274\n",
      "Epoch [398/10000], Loss: 0.9572983384132385\n",
      "Epoch [399/10000], Loss: 0.9572461843490601\n",
      "Epoch [400/10000], Loss: 0.9571939706802368\n",
      "Epoch [401/10000], Loss: 0.9571415185928345\n",
      "Epoch [402/10000], Loss: 0.9570890069007874\n",
      "Epoch [403/10000], Loss: 0.9570363759994507\n",
      "Epoch [404/10000], Loss: 0.9569836258888245\n",
      "Epoch [405/10000], Loss: 0.9569306969642639\n",
      "Epoch [406/10000], Loss: 0.9568777084350586\n",
      "Epoch [407/10000], Loss: 0.956824541091919\n",
      "Epoch [408/10000], Loss: 0.9567712545394897\n",
      "Epoch [409/10000], Loss: 0.956717848777771\n",
      "Epoch [410/10000], Loss: 0.9566643238067627\n",
      "Epoch [411/10000], Loss: 0.9566106796264648\n",
      "Epoch [412/10000], Loss: 0.9565569162368774\n",
      "Epoch [413/10000], Loss: 0.9565030336380005\n",
      "Epoch [414/10000], Loss: 0.9564489722251892\n",
      "Epoch [415/10000], Loss: 0.9563948512077332\n",
      "Epoch [416/10000], Loss: 0.9563406109809875\n",
      "Epoch [417/10000], Loss: 0.9562862515449524\n",
      "Epoch [418/10000], Loss: 0.9562317728996277\n",
      "Epoch [419/10000], Loss: 0.9561771750450134\n",
      "Epoch [420/10000], Loss: 0.9561224579811096\n",
      "Epoch [421/10000], Loss: 0.9560676217079163\n",
      "Epoch [422/10000], Loss: 0.9560126662254333\n",
      "Epoch [423/10000], Loss: 0.9559575915336609\n",
      "Epoch [424/10000], Loss: 0.9559024572372437\n",
      "Epoch [425/10000], Loss: 0.9558471441268921\n",
      "Epoch [426/10000], Loss: 0.9557917714118958\n",
      "Epoch [427/10000], Loss: 0.9557362794876099\n",
      "Epoch [428/10000], Loss: 0.9556806683540344\n",
      "Epoch [429/10000], Loss: 0.9556249380111694\n",
      "Epoch [430/10000], Loss: 0.9555691480636597\n",
      "Epoch [431/10000], Loss: 0.9555132389068604\n",
      "Epoch [432/10000], Loss: 0.9554572105407715\n",
      "Epoch [433/10000], Loss: 0.9554010629653931\n",
      "Epoch [434/10000], Loss: 0.9553448557853699\n",
      "Epoch [435/10000], Loss: 0.9552885890007019\n",
      "Epoch [436/10000], Loss: 0.9552321434020996\n",
      "Epoch [437/10000], Loss: 0.9551756978034973\n",
      "Epoch [438/10000], Loss: 0.9551190733909607\n",
      "Epoch [439/10000], Loss: 0.9550624489784241\n",
      "Epoch [440/10000], Loss: 0.9550056457519531\n",
      "Epoch [441/10000], Loss: 0.9549487829208374\n",
      "Epoch [442/10000], Loss: 0.9548918008804321\n",
      "Epoch [443/10000], Loss: 0.9548347592353821\n",
      "Epoch [444/10000], Loss: 0.9547775983810425\n",
      "Epoch [445/10000], Loss: 0.9547203779220581\n",
      "Epoch [446/10000], Loss: 0.9546630382537842\n",
      "Epoch [447/10000], Loss: 0.9546055793762207\n",
      "Epoch [448/10000], Loss: 0.9545480608940125\n",
      "Epoch [449/10000], Loss: 0.9544904232025146\n",
      "Epoch [450/10000], Loss: 0.9544327259063721\n",
      "Epoch [451/10000], Loss: 0.9543749094009399\n",
      "Epoch [452/10000], Loss: 0.9543169736862183\n",
      "Epoch [453/10000], Loss: 0.9542589783668518\n",
      "Epoch [454/10000], Loss: 0.9542008638381958\n",
      "Epoch [455/10000], Loss: 0.9541426301002502\n",
      "Epoch [456/10000], Loss: 0.9540843367576599\n",
      "Epoch [457/10000], Loss: 0.95402592420578\n",
      "Epoch [458/10000], Loss: 0.9539673924446106\n",
      "Epoch [459/10000], Loss: 0.9539087414741516\n",
      "Epoch [460/10000], Loss: 0.9538500308990479\n",
      "Epoch [461/10000], Loss: 0.9537911415100098\n",
      "Epoch [462/10000], Loss: 0.9537321925163269\n",
      "Epoch [463/10000], Loss: 0.9536731243133545\n",
      "Epoch [464/10000], Loss: 0.9536139965057373\n",
      "Epoch [465/10000], Loss: 0.9535546898841858\n",
      "Epoch [466/10000], Loss: 0.9534953236579895\n",
      "Epoch [467/10000], Loss: 0.9534357786178589\n",
      "Epoch [468/10000], Loss: 0.9533761739730835\n",
      "Epoch [469/10000], Loss: 0.9533164501190186\n",
      "Epoch [470/10000], Loss: 0.9532566666603088\n",
      "Epoch [471/10000], Loss: 0.9531967043876648\n",
      "Epoch [472/10000], Loss: 0.953136682510376\n",
      "Epoch [473/10000], Loss: 0.9530765414237976\n",
      "Epoch [474/10000], Loss: 0.9530162811279297\n",
      "Epoch [475/10000], Loss: 0.952955961227417\n",
      "Epoch [476/10000], Loss: 0.9528955221176147\n",
      "Epoch [477/10000], Loss: 0.9528349041938782\n",
      "Epoch [478/10000], Loss: 0.9527742862701416\n",
      "Epoch [479/10000], Loss: 0.9527134895324707\n",
      "Epoch [480/10000], Loss: 0.952652633190155\n",
      "Epoch [481/10000], Loss: 0.9525916576385498\n",
      "Epoch [482/10000], Loss: 0.952530562877655\n",
      "Epoch [483/10000], Loss: 0.9524693489074707\n",
      "Epoch [484/10000], Loss: 0.9524080157279968\n",
      "Epoch [485/10000], Loss: 0.9523466229438782\n",
      "Epoch [486/10000], Loss: 0.95228511095047\n",
      "Epoch [487/10000], Loss: 0.952223539352417\n",
      "Epoch [488/10000], Loss: 0.9521617889404297\n",
      "Epoch [489/10000], Loss: 0.9520999789237976\n",
      "Epoch [490/10000], Loss: 0.952038049697876\n",
      "Epoch [491/10000], Loss: 0.9519760608673096\n",
      "Epoch [492/10000], Loss: 0.9519139528274536\n",
      "Epoch [493/10000], Loss: 0.9518517255783081\n",
      "Epoch [494/10000], Loss: 0.951789379119873\n",
      "Epoch [495/10000], Loss: 0.9517269730567932\n",
      "Epoch [496/10000], Loss: 0.9516644477844238\n",
      "Epoch [497/10000], Loss: 0.9516018629074097\n",
      "Epoch [498/10000], Loss: 0.9515390992164612\n",
      "Epoch [499/10000], Loss: 0.9514762759208679\n",
      "Epoch [500/10000], Loss: 0.9514133930206299\n",
      "Epoch [501/10000], Loss: 0.9513503909111023\n",
      "Epoch [502/10000], Loss: 0.9512872695922852\n",
      "Epoch [503/10000], Loss: 0.9512240886688232\n",
      "Epoch [504/10000], Loss: 0.9511607885360718\n",
      "Epoch [505/10000], Loss: 0.9510974287986755\n",
      "Epoch [506/10000], Loss: 0.9510339498519897\n",
      "Epoch [507/10000], Loss: 0.9509703516960144\n",
      "Epoch [508/10000], Loss: 0.9509066939353943\n",
      "Epoch [509/10000], Loss: 0.9508429169654846\n",
      "Epoch [510/10000], Loss: 0.9507790803909302\n",
      "Epoch [511/10000], Loss: 0.9507151246070862\n",
      "Epoch [512/10000], Loss: 0.9506511092185974\n",
      "Epoch [513/10000], Loss: 0.9505869746208191\n",
      "Epoch [514/10000], Loss: 0.950522780418396\n",
      "Epoch [515/10000], Loss: 0.9504585266113281\n",
      "Epoch [516/10000], Loss: 0.9503941535949707\n",
      "Epoch [517/10000], Loss: 0.9503296613693237\n",
      "Epoch [518/10000], Loss: 0.950265109539032\n",
      "Epoch [519/10000], Loss: 0.9502004981040955\n",
      "Epoch [520/10000], Loss: 0.9501357674598694\n",
      "Epoch [521/10000], Loss: 0.9500709772109985\n",
      "Epoch [522/10000], Loss: 0.9500060677528381\n",
      "Epoch [523/10000], Loss: 0.949941098690033\n",
      "Epoch [524/10000], Loss: 0.9498760104179382\n",
      "Epoch [525/10000], Loss: 0.9498108625411987\n",
      "Epoch [526/10000], Loss: 0.9497456550598145\n",
      "Epoch [527/10000], Loss: 0.9496803879737854\n",
      "Epoch [528/10000], Loss: 0.949614942073822\n",
      "Epoch [529/10000], Loss: 0.9495494961738586\n",
      "Epoch [530/10000], Loss: 0.9494839310646057\n",
      "Epoch [531/10000], Loss: 0.949418306350708\n",
      "Epoch [532/10000], Loss: 0.9493525624275208\n",
      "Epoch [533/10000], Loss: 0.9492868185043335\n",
      "Epoch [534/10000], Loss: 0.9492208957672119\n",
      "Epoch [535/10000], Loss: 0.9491549730300903\n",
      "Epoch [536/10000], Loss: 0.9490889310836792\n",
      "Epoch [537/10000], Loss: 0.9490228295326233\n",
      "Epoch [538/10000], Loss: 0.9489566683769226\n",
      "Epoch [539/10000], Loss: 0.9488903880119324\n",
      "Epoch [540/10000], Loss: 0.9488240480422974\n",
      "Epoch [541/10000], Loss: 0.9487576484680176\n",
      "Epoch [542/10000], Loss: 0.9486911296844482\n",
      "Epoch [543/10000], Loss: 0.9486246109008789\n",
      "Epoch [544/10000], Loss: 0.94855797290802\n",
      "Epoch [545/10000], Loss: 0.9484912753105164\n",
      "Epoch [546/10000], Loss: 0.9484244585037231\n",
      "Epoch [547/10000], Loss: 0.9483576416969299\n",
      "Epoch [548/10000], Loss: 0.9482907056808472\n",
      "Epoch [549/10000], Loss: 0.9482237100601196\n",
      "Epoch [550/10000], Loss: 0.9481566548347473\n",
      "Epoch [551/10000], Loss: 0.9480894804000854\n",
      "Epoch [552/10000], Loss: 0.9480223059654236\n",
      "Epoch [553/10000], Loss: 0.9479550123214722\n",
      "Epoch [554/10000], Loss: 0.947887659072876\n",
      "Epoch [555/10000], Loss: 0.947820246219635\n",
      "Epoch [556/10000], Loss: 0.9477527141571045\n",
      "Epoch [557/10000], Loss: 0.9476851224899292\n",
      "Epoch [558/10000], Loss: 0.9476175308227539\n",
      "Epoch [559/10000], Loss: 0.9475498199462891\n",
      "Epoch [560/10000], Loss: 0.9474819898605347\n",
      "Epoch [561/10000], Loss: 0.9474141597747803\n",
      "Epoch [562/10000], Loss: 0.9473462700843811\n",
      "Epoch [563/10000], Loss: 0.9472782611846924\n",
      "Epoch [564/10000], Loss: 0.9472101926803589\n",
      "Epoch [565/10000], Loss: 0.9471420645713806\n",
      "Epoch [566/10000], Loss: 0.9470738768577576\n",
      "Epoch [567/10000], Loss: 0.9470056295394897\n",
      "Epoch [568/10000], Loss: 0.9469372630119324\n",
      "Epoch [569/10000], Loss: 0.946868896484375\n",
      "Epoch [570/10000], Loss: 0.9468004107475281\n",
      "Epoch [571/10000], Loss: 0.9467318654060364\n",
      "Epoch [572/10000], Loss: 0.9466632604598999\n",
      "Epoch [573/10000], Loss: 0.9465945959091187\n",
      "Epoch [574/10000], Loss: 0.9465258121490479\n",
      "Epoch [575/10000], Loss: 0.946457028388977\n",
      "Epoch [576/10000], Loss: 0.9463881254196167\n",
      "Epoch [577/10000], Loss: 0.9463191628456116\n",
      "Epoch [578/10000], Loss: 0.9462501406669617\n",
      "Epoch [579/10000], Loss: 0.946181058883667\n",
      "Epoch [580/10000], Loss: 0.9461118578910828\n",
      "Epoch [581/10000], Loss: 0.9460426568984985\n",
      "Epoch [582/10000], Loss: 0.9459733366966248\n",
      "Epoch [583/10000], Loss: 0.9459039568901062\n",
      "Epoch [584/10000], Loss: 0.9458345174789429\n",
      "Epoch [585/10000], Loss: 0.9457650184631348\n",
      "Epoch [586/10000], Loss: 0.9456954002380371\n",
      "Epoch [587/10000], Loss: 0.9456257820129395\n",
      "Epoch [588/10000], Loss: 0.9455560445785522\n",
      "Epoch [589/10000], Loss: 0.9454862475395203\n",
      "Epoch [590/10000], Loss: 0.9454163908958435\n",
      "Epoch [591/10000], Loss: 0.945346474647522\n",
      "Epoch [592/10000], Loss: 0.9452764391899109\n",
      "Epoch [593/10000], Loss: 0.945206344127655\n",
      "Epoch [594/10000], Loss: 0.9451361894607544\n",
      "Epoch [595/10000], Loss: 0.945065975189209\n",
      "Epoch [596/10000], Loss: 0.9449957013130188\n",
      "Epoch [597/10000], Loss: 0.9449253082275391\n",
      "Epoch [598/10000], Loss: 0.9448549151420593\n",
      "Epoch [599/10000], Loss: 0.94478440284729\n",
      "Epoch [600/10000], Loss: 0.944713830947876\n",
      "Epoch [601/10000], Loss: 0.9446431994438171\n",
      "Epoch [602/10000], Loss: 0.9445724487304688\n",
      "Epoch [603/10000], Loss: 0.9445016384124756\n",
      "Epoch [604/10000], Loss: 0.9444307684898376\n",
      "Epoch [605/10000], Loss: 0.9443598389625549\n",
      "Epoch [606/10000], Loss: 0.9442887902259827\n",
      "Epoch [607/10000], Loss: 0.9442176818847656\n",
      "Epoch [608/10000], Loss: 0.9441465139389038\n",
      "Epoch [609/10000], Loss: 0.9440752863883972\n",
      "Epoch [610/10000], Loss: 0.9440039396286011\n",
      "Epoch [611/10000], Loss: 0.9439325332641602\n",
      "Epoch [612/10000], Loss: 0.9438610672950745\n",
      "Epoch [613/10000], Loss: 0.943789541721344\n",
      "Epoch [614/10000], Loss: 0.943717896938324\n",
      "Epoch [615/10000], Loss: 0.9436461925506592\n",
      "Epoch [616/10000], Loss: 0.9435744285583496\n",
      "Epoch [617/10000], Loss: 0.9435025453567505\n",
      "Epoch [618/10000], Loss: 0.9434306025505066\n",
      "Epoch [619/10000], Loss: 0.9433585405349731\n",
      "Epoch [620/10000], Loss: 0.9432864785194397\n",
      "Epoch [621/10000], Loss: 0.9432142972946167\n",
      "Epoch [622/10000], Loss: 0.9431419968605042\n",
      "Epoch [623/10000], Loss: 0.9430696368217468\n",
      "Epoch [624/10000], Loss: 0.9429972171783447\n",
      "Epoch [625/10000], Loss: 0.9429246783256531\n",
      "Epoch [626/10000], Loss: 0.9428520798683167\n",
      "Epoch [627/10000], Loss: 0.9427794218063354\n",
      "Epoch [628/10000], Loss: 0.9427066445350647\n",
      "Epoch [629/10000], Loss: 0.9426338076591492\n",
      "Epoch [630/10000], Loss: 0.9425609111785889\n",
      "Epoch [631/10000], Loss: 0.942487895488739\n",
      "Epoch [632/10000], Loss: 0.9424147605895996\n",
      "Epoch [633/10000], Loss: 0.9423415660858154\n",
      "Epoch [634/10000], Loss: 0.9422683119773865\n",
      "Epoch [635/10000], Loss: 0.942194938659668\n",
      "Epoch [636/10000], Loss: 0.9421215057373047\n",
      "Epoch [637/10000], Loss: 0.9420479536056519\n",
      "Epoch [638/10000], Loss: 0.9419742822647095\n",
      "Epoch [639/10000], Loss: 0.9419005513191223\n",
      "Epoch [640/10000], Loss: 0.9418267607688904\n",
      "Epoch [641/10000], Loss: 0.9417528510093689\n",
      "Epoch [642/10000], Loss: 0.9416788220405579\n",
      "Epoch [643/10000], Loss: 0.941604733467102\n",
      "Epoch [644/10000], Loss: 0.9415305256843567\n",
      "Epoch [645/10000], Loss: 0.9414562582969666\n",
      "Epoch [646/10000], Loss: 0.9413818717002869\n",
      "Epoch [647/10000], Loss: 0.9413073658943176\n",
      "Epoch [648/10000], Loss: 0.9412328004837036\n",
      "Epoch [649/10000], Loss: 0.9411581158638\n",
      "Epoch [650/10000], Loss: 0.9410833120346069\n",
      "Epoch [651/10000], Loss: 0.941008448600769\n",
      "Epoch [652/10000], Loss: 0.9409334659576416\n",
      "Epoch [653/10000], Loss: 0.9408584237098694\n",
      "Epoch [654/10000], Loss: 0.9407832026481628\n",
      "Epoch [655/10000], Loss: 0.9407079815864563\n",
      "Epoch [656/10000], Loss: 0.9406325817108154\n",
      "Epoch [657/10000], Loss: 0.9405571222305298\n",
      "Epoch [658/10000], Loss: 0.9404814839363098\n",
      "Epoch [659/10000], Loss: 0.9404058456420898\n",
      "Epoch [660/10000], Loss: 0.9403300285339355\n",
      "Epoch [661/10000], Loss: 0.9402541518211365\n",
      "Epoch [662/10000], Loss: 0.9401780962944031\n",
      "Epoch [663/10000], Loss: 0.9401019811630249\n",
      "Epoch [664/10000], Loss: 0.940025806427002\n",
      "Epoch [665/10000], Loss: 0.9399494528770447\n",
      "Epoch [666/10000], Loss: 0.9398729801177979\n",
      "Epoch [667/10000], Loss: 0.9397964477539062\n",
      "Epoch [668/10000], Loss: 0.9397197961807251\n",
      "Epoch [669/10000], Loss: 0.9396429657936096\n",
      "Epoch [670/10000], Loss: 0.9395660758018494\n",
      "Epoch [671/10000], Loss: 0.9394890666007996\n",
      "Epoch [672/10000], Loss: 0.9394119381904602\n",
      "Epoch [673/10000], Loss: 0.9393346905708313\n",
      "Epoch [674/10000], Loss: 0.9392573237419128\n",
      "Epoch [675/10000], Loss: 0.9391798377037048\n",
      "Epoch [676/10000], Loss: 0.9391022324562073\n",
      "Epoch [677/10000], Loss: 0.9390245079994202\n",
      "Epoch [678/10000], Loss: 0.9389466643333435\n",
      "Epoch [679/10000], Loss: 0.9388687014579773\n",
      "Epoch [680/10000], Loss: 0.9387905597686768\n",
      "Epoch [681/10000], Loss: 0.9387123584747314\n",
      "Epoch [682/10000], Loss: 0.9386340379714966\n",
      "Epoch [683/10000], Loss: 0.9385555386543274\n",
      "Epoch [684/10000], Loss: 0.9384769797325134\n",
      "Epoch [685/10000], Loss: 0.9383982419967651\n",
      "Epoch [686/10000], Loss: 0.9383193850517273\n",
      "Epoch [687/10000], Loss: 0.9382404088973999\n",
      "Epoch [688/10000], Loss: 0.938161313533783\n",
      "Epoch [689/10000], Loss: 0.9380820393562317\n",
      "Epoch [690/10000], Loss: 0.9380027055740356\n",
      "Epoch [691/10000], Loss: 0.9379231929779053\n",
      "Epoch [692/10000], Loss: 0.9378435611724854\n",
      "Epoch [693/10000], Loss: 0.9377638101577759\n",
      "Epoch [694/10000], Loss: 0.9376838803291321\n",
      "Epoch [695/10000], Loss: 0.9376038312911987\n",
      "Epoch [696/10000], Loss: 0.9375236630439758\n",
      "Epoch [697/10000], Loss: 0.9374433159828186\n",
      "Epoch [698/10000], Loss: 0.9373628497123718\n",
      "Epoch [699/10000], Loss: 0.9372822642326355\n",
      "Epoch [700/10000], Loss: 0.9372014999389648\n",
      "Epoch [701/10000], Loss: 0.9371205568313599\n",
      "Epoch [702/10000], Loss: 0.9370395541191101\n",
      "Epoch [703/10000], Loss: 0.9369583129882812\n",
      "Epoch [704/10000], Loss: 0.9368770122528076\n",
      "Epoch [705/10000], Loss: 0.9367955327033997\n",
      "Epoch [706/10000], Loss: 0.9367138743400574\n",
      "Epoch [707/10000], Loss: 0.9366320967674255\n",
      "Epoch [708/10000], Loss: 0.9365501403808594\n",
      "Epoch [709/10000], Loss: 0.9364680647850037\n",
      "Epoch [710/10000], Loss: 0.9363858103752136\n",
      "Epoch [711/10000], Loss: 0.9363033771514893\n",
      "Epoch [712/10000], Loss: 0.9362208247184753\n",
      "Epoch [713/10000], Loss: 0.9361381530761719\n",
      "Epoch [714/10000], Loss: 0.9360552430152893\n",
      "Epoch [715/10000], Loss: 0.9359722137451172\n",
      "Epoch [716/10000], Loss: 0.9358890652656555\n",
      "Epoch [717/10000], Loss: 0.9358057379722595\n",
      "Epoch [718/10000], Loss: 0.9357222318649292\n",
      "Epoch [719/10000], Loss: 0.9356386065483093\n",
      "Epoch [720/10000], Loss: 0.9355548024177551\n",
      "Epoch [721/10000], Loss: 0.9354708194732666\n",
      "Epoch [722/10000], Loss: 0.9353867173194885\n",
      "Epoch [723/10000], Loss: 0.9353024363517761\n",
      "Epoch [724/10000], Loss: 0.9352179765701294\n",
      "Epoch [725/10000], Loss: 0.9351333379745483\n",
      "Epoch [726/10000], Loss: 0.935048520565033\n",
      "Epoch [727/10000], Loss: 0.934963583946228\n",
      "Epoch [728/10000], Loss: 0.9348784685134888\n",
      "Epoch [729/10000], Loss: 0.9347931742668152\n",
      "Epoch [730/10000], Loss: 0.9347077012062073\n",
      "Epoch [731/10000], Loss: 0.9346221089363098\n",
      "Epoch [732/10000], Loss: 0.9345362782478333\n",
      "Epoch [733/10000], Loss: 0.9344503283500671\n",
      "Epoch [734/10000], Loss: 0.9343641400337219\n",
      "Epoch [735/10000], Loss: 0.9342778325080872\n",
      "Epoch [736/10000], Loss: 0.9341913461685181\n",
      "Epoch [737/10000], Loss: 0.9341046214103699\n",
      "Epoch [738/10000], Loss: 0.9340177774429321\n",
      "Epoch [739/10000], Loss: 0.9339306950569153\n",
      "Epoch [740/10000], Loss: 0.9338434934616089\n",
      "Epoch [741/10000], Loss: 0.9337560534477234\n",
      "Epoch [742/10000], Loss: 0.9336684942245483\n",
      "Epoch [743/10000], Loss: 0.9335806965827942\n",
      "Epoch [744/10000], Loss: 0.9334927797317505\n",
      "Epoch [745/10000], Loss: 0.9334046244621277\n",
      "Epoch [746/10000], Loss: 0.9333162903785706\n",
      "Epoch [747/10000], Loss: 0.9332277774810791\n",
      "Epoch [748/10000], Loss: 0.9331390857696533\n",
      "Epoch [749/10000], Loss: 0.9330501556396484\n",
      "Epoch [750/10000], Loss: 0.932961106300354\n",
      "Epoch [751/10000], Loss: 0.9328718185424805\n",
      "Epoch [752/10000], Loss: 0.9327823519706726\n",
      "Epoch [753/10000], Loss: 0.9326927661895752\n",
      "Epoch [754/10000], Loss: 0.9326028823852539\n",
      "Epoch [755/10000], Loss: 0.9325128793716431\n",
      "Epoch [756/10000], Loss: 0.9324226379394531\n",
      "Epoch [757/10000], Loss: 0.9323322176933289\n",
      "Epoch [758/10000], Loss: 0.9322416186332703\n",
      "Epoch [759/10000], Loss: 0.9321507811546326\n",
      "Epoch [760/10000], Loss: 0.9320597648620605\n",
      "Epoch [761/10000], Loss: 0.9319685697555542\n",
      "Epoch [762/10000], Loss: 0.9318771362304688\n",
      "Epoch [763/10000], Loss: 0.9317855834960938\n",
      "Epoch [764/10000], Loss: 0.9316937327384949\n",
      "Epoch [765/10000], Loss: 0.9316017031669617\n",
      "Epoch [766/10000], Loss: 0.9315094947814941\n",
      "Epoch [767/10000], Loss: 0.9314170479774475\n",
      "Epoch [768/10000], Loss: 0.9313243627548218\n",
      "Epoch [769/10000], Loss: 0.9312315583229065\n",
      "Epoch [770/10000], Loss: 0.9311384558677673\n",
      "Epoch [771/10000], Loss: 0.9310451745986938\n",
      "Epoch [772/10000], Loss: 0.930951714515686\n",
      "Epoch [773/10000], Loss: 0.9308580160140991\n",
      "Epoch [774/10000], Loss: 0.9307641386985779\n",
      "Epoch [775/10000], Loss: 0.9306699633598328\n",
      "Epoch [776/10000], Loss: 0.9305756092071533\n",
      "Epoch [777/10000], Loss: 0.9304810762405396\n",
      "Epoch [778/10000], Loss: 0.9303862452507019\n",
      "Epoch [779/10000], Loss: 0.9302912354469299\n",
      "Epoch [780/10000], Loss: 0.9301959872245789\n",
      "Epoch [781/10000], Loss: 0.9301005005836487\n",
      "Epoch [782/10000], Loss: 0.9300047755241394\n",
      "Epoch [783/10000], Loss: 0.9299088716506958\n",
      "Epoch [784/10000], Loss: 0.9298127293586731\n",
      "Epoch [785/10000], Loss: 0.9297163486480713\n",
      "Epoch [786/10000], Loss: 0.9296197891235352\n",
      "Epoch [787/10000], Loss: 0.9295229315757751\n",
      "Epoch [788/10000], Loss: 0.9294258952140808\n",
      "Epoch [789/10000], Loss: 0.9293286204338074\n",
      "Epoch [790/10000], Loss: 0.9292311072349548\n",
      "Epoch [791/10000], Loss: 0.9291333556175232\n",
      "Epoch [792/10000], Loss: 0.9290353655815125\n",
      "Epoch [793/10000], Loss: 0.9289371371269226\n",
      "Epoch [794/10000], Loss: 0.9288386702537537\n",
      "Epoch [795/10000], Loss: 0.9287399649620056\n",
      "Epoch [796/10000], Loss: 0.9286410808563232\n",
      "Epoch [797/10000], Loss: 0.928541898727417\n",
      "Epoch [798/10000], Loss: 0.9284424781799316\n",
      "Epoch [799/10000], Loss: 0.928342878818512\n",
      "Epoch [800/10000], Loss: 0.9282429814338684\n",
      "Epoch [801/10000], Loss: 0.9281429052352905\n",
      "Epoch [802/10000], Loss: 0.9280425310134888\n",
      "Epoch [803/10000], Loss: 0.9279419779777527\n",
      "Epoch [804/10000], Loss: 0.9278411269187927\n",
      "Epoch [805/10000], Loss: 0.9277400374412537\n",
      "Epoch [806/10000], Loss: 0.9276387691497803\n",
      "Epoch [807/10000], Loss: 0.927537202835083\n",
      "Epoch [808/10000], Loss: 0.9274353981018066\n",
      "Epoch [809/10000], Loss: 0.9273333549499512\n",
      "Epoch [810/10000], Loss: 0.9272310733795166\n",
      "Epoch [811/10000], Loss: 0.9271284937858582\n",
      "Epoch [812/10000], Loss: 0.9270257353782654\n",
      "Epoch [813/10000], Loss: 0.9269226789474487\n",
      "Epoch [814/10000], Loss: 0.9268193244934082\n",
      "Epoch [815/10000], Loss: 0.9267157912254333\n",
      "Epoch [816/10000], Loss: 0.9266119599342346\n",
      "Epoch [817/10000], Loss: 0.9265078902244568\n",
      "Epoch [818/10000], Loss: 0.9264035820960999\n",
      "Epoch [819/10000], Loss: 0.926298975944519\n",
      "Epoch [820/10000], Loss: 0.9261941313743591\n",
      "Epoch [821/10000], Loss: 0.9260889887809753\n",
      "Epoch [822/10000], Loss: 0.9259836077690125\n",
      "Epoch [823/10000], Loss: 0.9258779287338257\n",
      "Epoch [824/10000], Loss: 0.9257720708847046\n",
      "Epoch [825/10000], Loss: 0.9256659150123596\n",
      "Epoch [826/10000], Loss: 0.9255594611167908\n",
      "Epoch [827/10000], Loss: 0.9254528284072876\n",
      "Epoch [828/10000], Loss: 0.9253458976745605\n",
      "Epoch [829/10000], Loss: 0.9252387285232544\n",
      "Epoch [830/10000], Loss: 0.9251312613487244\n",
      "Epoch [831/10000], Loss: 0.9250235557556152\n",
      "Epoch [832/10000], Loss: 0.924915611743927\n",
      "Epoch [833/10000], Loss: 0.9248073697090149\n",
      "Epoch [834/10000], Loss: 0.9246988892555237\n",
      "Epoch [835/10000], Loss: 0.9245901703834534\n",
      "Epoch [836/10000], Loss: 0.9244811534881592\n",
      "Epoch [837/10000], Loss: 0.9243718981742859\n",
      "Epoch [838/10000], Loss: 0.9242623448371887\n",
      "Epoch [839/10000], Loss: 0.9241524934768677\n",
      "Epoch [840/10000], Loss: 0.9240424036979675\n",
      "Epoch [841/10000], Loss: 0.9239320755004883\n",
      "Epoch [842/10000], Loss: 0.9238214492797852\n",
      "Epoch [843/10000], Loss: 0.9237105250358582\n",
      "Epoch [844/10000], Loss: 0.923599362373352\n",
      "Epoch [845/10000], Loss: 0.9234879016876221\n",
      "Epoch [846/10000], Loss: 0.923376202583313\n",
      "Epoch [847/10000], Loss: 0.92326420545578\n",
      "Epoch [848/10000], Loss: 0.923151969909668\n",
      "Epoch [849/10000], Loss: 0.9230393767356873\n",
      "Epoch [850/10000], Loss: 0.9229266047477722\n",
      "Epoch [851/10000], Loss: 0.9228134751319885\n",
      "Epoch [852/10000], Loss: 0.9227001070976257\n",
      "Epoch [853/10000], Loss: 0.9225864410400391\n",
      "Epoch [854/10000], Loss: 0.9224725365638733\n",
      "Epoch [855/10000], Loss: 0.9223583340644836\n",
      "Epoch [856/10000], Loss: 0.9222438335418701\n",
      "Epoch [857/10000], Loss: 0.9221290946006775\n",
      "Epoch [858/10000], Loss: 0.922014057636261\n",
      "Epoch [859/10000], Loss: 0.9218987822532654\n",
      "Epoch [860/10000], Loss: 0.9217832088470459\n",
      "Epoch [861/10000], Loss: 0.9216673970222473\n",
      "Epoch [862/10000], Loss: 0.9215512871742249\n",
      "Epoch [863/10000], Loss: 0.9214348793029785\n",
      "Epoch [864/10000], Loss: 0.9213182926177979\n",
      "Epoch [865/10000], Loss: 0.9212013483047485\n",
      "Epoch [866/10000], Loss: 0.9210841655731201\n",
      "Epoch [867/10000], Loss: 0.9209666848182678\n",
      "Epoch [868/10000], Loss: 0.9208489656448364\n",
      "Epoch [869/10000], Loss: 0.9207310080528259\n",
      "Epoch [870/10000], Loss: 0.9206127524375916\n",
      "Epoch [871/10000], Loss: 0.9204942584037781\n",
      "Epoch [872/10000], Loss: 0.920375406742096\n",
      "Epoch [873/10000], Loss: 0.9202563762664795\n",
      "Epoch [874/10000], Loss: 0.9201369881629944\n",
      "Epoch [875/10000], Loss: 0.9200173616409302\n",
      "Epoch [876/10000], Loss: 0.9198973774909973\n",
      "Epoch [877/10000], Loss: 0.9197772145271301\n",
      "Epoch [878/10000], Loss: 0.9196567535400391\n",
      "Epoch [879/10000], Loss: 0.9195359945297241\n",
      "Epoch [880/10000], Loss: 0.9194149971008301\n",
      "Epoch [881/10000], Loss: 0.9192937016487122\n",
      "Epoch [882/10000], Loss: 0.9191721081733704\n",
      "Epoch [883/10000], Loss: 0.9190502762794495\n",
      "Epoch [884/10000], Loss: 0.9189281463623047\n",
      "Epoch [885/10000], Loss: 0.9188057780265808\n",
      "Epoch [886/10000], Loss: 0.9186831116676331\n",
      "Epoch [887/10000], Loss: 0.9185602068901062\n",
      "Epoch [888/10000], Loss: 0.9184370636940002\n",
      "Epoch [889/10000], Loss: 0.9183136224746704\n",
      "Epoch [890/10000], Loss: 0.9181898832321167\n",
      "Epoch [891/10000], Loss: 0.9180659055709839\n",
      "Epoch [892/10000], Loss: 0.9179415702819824\n",
      "Epoch [893/10000], Loss: 0.9178170561790466\n",
      "Epoch [894/10000], Loss: 0.9176921844482422\n",
      "Epoch [895/10000], Loss: 0.9175670742988586\n",
      "Epoch [896/10000], Loss: 0.917441725730896\n",
      "Epoch [897/10000], Loss: 0.9173161387443542\n",
      "Epoch [898/10000], Loss: 0.9171902537345886\n",
      "Epoch [899/10000], Loss: 0.9170640707015991\n",
      "Epoch [900/10000], Loss: 0.9169377088546753\n",
      "Epoch [901/10000], Loss: 0.9168110489845276\n",
      "Epoch [902/10000], Loss: 0.916684091091156\n",
      "Epoch [903/10000], Loss: 0.9165568947792053\n",
      "Epoch [904/10000], Loss: 0.9164294004440308\n",
      "Epoch [905/10000], Loss: 0.9163016676902771\n",
      "Epoch [906/10000], Loss: 0.9161736369132996\n",
      "Epoch [907/10000], Loss: 0.9160453677177429\n",
      "Epoch [908/10000], Loss: 0.9159168004989624\n",
      "Epoch [909/10000], Loss: 0.9157879948616028\n",
      "Epoch [910/10000], Loss: 0.9156588912010193\n",
      "Epoch [911/10000], Loss: 0.9155295491218567\n",
      "Epoch [912/10000], Loss: 0.915399968624115\n",
      "Epoch [913/10000], Loss: 0.9152700901031494\n",
      "Epoch [914/10000], Loss: 0.9151399731636047\n",
      "Epoch [915/10000], Loss: 0.915009617805481\n",
      "Epoch [916/10000], Loss: 0.9148790240287781\n",
      "Epoch [917/10000], Loss: 0.9147481322288513\n",
      "Epoch [918/10000], Loss: 0.9146170020103455\n",
      "Epoch [919/10000], Loss: 0.9144855737686157\n",
      "Epoch [920/10000], Loss: 0.9143539667129517\n",
      "Epoch [921/10000], Loss: 0.9142220616340637\n",
      "Epoch [922/10000], Loss: 0.9140899777412415\n",
      "Epoch [923/10000], Loss: 0.9139575958251953\n",
      "Epoch [924/10000], Loss: 0.9138249754905701\n",
      "Epoch [925/10000], Loss: 0.9136921763420105\n",
      "Epoch [926/10000], Loss: 0.913559079170227\n",
      "Epoch [927/10000], Loss: 0.9134257435798645\n",
      "Epoch [928/10000], Loss: 0.9132921695709229\n",
      "Epoch [929/10000], Loss: 0.9131583571434021\n",
      "Epoch [930/10000], Loss: 0.913024365901947\n",
      "Epoch [931/10000], Loss: 0.9128901362419128\n",
      "Epoch [932/10000], Loss: 0.9127556681632996\n",
      "Epoch [933/10000], Loss: 0.9126209616661072\n",
      "Epoch [934/10000], Loss: 0.9124860167503357\n",
      "Epoch [935/10000], Loss: 0.9123508334159851\n",
      "Epoch [936/10000], Loss: 0.9122154712677002\n",
      "Epoch [937/10000], Loss: 0.9120798707008362\n",
      "Epoch [938/10000], Loss: 0.9119440317153931\n",
      "Epoch [939/10000], Loss: 0.9118079543113708\n",
      "Epoch [940/10000], Loss: 0.9116716980934143\n",
      "Epoch [941/10000], Loss: 0.9115351438522339\n",
      "Epoch [942/10000], Loss: 0.9113983511924744\n",
      "Epoch [943/10000], Loss: 0.9112613797187805\n",
      "Epoch [944/10000], Loss: 0.9111242294311523\n",
      "Epoch [945/10000], Loss: 0.9109868407249451\n",
      "Epoch [946/10000], Loss: 0.9108492136001587\n",
      "Epoch [947/10000], Loss: 0.910711407661438\n",
      "Epoch [948/10000], Loss: 0.9105733633041382\n",
      "Epoch [949/10000], Loss: 0.9104351997375488\n",
      "Epoch [950/10000], Loss: 0.9102967381477356\n",
      "Epoch [951/10000], Loss: 0.9101581573486328\n",
      "Epoch [952/10000], Loss: 0.9100193381309509\n",
      "Epoch [953/10000], Loss: 0.9098802804946899\n",
      "Epoch [954/10000], Loss: 0.9097410440444946\n",
      "Epoch [955/10000], Loss: 0.909601628780365\n",
      "Epoch [956/10000], Loss: 0.9094619154930115\n",
      "Epoch [957/10000], Loss: 0.9093220829963684\n",
      "Epoch [958/10000], Loss: 0.9091820120811462\n",
      "Epoch [959/10000], Loss: 0.9090417623519897\n",
      "Epoch [960/10000], Loss: 0.9089013338088989\n",
      "Epoch [961/10000], Loss: 0.908760666847229\n",
      "Epoch [962/10000], Loss: 0.9086198210716248\n",
      "Epoch [963/10000], Loss: 0.9084787368774414\n",
      "Epoch [964/10000], Loss: 0.9083375334739685\n",
      "Epoch [965/10000], Loss: 0.9081960916519165\n",
      "Epoch [966/10000], Loss: 0.9080544710159302\n",
      "Epoch [967/10000], Loss: 0.9079126119613647\n",
      "Epoch [968/10000], Loss: 0.907770574092865\n",
      "Epoch [969/10000], Loss: 0.9076284170150757\n",
      "Epoch [970/10000], Loss: 0.9074860215187073\n",
      "Epoch [971/10000], Loss: 0.9073434472084045\n",
      "Epoch [972/10000], Loss: 0.9072006940841675\n",
      "Epoch [973/10000], Loss: 0.9070577621459961\n",
      "Epoch [974/10000], Loss: 0.9069147109985352\n",
      "Epoch [975/10000], Loss: 0.9067714214324951\n",
      "Epoch [976/10000], Loss: 0.9066280126571655\n",
      "Epoch [977/10000], Loss: 0.9064843654632568\n",
      "Epoch [978/10000], Loss: 0.9063405990600586\n",
      "Epoch [979/10000], Loss: 0.9061965942382812\n",
      "Epoch [980/10000], Loss: 0.9060524702072144\n",
      "Epoch [981/10000], Loss: 0.9059081077575684\n",
      "Epoch [982/10000], Loss: 0.905763566493988\n",
      "Epoch [983/10000], Loss: 0.9056189060211182\n",
      "Epoch [984/10000], Loss: 0.905474066734314\n",
      "Epoch [985/10000], Loss: 0.9053289890289307\n",
      "Epoch [986/10000], Loss: 0.9051837921142578\n",
      "Epoch [987/10000], Loss: 0.9050384163856506\n",
      "Epoch [988/10000], Loss: 0.9048928618431091\n",
      "Epoch [989/10000], Loss: 0.9047471880912781\n",
      "Epoch [990/10000], Loss: 0.9046012759208679\n",
      "Epoch [991/10000], Loss: 0.9044552445411682\n",
      "Epoch [992/10000], Loss: 0.9043090343475342\n",
      "Epoch [993/10000], Loss: 0.9041627049446106\n",
      "Epoch [994/10000], Loss: 0.9040161371231079\n",
      "Epoch [995/10000], Loss: 0.9038694500923157\n",
      "Epoch [996/10000], Loss: 0.9037225842475891\n",
      "Epoch [997/10000], Loss: 0.9035755395889282\n",
      "Epoch [998/10000], Loss: 0.9034283757209778\n",
      "Epoch [999/10000], Loss: 0.9032809734344482\n",
      "Epoch [1000/10000], Loss: 0.9031335115432739\n",
      "Epoch [1001/10000], Loss: 0.9029858112335205\n",
      "Epoch [1002/10000], Loss: 0.9028379917144775\n",
      "Epoch [1003/10000], Loss: 0.902690052986145\n",
      "Epoch [1004/10000], Loss: 0.9025418758392334\n",
      "Epoch [1005/10000], Loss: 0.9023935794830322\n",
      "Epoch [1006/10000], Loss: 0.9022451639175415\n",
      "Epoch [1007/10000], Loss: 0.9020965099334717\n",
      "Epoch [1008/10000], Loss: 0.9019477367401123\n",
      "Epoch [1009/10000], Loss: 0.9017987847328186\n",
      "Epoch [1010/10000], Loss: 0.9016497135162354\n",
      "Epoch [1011/10000], Loss: 0.9015005230903625\n",
      "Epoch [1012/10000], Loss: 0.9013511538505554\n",
      "Epoch [1013/10000], Loss: 0.901201605796814\n",
      "Epoch [1014/10000], Loss: 0.901051938533783\n",
      "Epoch [1015/10000], Loss: 0.9009021520614624\n",
      "Epoch [1016/10000], Loss: 0.9007521867752075\n",
      "Epoch [1017/10000], Loss: 0.9006021022796631\n",
      "Epoch [1018/10000], Loss: 0.9004518389701843\n",
      "Epoch [1019/10000], Loss: 0.900301456451416\n",
      "Epoch [1020/10000], Loss: 0.9001508951187134\n",
      "Epoch [1021/10000], Loss: 0.900000274181366\n",
      "Epoch [1022/10000], Loss: 0.8998494148254395\n",
      "Epoch [1023/10000], Loss: 0.8996984958648682\n",
      "Epoch [1024/10000], Loss: 0.8995473980903625\n",
      "Epoch [1025/10000], Loss: 0.8993961811065674\n",
      "Epoch [1026/10000], Loss: 0.8992448449134827\n",
      "Epoch [1027/10000], Loss: 0.8990933299064636\n",
      "Epoch [1028/10000], Loss: 0.898941695690155\n",
      "Epoch [1029/10000], Loss: 0.8987898826599121\n",
      "Epoch [1030/10000], Loss: 0.8986379504203796\n",
      "Epoch [1031/10000], Loss: 0.8984858989715576\n",
      "Epoch [1032/10000], Loss: 0.898333728313446\n",
      "Epoch [1033/10000], Loss: 0.8981814384460449\n",
      "Epoch [1034/10000], Loss: 0.8980289697647095\n",
      "Epoch [1035/10000], Loss: 0.8978763818740845\n",
      "Epoch [1036/10000], Loss: 0.8977236747741699\n",
      "Epoch [1037/10000], Loss: 0.8975708484649658\n",
      "Epoch [1038/10000], Loss: 0.8974178433418274\n",
      "Epoch [1039/10000], Loss: 0.8972647190093994\n",
      "Epoch [1040/10000], Loss: 0.8971115350723267\n",
      "Epoch [1041/10000], Loss: 0.8969581723213196\n",
      "Epoch [1042/10000], Loss: 0.8968046307563782\n",
      "Epoch [1043/10000], Loss: 0.896651029586792\n",
      "Epoch [1044/10000], Loss: 0.8964972496032715\n",
      "Epoch [1045/10000], Loss: 0.8963434100151062\n",
      "Epoch [1046/10000], Loss: 0.8961893916130066\n",
      "Epoch [1047/10000], Loss: 0.8960351943969727\n",
      "Epoch [1048/10000], Loss: 0.895880937576294\n",
      "Epoch [1049/10000], Loss: 0.8957265615463257\n",
      "Epoch [1050/10000], Loss: 0.8955720067024231\n",
      "Epoch [1051/10000], Loss: 0.895417332649231\n",
      "Epoch [1052/10000], Loss: 0.8952625393867493\n",
      "Epoch [1053/10000], Loss: 0.895107626914978\n",
      "Epoch [1054/10000], Loss: 0.8949525952339172\n",
      "Epoch [1055/10000], Loss: 0.8947974443435669\n",
      "Epoch [1056/10000], Loss: 0.8946421146392822\n",
      "Epoch [1057/10000], Loss: 0.8944867253303528\n",
      "Epoch [1058/10000], Loss: 0.8943312168121338\n",
      "Epoch [1059/10000], Loss: 0.8941755890846252\n",
      "Epoch [1060/10000], Loss: 0.8940198421478271\n",
      "Epoch [1061/10000], Loss: 0.8938639760017395\n",
      "Epoch [1062/10000], Loss: 0.8937080502510071\n",
      "Epoch [1063/10000], Loss: 0.8935519456863403\n",
      "Epoch [1064/10000], Loss: 0.8933957815170288\n",
      "Epoch [1065/10000], Loss: 0.893239438533783\n",
      "Epoch [1066/10000], Loss: 0.8930830359458923\n",
      "Epoch [1067/10000], Loss: 0.8929265141487122\n",
      "Epoch [1068/10000], Loss: 0.8927698135375977\n",
      "Epoch [1069/10000], Loss: 0.8926130533218384\n",
      "Epoch [1070/10000], Loss: 0.8924561142921448\n",
      "Epoch [1071/10000], Loss: 0.8922991156578064\n",
      "Epoch [1072/10000], Loss: 0.8921419978141785\n",
      "Epoch [1073/10000], Loss: 0.891984760761261\n",
      "Epoch [1074/10000], Loss: 0.891827404499054\n",
      "Epoch [1075/10000], Loss: 0.8916699886322021\n",
      "Epoch [1076/10000], Loss: 0.8915124535560608\n",
      "Epoch [1077/10000], Loss: 0.8913547396659851\n",
      "Epoch [1078/10000], Loss: 0.8911969065666199\n",
      "Epoch [1079/10000], Loss: 0.8910390138626099\n",
      "Epoch [1080/10000], Loss: 0.8908810019493103\n",
      "Epoch [1081/10000], Loss: 0.890722930431366\n",
      "Epoch [1082/10000], Loss: 0.8905646800994873\n",
      "Epoch [1083/10000], Loss: 0.8904063701629639\n",
      "Epoch [1084/10000], Loss: 0.8902479410171509\n",
      "Epoch [1085/10000], Loss: 0.8900893926620483\n",
      "Epoch [1086/10000], Loss: 0.8899307250976562\n",
      "Epoch [1087/10000], Loss: 0.8897720575332642\n",
      "Epoch [1088/10000], Loss: 0.8896132111549377\n",
      "Epoch [1089/10000], Loss: 0.8894543647766113\n",
      "Epoch [1090/10000], Loss: 0.8892953395843506\n",
      "Epoch [1091/10000], Loss: 0.8891362547874451\n",
      "Epoch [1092/10000], Loss: 0.8889771103858948\n",
      "Epoch [1093/10000], Loss: 0.8888178467750549\n",
      "Epoch [1094/10000], Loss: 0.8886585831642151\n",
      "Epoch [1095/10000], Loss: 0.8884991407394409\n",
      "Epoch [1096/10000], Loss: 0.888339638710022\n",
      "Epoch [1097/10000], Loss: 0.8881800770759583\n",
      "Epoch [1098/10000], Loss: 0.888020396232605\n",
      "Epoch [1099/10000], Loss: 0.8878605961799622\n",
      "Epoch [1100/10000], Loss: 0.8877007961273193\n",
      "Epoch [1101/10000], Loss: 0.8875408172607422\n",
      "Epoch [1102/10000], Loss: 0.8873807787895203\n",
      "Epoch [1103/10000], Loss: 0.8872206807136536\n",
      "Epoch [1104/10000], Loss: 0.8870604634284973\n",
      "Epoch [1105/10000], Loss: 0.8869001865386963\n",
      "Epoch [1106/10000], Loss: 0.8867397904396057\n",
      "Epoch [1107/10000], Loss: 0.8865793347358704\n",
      "Epoch [1108/10000], Loss: 0.8864188194274902\n",
      "Epoch [1109/10000], Loss: 0.8862582445144653\n",
      "Epoch [1110/10000], Loss: 0.8860976099967957\n",
      "Epoch [1111/10000], Loss: 0.8859368562698364\n",
      "Epoch [1112/10000], Loss: 0.8857760429382324\n",
      "Epoch [1113/10000], Loss: 0.8856151700019836\n",
      "Epoch [1114/10000], Loss: 0.8854541778564453\n",
      "Epoch [1115/10000], Loss: 0.8852931261062622\n",
      "Epoch [1116/10000], Loss: 0.8851320147514343\n",
      "Epoch [1117/10000], Loss: 0.8849708437919617\n",
      "Epoch [1118/10000], Loss: 0.8848096132278442\n",
      "Epoch [1119/10000], Loss: 0.884648323059082\n",
      "Epoch [1120/10000], Loss: 0.8844869136810303\n",
      "Epoch [1121/10000], Loss: 0.8843255043029785\n",
      "Epoch [1122/10000], Loss: 0.8841639757156372\n",
      "Epoch [1123/10000], Loss: 0.8840024471282959\n",
      "Epoch [1124/10000], Loss: 0.8838408589363098\n",
      "Epoch [1125/10000], Loss: 0.8836792707443237\n",
      "Epoch [1126/10000], Loss: 0.8835175633430481\n",
      "Epoch [1127/10000], Loss: 0.8833558559417725\n",
      "Epoch [1128/10000], Loss: 0.883194088935852\n",
      "Epoch [1129/10000], Loss: 0.8830322623252869\n",
      "Epoch [1130/10000], Loss: 0.8828703761100769\n",
      "Epoch [1131/10000], Loss: 0.8827084302902222\n",
      "Epoch [1132/10000], Loss: 0.8825464844703674\n",
      "Epoch [1133/10000], Loss: 0.8823844790458679\n",
      "Epoch [1134/10000], Loss: 0.8822224736213684\n",
      "Epoch [1135/10000], Loss: 0.8820603489875793\n",
      "Epoch [1136/10000], Loss: 0.8818982243537903\n",
      "Epoch [1137/10000], Loss: 0.8817360401153564\n",
      "Epoch [1138/10000], Loss: 0.8815737962722778\n",
      "Epoch [1139/10000], Loss: 0.8814115524291992\n",
      "Epoch [1140/10000], Loss: 0.8812492489814758\n",
      "Epoch [1141/10000], Loss: 0.8810870051383972\n",
      "Epoch [1142/10000], Loss: 0.8809247612953186\n",
      "Epoch [1143/10000], Loss: 0.8807624578475952\n",
      "Epoch [1144/10000], Loss: 0.8806002140045166\n",
      "Epoch [1145/10000], Loss: 0.8804379105567932\n",
      "Epoch [1146/10000], Loss: 0.8802756667137146\n",
      "Epoch [1147/10000], Loss: 0.8801134824752808\n",
      "Epoch [1148/10000], Loss: 0.8799511790275574\n",
      "Epoch [1149/10000], Loss: 0.8797889351844788\n",
      "Epoch [1150/10000], Loss: 0.8796266913414001\n",
      "Epoch [1151/10000], Loss: 0.8794643878936768\n",
      "Epoch [1152/10000], Loss: 0.8793021440505981\n",
      "Epoch [1153/10000], Loss: 0.8791399002075195\n",
      "Epoch [1154/10000], Loss: 0.8789777159690857\n",
      "Epoch [1155/10000], Loss: 0.8788155317306519\n",
      "Epoch [1156/10000], Loss: 0.8786532878875732\n",
      "Epoch [1157/10000], Loss: 0.8784911632537842\n",
      "Epoch [1158/10000], Loss: 0.8783290982246399\n",
      "Epoch [1159/10000], Loss: 0.8781670331954956\n",
      "Epoch [1160/10000], Loss: 0.8780050873756409\n",
      "Epoch [1161/10000], Loss: 0.8778432011604309\n",
      "Epoch [1162/10000], Loss: 0.877681314945221\n",
      "Epoch [1163/10000], Loss: 0.8775194883346558\n",
      "Epoch [1164/10000], Loss: 0.8773578405380249\n",
      "Epoch [1165/10000], Loss: 0.8771963119506836\n",
      "Epoch [1166/10000], Loss: 0.8770348429679871\n",
      "Epoch [1167/10000], Loss: 0.8768734931945801\n",
      "Epoch [1168/10000], Loss: 0.8767121434211731\n",
      "Epoch [1169/10000], Loss: 0.8765509128570557\n",
      "Epoch [1170/10000], Loss: 0.8763896822929382\n",
      "Epoch [1171/10000], Loss: 0.8762285709381104\n",
      "Epoch [1172/10000], Loss: 0.8760674595832825\n",
      "Epoch [1173/10000], Loss: 0.8759064674377441\n",
      "Epoch [1174/10000], Loss: 0.8757455348968506\n",
      "Epoch [1175/10000], Loss: 0.8755847215652466\n",
      "Epoch [1176/10000], Loss: 0.8754238486289978\n",
      "Epoch [1177/10000], Loss: 0.8752630352973938\n",
      "Epoch [1178/10000], Loss: 0.8751022219657898\n",
      "Epoch [1179/10000], Loss: 0.8749414682388306\n",
      "Epoch [1180/10000], Loss: 0.8747807741165161\n",
      "Epoch [1181/10000], Loss: 0.8746200799942017\n",
      "Epoch [1182/10000], Loss: 0.874459445476532\n",
      "Epoch [1183/10000], Loss: 0.8742988705635071\n",
      "Epoch [1184/10000], Loss: 0.8741384148597717\n",
      "Epoch [1185/10000], Loss: 0.8739779591560364\n",
      "Epoch [1186/10000], Loss: 0.8738175630569458\n",
      "Epoch [1187/10000], Loss: 0.8736572861671448\n",
      "Epoch [1188/10000], Loss: 0.8734970688819885\n",
      "Epoch [1189/10000], Loss: 0.873336911201477\n",
      "Epoch [1190/10000], Loss: 0.8731768727302551\n",
      "Epoch [1191/10000], Loss: 0.873016893863678\n",
      "Epoch [1192/10000], Loss: 0.8728569746017456\n",
      "Epoch [1193/10000], Loss: 0.872697114944458\n",
      "Epoch [1194/10000], Loss: 0.8725373148918152\n",
      "Epoch [1195/10000], Loss: 0.8723775744438171\n",
      "Epoch [1196/10000], Loss: 0.8722178936004639\n",
      "Epoch [1197/10000], Loss: 0.8720582723617554\n",
      "Epoch [1198/10000], Loss: 0.8718988299369812\n",
      "Epoch [1199/10000], Loss: 0.871739387512207\n",
      "Epoch [1200/10000], Loss: 0.8715800642967224\n",
      "Epoch [1201/10000], Loss: 0.8714208006858826\n",
      "Epoch [1202/10000], Loss: 0.8712616562843323\n",
      "Epoch [1203/10000], Loss: 0.8711026310920715\n",
      "Epoch [1204/10000], Loss: 0.8709436655044556\n",
      "Epoch [1205/10000], Loss: 0.8707847595214844\n",
      "Epoch [1206/10000], Loss: 0.870625913143158\n",
      "Epoch [1207/10000], Loss: 0.8704671859741211\n",
      "Epoch [1208/10000], Loss: 0.870308518409729\n",
      "Epoch [1209/10000], Loss: 0.8701499700546265\n",
      "Epoch [1210/10000], Loss: 0.8699914813041687\n",
      "Epoch [1211/10000], Loss: 0.8698331117630005\n",
      "Epoch [1212/10000], Loss: 0.869674801826477\n",
      "Epoch [1213/10000], Loss: 0.8695165514945984\n",
      "Epoch [1214/10000], Loss: 0.8693583607673645\n",
      "Epoch [1215/10000], Loss: 0.8692003488540649\n",
      "Epoch [1216/10000], Loss: 0.8690423965454102\n",
      "Epoch [1217/10000], Loss: 0.8688845634460449\n",
      "Epoch [1218/10000], Loss: 0.8687267899513245\n",
      "Epoch [1219/10000], Loss: 0.8685690760612488\n",
      "Epoch [1220/10000], Loss: 0.8684114813804626\n",
      "Epoch [1221/10000], Loss: 0.8682540059089661\n",
      "Epoch [1222/10000], Loss: 0.8680965900421143\n",
      "Epoch [1223/10000], Loss: 0.867939293384552\n",
      "Epoch [1224/10000], Loss: 0.8677821159362793\n",
      "Epoch [1225/10000], Loss: 0.8676249980926514\n",
      "Epoch [1226/10000], Loss: 0.8674680590629578\n",
      "Epoch [1227/10000], Loss: 0.8673111796379089\n",
      "Epoch [1228/10000], Loss: 0.8671543598175049\n",
      "Epoch [1229/10000], Loss: 0.8669977188110352\n",
      "Epoch [1230/10000], Loss: 0.8668411374092102\n",
      "Epoch [1231/10000], Loss: 0.8666846752166748\n",
      "Epoch [1232/10000], Loss: 0.866528332233429\n",
      "Epoch [1233/10000], Loss: 0.8663720488548279\n",
      "Epoch [1234/10000], Loss: 0.8662158846855164\n",
      "Epoch [1235/10000], Loss: 0.8660598397254944\n",
      "Epoch [1236/10000], Loss: 0.865903913974762\n",
      "Epoch [1237/10000], Loss: 0.8657481074333191\n",
      "Epoch [1238/10000], Loss: 0.865592360496521\n",
      "Epoch [1239/10000], Loss: 0.8654367923736572\n",
      "Epoch [1240/10000], Loss: 0.865281343460083\n",
      "Epoch [1241/10000], Loss: 0.8651260137557983\n",
      "Epoch [1242/10000], Loss: 0.8649708032608032\n",
      "Epoch [1243/10000], Loss: 0.8648156523704529\n",
      "Epoch [1244/10000], Loss: 0.8646606802940369\n",
      "Epoch [1245/10000], Loss: 0.8645057678222656\n",
      "Epoch [1246/10000], Loss: 0.8643510341644287\n",
      "Epoch [1247/10000], Loss: 0.8641964197158813\n",
      "Epoch [1248/10000], Loss: 0.864041805267334\n",
      "Epoch [1249/10000], Loss: 0.8638874292373657\n",
      "Epoch [1250/10000], Loss: 0.8637330532073975\n",
      "Epoch [1251/10000], Loss: 0.8635789155960083\n",
      "Epoch [1252/10000], Loss: 0.8634248375892639\n",
      "Epoch [1253/10000], Loss: 0.8632708787918091\n",
      "Epoch [1254/10000], Loss: 0.8631170988082886\n",
      "Epoch [1255/10000], Loss: 0.8629634380340576\n",
      "Epoch [1256/10000], Loss: 0.8628098964691162\n",
      "Epoch [1257/10000], Loss: 0.8626564741134644\n",
      "Epoch [1258/10000], Loss: 0.862503170967102\n",
      "Epoch [1259/10000], Loss: 0.8623499870300293\n",
      "Epoch [1260/10000], Loss: 0.8621969223022461\n",
      "Epoch [1261/10000], Loss: 0.8620439767837524\n",
      "Epoch [1262/10000], Loss: 0.8618911504745483\n",
      "Epoch [1263/10000], Loss: 0.8617384433746338\n",
      "Epoch [1264/10000], Loss: 0.8615859746932983\n",
      "Epoch [1265/10000], Loss: 0.8614335656166077\n",
      "Epoch [1266/10000], Loss: 0.8612812757492065\n",
      "Epoch [1267/10000], Loss: 0.861129105091095\n",
      "Epoch [1268/10000], Loss: 0.8609771132469177\n",
      "Epoch [1269/10000], Loss: 0.86082524061203\n",
      "Epoch [1270/10000], Loss: 0.8606735467910767\n",
      "Epoch [1271/10000], Loss: 0.8605219125747681\n",
      "Epoch [1272/10000], Loss: 0.8603704571723938\n",
      "Epoch [1273/10000], Loss: 0.8602191209793091\n",
      "Epoch [1274/10000], Loss: 0.8600679039955139\n",
      "Epoch [1275/10000], Loss: 0.8599168062210083\n",
      "Epoch [1276/10000], Loss: 0.859765887260437\n",
      "Epoch [1277/10000], Loss: 0.8596150875091553\n",
      "Epoch [1278/10000], Loss: 0.8594644069671631\n",
      "Epoch [1279/10000], Loss: 0.8593138456344604\n",
      "Epoch [1280/10000], Loss: 0.8591634631156921\n",
      "Epoch [1281/10000], Loss: 0.8590131998062134\n",
      "Epoch [1282/10000], Loss: 0.858863115310669\n",
      "Epoch [1283/10000], Loss: 0.8587131500244141\n",
      "Epoch [1284/10000], Loss: 0.8585633039474487\n",
      "Epoch [1285/10000], Loss: 0.858413577079773\n",
      "Epoch [1286/10000], Loss: 0.8582640290260315\n",
      "Epoch [1287/10000], Loss: 0.8581146001815796\n",
      "Epoch [1288/10000], Loss: 0.857965350151062\n",
      "Epoch [1289/10000], Loss: 0.857816219329834\n",
      "Epoch [1290/10000], Loss: 0.8576672077178955\n",
      "Epoch [1291/10000], Loss: 0.8575183153152466\n",
      "Epoch [1292/10000], Loss: 0.857369601726532\n",
      "Epoch [1293/10000], Loss: 0.8572210073471069\n",
      "Epoch [1294/10000], Loss: 0.8570725917816162\n",
      "Epoch [1295/10000], Loss: 0.8569242358207703\n",
      "Epoch [1296/10000], Loss: 0.8567760586738586\n",
      "Epoch [1297/10000], Loss: 0.8566280007362366\n",
      "Epoch [1298/10000], Loss: 0.8564801216125488\n",
      "Epoch [1299/10000], Loss: 0.8563323020935059\n",
      "Epoch [1300/10000], Loss: 0.856184720993042\n",
      "Epoch [1301/10000], Loss: 0.8560372591018677\n",
      "Epoch [1302/10000], Loss: 0.8558899164199829\n",
      "Epoch [1303/10000], Loss: 0.8557427525520325\n",
      "Epoch [1304/10000], Loss: 0.8555957078933716\n",
      "Epoch [1305/10000], Loss: 0.855448842048645\n",
      "Epoch [1306/10000], Loss: 0.855302095413208\n",
      "Epoch [1307/10000], Loss: 0.8551554679870605\n",
      "Epoch [1308/10000], Loss: 0.8550089597702026\n",
      "Epoch [1309/10000], Loss: 0.8548625707626343\n",
      "Epoch [1310/10000], Loss: 0.854716420173645\n",
      "Epoch [1311/10000], Loss: 0.8545703291893005\n",
      "Epoch [1312/10000], Loss: 0.8544243574142456\n",
      "Epoch [1313/10000], Loss: 0.8542786240577698\n",
      "Epoch [1314/10000], Loss: 0.8541329503059387\n",
      "Epoch [1315/10000], Loss: 0.853987455368042\n",
      "Epoch [1316/10000], Loss: 0.8538421392440796\n",
      "Epoch [1317/10000], Loss: 0.853696882724762\n",
      "Epoch [1318/10000], Loss: 0.8535517454147339\n",
      "Epoch [1319/10000], Loss: 0.8534068465232849\n",
      "Epoch [1320/10000], Loss: 0.8532620668411255\n",
      "Epoch [1321/10000], Loss: 0.8531174063682556\n",
      "Epoch [1322/10000], Loss: 0.8529729247093201\n",
      "Epoch [1323/10000], Loss: 0.8528286218643188\n",
      "Epoch [1324/10000], Loss: 0.8526843786239624\n",
      "Epoch [1325/10000], Loss: 0.8525402545928955\n",
      "Epoch [1326/10000], Loss: 0.8523963689804077\n",
      "Epoch [1327/10000], Loss: 0.8522525429725647\n",
      "Epoch [1328/10000], Loss: 0.852108895778656\n",
      "Epoch [1329/10000], Loss: 0.8519654273986816\n",
      "Epoch [1330/10000], Loss: 0.851822018623352\n",
      "Epoch [1331/10000], Loss: 0.8516787886619568\n",
      "Epoch [1332/10000], Loss: 0.8515356779098511\n",
      "Epoch [1333/10000], Loss: 0.8513926863670349\n",
      "Epoch [1334/10000], Loss: 0.8512498140335083\n",
      "Epoch [1335/10000], Loss: 0.851107120513916\n",
      "Epoch [1336/10000], Loss: 0.8509646654129028\n",
      "Epoch [1337/10000], Loss: 0.8508222103118896\n",
      "Epoch [1338/10000], Loss: 0.8506799340248108\n",
      "Epoch [1339/10000], Loss: 0.8505377769470215\n",
      "Epoch [1340/10000], Loss: 0.8503957986831665\n",
      "Epoch [1341/10000], Loss: 0.8502539396286011\n",
      "Epoch [1342/10000], Loss: 0.8501123189926147\n",
      "Epoch [1343/10000], Loss: 0.8499706983566284\n",
      "Epoch [1344/10000], Loss: 0.8498293161392212\n",
      "Epoch [1345/10000], Loss: 0.8496880531311035\n",
      "Epoch [1346/10000], Loss: 0.8495469093322754\n",
      "Epoch [1347/10000], Loss: 0.8494058847427368\n",
      "Epoch [1348/10000], Loss: 0.8492650389671326\n",
      "Epoch [1349/10000], Loss: 0.8491243124008179\n",
      "Epoch [1350/10000], Loss: 0.848983645439148\n",
      "Epoch [1351/10000], Loss: 0.8488432168960571\n",
      "Epoch [1352/10000], Loss: 0.8487028479576111\n",
      "Epoch [1353/10000], Loss: 0.8485625982284546\n",
      "Epoch [1354/10000], Loss: 0.8484225273132324\n",
      "Epoch [1355/10000], Loss: 0.8482826352119446\n",
      "Epoch [1356/10000], Loss: 0.8481428623199463\n",
      "Epoch [1357/10000], Loss: 0.8480032086372375\n",
      "Epoch [1358/10000], Loss: 0.8478636741638184\n",
      "Epoch [1359/10000], Loss: 0.8477243185043335\n",
      "Epoch [1360/10000], Loss: 0.847585141658783\n",
      "Epoch [1361/10000], Loss: 0.847446084022522\n",
      "Epoch [1362/10000], Loss: 0.8473070859909058\n",
      "Epoch [1363/10000], Loss: 0.8471682071685791\n",
      "Epoch [1364/10000], Loss: 0.8470295667648315\n",
      "Epoch [1365/10000], Loss: 0.846890926361084\n",
      "Epoch [1366/10000], Loss: 0.8467524647712708\n",
      "Epoch [1367/10000], Loss: 0.8466141223907471\n",
      "Epoch [1368/10000], Loss: 0.8464759588241577\n",
      "Epoch [1369/10000], Loss: 0.8463379144668579\n",
      "Epoch [1370/10000], Loss: 0.8461999297142029\n",
      "Epoch [1371/10000], Loss: 0.8460620641708374\n",
      "Epoch [1372/10000], Loss: 0.8459243774414062\n",
      "Epoch [1373/10000], Loss: 0.8457868099212646\n",
      "Epoch [1374/10000], Loss: 0.8456493616104126\n",
      "Epoch [1375/10000], Loss: 0.8455120325088501\n",
      "Epoch [1376/10000], Loss: 0.8453748226165771\n",
      "Epoch [1377/10000], Loss: 0.8452377915382385\n",
      "Epoch [1378/10000], Loss: 0.8451007604598999\n",
      "Epoch [1379/10000], Loss: 0.8449639081954956\n",
      "Epoch [1380/10000], Loss: 0.8448271155357361\n",
      "Epoch [1381/10000], Loss: 0.8446905016899109\n",
      "Epoch [1382/10000], Loss: 0.8445540070533752\n",
      "Epoch [1383/10000], Loss: 0.8444175720214844\n",
      "Epoch [1384/10000], Loss: 0.8442813158035278\n",
      "Epoch [1385/10000], Loss: 0.8441450595855713\n",
      "Epoch [1386/10000], Loss: 0.8440090417861938\n",
      "Epoch [1387/10000], Loss: 0.8438730835914612\n",
      "Epoch [1388/10000], Loss: 0.8437372446060181\n",
      "Epoch [1389/10000], Loss: 0.8436015844345093\n",
      "Epoch [1390/10000], Loss: 0.84346604347229\n",
      "Epoch [1391/10000], Loss: 0.8433305621147156\n",
      "Epoch [1392/10000], Loss: 0.8431952595710754\n",
      "Epoch [1393/10000], Loss: 0.8430600762367249\n",
      "Epoch [1394/10000], Loss: 0.8429250121116638\n",
      "Epoch [1395/10000], Loss: 0.8427900671958923\n",
      "Epoch [1396/10000], Loss: 0.8426552414894104\n",
      "Epoch [1397/10000], Loss: 0.8425204753875732\n",
      "Epoch [1398/10000], Loss: 0.8423858880996704\n",
      "Epoch [1399/10000], Loss: 0.8422514796257019\n",
      "Epoch [1400/10000], Loss: 0.842117190361023\n",
      "Epoch [1401/10000], Loss: 0.8419829607009888\n",
      "Epoch [1402/10000], Loss: 0.8418488502502441\n",
      "Epoch [1403/10000], Loss: 0.8417149186134338\n",
      "Epoch [1404/10000], Loss: 0.8415811061859131\n",
      "Epoch [1405/10000], Loss: 0.8414474725723267\n",
      "Epoch [1406/10000], Loss: 0.8413139581680298\n",
      "Epoch [1407/10000], Loss: 0.8411805629730225\n",
      "Epoch [1408/10000], Loss: 0.8410473465919495\n",
      "Epoch [1409/10000], Loss: 0.840914249420166\n",
      "Epoch [1410/10000], Loss: 0.8407812714576721\n",
      "Epoch [1411/10000], Loss: 0.8406484723091125\n",
      "Epoch [1412/10000], Loss: 0.8405157327651978\n",
      "Epoch [1413/10000], Loss: 0.8403831720352173\n",
      "Epoch [1414/10000], Loss: 0.8402507305145264\n",
      "Epoch [1415/10000], Loss: 0.840118408203125\n",
      "Epoch [1416/10000], Loss: 0.839986264705658\n",
      "Epoch [1417/10000], Loss: 0.8398542404174805\n",
      "Epoch [1418/10000], Loss: 0.8397222757339478\n",
      "Epoch [1419/10000], Loss: 0.8395905494689941\n",
      "Epoch [1420/10000], Loss: 0.8394588828086853\n",
      "Epoch [1421/10000], Loss: 0.839327335357666\n",
      "Epoch [1422/10000], Loss: 0.8391958475112915\n",
      "Epoch [1423/10000], Loss: 0.8390645980834961\n",
      "Epoch [1424/10000], Loss: 0.8389334678649902\n",
      "Epoch [1425/10000], Loss: 0.8388023376464844\n",
      "Epoch [1426/10000], Loss: 0.8386714458465576\n",
      "Epoch [1427/10000], Loss: 0.8385406732559204\n",
      "Epoch [1428/10000], Loss: 0.8384099006652832\n",
      "Epoch [1429/10000], Loss: 0.8382793664932251\n",
      "Epoch [1430/10000], Loss: 0.838148832321167\n",
      "Epoch [1431/10000], Loss: 0.838018536567688\n",
      "Epoch [1432/10000], Loss: 0.8378883600234985\n",
      "Epoch [1433/10000], Loss: 0.8377581834793091\n",
      "Epoch [1434/10000], Loss: 0.837628185749054\n",
      "Epoch [1435/10000], Loss: 0.8374983072280884\n",
      "Epoch [1436/10000], Loss: 0.8373684883117676\n",
      "Epoch [1437/10000], Loss: 0.8372388482093811\n",
      "Epoch [1438/10000], Loss: 0.8371093273162842\n",
      "Epoch [1439/10000], Loss: 0.836979866027832\n",
      "Epoch [1440/10000], Loss: 0.8368505239486694\n",
      "Epoch [1441/10000], Loss: 0.8367213010787964\n",
      "Epoch [1442/10000], Loss: 0.8365920782089233\n",
      "Epoch [1443/10000], Loss: 0.8364630937576294\n",
      "Epoch [1444/10000], Loss: 0.8363341093063354\n",
      "Epoch [1445/10000], Loss: 0.836205244064331\n",
      "Epoch [1446/10000], Loss: 0.8360764980316162\n",
      "Epoch [1447/10000], Loss: 0.8359478712081909\n",
      "Epoch [1448/10000], Loss: 0.8358192443847656\n",
      "Epoch [1449/10000], Loss: 0.8356907367706299\n",
      "Epoch [1450/10000], Loss: 0.8355623483657837\n",
      "Epoch [1451/10000], Loss: 0.8354339599609375\n",
      "Epoch [1452/10000], Loss: 0.8353056907653809\n",
      "Epoch [1453/10000], Loss: 0.8351775407791138\n",
      "Epoch [1454/10000], Loss: 0.8350494503974915\n",
      "Epoch [1455/10000], Loss: 0.8349214792251587\n",
      "Epoch [1456/10000], Loss: 0.8347935676574707\n",
      "Epoch [1457/10000], Loss: 0.834665834903717\n",
      "Epoch [1458/10000], Loss: 0.8345381617546082\n",
      "Epoch [1459/10000], Loss: 0.8344106078147888\n",
      "Epoch [1460/10000], Loss: 0.8342831134796143\n",
      "Epoch [1461/10000], Loss: 0.834155797958374\n",
      "Epoch [1462/10000], Loss: 0.8340286016464233\n",
      "Epoch [1463/10000], Loss: 0.8339015245437622\n",
      "Epoch [1464/10000], Loss: 0.8337744474411011\n",
      "Epoch [1465/10000], Loss: 0.8336474895477295\n",
      "Epoch [1466/10000], Loss: 0.8335206508636475\n",
      "Epoch [1467/10000], Loss: 0.833393931388855\n",
      "Epoch [1468/10000], Loss: 0.833267331123352\n",
      "Epoch [1469/10000], Loss: 0.8331408500671387\n",
      "Epoch [1470/10000], Loss: 0.8330145478248596\n",
      "Epoch [1471/10000], Loss: 0.8328883647918701\n",
      "Epoch [1472/10000], Loss: 0.8327622413635254\n",
      "Epoch [1473/10000], Loss: 0.8326363563537598\n",
      "Epoch [1474/10000], Loss: 0.8325105905532837\n",
      "Epoch [1475/10000], Loss: 0.8323849439620972\n",
      "Epoch [1476/10000], Loss: 0.832259476184845\n",
      "Epoch [1477/10000], Loss: 0.8321341276168823\n",
      "Epoch [1478/10000], Loss: 0.8320088386535645\n",
      "Epoch [1479/10000], Loss: 0.8318836688995361\n",
      "Epoch [1480/10000], Loss: 0.8317586183547974\n",
      "Epoch [1481/10000], Loss: 0.8316338062286377\n",
      "Epoch [1482/10000], Loss: 0.8315090537071228\n",
      "Epoch [1483/10000], Loss: 0.8313844799995422\n",
      "Epoch [1484/10000], Loss: 0.8312600255012512\n",
      "Epoch [1485/10000], Loss: 0.8311358094215393\n",
      "Epoch [1486/10000], Loss: 0.8310117721557617\n",
      "Epoch [1487/10000], Loss: 0.8308877944946289\n",
      "Epoch [1488/10000], Loss: 0.8307640552520752\n",
      "Epoch [1489/10000], Loss: 0.830640435218811\n",
      "Epoch [1490/10000], Loss: 0.8305169343948364\n",
      "Epoch [1491/10000], Loss: 0.8303935527801514\n",
      "Epoch [1492/10000], Loss: 0.8302703499794006\n",
      "Epoch [1493/10000], Loss: 0.8301472663879395\n",
      "Epoch [1494/10000], Loss: 0.830024242401123\n",
      "Epoch [1495/10000], Loss: 0.8299013376235962\n",
      "Epoch [1496/10000], Loss: 0.8297786116600037\n",
      "Epoch [1497/10000], Loss: 0.8296560049057007\n",
      "Epoch [1498/10000], Loss: 0.8295334577560425\n",
      "Epoch [1499/10000], Loss: 0.8294110298156738\n",
      "Epoch [1500/10000], Loss: 0.8292887210845947\n",
      "Epoch [1501/10000], Loss: 0.82916659116745\n",
      "Epoch [1502/10000], Loss: 0.8290445804595947\n",
      "Epoch [1503/10000], Loss: 0.8289227485656738\n",
      "Epoch [1504/10000], Loss: 0.8288009166717529\n",
      "Epoch [1505/10000], Loss: 0.8286793231964111\n",
      "Epoch [1506/10000], Loss: 0.8285578489303589\n",
      "Epoch [1507/10000], Loss: 0.8284364938735962\n",
      "Epoch [1508/10000], Loss: 0.828315258026123\n",
      "Epoch [1509/10000], Loss: 0.8281941413879395\n",
      "Epoch [1510/10000], Loss: 0.8280731439590454\n",
      "Epoch [1511/10000], Loss: 0.8279523849487305\n",
      "Epoch [1512/10000], Loss: 0.8278316259384155\n",
      "Epoch [1513/10000], Loss: 0.8277111053466797\n",
      "Epoch [1514/10000], Loss: 0.8275905847549438\n",
      "Epoch [1515/10000], Loss: 0.8274703025817871\n",
      "Epoch [1516/10000], Loss: 0.8273500204086304\n",
      "Epoch [1517/10000], Loss: 0.8272299766540527\n",
      "Epoch [1518/10000], Loss: 0.8271099925041199\n",
      "Epoch [1519/10000], Loss: 0.8269901275634766\n",
      "Epoch [1520/10000], Loss: 0.8268703818321228\n",
      "Epoch [1521/10000], Loss: 0.8267508149147034\n",
      "Epoch [1522/10000], Loss: 0.8266313076019287\n",
      "Epoch [1523/10000], Loss: 0.8265119791030884\n",
      "Epoch [1524/10000], Loss: 0.8263927698135376\n",
      "Epoch [1525/10000], Loss: 0.8262736797332764\n",
      "Epoch [1526/10000], Loss: 0.8261546492576599\n",
      "Epoch [1527/10000], Loss: 0.8260357975959778\n",
      "Epoch [1528/10000], Loss: 0.8259170055389404\n",
      "Epoch [1529/10000], Loss: 0.8257983922958374\n",
      "Epoch [1530/10000], Loss: 0.8256798982620239\n",
      "Epoch [1531/10000], Loss: 0.8255614638328552\n",
      "Epoch [1532/10000], Loss: 0.8254431486129761\n",
      "Epoch [1533/10000], Loss: 0.825325071811676\n",
      "Epoch [1534/10000], Loss: 0.825206995010376\n",
      "Epoch [1535/10000], Loss: 0.825089156627655\n",
      "Epoch [1536/10000], Loss: 0.8249713182449341\n",
      "Epoch [1537/10000], Loss: 0.8248536586761475\n",
      "Epoch [1538/10000], Loss: 0.8247361183166504\n",
      "Epoch [1539/10000], Loss: 0.8246186971664429\n",
      "Epoch [1540/10000], Loss: 0.8245013356208801\n",
      "Epoch [1541/10000], Loss: 0.8243840932846069\n",
      "Epoch [1542/10000], Loss: 0.8242670297622681\n",
      "Epoch [1543/10000], Loss: 0.8241500854492188\n",
      "Epoch [1544/10000], Loss: 0.824033260345459\n",
      "Epoch [1545/10000], Loss: 0.8239164352416992\n",
      "Epoch [1546/10000], Loss: 0.8237998485565186\n",
      "Epoch [1547/10000], Loss: 0.8236833810806274\n",
      "Epoch [1548/10000], Loss: 0.8235670328140259\n",
      "Epoch [1549/10000], Loss: 0.8234508037567139\n",
      "Epoch [1550/10000], Loss: 0.8233346343040466\n",
      "Epoch [1551/10000], Loss: 0.823218584060669\n",
      "Epoch [1552/10000], Loss: 0.8231027126312256\n",
      "Epoch [1553/10000], Loss: 0.8229868412017822\n",
      "Epoch [1554/10000], Loss: 0.822871208190918\n",
      "Epoch [1555/10000], Loss: 0.8227555751800537\n",
      "Epoch [1556/10000], Loss: 0.8226401805877686\n",
      "Epoch [1557/10000], Loss: 0.8225247859954834\n",
      "Epoch [1558/10000], Loss: 0.8224095106124878\n",
      "Epoch [1559/10000], Loss: 0.8222944736480713\n",
      "Epoch [1560/10000], Loss: 0.8221794366836548\n",
      "Epoch [1561/10000], Loss: 0.8220645189285278\n",
      "Epoch [1562/10000], Loss: 0.8219497203826904\n",
      "Epoch [1563/10000], Loss: 0.8218351006507874\n",
      "Epoch [1564/10000], Loss: 0.8217204809188843\n",
      "Epoch [1565/10000], Loss: 0.8216060996055603\n",
      "Epoch [1566/10000], Loss: 0.8214917182922363\n",
      "Epoch [1567/10000], Loss: 0.8213775157928467\n",
      "Epoch [1568/10000], Loss: 0.8212634325027466\n",
      "Epoch [1569/10000], Loss: 0.8211494088172913\n",
      "Epoch [1570/10000], Loss: 0.8210355043411255\n",
      "Epoch [1571/10000], Loss: 0.820921778678894\n",
      "Epoch [1572/10000], Loss: 0.8208080530166626\n",
      "Epoch [1573/10000], Loss: 0.8206944465637207\n",
      "Epoch [1574/10000], Loss: 0.8205810785293579\n",
      "Epoch [1575/10000], Loss: 0.8204677104949951\n",
      "Epoch [1576/10000], Loss: 0.8203544616699219\n",
      "Epoch [1577/10000], Loss: 0.820241391658783\n",
      "Epoch [1578/10000], Loss: 0.8201283812522888\n",
      "Epoch [1579/10000], Loss: 0.8200154900550842\n",
      "Epoch [1580/10000], Loss: 0.8199026584625244\n",
      "Epoch [1581/10000], Loss: 0.8197900056838989\n",
      "Epoch [1582/10000], Loss: 0.819677472114563\n",
      "Epoch [1583/10000], Loss: 0.819564938545227\n",
      "Epoch [1584/10000], Loss: 0.8194525837898254\n",
      "Epoch [1585/10000], Loss: 0.8193402886390686\n",
      "Epoch [1586/10000], Loss: 0.8192281723022461\n",
      "Epoch [1587/10000], Loss: 0.8191161155700684\n",
      "Epoch [1588/10000], Loss: 0.8190041184425354\n",
      "Epoch [1589/10000], Loss: 0.8188923001289368\n",
      "Epoch [1590/10000], Loss: 0.8187805414199829\n",
      "Epoch [1591/10000], Loss: 0.8186688423156738\n",
      "Epoch [1592/10000], Loss: 0.8185573220252991\n",
      "Epoch [1593/10000], Loss: 0.8184458613395691\n",
      "Epoch [1594/10000], Loss: 0.8183345198631287\n",
      "Epoch [1595/10000], Loss: 0.818223237991333\n",
      "Epoch [1596/10000], Loss: 0.8181121349334717\n",
      "Epoch [1597/10000], Loss: 0.8180010914802551\n",
      "Epoch [1598/10000], Loss: 0.8178901672363281\n",
      "Epoch [1599/10000], Loss: 0.8177793025970459\n",
      "Epoch [1600/10000], Loss: 0.8176684975624084\n",
      "Epoch [1601/10000], Loss: 0.8175578117370605\n",
      "Epoch [1602/10000], Loss: 0.817447304725647\n",
      "Epoch [1603/10000], Loss: 0.8173367977142334\n",
      "Epoch [1604/10000], Loss: 0.8172264695167542\n",
      "Epoch [1605/10000], Loss: 0.8171162009239197\n",
      "Epoch [1606/10000], Loss: 0.8170060515403748\n",
      "Epoch [1607/10000], Loss: 0.8168959617614746\n",
      "Epoch [1608/10000], Loss: 0.816785991191864\n",
      "Epoch [1609/10000], Loss: 0.816676139831543\n",
      "Epoch [1610/10000], Loss: 0.8165663480758667\n",
      "Epoch [1611/10000], Loss: 0.81645667552948\n",
      "Epoch [1612/10000], Loss: 0.8163471221923828\n",
      "Epoch [1613/10000], Loss: 0.8162376880645752\n",
      "Epoch [1614/10000], Loss: 0.8161282539367676\n",
      "Epoch [1615/10000], Loss: 0.8160189986228943\n",
      "Epoch [1616/10000], Loss: 0.8159098029136658\n",
      "Epoch [1617/10000], Loss: 0.8158007264137268\n",
      "Epoch [1618/10000], Loss: 0.8156917095184326\n",
      "Epoch [1619/10000], Loss: 0.815582811832428\n",
      "Epoch [1620/10000], Loss: 0.8154740333557129\n",
      "Epoch [1621/10000], Loss: 0.8153653144836426\n",
      "Epoch [1622/10000], Loss: 0.8152565956115723\n",
      "Epoch [1623/10000], Loss: 0.815148115158081\n",
      "Epoch [1624/10000], Loss: 0.8150396347045898\n",
      "Epoch [1625/10000], Loss: 0.8149312734603882\n",
      "Epoch [1626/10000], Loss: 0.8148229718208313\n",
      "Epoch [1627/10000], Loss: 0.814714789390564\n",
      "Epoch [1628/10000], Loss: 0.8146066665649414\n",
      "Epoch [1629/10000], Loss: 0.8144986629486084\n",
      "Epoch [1630/10000], Loss: 0.8143907785415649\n",
      "Epoch [1631/10000], Loss: 0.8142828941345215\n",
      "Epoch [1632/10000], Loss: 0.8141751289367676\n",
      "Epoch [1633/10000], Loss: 0.8140674829483032\n",
      "Epoch [1634/10000], Loss: 0.8139598965644836\n",
      "Epoch [1635/10000], Loss: 0.8138524293899536\n",
      "Epoch [1636/10000], Loss: 0.8137449622154236\n",
      "Epoch [1637/10000], Loss: 0.8136376142501831\n",
      "Epoch [1638/10000], Loss: 0.8135303854942322\n",
      "Epoch [1639/10000], Loss: 0.813423216342926\n",
      "Epoch [1640/10000], Loss: 0.8133161067962646\n",
      "Epoch [1641/10000], Loss: 0.813209056854248\n",
      "Epoch [1642/10000], Loss: 0.813102126121521\n",
      "Epoch [1643/10000], Loss: 0.8129953145980835\n",
      "Epoch [1644/10000], Loss: 0.8128885626792908\n",
      "Epoch [1645/10000], Loss: 0.812781810760498\n",
      "Epoch [1646/10000], Loss: 0.8126752376556396\n",
      "Epoch [1647/10000], Loss: 0.8125686645507812\n",
      "Epoch [1648/10000], Loss: 0.8124622106552124\n",
      "Epoch [1649/10000], Loss: 0.8123558759689331\n",
      "Epoch [1650/10000], Loss: 0.8122495412826538\n",
      "Epoch [1651/10000], Loss: 0.8121432662010193\n",
      "Epoch [1652/10000], Loss: 0.8120371103286743\n",
      "Epoch [1653/10000], Loss: 0.8119310140609741\n",
      "Epoch [1654/10000], Loss: 0.8118249773979187\n",
      "Epoch [1655/10000], Loss: 0.8117190003395081\n",
      "Epoch [1656/10000], Loss: 0.8116130828857422\n",
      "Epoch [1657/10000], Loss: 0.8115072846412659\n",
      "Epoch [1658/10000], Loss: 0.8114016056060791\n",
      "Epoch [1659/10000], Loss: 0.8112958669662476\n",
      "Epoch [1660/10000], Loss: 0.8111903071403503\n",
      "Epoch [1661/10000], Loss: 0.8110847473144531\n",
      "Epoch [1662/10000], Loss: 0.8109793066978455\n",
      "Epoch [1663/10000], Loss: 0.8108739256858826\n",
      "Epoch [1664/10000], Loss: 0.8107686042785645\n",
      "Epoch [1665/10000], Loss: 0.8106633424758911\n",
      "Epoch [1666/10000], Loss: 0.8105581998825073\n",
      "Epoch [1667/10000], Loss: 0.8104530572891235\n",
      "Epoch [1668/10000], Loss: 0.8103480339050293\n",
      "Epoch [1669/10000], Loss: 0.8102431297302246\n",
      "Epoch [1670/10000], Loss: 0.8101382255554199\n",
      "Epoch [1671/10000], Loss: 0.81003338098526\n",
      "Epoch [1672/10000], Loss: 0.8099285960197449\n",
      "Epoch [1673/10000], Loss: 0.8098238706588745\n",
      "Epoch [1674/10000], Loss: 0.8097192049026489\n",
      "Epoch [1675/10000], Loss: 0.8096146583557129\n",
      "Epoch [1676/10000], Loss: 0.8095101714134216\n",
      "Epoch [1677/10000], Loss: 0.8094057440757751\n",
      "Epoch [1678/10000], Loss: 0.8093013167381287\n",
      "Epoch [1679/10000], Loss: 0.8091970682144165\n",
      "Epoch [1680/10000], Loss: 0.8090927600860596\n",
      "Epoch [1681/10000], Loss: 0.8089885711669922\n",
      "Epoch [1682/10000], Loss: 0.8088844418525696\n",
      "Epoch [1683/10000], Loss: 0.8087803721427917\n",
      "Epoch [1684/10000], Loss: 0.8086763620376587\n",
      "Epoch [1685/10000], Loss: 0.8085724115371704\n",
      "Epoch [1686/10000], Loss: 0.8084684610366821\n",
      "Epoch [1687/10000], Loss: 0.8083645701408386\n",
      "Epoch [1688/10000], Loss: 0.8082607388496399\n",
      "Epoch [1689/10000], Loss: 0.8081569671630859\n",
      "Epoch [1690/10000], Loss: 0.8080532550811768\n",
      "Epoch [1691/10000], Loss: 0.8079495429992676\n",
      "Epoch [1692/10000], Loss: 0.807845950126648\n",
      "Epoch [1693/10000], Loss: 0.8077424168586731\n",
      "Epoch [1694/10000], Loss: 0.8076388835906982\n",
      "Epoch [1695/10000], Loss: 0.8075354099273682\n",
      "Epoch [1696/10000], Loss: 0.8074320554733276\n",
      "Epoch [1697/10000], Loss: 0.8073287010192871\n",
      "Epoch [1698/10000], Loss: 0.8072253465652466\n",
      "Epoch [1699/10000], Loss: 0.8071221113204956\n",
      "Epoch [1700/10000], Loss: 0.8070188760757446\n",
      "Epoch [1701/10000], Loss: 0.8069157004356384\n",
      "Epoch [1702/10000], Loss: 0.8068125247955322\n",
      "Epoch [1703/10000], Loss: 0.8067094683647156\n",
      "Epoch [1704/10000], Loss: 0.8066064119338989\n",
      "Epoch [1705/10000], Loss: 0.806503415107727\n",
      "Epoch [1706/10000], Loss: 0.8064004182815552\n",
      "Epoch [1707/10000], Loss: 0.8062975406646729\n",
      "Epoch [1708/10000], Loss: 0.8061946034431458\n",
      "Epoch [1709/10000], Loss: 0.8060917854309082\n",
      "Epoch [1710/10000], Loss: 0.8059889078140259\n",
      "Epoch [1711/10000], Loss: 0.8058861494064331\n",
      "Epoch [1712/10000], Loss: 0.8057833909988403\n",
      "Epoch [1713/10000], Loss: 0.8056806921958923\n",
      "Epoch [1714/10000], Loss: 0.8055779933929443\n",
      "Epoch [1715/10000], Loss: 0.8054753541946411\n",
      "Epoch [1716/10000], Loss: 0.8053727746009827\n",
      "Epoch [1717/10000], Loss: 0.8052701950073242\n",
      "Epoch [1718/10000], Loss: 0.8051676750183105\n",
      "Epoch [1719/10000], Loss: 0.8050652146339417\n",
      "Epoch [1720/10000], Loss: 0.8049627542495728\n",
      "Epoch [1721/10000], Loss: 0.8048602938652039\n",
      "Epoch [1722/10000], Loss: 0.8047578930854797\n",
      "Epoch [1723/10000], Loss: 0.8046554923057556\n",
      "Epoch [1724/10000], Loss: 0.8045531511306763\n",
      "Epoch [1725/10000], Loss: 0.8044508099555969\n",
      "Epoch [1726/10000], Loss: 0.8043485879898071\n",
      "Epoch [1727/10000], Loss: 0.8042463064193726\n",
      "Epoch [1728/10000], Loss: 0.804144024848938\n",
      "Epoch [1729/10000], Loss: 0.8040418028831482\n",
      "Epoch [1730/10000], Loss: 0.8039395809173584\n",
      "Epoch [1731/10000], Loss: 0.8038374185562134\n",
      "Epoch [1732/10000], Loss: 0.8037352561950684\n",
      "Epoch [1733/10000], Loss: 0.8036332130432129\n",
      "Epoch [1734/10000], Loss: 0.8035310506820679\n",
      "Epoch [1735/10000], Loss: 0.8034290075302124\n",
      "Epoch [1736/10000], Loss: 0.8033269643783569\n",
      "Epoch [1737/10000], Loss: 0.8032249212265015\n",
      "Epoch [1738/10000], Loss: 0.803122878074646\n",
      "Epoch [1739/10000], Loss: 0.8030208349227905\n",
      "Epoch [1740/10000], Loss: 0.8029188513755798\n",
      "Epoch [1741/10000], Loss: 0.8028168678283691\n",
      "Epoch [1742/10000], Loss: 0.8027148842811584\n",
      "Epoch [1743/10000], Loss: 0.8026129007339478\n",
      "Epoch [1744/10000], Loss: 0.8025109767913818\n",
      "Epoch [1745/10000], Loss: 0.8024090528488159\n",
      "Epoch [1746/10000], Loss: 0.80230712890625\n",
      "Epoch [1747/10000], Loss: 0.8022052049636841\n",
      "Epoch [1748/10000], Loss: 0.8021032810211182\n",
      "Epoch [1749/10000], Loss: 0.802001416683197\n",
      "Epoch [1750/10000], Loss: 0.8018995523452759\n",
      "Epoch [1751/10000], Loss: 0.80179762840271\n",
      "Epoch [1752/10000], Loss: 0.8016958236694336\n",
      "Epoch [1753/10000], Loss: 0.8015938997268677\n",
      "Epoch [1754/10000], Loss: 0.8014920949935913\n",
      "Epoch [1755/10000], Loss: 0.8013901710510254\n",
      "Epoch [1756/10000], Loss: 0.801288366317749\n",
      "Epoch [1757/10000], Loss: 0.8011865615844727\n",
      "Epoch [1758/10000], Loss: 0.8010846972465515\n",
      "Epoch [1759/10000], Loss: 0.8009828329086304\n",
      "Epoch [1760/10000], Loss: 0.800881028175354\n",
      "Epoch [1761/10000], Loss: 0.8007792234420776\n",
      "Epoch [1762/10000], Loss: 0.8006773591041565\n",
      "Epoch [1763/10000], Loss: 0.8005755543708801\n",
      "Epoch [1764/10000], Loss: 0.800473690032959\n",
      "Epoch [1765/10000], Loss: 0.8003718852996826\n",
      "Epoch [1766/10000], Loss: 0.8002700209617615\n",
      "Epoch [1767/10000], Loss: 0.8001682162284851\n",
      "Epoch [1768/10000], Loss: 0.800066351890564\n",
      "Epoch [1769/10000], Loss: 0.7999644875526428\n",
      "Epoch [1770/10000], Loss: 0.7998626232147217\n",
      "Epoch [1771/10000], Loss: 0.7997608184814453\n",
      "Epoch [1772/10000], Loss: 0.7996588945388794\n",
      "Epoch [1773/10000], Loss: 0.799557089805603\n",
      "Epoch [1774/10000], Loss: 0.7994551658630371\n",
      "Epoch [1775/10000], Loss: 0.7993532419204712\n",
      "Epoch [1776/10000], Loss: 0.7992513179779053\n",
      "Epoch [1777/10000], Loss: 0.7991494536399841\n",
      "Epoch [1778/10000], Loss: 0.799047589302063\n",
      "Epoch [1779/10000], Loss: 0.7989456653594971\n",
      "Epoch [1780/10000], Loss: 0.7988437414169312\n",
      "Epoch [1781/10000], Loss: 0.7987418174743652\n",
      "Epoch [1782/10000], Loss: 0.7986398339271545\n",
      "Epoch [1783/10000], Loss: 0.7985378503799438\n",
      "Epoch [1784/10000], Loss: 0.7984358668327332\n",
      "Epoch [1785/10000], Loss: 0.7983338832855225\n",
      "Epoch [1786/10000], Loss: 0.798231840133667\n",
      "Epoch [1787/10000], Loss: 0.7981298565864563\n",
      "Epoch [1788/10000], Loss: 0.798027753829956\n",
      "Epoch [1789/10000], Loss: 0.7979257106781006\n",
      "Epoch [1790/10000], Loss: 0.7978236675262451\n",
      "Epoch [1791/10000], Loss: 0.7977215051651001\n",
      "Epoch [1792/10000], Loss: 0.7976194024085999\n",
      "Epoch [1793/10000], Loss: 0.7975172996520996\n",
      "Epoch [1794/10000], Loss: 0.7974151372909546\n",
      "Epoch [1795/10000], Loss: 0.79731285572052\n",
      "Epoch [1796/10000], Loss: 0.797210693359375\n",
      "Epoch [1797/10000], Loss: 0.7971084117889404\n",
      "Epoch [1798/10000], Loss: 0.7970061898231506\n",
      "Epoch [1799/10000], Loss: 0.7969038486480713\n",
      "Epoch [1800/10000], Loss: 0.7968015670776367\n",
      "Epoch [1801/10000], Loss: 0.7966991662979126\n",
      "Epoch [1802/10000], Loss: 0.7965967655181885\n",
      "Epoch [1803/10000], Loss: 0.7964943647384644\n",
      "Epoch [1804/10000], Loss: 0.7963919639587402\n",
      "Epoch [1805/10000], Loss: 0.7962894439697266\n",
      "Epoch [1806/10000], Loss: 0.7961869239807129\n",
      "Epoch [1807/10000], Loss: 0.7960844039916992\n",
      "Epoch [1808/10000], Loss: 0.7959818243980408\n",
      "Epoch [1809/10000], Loss: 0.7958791851997375\n",
      "Epoch [1810/10000], Loss: 0.7957765460014343\n",
      "Epoch [1811/10000], Loss: 0.7956738471984863\n",
      "Epoch [1812/10000], Loss: 0.7955710887908936\n",
      "Epoch [1813/10000], Loss: 0.7954683303833008\n",
      "Epoch [1814/10000], Loss: 0.795365571975708\n",
      "Epoch [1815/10000], Loss: 0.7952626943588257\n",
      "Epoch [1816/10000], Loss: 0.7951598167419434\n",
      "Epoch [1817/10000], Loss: 0.795056939125061\n",
      "Epoch [1818/10000], Loss: 0.7949539422988892\n",
      "Epoch [1819/10000], Loss: 0.7948509454727173\n",
      "Epoch [1820/10000], Loss: 0.7947478890419006\n",
      "Epoch [1821/10000], Loss: 0.794644832611084\n",
      "Epoch [1822/10000], Loss: 0.7945417165756226\n",
      "Epoch [1823/10000], Loss: 0.7944384813308716\n",
      "Epoch [1824/10000], Loss: 0.7943353056907654\n",
      "Epoch [1825/10000], Loss: 0.7942320108413696\n",
      "Epoch [1826/10000], Loss: 0.7941286563873291\n",
      "Epoch [1827/10000], Loss: 0.7940253019332886\n",
      "Epoch [1828/10000], Loss: 0.7939218878746033\n",
      "Epoch [1829/10000], Loss: 0.7938184142112732\n",
      "Epoch [1830/10000], Loss: 0.7937148809432983\n",
      "Epoch [1831/10000], Loss: 0.7936113476753235\n",
      "Epoch [1832/10000], Loss: 0.7935076951980591\n",
      "Epoch [1833/10000], Loss: 0.7934041023254395\n",
      "Epoch [1834/10000], Loss: 0.7933003306388855\n",
      "Epoch [1835/10000], Loss: 0.7931965589523315\n",
      "Epoch [1836/10000], Loss: 0.7930927276611328\n",
      "Epoch [1837/10000], Loss: 0.7929887771606445\n",
      "Epoch [1838/10000], Loss: 0.7928848266601562\n",
      "Epoch [1839/10000], Loss: 0.7927808165550232\n",
      "Epoch [1840/10000], Loss: 0.7926767468452454\n",
      "Epoch [1841/10000], Loss: 0.7925726175308228\n",
      "Epoch [1842/10000], Loss: 0.7924684286117554\n",
      "Epoch [1843/10000], Loss: 0.7923641204833984\n",
      "Epoch [1844/10000], Loss: 0.7922598123550415\n",
      "Epoch [1845/10000], Loss: 0.792155385017395\n",
      "Epoch [1846/10000], Loss: 0.7920508980751038\n",
      "Epoch [1847/10000], Loss: 0.7919464111328125\n",
      "Epoch [1848/10000], Loss: 0.7918417453765869\n",
      "Epoch [1849/10000], Loss: 0.7917370796203613\n",
      "Epoch [1850/10000], Loss: 0.7916324138641357\n",
      "Epoch [1851/10000], Loss: 0.7915275692939758\n",
      "Epoch [1852/10000], Loss: 0.7914226651191711\n",
      "Epoch [1853/10000], Loss: 0.7913177609443665\n",
      "Epoch [1854/10000], Loss: 0.7912127375602722\n",
      "Epoch [1855/10000], Loss: 0.7911076545715332\n",
      "Epoch [1856/10000], Loss: 0.7910025119781494\n",
      "Epoch [1857/10000], Loss: 0.7908972501754761\n",
      "Epoch [1858/10000], Loss: 0.790791928768158\n",
      "Epoch [1859/10000], Loss: 0.7906866073608398\n",
      "Epoch [1860/10000], Loss: 0.7905811071395874\n",
      "Epoch [1861/10000], Loss: 0.790475606918335\n",
      "Epoch [1862/10000], Loss: 0.790369987487793\n",
      "Epoch [1863/10000], Loss: 0.7902643084526062\n",
      "Epoch [1864/10000], Loss: 0.7901585698127747\n",
      "Epoch [1865/10000], Loss: 0.7900527119636536\n",
      "Epoch [1866/10000], Loss: 0.7899468541145325\n",
      "Epoch [1867/10000], Loss: 0.7898408770561218\n",
      "Epoch [1868/10000], Loss: 0.7897348403930664\n",
      "Epoch [1869/10000], Loss: 0.7896286845207214\n",
      "Epoch [1870/10000], Loss: 0.7895224094390869\n",
      "Epoch [1871/10000], Loss: 0.7894161343574524\n",
      "Epoch [1872/10000], Loss: 0.7893097400665283\n",
      "Epoch [1873/10000], Loss: 0.7892032861709595\n",
      "Epoch [1874/10000], Loss: 0.7890967130661011\n",
      "Epoch [1875/10000], Loss: 0.7889900803565979\n",
      "Epoch [1876/10000], Loss: 0.7888833284378052\n",
      "Epoch [1877/10000], Loss: 0.7887765169143677\n",
      "Epoch [1878/10000], Loss: 0.7886695861816406\n",
      "Epoch [1879/10000], Loss: 0.7885625958442688\n",
      "Epoch [1880/10000], Loss: 0.7884555459022522\n",
      "Epoch [1881/10000], Loss: 0.788348376750946\n",
      "Epoch [1882/10000], Loss: 0.7882411479949951\n",
      "Epoch [1883/10000], Loss: 0.7881338000297546\n",
      "Epoch [1884/10000], Loss: 0.7880263328552246\n",
      "Epoch [1885/10000], Loss: 0.7879188656806946\n",
      "Epoch [1886/10000], Loss: 0.787811279296875\n",
      "Epoch [1887/10000], Loss: 0.7877036333084106\n",
      "Epoch [1888/10000], Loss: 0.787595808506012\n",
      "Epoch [1889/10000], Loss: 0.7874879837036133\n",
      "Epoch [1890/10000], Loss: 0.7873799800872803\n",
      "Epoch [1891/10000], Loss: 0.7872719168663025\n",
      "Epoch [1892/10000], Loss: 0.7871637344360352\n",
      "Epoch [1893/10000], Loss: 0.7870555520057678\n",
      "Epoch [1894/10000], Loss: 0.7869471907615662\n",
      "Epoch [1895/10000], Loss: 0.7868387699127197\n",
      "Epoch [1896/10000], Loss: 0.786730170249939\n",
      "Epoch [1897/10000], Loss: 0.7866215705871582\n",
      "Epoch [1898/10000], Loss: 0.7865128517150879\n",
      "Epoch [1899/10000], Loss: 0.7864039540290833\n",
      "Epoch [1900/10000], Loss: 0.7862949967384338\n",
      "Epoch [1901/10000], Loss: 0.7861859798431396\n",
      "Epoch [1902/10000], Loss: 0.7860767841339111\n",
      "Epoch [1903/10000], Loss: 0.7859674692153931\n",
      "Epoch [1904/10000], Loss: 0.7858580946922302\n",
      "Epoch [1905/10000], Loss: 0.7857486009597778\n",
      "Epoch [1906/10000], Loss: 0.7856390476226807\n",
      "Epoch [1907/10000], Loss: 0.785529375076294\n",
      "Epoch [1908/10000], Loss: 0.7854195833206177\n",
      "Epoch [1909/10000], Loss: 0.7853096723556519\n",
      "Epoch [1910/10000], Loss: 0.7851996421813965\n",
      "Epoch [1911/10000], Loss: 0.7850894927978516\n",
      "Epoch [1912/10000], Loss: 0.7849792242050171\n",
      "Epoch [1913/10000], Loss: 0.7848688960075378\n",
      "Epoch [1914/10000], Loss: 0.7847583889961243\n",
      "Epoch [1915/10000], Loss: 0.7846478223800659\n",
      "Epoch [1916/10000], Loss: 0.784537136554718\n",
      "Epoch [1917/10000], Loss: 0.7844263315200806\n",
      "Epoch [1918/10000], Loss: 0.7843154072761536\n",
      "Epoch [1919/10000], Loss: 0.784204363822937\n",
      "Epoch [1920/10000], Loss: 0.7840932011604309\n",
      "Epoch [1921/10000], Loss: 0.7839819192886353\n",
      "Epoch [1922/10000], Loss: 0.78387051820755\n",
      "Epoch [1923/10000], Loss: 0.7837589979171753\n",
      "Epoch [1924/10000], Loss: 0.7836474180221558\n",
      "Epoch [1925/10000], Loss: 0.7835355997085571\n",
      "Epoch [1926/10000], Loss: 0.7834237813949585\n",
      "Epoch [1927/10000], Loss: 0.7833118438720703\n",
      "Epoch [1928/10000], Loss: 0.7831997275352478\n",
      "Epoch [1929/10000], Loss: 0.7830875515937805\n",
      "Epoch [1930/10000], Loss: 0.7829751968383789\n",
      "Epoch [1931/10000], Loss: 0.7828627824783325\n",
      "Epoch [1932/10000], Loss: 0.7827502489089966\n",
      "Epoch [1933/10000], Loss: 0.7826375365257263\n",
      "Epoch [1934/10000], Loss: 0.7825247645378113\n",
      "Epoch [1935/10000], Loss: 0.7824118137359619\n",
      "Epoch [1936/10000], Loss: 0.782298743724823\n",
      "Epoch [1937/10000], Loss: 0.7821855545043945\n",
      "Epoch [1938/10000], Loss: 0.7820722460746765\n",
      "Epoch [1939/10000], Loss: 0.781958818435669\n",
      "Epoch [1940/10000], Loss: 0.7818453311920166\n",
      "Epoch [1941/10000], Loss: 0.7817316055297852\n",
      "Epoch [1942/10000], Loss: 0.7816178202629089\n",
      "Epoch [1943/10000], Loss: 0.7815039157867432\n",
      "Epoch [1944/10000], Loss: 0.7813898324966431\n",
      "Epoch [1945/10000], Loss: 0.7812756896018982\n",
      "Epoch [1946/10000], Loss: 0.7811614274978638\n",
      "Epoch [1947/10000], Loss: 0.781046986579895\n",
      "Epoch [1948/10000], Loss: 0.7809324264526367\n",
      "Epoch [1949/10000], Loss: 0.7808177471160889\n",
      "Epoch [1950/10000], Loss: 0.7807029485702515\n",
      "Epoch [1951/10000], Loss: 0.7805880308151245\n",
      "Epoch [1952/10000], Loss: 0.7804729342460632\n",
      "Epoch [1953/10000], Loss: 0.7803577184677124\n",
      "Epoch [1954/10000], Loss: 0.7802424430847168\n",
      "Epoch [1955/10000], Loss: 0.7801269888877869\n",
      "Epoch [1956/10000], Loss: 0.7800114154815674\n",
      "Epoch [1957/10000], Loss: 0.7798956632614136\n",
      "Epoch [1958/10000], Loss: 0.7797797918319702\n",
      "Epoch [1959/10000], Loss: 0.7796638011932373\n",
      "Epoch [1960/10000], Loss: 0.7795476913452148\n",
      "Epoch [1961/10000], Loss: 0.7794314622879028\n",
      "Epoch [1962/10000], Loss: 0.7793150544166565\n",
      "Epoch [1963/10000], Loss: 0.7791985273361206\n",
      "Epoch [1964/10000], Loss: 0.7790818810462952\n",
      "Epoch [1965/10000], Loss: 0.7789651155471802\n",
      "Epoch [1966/10000], Loss: 0.7788481712341309\n",
      "Epoch [1967/10000], Loss: 0.778731107711792\n",
      "Epoch [1968/10000], Loss: 0.7786139249801636\n",
      "Epoch [1969/10000], Loss: 0.7784966230392456\n",
      "Epoch [1970/10000], Loss: 0.7783791422843933\n",
      "Epoch [1971/10000], Loss: 0.7782615423202515\n",
      "Epoch [1972/10000], Loss: 0.7781438827514648\n",
      "Epoch [1973/10000], Loss: 0.7780259847640991\n",
      "Epoch [1974/10000], Loss: 0.7779080271720886\n",
      "Epoch [1975/10000], Loss: 0.7777898907661438\n",
      "Epoch [1976/10000], Loss: 0.7776716351509094\n",
      "Epoch [1977/10000], Loss: 0.7775532007217407\n",
      "Epoch [1978/10000], Loss: 0.7774347066879272\n",
      "Epoch [1979/10000], Loss: 0.7773160338401794\n",
      "Epoch [1980/10000], Loss: 0.7771972417831421\n",
      "Epoch [1981/10000], Loss: 0.7770782709121704\n",
      "Epoch [1982/10000], Loss: 0.7769591212272644\n",
      "Epoch [1983/10000], Loss: 0.7768399119377136\n",
      "Epoch [1984/10000], Loss: 0.7767205238342285\n",
      "Epoch [1985/10000], Loss: 0.7766010165214539\n",
      "Epoch [1986/10000], Loss: 0.7764813899993896\n",
      "Epoch [1987/10000], Loss: 0.7763615846633911\n",
      "Epoch [1988/10000], Loss: 0.776241660118103\n",
      "Epoch [1989/10000], Loss: 0.7761216163635254\n",
      "Epoch [1990/10000], Loss: 0.7760013937950134\n",
      "Epoch [1991/10000], Loss: 0.7758810520172119\n",
      "Epoch [1992/10000], Loss: 0.7757605314254761\n",
      "Epoch [1993/10000], Loss: 0.7756399512290955\n",
      "Epoch [1994/10000], Loss: 0.7755191922187805\n",
      "Epoch [1995/10000], Loss: 0.7753982543945312\n",
      "Epoch [1996/10000], Loss: 0.7752772569656372\n",
      "Epoch [1997/10000], Loss: 0.7751560211181641\n",
      "Epoch [1998/10000], Loss: 0.7750347256660461\n",
      "Epoch [1999/10000], Loss: 0.7749131917953491\n",
      "Epoch [2000/10000], Loss: 0.7747915983200073\n",
      "Epoch [2001/10000], Loss: 0.7746698260307312\n",
      "Epoch [2002/10000], Loss: 0.7745479345321655\n",
      "Epoch [2003/10000], Loss: 0.7744258642196655\n",
      "Epoch [2004/10000], Loss: 0.774303674697876\n",
      "Epoch [2005/10000], Loss: 0.7741813659667969\n",
      "Epoch [2006/10000], Loss: 0.7740588784217834\n",
      "Epoch [2007/10000], Loss: 0.7739362716674805\n",
      "Epoch [2008/10000], Loss: 0.7738135457038879\n",
      "Epoch [2009/10000], Loss: 0.7736905813217163\n",
      "Epoch [2010/10000], Loss: 0.7735675573348999\n",
      "Epoch [2011/10000], Loss: 0.7734443545341492\n",
      "Epoch [2012/10000], Loss: 0.7733210325241089\n",
      "Epoch [2013/10000], Loss: 0.7731975317001343\n",
      "Epoch [2014/10000], Loss: 0.7730739116668701\n",
      "Epoch [2015/10000], Loss: 0.7729501128196716\n",
      "Epoch [2016/10000], Loss: 0.7728261947631836\n",
      "Epoch [2017/10000], Loss: 0.7727020978927612\n",
      "Epoch [2018/10000], Loss: 0.7725778818130493\n",
      "Epoch [2019/10000], Loss: 0.7724535465240479\n",
      "Epoch [2020/10000], Loss: 0.7723290920257568\n",
      "Epoch [2021/10000], Loss: 0.7722043991088867\n",
      "Epoch [2022/10000], Loss: 0.772079586982727\n",
      "Epoch [2023/10000], Loss: 0.7719546556472778\n",
      "Epoch [2024/10000], Loss: 0.7718296051025391\n",
      "Epoch [2025/10000], Loss: 0.7717043161392212\n",
      "Epoch [2026/10000], Loss: 0.7715790271759033\n",
      "Epoch [2027/10000], Loss: 0.7714534401893616\n",
      "Epoch [2028/10000], Loss: 0.771327793598175\n",
      "Epoch [2029/10000], Loss: 0.7712019681930542\n",
      "Epoch [2030/10000], Loss: 0.7710760235786438\n",
      "Epoch [2031/10000], Loss: 0.7709499597549438\n",
      "Epoch [2032/10000], Loss: 0.7708237171173096\n",
      "Epoch [2033/10000], Loss: 0.770697295665741\n",
      "Epoch [2034/10000], Loss: 0.7705708146095276\n",
      "Epoch [2035/10000], Loss: 0.7704441547393799\n",
      "Epoch [2036/10000], Loss: 0.7703173160552979\n",
      "Epoch [2037/10000], Loss: 0.7701903581619263\n",
      "Epoch [2038/10000], Loss: 0.7700632214546204\n",
      "Epoch [2039/10000], Loss: 0.7699359655380249\n",
      "Epoch [2040/10000], Loss: 0.7698085308074951\n",
      "Epoch [2041/10000], Loss: 0.7696809768676758\n",
      "Epoch [2042/10000], Loss: 0.7695533037185669\n",
      "Epoch [2043/10000], Loss: 0.7694254517555237\n",
      "Epoch [2044/10000], Loss: 0.7692974805831909\n",
      "Epoch [2045/10000], Loss: 0.7691693305969238\n",
      "Epoch [2046/10000], Loss: 0.7690410614013672\n",
      "Epoch [2047/10000], Loss: 0.7689126133918762\n",
      "Epoch [2048/10000], Loss: 0.7687840461730957\n",
      "Epoch [2049/10000], Loss: 0.7686553597450256\n",
      "Epoch [2050/10000], Loss: 0.7685264945030212\n",
      "Epoch [2051/10000], Loss: 0.7683974504470825\n",
      "Epoch [2052/10000], Loss: 0.7682682871818542\n",
      "Epoch [2053/10000], Loss: 0.7681390047073364\n",
      "Epoch [2054/10000], Loss: 0.7680095434188843\n",
      "Epoch [2055/10000], Loss: 0.7678799629211426\n",
      "Epoch [2056/10000], Loss: 0.7677502036094666\n",
      "Epoch [2057/10000], Loss: 0.767620325088501\n",
      "Epoch [2058/10000], Loss: 0.7674902677536011\n",
      "Epoch [2059/10000], Loss: 0.7673600912094116\n",
      "Epoch [2060/10000], Loss: 0.7672297954559326\n",
      "Epoch [2061/10000], Loss: 0.7670993804931641\n",
      "Epoch [2062/10000], Loss: 0.7669687867164612\n",
      "Epoch [2063/10000], Loss: 0.7668380737304688\n",
      "Epoch [2064/10000], Loss: 0.766707181930542\n",
      "Epoch [2065/10000], Loss: 0.7665761709213257\n",
      "Epoch [2066/10000], Loss: 0.766444981098175\n",
      "Epoch [2067/10000], Loss: 0.7663136720657349\n",
      "Epoch [2068/10000], Loss: 0.7661821842193604\n",
      "Epoch [2069/10000], Loss: 0.7660505771636963\n",
      "Epoch [2070/10000], Loss: 0.7659188508987427\n",
      "Epoch [2071/10000], Loss: 0.76578688621521\n",
      "Epoch [2072/10000], Loss: 0.7656548619270325\n",
      "Epoch [2073/10000], Loss: 0.7655226588249207\n",
      "Epoch [2074/10000], Loss: 0.7653903365135193\n",
      "Epoch [2075/10000], Loss: 0.7652578353881836\n",
      "Epoch [2076/10000], Loss: 0.7651252746582031\n",
      "Epoch [2077/10000], Loss: 0.7649924755096436\n",
      "Epoch [2078/10000], Loss: 0.7648595571517944\n",
      "Epoch [2079/10000], Loss: 0.7647265195846558\n",
      "Epoch [2080/10000], Loss: 0.7645933628082275\n",
      "Epoch [2081/10000], Loss: 0.7644599676132202\n",
      "Epoch [2082/10000], Loss: 0.7643265724182129\n",
      "Epoch [2083/10000], Loss: 0.7641929388046265\n",
      "Epoch [2084/10000], Loss: 0.7640591859817505\n",
      "Epoch [2085/10000], Loss: 0.7639252543449402\n",
      "Epoch [2086/10000], Loss: 0.7637912034988403\n",
      "Epoch [2087/10000], Loss: 0.7636569738388062\n",
      "Epoch [2088/10000], Loss: 0.7635226845741272\n",
      "Epoch [2089/10000], Loss: 0.7633882761001587\n",
      "Epoch [2090/10000], Loss: 0.7632536292076111\n",
      "Epoch [2091/10000], Loss: 0.7631188631057739\n",
      "Epoch [2092/10000], Loss: 0.7629839777946472\n",
      "Epoch [2093/10000], Loss: 0.762848973274231\n",
      "Epoch [2094/10000], Loss: 0.7627137303352356\n",
      "Epoch [2095/10000], Loss: 0.7625783681869507\n",
      "Epoch [2096/10000], Loss: 0.762442946434021\n",
      "Epoch [2097/10000], Loss: 0.7623072862625122\n",
      "Epoch [2098/10000], Loss: 0.7621715068817139\n",
      "Epoch [2099/10000], Loss: 0.762035608291626\n",
      "Epoch [2100/10000], Loss: 0.7618995904922485\n",
      "Epoch [2101/10000], Loss: 0.7617634534835815\n",
      "Epoch [2102/10000], Loss: 0.7616270780563354\n",
      "Epoch [2103/10000], Loss: 0.7614905834197998\n",
      "Epoch [2104/10000], Loss: 0.7613539695739746\n",
      "Epoch [2105/10000], Loss: 0.7612172365188599\n",
      "Epoch [2106/10000], Loss: 0.761080265045166\n",
      "Epoch [2107/10000], Loss: 0.7609432935714722\n",
      "Epoch [2108/10000], Loss: 0.7608060836791992\n",
      "Epoch [2109/10000], Loss: 0.7606687545776367\n",
      "Epoch [2110/10000], Loss: 0.7605312466621399\n",
      "Epoch [2111/10000], Loss: 0.7603936195373535\n",
      "Epoch [2112/10000], Loss: 0.7602558732032776\n",
      "Epoch [2113/10000], Loss: 0.7601180076599121\n",
      "Epoch [2114/10000], Loss: 0.7599799633026123\n",
      "Epoch [2115/10000], Loss: 0.759841799736023\n",
      "Epoch [2116/10000], Loss: 0.7597033977508545\n",
      "Epoch [2117/10000], Loss: 0.7595649361610413\n",
      "Epoch [2118/10000], Loss: 0.7594263553619385\n",
      "Epoch [2119/10000], Loss: 0.7592875361442566\n",
      "Epoch [2120/10000], Loss: 0.7591486573219299\n",
      "Epoch [2121/10000], Loss: 0.759009599685669\n",
      "Epoch [2122/10000], Loss: 0.7588704228401184\n",
      "Epoch [2123/10000], Loss: 0.7587310671806335\n",
      "Epoch [2124/10000], Loss: 0.7585916519165039\n",
      "Epoch [2125/10000], Loss: 0.7584520578384399\n",
      "Epoch [2126/10000], Loss: 0.7583123445510864\n",
      "Epoch [2127/10000], Loss: 0.7581723928451538\n",
      "Epoch [2128/10000], Loss: 0.7580324411392212\n",
      "Epoch [2129/10000], Loss: 0.7578922510147095\n",
      "Epoch [2130/10000], Loss: 0.7577519416809082\n",
      "Epoch [2131/10000], Loss: 0.7576115131378174\n",
      "Epoch [2132/10000], Loss: 0.757470965385437\n",
      "Epoch [2133/10000], Loss: 0.7573301792144775\n",
      "Epoch [2134/10000], Loss: 0.7571893334388733\n",
      "Epoch [2135/10000], Loss: 0.7570483684539795\n",
      "Epoch [2136/10000], Loss: 0.7569072246551514\n",
      "Epoch [2137/10000], Loss: 0.7567659616470337\n",
      "Epoch [2138/10000], Loss: 0.7566244602203369\n",
      "Epoch [2139/10000], Loss: 0.7564828991889954\n",
      "Epoch [2140/10000], Loss: 0.7563412189483643\n",
      "Epoch [2141/10000], Loss: 0.7561993598937988\n",
      "Epoch [2142/10000], Loss: 0.7560573220252991\n",
      "Epoch [2143/10000], Loss: 0.7559151649475098\n",
      "Epoch [2144/10000], Loss: 0.7557729482650757\n",
      "Epoch [2145/10000], Loss: 0.7556304931640625\n",
      "Epoch [2146/10000], Loss: 0.7554879188537598\n",
      "Epoch [2147/10000], Loss: 0.7553452253341675\n",
      "Epoch [2148/10000], Loss: 0.7552024126052856\n",
      "Epoch [2149/10000], Loss: 0.7550594806671143\n",
      "Epoch [2150/10000], Loss: 0.7549163103103638\n",
      "Epoch [2151/10000], Loss: 0.7547730803489685\n",
      "Epoch [2152/10000], Loss: 0.7546296119689941\n",
      "Epoch [2153/10000], Loss: 0.754486083984375\n",
      "Epoch [2154/10000], Loss: 0.7543424367904663\n",
      "Epoch [2155/10000], Loss: 0.7541985511779785\n",
      "Epoch [2156/10000], Loss: 0.7540545463562012\n",
      "Epoch [2157/10000], Loss: 0.753910481929779\n",
      "Epoch [2158/10000], Loss: 0.7537661790847778\n",
      "Epoch [2159/10000], Loss: 0.7536218166351318\n",
      "Epoch [2160/10000], Loss: 0.7534772157669067\n",
      "Epoch [2161/10000], Loss: 0.7533325552940369\n",
      "Epoch [2162/10000], Loss: 0.7531877756118774\n",
      "Epoch [2163/10000], Loss: 0.7530428171157837\n",
      "Epoch [2164/10000], Loss: 0.7528977394104004\n",
      "Epoch [2165/10000], Loss: 0.752752423286438\n",
      "Epoch [2166/10000], Loss: 0.7526070475578308\n",
      "Epoch [2167/10000], Loss: 0.7524615526199341\n",
      "Epoch [2168/10000], Loss: 0.752315878868103\n",
      "Epoch [2169/10000], Loss: 0.7521700859069824\n",
      "Epoch [2170/10000], Loss: 0.7520241737365723\n",
      "Epoch [2171/10000], Loss: 0.7518780827522278\n",
      "Epoch [2172/10000], Loss: 0.751731812953949\n",
      "Epoch [2173/10000], Loss: 0.7515854835510254\n",
      "Epoch [2174/10000], Loss: 0.7514389753341675\n",
      "Epoch [2175/10000], Loss: 0.7512922883033752\n",
      "Epoch [2176/10000], Loss: 0.7511454820632935\n",
      "Epoch [2177/10000], Loss: 0.7509984970092773\n",
      "Epoch [2178/10000], Loss: 0.7508514523506165\n",
      "Epoch [2179/10000], Loss: 0.7507042288780212\n",
      "Epoch [2180/10000], Loss: 0.7505568265914917\n",
      "Epoch [2181/10000], Loss: 0.7504093647003174\n",
      "Epoch [2182/10000], Loss: 0.7502617835998535\n",
      "Epoch [2183/10000], Loss: 0.7501139640808105\n",
      "Epoch [2184/10000], Loss: 0.749966025352478\n",
      "Epoch [2185/10000], Loss: 0.749817967414856\n",
      "Epoch [2186/10000], Loss: 0.7496697902679443\n",
      "Epoch [2187/10000], Loss: 0.7495214343070984\n",
      "Epoch [2188/10000], Loss: 0.7493729591369629\n",
      "Epoch [2189/10000], Loss: 0.7492243051528931\n",
      "Epoch [2190/10000], Loss: 0.7490755319595337\n",
      "Epoch [2191/10000], Loss: 0.7489266395568848\n",
      "Epoch [2192/10000], Loss: 0.7487776279449463\n",
      "Epoch [2193/10000], Loss: 0.7486283779144287\n",
      "Epoch [2194/10000], Loss: 0.7484790086746216\n",
      "Epoch [2195/10000], Loss: 0.7483295202255249\n",
      "Epoch [2196/10000], Loss: 0.7481799125671387\n",
      "Epoch [2197/10000], Loss: 0.7480300664901733\n",
      "Epoch [2198/10000], Loss: 0.7478801012039185\n",
      "Epoch [2199/10000], Loss: 0.7477300763130188\n",
      "Epoch [2200/10000], Loss: 0.74757981300354\n",
      "Epoch [2201/10000], Loss: 0.7474294900894165\n",
      "Epoch [2202/10000], Loss: 0.7472789287567139\n",
      "Epoch [2203/10000], Loss: 0.7471282482147217\n",
      "Epoch [2204/10000], Loss: 0.7469774484634399\n",
      "Epoch [2205/10000], Loss: 0.7468265295028687\n",
      "Epoch [2206/10000], Loss: 0.7466753721237183\n",
      "Epoch [2207/10000], Loss: 0.7465240955352783\n",
      "Epoch [2208/10000], Loss: 0.7463726997375488\n",
      "Epoch [2209/10000], Loss: 0.7462210655212402\n",
      "Epoch [2210/10000], Loss: 0.7460693120956421\n",
      "Epoch [2211/10000], Loss: 0.7459174394607544\n",
      "Epoch [2212/10000], Loss: 0.7457654476165771\n",
      "Epoch [2213/10000], Loss: 0.7456132173538208\n",
      "Epoch [2214/10000], Loss: 0.7454608678817749\n",
      "Epoch [2215/10000], Loss: 0.7453083992004395\n",
      "Epoch [2216/10000], Loss: 0.7451557517051697\n",
      "Epoch [2217/10000], Loss: 0.7450029850006104\n",
      "Epoch [2218/10000], Loss: 0.7448500394821167\n",
      "Epoch [2219/10000], Loss: 0.7446969151496887\n",
      "Epoch [2220/10000], Loss: 0.7445436716079712\n",
      "Epoch [2221/10000], Loss: 0.7443902492523193\n",
      "Epoch [2222/10000], Loss: 0.7442367076873779\n",
      "Epoch [2223/10000], Loss: 0.744083046913147\n",
      "Epoch [2224/10000], Loss: 0.7439291477203369\n",
      "Epoch [2225/10000], Loss: 0.7437751293182373\n",
      "Epoch [2226/10000], Loss: 0.7436209917068481\n",
      "Epoch [2227/10000], Loss: 0.7434666752815247\n",
      "Epoch [2228/10000], Loss: 0.7433122396469116\n",
      "Epoch [2229/10000], Loss: 0.7431575655937195\n",
      "Epoch [2230/10000], Loss: 0.7430027723312378\n",
      "Epoch [2231/10000], Loss: 0.742847740650177\n",
      "Epoch [2232/10000], Loss: 0.7426925897598267\n",
      "Epoch [2233/10000], Loss: 0.7425373196601868\n",
      "Epoch [2234/10000], Loss: 0.7423818707466125\n",
      "Epoch [2235/10000], Loss: 0.742226243019104\n",
      "Epoch [2236/10000], Loss: 0.7420704960823059\n",
      "Epoch [2237/10000], Loss: 0.7419145107269287\n",
      "Epoch [2238/10000], Loss: 0.7417584657669067\n",
      "Epoch [2239/10000], Loss: 0.7416021823883057\n",
      "Epoch [2240/10000], Loss: 0.741445779800415\n",
      "Epoch [2241/10000], Loss: 0.7412891387939453\n",
      "Epoch [2242/10000], Loss: 0.741132378578186\n",
      "Epoch [2243/10000], Loss: 0.7409754991531372\n",
      "Epoch [2244/10000], Loss: 0.7408183813095093\n",
      "Epoch [2245/10000], Loss: 0.7406611442565918\n",
      "Epoch [2246/10000], Loss: 0.7405036687850952\n",
      "Epoch [2247/10000], Loss: 0.7403460741043091\n",
      "Epoch [2248/10000], Loss: 0.7401883602142334\n",
      "Epoch [2249/10000], Loss: 0.7400304079055786\n",
      "Epoch [2250/10000], Loss: 0.7398723363876343\n",
      "Epoch [2251/10000], Loss: 0.7397140264511108\n",
      "Epoch [2252/10000], Loss: 0.7395555973052979\n",
      "Epoch [2253/10000], Loss: 0.7393970489501953\n",
      "Epoch [2254/10000], Loss: 0.7392382621765137\n",
      "Epoch [2255/10000], Loss: 0.7390792965888977\n",
      "Epoch [2256/10000], Loss: 0.7389201521873474\n",
      "Epoch [2257/10000], Loss: 0.7387608289718628\n",
      "Epoch [2258/10000], Loss: 0.7386013269424438\n",
      "Epoch [2259/10000], Loss: 0.7384417057037354\n",
      "Epoch [2260/10000], Loss: 0.7382818460464478\n",
      "Epoch [2261/10000], Loss: 0.7381218075752258\n",
      "Epoch [2262/10000], Loss: 0.7379615902900696\n",
      "Epoch [2263/10000], Loss: 0.737801194190979\n",
      "Epoch [2264/10000], Loss: 0.7376406192779541\n",
      "Epoch [2265/10000], Loss: 0.7374798655509949\n",
      "Epoch [2266/10000], Loss: 0.7373188734054565\n",
      "Epoch [2267/10000], Loss: 0.7371577024459839\n",
      "Epoch [2268/10000], Loss: 0.7369964122772217\n",
      "Epoch [2269/10000], Loss: 0.7368348836898804\n",
      "Epoch [2270/10000], Loss: 0.7366731762886047\n",
      "Epoch [2271/10000], Loss: 0.7365112900733948\n",
      "Epoch [2272/10000], Loss: 0.7363491654396057\n",
      "Epoch [2273/10000], Loss: 0.7361869215965271\n",
      "Epoch [2274/10000], Loss: 0.7360243797302246\n",
      "Epoch [2275/10000], Loss: 0.7358617782592773\n",
      "Epoch [2276/10000], Loss: 0.7356988191604614\n",
      "Epoch [2277/10000], Loss: 0.7355356216430664\n",
      "Epoch [2278/10000], Loss: 0.7353723049163818\n",
      "Epoch [2279/10000], Loss: 0.7352087497711182\n",
      "Epoch [2280/10000], Loss: 0.7350450754165649\n",
      "Epoch [2281/10000], Loss: 0.7348812818527222\n",
      "Epoch [2282/10000], Loss: 0.7347172498703003\n",
      "Epoch [2283/10000], Loss: 0.7345529794692993\n",
      "Epoch [2284/10000], Loss: 0.734388530254364\n",
      "Epoch [2285/10000], Loss: 0.7342237234115601\n",
      "Epoch [2286/10000], Loss: 0.7340588569641113\n",
      "Epoch [2287/10000], Loss: 0.7338937520980835\n",
      "Epoch [2288/10000], Loss: 0.7337284684181213\n",
      "Epoch [2289/10000], Loss: 0.7335628867149353\n",
      "Epoch [2290/10000], Loss: 0.7333971858024597\n",
      "Epoch [2291/10000], Loss: 0.7332311868667603\n",
      "Epoch [2292/10000], Loss: 0.7330648899078369\n",
      "Epoch [2293/10000], Loss: 0.732898473739624\n",
      "Epoch [2294/10000], Loss: 0.7327318787574768\n",
      "Epoch [2295/10000], Loss: 0.7325650453567505\n",
      "Epoch [2296/10000], Loss: 0.7323980331420898\n",
      "Epoch [2297/10000], Loss: 0.7322307825088501\n",
      "Epoch [2298/10000], Loss: 0.7320632934570312\n",
      "Epoch [2299/10000], Loss: 0.7318955659866333\n",
      "Epoch [2300/10000], Loss: 0.7317274808883667\n",
      "Epoch [2301/10000], Loss: 0.7315592169761658\n",
      "Epoch [2302/10000], Loss: 0.7313907146453857\n",
      "Epoch [2303/10000], Loss: 0.7312219142913818\n",
      "Epoch [2304/10000], Loss: 0.7310529947280884\n",
      "Epoch [2305/10000], Loss: 0.7308838963508606\n",
      "Epoch [2306/10000], Loss: 0.7307144999504089\n",
      "Epoch [2307/10000], Loss: 0.7305448055267334\n",
      "Epoch [2308/10000], Loss: 0.7303749322891235\n",
      "Epoch [2309/10000], Loss: 0.7302046418190002\n",
      "Epoch [2310/10000], Loss: 0.7300340533256531\n",
      "Epoch [2311/10000], Loss: 0.7298633456230164\n",
      "Epoch [2312/10000], Loss: 0.7296925187110901\n",
      "Epoch [2313/10000], Loss: 0.7295215129852295\n",
      "Epoch [2314/10000], Loss: 0.7293502688407898\n",
      "Epoch [2315/10000], Loss: 0.729178786277771\n",
      "Epoch [2316/10000], Loss: 0.7290071249008179\n",
      "Epoch [2317/10000], Loss: 0.7288353443145752\n",
      "Epoch [2318/10000], Loss: 0.7286632061004639\n",
      "Epoch [2319/10000], Loss: 0.7284907102584839\n",
      "Epoch [2320/10000], Loss: 0.7283179759979248\n",
      "Epoch [2321/10000], Loss: 0.7281450629234314\n",
      "Epoch [2322/10000], Loss: 0.7279719114303589\n",
      "Epoch [2323/10000], Loss: 0.7277984619140625\n",
      "Epoch [2324/10000], Loss: 0.7276247143745422\n",
      "Epoch [2325/10000], Loss: 0.7274507284164429\n",
      "Epoch [2326/10000], Loss: 0.7272766828536987\n",
      "Epoch [2327/10000], Loss: 0.727102518081665\n",
      "Epoch [2328/10000], Loss: 0.7269280552864075\n",
      "Epoch [2329/10000], Loss: 0.7267533540725708\n",
      "Epoch [2330/10000], Loss: 0.7265784740447998\n",
      "Epoch [2331/10000], Loss: 0.7264033555984497\n",
      "Epoch [2332/10000], Loss: 0.7262279987335205\n",
      "Epoch [2333/10000], Loss: 0.7260524034500122\n",
      "Epoch [2334/10000], Loss: 0.72587651014328\n",
      "Epoch [2335/10000], Loss: 0.7257002592086792\n",
      "Epoch [2336/10000], Loss: 0.725523829460144\n",
      "Epoch [2337/10000], Loss: 0.7253471612930298\n",
      "Epoch [2338/10000], Loss: 0.7251703143119812\n",
      "Epoch [2339/10000], Loss: 0.7249931693077087\n",
      "Epoch [2340/10000], Loss: 0.7248157262802124\n",
      "Epoch [2341/10000], Loss: 0.7246381044387817\n",
      "Epoch [2342/10000], Loss: 0.7244604825973511\n",
      "Epoch [2343/10000], Loss: 0.7242826223373413\n",
      "Epoch [2344/10000], Loss: 0.724104642868042\n",
      "Epoch [2345/10000], Loss: 0.7239266037940979\n",
      "Epoch [2346/10000], Loss: 0.7237483263015747\n",
      "Epoch [2347/10000], Loss: 0.7235698699951172\n",
      "Epoch [2348/10000], Loss: 0.7233912348747253\n",
      "Epoch [2349/10000], Loss: 0.7232124209403992\n",
      "Epoch [2350/10000], Loss: 0.7230334281921387\n",
      "Epoch [2351/10000], Loss: 0.7228543162345886\n",
      "Epoch [2352/10000], Loss: 0.722675085067749\n",
      "Epoch [2353/10000], Loss: 0.7224956750869751\n",
      "Epoch [2354/10000], Loss: 0.7223161458969116\n",
      "Epoch [2355/10000], Loss: 0.722136378288269\n",
      "Epoch [2356/10000], Loss: 0.7219563722610474\n",
      "Epoch [2357/10000], Loss: 0.7217761874198914\n",
      "Epoch [2358/10000], Loss: 0.721595823764801\n",
      "Epoch [2359/10000], Loss: 0.7214152812957764\n",
      "Epoch [2360/10000], Loss: 0.7212344408035278\n",
      "Epoch [2361/10000], Loss: 0.7210534811019897\n",
      "Epoch [2362/10000], Loss: 0.7208722829818726\n",
      "Epoch [2363/10000], Loss: 0.7206909656524658\n",
      "Epoch [2364/10000], Loss: 0.7205093502998352\n",
      "Epoch [2365/10000], Loss: 0.720327615737915\n",
      "Epoch [2366/10000], Loss: 0.7201456427574158\n",
      "Epoch [2367/10000], Loss: 0.7199634313583374\n",
      "Epoch [2368/10000], Loss: 0.7197811007499695\n",
      "Epoch [2369/10000], Loss: 0.7195984721183777\n",
      "Epoch [2370/10000], Loss: 0.7194156646728516\n",
      "Epoch [2371/10000], Loss: 0.7192326784133911\n",
      "Epoch [2372/10000], Loss: 0.7190494537353516\n",
      "Epoch [2373/10000], Loss: 0.7188659906387329\n",
      "Epoch [2374/10000], Loss: 0.7186823487281799\n",
      "Epoch [2375/10000], Loss: 0.7184985280036926\n",
      "Epoch [2376/10000], Loss: 0.7183144688606262\n",
      "Epoch [2377/10000], Loss: 0.7181302309036255\n",
      "Epoch [2378/10000], Loss: 0.7179458141326904\n",
      "Epoch [2379/10000], Loss: 0.7177611589431763\n",
      "Epoch [2380/10000], Loss: 0.7175763249397278\n",
      "Epoch [2381/10000], Loss: 0.7173912525177002\n",
      "Epoch [2382/10000], Loss: 0.7172060012817383\n",
      "Epoch [2383/10000], Loss: 0.7170206308364868\n",
      "Epoch [2384/10000], Loss: 0.7168349623680115\n",
      "Epoch [2385/10000], Loss: 0.7166491150856018\n",
      "Epoch [2386/10000], Loss: 0.7164630889892578\n",
      "Epoch [2387/10000], Loss: 0.7162768244743347\n",
      "Epoch [2388/10000], Loss: 0.7160903811454773\n",
      "Epoch [2389/10000], Loss: 0.7159037590026855\n",
      "Epoch [2390/10000], Loss: 0.7157169580459595\n",
      "Epoch [2391/10000], Loss: 0.7155299186706543\n",
      "Epoch [2392/10000], Loss: 0.71534264087677\n",
      "Epoch [2393/10000], Loss: 0.7151552438735962\n",
      "Epoch [2394/10000], Loss: 0.714967668056488\n",
      "Epoch [2395/10000], Loss: 0.714779794216156\n",
      "Epoch [2396/10000], Loss: 0.7145918607711792\n",
      "Epoch [2397/10000], Loss: 0.7144036293029785\n",
      "Epoch [2398/10000], Loss: 0.7142152786254883\n",
      "Epoch [2399/10000], Loss: 0.714026689529419\n",
      "Epoch [2400/10000], Loss: 0.7138378620147705\n",
      "Epoch [2401/10000], Loss: 0.7136489152908325\n",
      "Epoch [2402/10000], Loss: 0.7134597897529602\n",
      "Epoch [2403/10000], Loss: 0.7132704257965088\n",
      "Epoch [2404/10000], Loss: 0.7130809426307678\n",
      "Epoch [2405/10000], Loss: 0.7128912210464478\n",
      "Epoch [2406/10000], Loss: 0.7127013206481934\n",
      "Epoch [2407/10000], Loss: 0.7125113010406494\n",
      "Epoch [2408/10000], Loss: 0.7123209834098816\n",
      "Epoch [2409/10000], Loss: 0.7121305465698242\n",
      "Epoch [2410/10000], Loss: 0.7119398713111877\n",
      "Epoch [2411/10000], Loss: 0.7117490172386169\n",
      "Epoch [2412/10000], Loss: 0.7115579843521118\n",
      "Epoch [2413/10000], Loss: 0.7113667726516724\n",
      "Epoch [2414/10000], Loss: 0.7111753225326538\n",
      "Epoch [2415/10000], Loss: 0.7109837532043457\n",
      "Epoch [2416/10000], Loss: 0.7107919454574585\n",
      "Epoch [2417/10000], Loss: 0.710599958896637\n",
      "Epoch [2418/10000], Loss: 0.7104077935218811\n",
      "Epoch [2419/10000], Loss: 0.7102154493331909\n",
      "Epoch [2420/10000], Loss: 0.7100229263305664\n",
      "Epoch [2421/10000], Loss: 0.7098302841186523\n",
      "Epoch [2422/10000], Loss: 0.7096374034881592\n",
      "Epoch [2423/10000], Loss: 0.7094442844390869\n",
      "Epoch [2424/10000], Loss: 0.7092511057853699\n",
      "Epoch [2425/10000], Loss: 0.7090576887130737\n",
      "Epoch [2426/10000], Loss: 0.7088640928268433\n",
      "Epoch [2427/10000], Loss: 0.7086702585220337\n",
      "Epoch [2428/10000], Loss: 0.7084763050079346\n",
      "Epoch [2429/10000], Loss: 0.7082822322845459\n",
      "Epoch [2430/10000], Loss: 0.7080878615379333\n",
      "Epoch [2431/10000], Loss: 0.7078933715820312\n",
      "Epoch [2432/10000], Loss: 0.7076987028121948\n",
      "Epoch [2433/10000], Loss: 0.7075039148330688\n",
      "Epoch [2434/10000], Loss: 0.7073088884353638\n",
      "Epoch [2435/10000], Loss: 0.7071137428283691\n",
      "Epoch [2436/10000], Loss: 0.7069184184074402\n",
      "Epoch [2437/10000], Loss: 0.7067229747772217\n",
      "Epoch [2438/10000], Loss: 0.7065272927284241\n",
      "Epoch [2439/10000], Loss: 0.7063314914703369\n",
      "Epoch [2440/10000], Loss: 0.7061355113983154\n",
      "Epoch [2441/10000], Loss: 0.7059394121170044\n",
      "Epoch [2442/10000], Loss: 0.705743134021759\n",
      "Epoch [2443/10000], Loss: 0.7055467367172241\n",
      "Epoch [2444/10000], Loss: 0.7053501009941101\n",
      "Epoch [2445/10000], Loss: 0.7051533460617065\n",
      "Epoch [2446/10000], Loss: 0.7049564719200134\n",
      "Epoch [2447/10000], Loss: 0.704759418964386\n",
      "Epoch [2448/10000], Loss: 0.704562246799469\n",
      "Epoch [2449/10000], Loss: 0.7043648958206177\n",
      "Epoch [2450/10000], Loss: 0.704167366027832\n",
      "Epoch [2451/10000], Loss: 0.7039697170257568\n",
      "Epoch [2452/10000], Loss: 0.7037718892097473\n",
      "Epoch [2453/10000], Loss: 0.7035739421844482\n",
      "Epoch [2454/10000], Loss: 0.7033757567405701\n",
      "Epoch [2455/10000], Loss: 0.7031775712966919\n",
      "Epoch [2456/10000], Loss: 0.7029792070388794\n",
      "Epoch [2457/10000], Loss: 0.7027807235717773\n",
      "Epoch [2458/10000], Loss: 0.7025821208953857\n",
      "Epoch [2459/10000], Loss: 0.702383279800415\n",
      "Epoch [2460/10000], Loss: 0.7021843791007996\n",
      "Epoch [2461/10000], Loss: 0.7019853591918945\n",
      "Epoch [2462/10000], Loss: 0.7017861008644104\n",
      "Epoch [2463/10000], Loss: 0.7015867829322815\n",
      "Epoch [2464/10000], Loss: 0.701387345790863\n",
      "Epoch [2465/10000], Loss: 0.7011877298355103\n",
      "Epoch [2466/10000], Loss: 0.7009880542755127\n",
      "Epoch [2467/10000], Loss: 0.700788140296936\n",
      "Epoch [2468/10000], Loss: 0.7005881667137146\n",
      "Epoch [2469/10000], Loss: 0.7003880739212036\n",
      "Epoch [2470/10000], Loss: 0.7001878023147583\n",
      "Epoch [2471/10000], Loss: 0.6999873518943787\n",
      "Epoch [2472/10000], Loss: 0.699786901473999\n",
      "Epoch [2473/10000], Loss: 0.6995862722396851\n",
      "Epoch [2474/10000], Loss: 0.6993855237960815\n",
      "Epoch [2475/10000], Loss: 0.6991846561431885\n",
      "Epoch [2476/10000], Loss: 0.6989836692810059\n",
      "Epoch [2477/10000], Loss: 0.6987825632095337\n",
      "Epoch [2478/10000], Loss: 0.698581337928772\n",
      "Epoch [2479/10000], Loss: 0.6983799934387207\n",
      "Epoch [2480/10000], Loss: 0.6981785297393799\n",
      "Epoch [2481/10000], Loss: 0.6979769468307495\n",
      "Epoch [2482/10000], Loss: 0.6977752447128296\n",
      "Epoch [2483/10000], Loss: 0.6975734233856201\n",
      "Epoch [2484/10000], Loss: 0.6973714828491211\n",
      "Epoch [2485/10000], Loss: 0.6971693634986877\n",
      "Epoch [2486/10000], Loss: 0.6969672441482544\n",
      "Epoch [2487/10000], Loss: 0.6967649459838867\n",
      "Epoch [2488/10000], Loss: 0.6965625286102295\n",
      "Epoch [2489/10000], Loss: 0.6963600516319275\n",
      "Epoch [2490/10000], Loss: 0.6961573958396912\n",
      "Epoch [2491/10000], Loss: 0.6959546804428101\n",
      "Epoch [2492/10000], Loss: 0.6957518458366394\n",
      "Epoch [2493/10000], Loss: 0.6955488920211792\n",
      "Epoch [2494/10000], Loss: 0.6953458786010742\n",
      "Epoch [2495/10000], Loss: 0.6951427459716797\n",
      "Epoch [2496/10000], Loss: 0.6949394345283508\n",
      "Epoch [2497/10000], Loss: 0.6947360038757324\n",
      "Epoch [2498/10000], Loss: 0.6945326328277588\n",
      "Epoch [2499/10000], Loss: 0.694329023361206\n",
      "Epoch [2500/10000], Loss: 0.6941252946853638\n",
      "Epoch [2501/10000], Loss: 0.6939215660095215\n",
      "Epoch [2502/10000], Loss: 0.6937175989151001\n",
      "Epoch [2503/10000], Loss: 0.6935136318206787\n",
      "Epoch [2504/10000], Loss: 0.6933095455169678\n",
      "Epoch [2505/10000], Loss: 0.6931053400039673\n",
      "Epoch [2506/10000], Loss: 0.6929010152816772\n",
      "Epoch [2507/10000], Loss: 0.6926966905593872\n",
      "Epoch [2508/10000], Loss: 0.6924922466278076\n",
      "Epoch [2509/10000], Loss: 0.6922876834869385\n",
      "Epoch [2510/10000], Loss: 0.6920830607414246\n",
      "Epoch [2511/10000], Loss: 0.6918783187866211\n",
      "Epoch [2512/10000], Loss: 0.6916735172271729\n",
      "Epoch [2513/10000], Loss: 0.6914685964584351\n",
      "Epoch [2514/10000], Loss: 0.6912635564804077\n",
      "Epoch [2515/10000], Loss: 0.6910585165023804\n",
      "Epoch [2516/10000], Loss: 0.6908533573150635\n",
      "Epoch [2517/10000], Loss: 0.690648078918457\n",
      "Epoch [2518/10000], Loss: 0.6904427409172058\n",
      "Epoch [2519/10000], Loss: 0.690237283706665\n",
      "Epoch [2520/10000], Loss: 0.6900317668914795\n",
      "Epoch [2521/10000], Loss: 0.6898261308670044\n",
      "Epoch [2522/10000], Loss: 0.6896203756332397\n",
      "Epoch [2523/10000], Loss: 0.6894145607948303\n",
      "Epoch [2524/10000], Loss: 0.6892086863517761\n",
      "Epoch [2525/10000], Loss: 0.6890027523040771\n",
      "Epoch [2526/10000], Loss: 0.6887966990470886\n",
      "Epoch [2527/10000], Loss: 0.6885905265808105\n",
      "Epoch [2528/10000], Loss: 0.6883842945098877\n",
      "Epoch [2529/10000], Loss: 0.6881780028343201\n",
      "Epoch [2530/10000], Loss: 0.6879715919494629\n",
      "Epoch [2531/10000], Loss: 0.6877651214599609\n",
      "Epoch [2532/10000], Loss: 0.6875585317611694\n",
      "Epoch [2533/10000], Loss: 0.6873518824577332\n",
      "Epoch [2534/10000], Loss: 0.6871451139450073\n",
      "Epoch [2535/10000], Loss: 0.6869382858276367\n",
      "Epoch [2536/10000], Loss: 0.6867314577102661\n",
      "Epoch [2537/10000], Loss: 0.6865244507789612\n",
      "Epoch [2538/10000], Loss: 0.6863174438476562\n",
      "Epoch [2539/10000], Loss: 0.686110258102417\n",
      "Epoch [2540/10000], Loss: 0.685903012752533\n",
      "Epoch [2541/10000], Loss: 0.6856956481933594\n",
      "Epoch [2542/10000], Loss: 0.6854882836341858\n",
      "Epoch [2543/10000], Loss: 0.6852807998657227\n",
      "Epoch [2544/10000], Loss: 0.6850732564926147\n",
      "Epoch [2545/10000], Loss: 0.6848656535148621\n",
      "Epoch [2546/10000], Loss: 0.6846579313278198\n",
      "Epoch [2547/10000], Loss: 0.6844501495361328\n",
      "Epoch [2548/10000], Loss: 0.6842423677444458\n",
      "Epoch [2549/10000], Loss: 0.6840344667434692\n",
      "Epoch [2550/10000], Loss: 0.6838264465332031\n",
      "Epoch [2551/10000], Loss: 0.683618426322937\n",
      "Epoch [2552/10000], Loss: 0.6834101676940918\n",
      "Epoch [2553/10000], Loss: 0.6832020282745361\n",
      "Epoch [2554/10000], Loss: 0.6829936504364014\n",
      "Epoch [2555/10000], Loss: 0.6827852725982666\n",
      "Epoch [2556/10000], Loss: 0.6825767755508423\n",
      "Epoch [2557/10000], Loss: 0.6823682188987732\n",
      "Epoch [2558/10000], Loss: 0.6821594834327698\n",
      "Epoch [2559/10000], Loss: 0.6819506883621216\n",
      "Epoch [2560/10000], Loss: 0.6817418932914734\n",
      "Epoch [2561/10000], Loss: 0.6815329194068909\n",
      "Epoch [2562/10000], Loss: 0.6813238859176636\n",
      "Epoch [2563/10000], Loss: 0.6811147928237915\n",
      "Epoch [2564/10000], Loss: 0.6809055805206299\n",
      "Epoch [2565/10000], Loss: 0.6806962490081787\n",
      "Epoch [2566/10000], Loss: 0.6804869174957275\n",
      "Epoch [2567/10000], Loss: 0.6802774667739868\n",
      "Epoch [2568/10000], Loss: 0.6800680160522461\n",
      "Epoch [2569/10000], Loss: 0.679858386516571\n",
      "Epoch [2570/10000], Loss: 0.6796486973762512\n",
      "Epoch [2571/10000], Loss: 0.6794389486312866\n",
      "Epoch [2572/10000], Loss: 0.6792290210723877\n",
      "Epoch [2573/10000], Loss: 0.6790190935134888\n",
      "Epoch [2574/10000], Loss: 0.6788090467453003\n",
      "Epoch [2575/10000], Loss: 0.6785988807678223\n",
      "Epoch [2576/10000], Loss: 0.6783887147903442\n",
      "Epoch [2577/10000], Loss: 0.6781784296035767\n",
      "Epoch [2578/10000], Loss: 0.6779680848121643\n",
      "Epoch [2579/10000], Loss: 0.6777576208114624\n",
      "Epoch [2580/10000], Loss: 0.6775470972061157\n",
      "Epoch [2581/10000], Loss: 0.6773364543914795\n",
      "Epoch [2582/10000], Loss: 0.6771257519721985\n",
      "Epoch [2583/10000], Loss: 0.6769150495529175\n",
      "Epoch [2584/10000], Loss: 0.6767041683197021\n",
      "Epoch [2585/10000], Loss: 0.6764931678771973\n",
      "Epoch [2586/10000], Loss: 0.6762821674346924\n",
      "Epoch [2587/10000], Loss: 0.676071047782898\n",
      "Epoch [2588/10000], Loss: 0.675859808921814\n",
      "Epoch [2589/10000], Loss: 0.6756484508514404\n",
      "Epoch [2590/10000], Loss: 0.6754371523857117\n",
      "Epoch [2591/10000], Loss: 0.6752256155014038\n",
      "Epoch [2592/10000], Loss: 0.6750140190124512\n",
      "Epoch [2593/10000], Loss: 0.6748024225234985\n",
      "Epoch [2594/10000], Loss: 0.6745907068252563\n",
      "Epoch [2595/10000], Loss: 0.6743788719177246\n",
      "Epoch [2596/10000], Loss: 0.6741669178009033\n",
      "Epoch [2597/10000], Loss: 0.673954963684082\n",
      "Epoch [2598/10000], Loss: 0.6737428903579712\n",
      "Epoch [2599/10000], Loss: 0.6735306978225708\n",
      "Epoch [2600/10000], Loss: 0.6733185052871704\n",
      "Epoch [2601/10000], Loss: 0.6731061935424805\n",
      "Epoch [2602/10000], Loss: 0.672893762588501\n",
      "Epoch [2603/10000], Loss: 0.6726812124252319\n",
      "Epoch [2604/10000], Loss: 0.6724686622619629\n",
      "Epoch [2605/10000], Loss: 0.6722559928894043\n",
      "Epoch [2606/10000], Loss: 0.6720432043075562\n",
      "Epoch [2607/10000], Loss: 0.6718303561210632\n",
      "Epoch [2608/10000], Loss: 0.6716173887252808\n",
      "Epoch [2609/10000], Loss: 0.6714043617248535\n",
      "Epoch [2610/10000], Loss: 0.6711912155151367\n",
      "Epoch [2611/10000], Loss: 0.6709780693054199\n",
      "Epoch [2612/10000], Loss: 0.670764684677124\n",
      "Epoch [2613/10000], Loss: 0.6705513000488281\n",
      "Epoch [2614/10000], Loss: 0.6703379154205322\n",
      "Epoch [2615/10000], Loss: 0.6701242923736572\n",
      "Epoch [2616/10000], Loss: 0.6699106693267822\n",
      "Epoch [2617/10000], Loss: 0.6696969270706177\n",
      "Epoch [2618/10000], Loss: 0.6694830656051636\n",
      "Epoch [2619/10000], Loss: 0.6692692041397095\n",
      "Epoch [2620/10000], Loss: 0.6690552234649658\n",
      "Epoch [2621/10000], Loss: 0.6688411235809326\n",
      "Epoch [2622/10000], Loss: 0.6686269044876099\n",
      "Epoch [2623/10000], Loss: 0.6684126257896423\n",
      "Epoch [2624/10000], Loss: 0.6681982278823853\n",
      "Epoch [2625/10000], Loss: 0.6679837703704834\n",
      "Epoch [2626/10000], Loss: 0.6677693128585815\n",
      "Epoch [2627/10000], Loss: 0.6675547361373901\n",
      "Epoch [2628/10000], Loss: 0.6673400402069092\n",
      "Epoch [2629/10000], Loss: 0.6671252250671387\n",
      "Epoch [2630/10000], Loss: 0.6669104695320129\n",
      "Epoch [2631/10000], Loss: 0.6666955947875977\n",
      "Epoch [2632/10000], Loss: 0.666480541229248\n",
      "Epoch [2633/10000], Loss: 0.6662654876708984\n",
      "Epoch [2634/10000], Loss: 0.6660504341125488\n",
      "Epoch [2635/10000], Loss: 0.6658351421356201\n",
      "Epoch [2636/10000], Loss: 0.6656198501586914\n",
      "Epoch [2637/10000], Loss: 0.6654044389724731\n",
      "Epoch [2638/10000], Loss: 0.6651890277862549\n",
      "Epoch [2639/10000], Loss: 0.6649734973907471\n",
      "Epoch [2640/10000], Loss: 0.6647579669952393\n",
      "Epoch [2641/10000], Loss: 0.6645423769950867\n",
      "Epoch [2642/10000], Loss: 0.6643266677856445\n",
      "Epoch [2643/10000], Loss: 0.6641108989715576\n",
      "Epoch [2644/10000], Loss: 0.6638950109481812\n",
      "Epoch [2645/10000], Loss: 0.6636791229248047\n",
      "Epoch [2646/10000], Loss: 0.6634631156921387\n",
      "Epoch [2647/10000], Loss: 0.6632470488548279\n",
      "Epoch [2648/10000], Loss: 0.6630309224128723\n",
      "Epoch [2649/10000], Loss: 0.662814736366272\n",
      "Epoch [2650/10000], Loss: 0.6625984311103821\n",
      "Epoch [2651/10000], Loss: 0.6623821258544922\n",
      "Epoch [2652/10000], Loss: 0.6621657609939575\n",
      "Epoch [2653/10000], Loss: 0.6619493365287781\n",
      "Epoch [2654/10000], Loss: 0.6617327928543091\n",
      "Epoch [2655/10000], Loss: 0.6615161895751953\n",
      "Epoch [2656/10000], Loss: 0.6612995862960815\n",
      "Epoch [2657/10000], Loss: 0.6610828638076782\n",
      "Epoch [2658/10000], Loss: 0.6608660817146301\n",
      "Epoch [2659/10000], Loss: 0.660649299621582\n",
      "Epoch [2660/10000], Loss: 0.6604323387145996\n",
      "Epoch [2661/10000], Loss: 0.6602153778076172\n",
      "Epoch [2662/10000], Loss: 0.6599984169006348\n",
      "Epoch [2663/10000], Loss: 0.6597813367843628\n",
      "Epoch [2664/10000], Loss: 0.6595642566680908\n",
      "Epoch [2665/10000], Loss: 0.6593470573425293\n",
      "Epoch [2666/10000], Loss: 0.6591298580169678\n",
      "Epoch [2667/10000], Loss: 0.6589125990867615\n",
      "Epoch [2668/10000], Loss: 0.6586952209472656\n",
      "Epoch [2669/10000], Loss: 0.658477783203125\n",
      "Epoch [2670/10000], Loss: 0.6582604050636292\n",
      "Epoch [2671/10000], Loss: 0.6580429077148438\n",
      "Epoch [2672/10000], Loss: 0.6578254103660583\n",
      "Epoch [2673/10000], Loss: 0.6576077938079834\n",
      "Epoch [2674/10000], Loss: 0.6573900580406189\n",
      "Epoch [2675/10000], Loss: 0.6571723222732544\n",
      "Epoch [2676/10000], Loss: 0.6569545269012451\n",
      "Epoch [2677/10000], Loss: 0.6567366719245911\n",
      "Epoch [2678/10000], Loss: 0.656518816947937\n",
      "Epoch [2679/10000], Loss: 0.6563009023666382\n",
      "Epoch [2680/10000], Loss: 0.6560829281806946\n",
      "Epoch [2681/10000], Loss: 0.6558648943901062\n",
      "Epoch [2682/10000], Loss: 0.6556468605995178\n",
      "Epoch [2683/10000], Loss: 0.6554287672042847\n",
      "Epoch [2684/10000], Loss: 0.6552106142044067\n",
      "Epoch [2685/10000], Loss: 0.6549924612045288\n",
      "Epoch [2686/10000], Loss: 0.6547742486000061\n",
      "Epoch [2687/10000], Loss: 0.6545560359954834\n",
      "Epoch [2688/10000], Loss: 0.6543377637863159\n",
      "Epoch [2689/10000], Loss: 0.6541194915771484\n",
      "Epoch [2690/10000], Loss: 0.6539011001586914\n",
      "Epoch [2691/10000], Loss: 0.6536827683448792\n",
      "Epoch [2692/10000], Loss: 0.6534644365310669\n",
      "Epoch [2693/10000], Loss: 0.6532459855079651\n",
      "Epoch [2694/10000], Loss: 0.6530275344848633\n",
      "Epoch [2695/10000], Loss: 0.6528090834617615\n",
      "Epoch [2696/10000], Loss: 0.6525905132293701\n",
      "Epoch [2697/10000], Loss: 0.6523720622062683\n",
      "Epoch [2698/10000], Loss: 0.652153491973877\n",
      "Epoch [2699/10000], Loss: 0.6519349217414856\n",
      "Epoch [2700/10000], Loss: 0.6517163515090942\n",
      "Epoch [2701/10000], Loss: 0.6514977812767029\n",
      "Epoch [2702/10000], Loss: 0.6512791514396667\n",
      "Epoch [2703/10000], Loss: 0.6510605812072754\n",
      "Epoch [2704/10000], Loss: 0.6508419513702393\n",
      "Epoch [2705/10000], Loss: 0.6506233215332031\n",
      "Epoch [2706/10000], Loss: 0.650404691696167\n",
      "Epoch [2707/10000], Loss: 0.6501860618591309\n",
      "Epoch [2708/10000], Loss: 0.6499674320220947\n",
      "Epoch [2709/10000], Loss: 0.6497487425804138\n",
      "Epoch [2710/10000], Loss: 0.6495301127433777\n",
      "Epoch [2711/10000], Loss: 0.6493114829063416\n",
      "Epoch [2712/10000], Loss: 0.6490928530693054\n",
      "Epoch [2713/10000], Loss: 0.6488742828369141\n",
      "Epoch [2714/10000], Loss: 0.6486556529998779\n",
      "Epoch [2715/10000], Loss: 0.6484370231628418\n",
      "Epoch [2716/10000], Loss: 0.6482183933258057\n",
      "Epoch [2717/10000], Loss: 0.6479997634887695\n",
      "Epoch [2718/10000], Loss: 0.6477811932563782\n",
      "Epoch [2719/10000], Loss: 0.647562563419342\n",
      "Epoch [2720/10000], Loss: 0.6473439931869507\n",
      "Epoch [2721/10000], Loss: 0.6471254825592041\n",
      "Epoch [2722/10000], Loss: 0.6469069719314575\n",
      "Epoch [2723/10000], Loss: 0.6466885209083557\n",
      "Epoch [2724/10000], Loss: 0.6464701294898987\n",
      "Epoch [2725/10000], Loss: 0.6462517976760864\n",
      "Epoch [2726/10000], Loss: 0.6460334062576294\n",
      "Epoch [2727/10000], Loss: 0.6458151340484619\n",
      "Epoch [2728/10000], Loss: 0.6455967426300049\n",
      "Epoch [2729/10000], Loss: 0.6453784704208374\n",
      "Epoch [2730/10000], Loss: 0.6451602578163147\n",
      "Epoch [2731/10000], Loss: 0.644942045211792\n",
      "Epoch [2732/10000], Loss: 0.6447238326072693\n",
      "Epoch [2733/10000], Loss: 0.6445057392120361\n",
      "Epoch [2734/10000], Loss: 0.6442875862121582\n",
      "Epoch [2735/10000], Loss: 0.6440694332122803\n",
      "Epoch [2736/10000], Loss: 0.6438514590263367\n",
      "Epoch [2737/10000], Loss: 0.6436334848403931\n",
      "Epoch [2738/10000], Loss: 0.6434155106544495\n",
      "Epoch [2739/10000], Loss: 0.6431975364685059\n",
      "Epoch [2740/10000], Loss: 0.6429796814918518\n",
      "Epoch [2741/10000], Loss: 0.6427618265151978\n",
      "Epoch [2742/10000], Loss: 0.6425440311431885\n",
      "Epoch [2743/10000], Loss: 0.6423263549804688\n",
      "Epoch [2744/10000], Loss: 0.642108678817749\n",
      "Epoch [2745/10000], Loss: 0.6418910026550293\n",
      "Epoch [2746/10000], Loss: 0.6416734457015991\n",
      "Epoch [2747/10000], Loss: 0.6414560079574585\n",
      "Epoch [2748/10000], Loss: 0.6412385702133179\n",
      "Epoch [2749/10000], Loss: 0.641021192073822\n",
      "Epoch [2750/10000], Loss: 0.6408039331436157\n",
      "Epoch [2751/10000], Loss: 0.640586793422699\n",
      "Epoch [2752/10000], Loss: 0.6403696537017822\n",
      "Epoch [2753/10000], Loss: 0.6401525735855103\n",
      "Epoch [2754/10000], Loss: 0.6399356722831726\n",
      "Epoch [2755/10000], Loss: 0.6397188305854797\n",
      "Epoch [2756/10000], Loss: 0.6395020484924316\n",
      "Epoch [2757/10000], Loss: 0.6392853260040283\n",
      "Epoch [2758/10000], Loss: 0.6390687227249146\n",
      "Epoch [2759/10000], Loss: 0.6388521194458008\n",
      "Epoch [2760/10000], Loss: 0.6386356353759766\n",
      "Epoch [2761/10000], Loss: 0.6384192705154419\n",
      "Epoch [2762/10000], Loss: 0.6382029056549072\n",
      "Epoch [2763/10000], Loss: 0.6379866600036621\n",
      "Epoch [2764/10000], Loss: 0.6377706527709961\n",
      "Epoch [2765/10000], Loss: 0.6375545263290405\n",
      "Epoch [2766/10000], Loss: 0.6373385190963745\n",
      "Epoch [2767/10000], Loss: 0.637122631072998\n",
      "Epoch [2768/10000], Loss: 0.6369068622589111\n",
      "Epoch [2769/10000], Loss: 0.6366912126541138\n",
      "Epoch [2770/10000], Loss: 0.6364756226539612\n",
      "Epoch [2771/10000], Loss: 0.6362602114677429\n",
      "Epoch [2772/10000], Loss: 0.6360448002815247\n",
      "Epoch [2773/10000], Loss: 0.6358295679092407\n",
      "Epoch [2774/10000], Loss: 0.6356143951416016\n",
      "Epoch [2775/10000], Loss: 0.635399341583252\n",
      "Epoch [2776/10000], Loss: 0.6351843476295471\n",
      "Epoch [2777/10000], Loss: 0.6349694728851318\n",
      "Epoch [2778/10000], Loss: 0.6347547769546509\n",
      "Epoch [2779/10000], Loss: 0.6345400810241699\n",
      "Epoch [2780/10000], Loss: 0.6343255639076233\n",
      "Epoch [2781/10000], Loss: 0.6341111660003662\n",
      "Epoch [2782/10000], Loss: 0.6338968276977539\n",
      "Epoch [2783/10000], Loss: 0.6336827278137207\n",
      "Epoch [2784/10000], Loss: 0.6334686279296875\n",
      "Epoch [2785/10000], Loss: 0.6332546472549438\n",
      "Epoch [2786/10000], Loss: 0.6330408453941345\n",
      "Epoch [2787/10000], Loss: 0.6328271627426147\n",
      "Epoch [2788/10000], Loss: 0.6326135396957397\n",
      "Epoch [2789/10000], Loss: 0.6324000954627991\n",
      "Epoch [2790/10000], Loss: 0.632186770439148\n",
      "Epoch [2791/10000], Loss: 0.6319736242294312\n",
      "Epoch [2792/10000], Loss: 0.6317604780197144\n",
      "Epoch [2793/10000], Loss: 0.6315476298332214\n",
      "Epoch [2794/10000], Loss: 0.6313348412513733\n",
      "Epoch [2795/10000], Loss: 0.6311222314834595\n",
      "Epoch [2796/10000], Loss: 0.6309096813201904\n",
      "Epoch [2797/10000], Loss: 0.6306973695755005\n",
      "Epoch [2798/10000], Loss: 0.6304851174354553\n",
      "Epoch [2799/10000], Loss: 0.6302730441093445\n",
      "Epoch [2800/10000], Loss: 0.630061149597168\n",
      "Epoch [2801/10000], Loss: 0.6298492550849915\n",
      "Epoch [2802/10000], Loss: 0.629637598991394\n",
      "Epoch [2803/10000], Loss: 0.629426121711731\n",
      "Epoch [2804/10000], Loss: 0.6292146444320679\n",
      "Epoch [2805/10000], Loss: 0.6290034055709839\n",
      "Epoch [2806/10000], Loss: 0.6287923455238342\n",
      "Epoch [2807/10000], Loss: 0.6285814046859741\n",
      "Epoch [2808/10000], Loss: 0.6283705234527588\n",
      "Epoch [2809/10000], Loss: 0.6281598806381226\n",
      "Epoch [2810/10000], Loss: 0.6279492974281311\n",
      "Epoch [2811/10000], Loss: 0.6277389526367188\n",
      "Epoch [2812/10000], Loss: 0.6275286078453064\n",
      "Epoch [2813/10000], Loss: 0.6273185014724731\n",
      "Epoch [2814/10000], Loss: 0.6271085739135742\n",
      "Epoch [2815/10000], Loss: 0.6268987059593201\n",
      "Epoch [2816/10000], Loss: 0.626689076423645\n",
      "Epoch [2817/10000], Loss: 0.6264796257019043\n",
      "Epoch [2818/10000], Loss: 0.6262702345848083\n",
      "Epoch [2819/10000], Loss: 0.6260610818862915\n",
      "Epoch [2820/10000], Loss: 0.6258519887924194\n",
      "Epoch [2821/10000], Loss: 0.6256431937217712\n",
      "Epoch [2822/10000], Loss: 0.6254344582557678\n",
      "Epoch [2823/10000], Loss: 0.6252259016036987\n",
      "Epoch [2824/10000], Loss: 0.6250175833702087\n",
      "Epoch [2825/10000], Loss: 0.6248093843460083\n",
      "Epoch [2826/10000], Loss: 0.6246013641357422\n",
      "Epoch [2827/10000], Loss: 0.6243934631347656\n",
      "Epoch [2828/10000], Loss: 0.6241858005523682\n",
      "Epoch [2829/10000], Loss: 0.6239782571792603\n",
      "Epoch [2830/10000], Loss: 0.6237709522247314\n",
      "Epoch [2831/10000], Loss: 0.623563826084137\n",
      "Epoch [2832/10000], Loss: 0.623356819152832\n",
      "Epoch [2833/10000], Loss: 0.6231499910354614\n",
      "Epoch [2834/10000], Loss: 0.6229432225227356\n",
      "Epoch [2835/10000], Loss: 0.6227368116378784\n",
      "Epoch [2836/10000], Loss: 0.6225306391716003\n",
      "Epoch [2837/10000], Loss: 0.6223245859146118\n",
      "Epoch [2838/10000], Loss: 0.6221188306808472\n",
      "Epoch [2839/10000], Loss: 0.6219131946563721\n",
      "Epoch [2840/10000], Loss: 0.6217076778411865\n",
      "Epoch [2841/10000], Loss: 0.6215023994445801\n",
      "Epoch [2842/10000], Loss: 0.6212972402572632\n",
      "Epoch [2843/10000], Loss: 0.6210923194885254\n",
      "Epoch [2844/10000], Loss: 0.6208875179290771\n",
      "Epoch [2845/10000], Loss: 0.620682954788208\n",
      "Epoch [2846/10000], Loss: 0.620478630065918\n",
      "Epoch [2847/10000], Loss: 0.6202744245529175\n",
      "Epoch [2848/10000], Loss: 0.6200704574584961\n",
      "Epoch [2849/10000], Loss: 0.6198666095733643\n",
      "Epoch [2850/10000], Loss: 0.619662880897522\n",
      "Epoch [2851/10000], Loss: 0.6194595098495483\n",
      "Epoch [2852/10000], Loss: 0.6192562580108643\n",
      "Epoch [2853/10000], Loss: 0.6190531253814697\n",
      "Epoch [2854/10000], Loss: 0.6188501715660095\n",
      "Epoch [2855/10000], Loss: 0.6186474561691284\n",
      "Epoch [2856/10000], Loss: 0.6184449195861816\n",
      "Epoch [2857/10000], Loss: 0.618242621421814\n",
      "Epoch [2858/10000], Loss: 0.6180404424667358\n",
      "Epoch [2859/10000], Loss: 0.6178385019302368\n",
      "Epoch [2860/10000], Loss: 0.6176366806030273\n",
      "Epoch [2861/10000], Loss: 0.617435097694397\n",
      "Epoch [2862/10000], Loss: 0.6172337532043457\n",
      "Epoch [2863/10000], Loss: 0.617032527923584\n",
      "Epoch [2864/10000], Loss: 0.6168314814567566\n",
      "Epoch [2865/10000], Loss: 0.6166306734085083\n",
      "Epoch [2866/10000], Loss: 0.6164300441741943\n",
      "Epoch [2867/10000], Loss: 0.6162295341491699\n",
      "Epoch [2868/10000], Loss: 0.6160292625427246\n",
      "Epoch [2869/10000], Loss: 0.6158292293548584\n",
      "Epoch [2870/10000], Loss: 0.6156294345855713\n",
      "Epoch [2871/10000], Loss: 0.615429699420929\n",
      "Epoch [2872/10000], Loss: 0.6152302026748657\n",
      "Epoch [2873/10000], Loss: 0.6150310039520264\n",
      "Epoch [2874/10000], Loss: 0.6148319244384766\n",
      "Epoch [2875/10000], Loss: 0.6146330833435059\n",
      "Epoch [2876/10000], Loss: 0.6144343614578247\n",
      "Epoch [2877/10000], Loss: 0.6142358779907227\n",
      "Epoch [2878/10000], Loss: 0.6140375733375549\n",
      "Epoch [2879/10000], Loss: 0.6138395667076111\n",
      "Epoch [2880/10000], Loss: 0.6136416792869568\n",
      "Epoch [2881/10000], Loss: 0.6134439706802368\n",
      "Epoch [2882/10000], Loss: 0.613246500492096\n",
      "Epoch [2883/10000], Loss: 0.6130492687225342\n",
      "Epoch [2884/10000], Loss: 0.6128520965576172\n",
      "Epoch [2885/10000], Loss: 0.6126552820205688\n",
      "Epoch [2886/10000], Loss: 0.6124585866928101\n",
      "Epoch [2887/10000], Loss: 0.6122621297836304\n",
      "Epoch [2888/10000], Loss: 0.612065851688385\n",
      "Epoch [2889/10000], Loss: 0.6118698120117188\n",
      "Epoch [2890/10000], Loss: 0.6116739511489868\n",
      "Epoch [2891/10000], Loss: 0.6114782094955444\n",
      "Epoch [2892/10000], Loss: 0.6112828254699707\n",
      "Epoch [2893/10000], Loss: 0.6110875606536865\n",
      "Epoch [2894/10000], Loss: 0.6108924746513367\n",
      "Epoch [2895/10000], Loss: 0.6106976270675659\n",
      "Epoch [2896/10000], Loss: 0.6105030179023743\n",
      "Epoch [2897/10000], Loss: 0.6103085875511169\n",
      "Epoch [2898/10000], Loss: 0.610114336013794\n",
      "Epoch [2899/10000], Loss: 0.60992032289505\n",
      "Epoch [2900/10000], Loss: 0.6097264289855957\n",
      "Epoch [2901/10000], Loss: 0.6095327734947205\n",
      "Epoch [2902/10000], Loss: 0.6093393564224243\n",
      "Epoch [2903/10000], Loss: 0.6091461181640625\n",
      "Epoch [2904/10000], Loss: 0.6089529991149902\n",
      "Epoch [2905/10000], Loss: 0.6087602376937866\n",
      "Epoch [2906/10000], Loss: 0.6085675954818726\n",
      "Epoch [2907/10000], Loss: 0.6083751320838928\n",
      "Epoch [2908/10000], Loss: 0.6081829071044922\n",
      "Epoch [2909/10000], Loss: 0.6079908609390259\n",
      "Epoch [2910/10000], Loss: 0.6077990531921387\n",
      "Epoch [2911/10000], Loss: 0.6076074242591858\n",
      "Epoch [2912/10000], Loss: 0.607416033744812\n",
      "Epoch [2913/10000], Loss: 0.6072248220443726\n",
      "Epoch [2914/10000], Loss: 0.6070337891578674\n",
      "Epoch [2915/10000], Loss: 0.6068429946899414\n",
      "Epoch [2916/10000], Loss: 0.6066523790359497\n",
      "Epoch [2917/10000], Loss: 0.6064620018005371\n",
      "Epoch [2918/10000], Loss: 0.6062718629837036\n",
      "Epoch [2919/10000], Loss: 0.6060818433761597\n",
      "Epoch [2920/10000], Loss: 0.6058920621871948\n",
      "Epoch [2921/10000], Loss: 0.6057025194168091\n",
      "Epoch [2922/10000], Loss: 0.6055130958557129\n",
      "Epoch [2923/10000], Loss: 0.6053239703178406\n",
      "Epoch [2924/10000], Loss: 0.6051349639892578\n",
      "Epoch [2925/10000], Loss: 0.6049462556838989\n",
      "Epoch [2926/10000], Loss: 0.6047577261924744\n",
      "Epoch [2927/10000], Loss: 0.6045693755149841\n",
      "Epoch [2928/10000], Loss: 0.604381263256073\n",
      "Epoch [2929/10000], Loss: 0.6041933298110962\n",
      "Epoch [2930/10000], Loss: 0.6040055751800537\n",
      "Epoch [2931/10000], Loss: 0.6038181185722351\n",
      "Epoch [2932/10000], Loss: 0.6036308407783508\n",
      "Epoch [2933/10000], Loss: 0.6034436821937561\n",
      "Epoch [2934/10000], Loss: 0.6032568216323853\n",
      "Epoch [2935/10000], Loss: 0.6030701398849487\n",
      "Epoch [2936/10000], Loss: 0.6028836369514465\n",
      "Epoch [2937/10000], Loss: 0.6026973724365234\n",
      "Epoch [2938/10000], Loss: 0.6025112271308899\n",
      "Epoch [2939/10000], Loss: 0.6023253798484802\n",
      "Epoch [2940/10000], Loss: 0.6021397709846497\n",
      "Epoch [2941/10000], Loss: 0.6019543409347534\n",
      "Epoch [2942/10000], Loss: 0.6017690896987915\n",
      "Epoch [2943/10000], Loss: 0.6015840172767639\n",
      "Epoch [2944/10000], Loss: 0.6013991832733154\n",
      "Epoch [2945/10000], Loss: 0.6012145280838013\n",
      "Epoch [2946/10000], Loss: 0.6010301113128662\n",
      "Epoch [2947/10000], Loss: 0.6008459329605103\n",
      "Epoch [2948/10000], Loss: 0.6006618738174438\n",
      "Epoch [2949/10000], Loss: 0.6004779934883118\n",
      "Epoch [2950/10000], Loss: 0.6002943515777588\n",
      "Epoch [2951/10000], Loss: 0.6001108884811401\n",
      "Epoch [2952/10000], Loss: 0.5999276638031006\n",
      "Epoch [2953/10000], Loss: 0.5997445583343506\n",
      "Epoch [2954/10000], Loss: 0.5995616912841797\n",
      "Epoch [2955/10000], Loss: 0.5993789434432983\n",
      "Epoch [2956/10000], Loss: 0.5991964936256409\n",
      "Epoch [2957/10000], Loss: 0.5990142822265625\n",
      "Epoch [2958/10000], Loss: 0.5988321900367737\n",
      "Epoch [2959/10000], Loss: 0.5986502766609192\n",
      "Epoch [2960/10000], Loss: 0.5984686613082886\n",
      "Epoch [2961/10000], Loss: 0.5982872247695923\n",
      "Epoch [2962/10000], Loss: 0.5981060266494751\n",
      "Epoch [2963/10000], Loss: 0.5979249477386475\n",
      "Epoch [2964/10000], Loss: 0.5977441072463989\n",
      "Epoch [2965/10000], Loss: 0.5975635051727295\n",
      "Epoch [2966/10000], Loss: 0.5973830223083496\n",
      "Epoch [2967/10000], Loss: 0.5972027778625488\n",
      "Epoch [2968/10000], Loss: 0.5970227718353271\n",
      "Epoch [2969/10000], Loss: 0.5968429446220398\n",
      "Epoch [2970/10000], Loss: 0.5966633558273315\n",
      "Epoch [2971/10000], Loss: 0.5964839458465576\n",
      "Epoch [2972/10000], Loss: 0.5963047742843628\n",
      "Epoch [2973/10000], Loss: 0.5961257219314575\n",
      "Epoch [2974/10000], Loss: 0.5959469676017761\n",
      "Epoch [2975/10000], Loss: 0.595768392086029\n",
      "Epoch [2976/10000], Loss: 0.5955899953842163\n",
      "Epoch [2977/10000], Loss: 0.5954118371009827\n",
      "Epoch [2978/10000], Loss: 0.5952338576316833\n",
      "Epoch [2979/10000], Loss: 0.5950560569763184\n",
      "Epoch [2980/10000], Loss: 0.5948784351348877\n",
      "Epoch [2981/10000], Loss: 0.5947010517120361\n",
      "Epoch [2982/10000], Loss: 0.5945239067077637\n",
      "Epoch [2983/10000], Loss: 0.5943468809127808\n",
      "Epoch [2984/10000], Loss: 0.594170093536377\n",
      "Epoch [2985/10000], Loss: 0.5939935445785522\n",
      "Epoch [2986/10000], Loss: 0.5938171744346619\n",
      "Epoch [2987/10000], Loss: 0.5936410427093506\n",
      "Epoch [2988/10000], Loss: 0.5934650897979736\n",
      "Epoch [2989/10000], Loss: 0.593289315700531\n",
      "Epoch [2990/10000], Loss: 0.5931137800216675\n",
      "Epoch [2991/10000], Loss: 0.5929383635520935\n",
      "Epoch [2992/10000], Loss: 0.5927631855010986\n",
      "Epoch [2993/10000], Loss: 0.5925882458686829\n",
      "Epoch [2994/10000], Loss: 0.5924134254455566\n",
      "Epoch [2995/10000], Loss: 0.5922389030456543\n",
      "Epoch [2996/10000], Loss: 0.592064619064331\n",
      "Epoch [2997/10000], Loss: 0.5918904542922974\n",
      "Epoch [2998/10000], Loss: 0.5917165875434875\n",
      "Epoch [2999/10000], Loss: 0.5915428400039673\n",
      "Epoch [3000/10000], Loss: 0.5913693308830261\n",
      "Epoch [3001/10000], Loss: 0.5911960005760193\n",
      "Epoch [3002/10000], Loss: 0.5910228490829468\n",
      "Epoch [3003/10000], Loss: 0.5908499360084534\n",
      "Epoch [3004/10000], Loss: 0.5906772613525391\n",
      "Epoch [3005/10000], Loss: 0.5905047059059143\n",
      "Epoch [3006/10000], Loss: 0.5903322696685791\n",
      "Epoch [3007/10000], Loss: 0.5901601314544678\n",
      "Epoch [3008/10000], Loss: 0.5899881720542908\n",
      "Epoch [3009/10000], Loss: 0.5898164510726929\n",
      "Epoch [3010/10000], Loss: 0.5896449089050293\n",
      "Epoch [3011/10000], Loss: 0.5894734859466553\n",
      "Epoch [3012/10000], Loss: 0.5893023014068604\n",
      "Epoch [3013/10000], Loss: 0.5891313552856445\n",
      "Epoch [3014/10000], Loss: 0.5889605283737183\n",
      "Epoch [3015/10000], Loss: 0.5887899398803711\n",
      "Epoch [3016/10000], Loss: 0.588619589805603\n",
      "Epoch [3017/10000], Loss: 0.5884493589401245\n",
      "Epoch [3018/10000], Loss: 0.5882793068885803\n",
      "Epoch [3019/10000], Loss: 0.5881094932556152\n",
      "Epoch [3020/10000], Loss: 0.5879398584365845\n",
      "Epoch [3021/10000], Loss: 0.5877703428268433\n",
      "Epoch [3022/10000], Loss: 0.5876010656356812\n",
      "Epoch [3023/10000], Loss: 0.5874319076538086\n",
      "Epoch [3024/10000], Loss: 0.5872631072998047\n",
      "Epoch [3025/10000], Loss: 0.5870943069458008\n",
      "Epoch [3026/10000], Loss: 0.5869258046150208\n",
      "Epoch [3027/10000], Loss: 0.5867575407028198\n",
      "Epoch [3028/10000], Loss: 0.5865893363952637\n",
      "Epoch [3029/10000], Loss: 0.5864214301109314\n",
      "Epoch [3030/10000], Loss: 0.5862536430358887\n",
      "Epoch [3031/10000], Loss: 0.5860860347747803\n",
      "Epoch [3032/10000], Loss: 0.585918664932251\n",
      "Epoch [3033/10000], Loss: 0.585751473903656\n",
      "Epoch [3034/10000], Loss: 0.5855844020843506\n",
      "Epoch [3035/10000], Loss: 0.5854175686836243\n",
      "Epoch [3036/10000], Loss: 0.5852509140968323\n",
      "Epoch [3037/10000], Loss: 0.5850844383239746\n",
      "Epoch [3038/10000], Loss: 0.5849181413650513\n",
      "Epoch [3039/10000], Loss: 0.5847520232200623\n",
      "Epoch [3040/10000], Loss: 0.5845861434936523\n",
      "Epoch [3041/10000], Loss: 0.584420382976532\n",
      "Epoch [3042/10000], Loss: 0.5842548608779907\n",
      "Epoch [3043/10000], Loss: 0.584089457988739\n",
      "Epoch [3044/10000], Loss: 0.5839242935180664\n",
      "Epoch [3045/10000], Loss: 0.5837593078613281\n",
      "Epoch [3046/10000], Loss: 0.5835945010185242\n",
      "Epoch [3047/10000], Loss: 0.5834298133850098\n",
      "Epoch [3048/10000], Loss: 0.5832653045654297\n",
      "Epoch [3049/10000], Loss: 0.5831010341644287\n",
      "Epoch [3050/10000], Loss: 0.5829370021820068\n",
      "Epoch [3051/10000], Loss: 0.5827730298042297\n",
      "Epoch [3052/10000], Loss: 0.5826092958450317\n",
      "Epoch [3053/10000], Loss: 0.5824457406997681\n",
      "Epoch [3054/10000], Loss: 0.582282304763794\n",
      "Epoch [3055/10000], Loss: 0.5821191072463989\n",
      "Epoch [3056/10000], Loss: 0.5819560289382935\n",
      "Epoch [3057/10000], Loss: 0.5817931294441223\n",
      "Epoch [3058/10000], Loss: 0.5816304683685303\n",
      "Epoch [3059/10000], Loss: 0.5814679861068726\n",
      "Epoch [3060/10000], Loss: 0.5813056230545044\n",
      "Epoch [3061/10000], Loss: 0.5811434388160706\n",
      "Epoch [3062/10000], Loss: 0.5809814929962158\n",
      "Epoch [3063/10000], Loss: 0.5808196663856506\n",
      "Epoch [3064/10000], Loss: 0.580657958984375\n",
      "Epoch [3065/10000], Loss: 0.5804965496063232\n",
      "Epoch [3066/10000], Loss: 0.5803353190422058\n",
      "Epoch [3067/10000], Loss: 0.5801742076873779\n",
      "Epoch [3068/10000], Loss: 0.5800132751464844\n",
      "Epoch [3069/10000], Loss: 0.5798525214195251\n",
      "Epoch [3070/10000], Loss: 0.5796918869018555\n",
      "Epoch [3071/10000], Loss: 0.5795314908027649\n",
      "Epoch [3072/10000], Loss: 0.5793712139129639\n",
      "Epoch [3073/10000], Loss: 0.5792111158370972\n",
      "Epoch [3074/10000], Loss: 0.5790512561798096\n",
      "Epoch [3075/10000], Loss: 0.5788915753364563\n",
      "Epoch [3076/10000], Loss: 0.5787320137023926\n",
      "Epoch [3077/10000], Loss: 0.5785726308822632\n",
      "Epoch [3078/10000], Loss: 0.5784134268760681\n",
      "Epoch [3079/10000], Loss: 0.5782543420791626\n",
      "Epoch [3080/10000], Loss: 0.5780954957008362\n",
      "Epoch [3081/10000], Loss: 0.5779367685317993\n",
      "Epoch [3082/10000], Loss: 0.5777782201766968\n",
      "Epoch [3083/10000], Loss: 0.5776199102401733\n",
      "Epoch [3084/10000], Loss: 0.5774616599082947\n",
      "Epoch [3085/10000], Loss: 0.5773036479949951\n",
      "Epoch [3086/10000], Loss: 0.5771457552909851\n",
      "Epoch [3087/10000], Loss: 0.5769881010055542\n",
      "Epoch [3088/10000], Loss: 0.5768305659294128\n",
      "Epoch [3089/10000], Loss: 0.5766732692718506\n",
      "Epoch [3090/10000], Loss: 0.5765160322189331\n",
      "Epoch [3091/10000], Loss: 0.5763590335845947\n",
      "Epoch [3092/10000], Loss: 0.5762021541595459\n",
      "Epoch [3093/10000], Loss: 0.5760454535484314\n",
      "Epoch [3094/10000], Loss: 0.5758888721466064\n",
      "Epoch [3095/10000], Loss: 0.5757326483726501\n",
      "Epoch [3096/10000], Loss: 0.5755764245986938\n",
      "Epoch [3097/10000], Loss: 0.5754203796386719\n",
      "Epoch [3098/10000], Loss: 0.5752645134925842\n",
      "Epoch [3099/10000], Loss: 0.5751088261604309\n",
      "Epoch [3100/10000], Loss: 0.5749533176422119\n",
      "Epoch [3101/10000], Loss: 0.5747978687286377\n",
      "Epoch [3102/10000], Loss: 0.5746426582336426\n",
      "Epoch [3103/10000], Loss: 0.5744876861572266\n",
      "Epoch [3104/10000], Loss: 0.5743327736854553\n",
      "Epoch [3105/10000], Loss: 0.5741780996322632\n",
      "Epoch [3106/10000], Loss: 0.5740234851837158\n",
      "Epoch [3107/10000], Loss: 0.5738691091537476\n",
      "Epoch [3108/10000], Loss: 0.5737149119377136\n",
      "Epoch [3109/10000], Loss: 0.5735608339309692\n",
      "Epoch [3110/10000], Loss: 0.5734068751335144\n",
      "Epoch [3111/10000], Loss: 0.5732530355453491\n",
      "Epoch [3112/10000], Loss: 0.5730994939804077\n",
      "Epoch [3113/10000], Loss: 0.5729460716247559\n",
      "Epoch [3114/10000], Loss: 0.5727927684783936\n",
      "Epoch [3115/10000], Loss: 0.5726396441459656\n",
      "Epoch [3116/10000], Loss: 0.5724866390228271\n",
      "Epoch [3117/10000], Loss: 0.572333812713623\n",
      "Epoch [3118/10000], Loss: 0.5721811652183533\n",
      "Epoch [3119/10000], Loss: 0.5720286965370178\n",
      "Epoch [3120/10000], Loss: 0.5718762874603271\n",
      "Epoch [3121/10000], Loss: 0.5717241168022156\n",
      "Epoch [3122/10000], Loss: 0.5715720653533936\n",
      "Epoch [3123/10000], Loss: 0.5714201927185059\n",
      "Epoch [3124/10000], Loss: 0.5712685585021973\n",
      "Epoch [3125/10000], Loss: 0.5711169242858887\n",
      "Epoch [3126/10000], Loss: 0.5709655284881592\n",
      "Epoch [3127/10000], Loss: 0.570814311504364\n",
      "Epoch [3128/10000], Loss: 0.5706632137298584\n",
      "Epoch [3129/10000], Loss: 0.5705122947692871\n",
      "Epoch [3130/10000], Loss: 0.5703614950180054\n",
      "Epoch [3131/10000], Loss: 0.5702109336853027\n",
      "Epoch [3132/10000], Loss: 0.5700604915618896\n",
      "Epoch [3133/10000], Loss: 0.5699101090431213\n",
      "Epoch [3134/10000], Loss: 0.5697599649429321\n",
      "Epoch [3135/10000], Loss: 0.5696099996566772\n",
      "Epoch [3136/10000], Loss: 0.5694601535797119\n",
      "Epoch [3137/10000], Loss: 0.5693104267120361\n",
      "Epoch [3138/10000], Loss: 0.5691608786582947\n",
      "Epoch [3139/10000], Loss: 0.5690114498138428\n",
      "Epoch [3140/10000], Loss: 0.5688621997833252\n",
      "Epoch [3141/10000], Loss: 0.5687130689620972\n",
      "Epoch [3142/10000], Loss: 0.5685640573501587\n",
      "Epoch [3143/10000], Loss: 0.5684152841567993\n",
      "Epoch [3144/10000], Loss: 0.5682665109634399\n",
      "Epoch [3145/10000], Loss: 0.5681179761886597\n",
      "Epoch [3146/10000], Loss: 0.5679696202278137\n",
      "Epoch [3147/10000], Loss: 0.5678213834762573\n",
      "Epoch [3148/10000], Loss: 0.5676732063293457\n",
      "Epoch [3149/10000], Loss: 0.567525327205658\n",
      "Epoch [3150/10000], Loss: 0.5673775672912598\n",
      "Epoch [3151/10000], Loss: 0.5672298669815063\n",
      "Epoch [3152/10000], Loss: 0.5670823454856873\n",
      "Epoch [3153/10000], Loss: 0.5669350624084473\n",
      "Epoch [3154/10000], Loss: 0.5667877793312073\n",
      "Epoch [3155/10000], Loss: 0.5666406154632568\n",
      "Epoch [3156/10000], Loss: 0.5664937496185303\n",
      "Epoch [3157/10000], Loss: 0.5663470029830933\n",
      "Epoch [3158/10000], Loss: 0.5662002563476562\n",
      "Epoch [3159/10000], Loss: 0.5660538673400879\n",
      "Epoch [3160/10000], Loss: 0.5659074783325195\n",
      "Epoch [3161/10000], Loss: 0.5657613277435303\n",
      "Epoch [3162/10000], Loss: 0.565615177154541\n",
      "Epoch [3163/10000], Loss: 0.5654692649841309\n",
      "Epoch [3164/10000], Loss: 0.565323531627655\n",
      "Epoch [3165/10000], Loss: 0.565177857875824\n",
      "Epoch [3166/10000], Loss: 0.5650323629379272\n",
      "Epoch [3167/10000], Loss: 0.5648869872093201\n",
      "Epoch [3168/10000], Loss: 0.5647417306900024\n",
      "Epoch [3169/10000], Loss: 0.5645966529846191\n",
      "Epoch [3170/10000], Loss: 0.5644516944885254\n",
      "Epoch [3171/10000], Loss: 0.5643068552017212\n",
      "Epoch [3172/10000], Loss: 0.5641621947288513\n",
      "Epoch [3173/10000], Loss: 0.5640177130699158\n",
      "Epoch [3174/10000], Loss: 0.563873291015625\n",
      "Epoch [3175/10000], Loss: 0.5637289881706238\n",
      "Epoch [3176/10000], Loss: 0.5635848045349121\n",
      "Epoch [3177/10000], Loss: 0.5634409189224243\n",
      "Epoch [3178/10000], Loss: 0.5632970333099365\n",
      "Epoch [3179/10000], Loss: 0.5631532669067383\n",
      "Epoch [3180/10000], Loss: 0.5630097389221191\n",
      "Epoch [3181/10000], Loss: 0.5628663301467896\n",
      "Epoch [3182/10000], Loss: 0.5627230405807495\n",
      "Epoch [3183/10000], Loss: 0.562579870223999\n",
      "Epoch [3184/10000], Loss: 0.5624369382858276\n",
      "Epoch [3185/10000], Loss: 0.5622940063476562\n",
      "Epoch [3186/10000], Loss: 0.5621511936187744\n",
      "Epoch [3187/10000], Loss: 0.5620086193084717\n",
      "Epoch [3188/10000], Loss: 0.5618661642074585\n",
      "Epoch [3189/10000], Loss: 0.5617238283157349\n",
      "Epoch [3190/10000], Loss: 0.5615816116333008\n",
      "Epoch [3191/10000], Loss: 0.561439573764801\n",
      "Epoch [3192/10000], Loss: 0.5612976551055908\n",
      "Epoch [3193/10000], Loss: 0.5611557960510254\n",
      "Epoch [3194/10000], Loss: 0.5610141754150391\n",
      "Epoch [3195/10000], Loss: 0.5608726143836975\n",
      "Epoch [3196/10000], Loss: 0.5607311725616455\n",
      "Epoch [3197/10000], Loss: 0.5605898499488831\n",
      "Epoch [3198/10000], Loss: 0.5604487061500549\n",
      "Epoch [3199/10000], Loss: 0.5603076219558716\n",
      "Epoch [3200/10000], Loss: 0.5601667165756226\n",
      "Epoch [3201/10000], Loss: 0.5600260496139526\n",
      "Epoch [3202/10000], Loss: 0.5598853230476379\n",
      "Epoch [3203/10000], Loss: 0.5597448348999023\n",
      "Epoch [3204/10000], Loss: 0.5596044063568115\n",
      "Epoch [3205/10000], Loss: 0.559464156627655\n",
      "Epoch [3206/10000], Loss: 0.5593240261077881\n",
      "Epoch [3207/10000], Loss: 0.5591840744018555\n",
      "Epoch [3208/10000], Loss: 0.5590441823005676\n",
      "Epoch [3209/10000], Loss: 0.5589044094085693\n",
      "Epoch [3210/10000], Loss: 0.5587648153305054\n",
      "Epoch [3211/10000], Loss: 0.558625340461731\n",
      "Epoch [3212/10000], Loss: 0.5584859848022461\n",
      "Epoch [3213/10000], Loss: 0.5583468079566956\n",
      "Epoch [3214/10000], Loss: 0.5582077503204346\n",
      "Epoch [3215/10000], Loss: 0.5580687522888184\n",
      "Epoch [3216/10000], Loss: 0.5579299330711365\n",
      "Epoch [3217/10000], Loss: 0.5577912926673889\n",
      "Epoch [3218/10000], Loss: 0.5576527118682861\n",
      "Epoch [3219/10000], Loss: 0.5575143098831177\n",
      "Epoch [3220/10000], Loss: 0.557375967502594\n",
      "Epoch [3221/10000], Loss: 0.5572377443313599\n",
      "Epoch [3222/10000], Loss: 0.5570997595787048\n",
      "Epoch [3223/10000], Loss: 0.5569617748260498\n",
      "Epoch [3224/10000], Loss: 0.5568239688873291\n",
      "Epoch [3225/10000], Loss: 0.5566864013671875\n",
      "Epoch [3226/10000], Loss: 0.5565488338470459\n",
      "Epoch [3227/10000], Loss: 0.5564113259315491\n",
      "Epoch [3228/10000], Loss: 0.5562741160392761\n",
      "Epoch [3229/10000], Loss: 0.5561369061470032\n",
      "Epoch [3230/10000], Loss: 0.5559998154640198\n",
      "Epoch [3231/10000], Loss: 0.5558629035949707\n",
      "Epoch [3232/10000], Loss: 0.5557260513305664\n",
      "Epoch [3233/10000], Loss: 0.5555894374847412\n",
      "Epoch [3234/10000], Loss: 0.555452823638916\n",
      "Epoch [3235/10000], Loss: 0.5553163886070251\n",
      "Epoch [3236/10000], Loss: 0.5551800727844238\n",
      "Epoch [3237/10000], Loss: 0.5550439357757568\n",
      "Epoch [3238/10000], Loss: 0.5549077987670898\n",
      "Epoch [3239/10000], Loss: 0.5547717809677124\n",
      "Epoch [3240/10000], Loss: 0.5546360015869141\n",
      "Epoch [3241/10000], Loss: 0.5545002818107605\n",
      "Epoch [3242/10000], Loss: 0.5543646812438965\n",
      "Epoch [3243/10000], Loss: 0.5542292594909668\n",
      "Epoch [3244/10000], Loss: 0.5540938377380371\n",
      "Epoch [3245/10000], Loss: 0.5539586544036865\n",
      "Epoch [3246/10000], Loss: 0.5538234710693359\n",
      "Epoch [3247/10000], Loss: 0.5536885261535645\n",
      "Epoch [3248/10000], Loss: 0.5535537004470825\n",
      "Epoch [3249/10000], Loss: 0.5534189343452454\n",
      "Epoch [3250/10000], Loss: 0.5532843470573425\n",
      "Epoch [3251/10000], Loss: 0.5531498193740845\n",
      "Epoch [3252/10000], Loss: 0.5530155301094055\n",
      "Epoch [3253/10000], Loss: 0.5528812408447266\n",
      "Epoch [3254/10000], Loss: 0.5527471303939819\n",
      "Epoch [3255/10000], Loss: 0.5526130795478821\n",
      "Epoch [3256/10000], Loss: 0.5524791479110718\n",
      "Epoch [3257/10000], Loss: 0.5523453950881958\n",
      "Epoch [3258/10000], Loss: 0.5522117614746094\n",
      "Epoch [3259/10000], Loss: 0.552078127861023\n",
      "Epoch [3260/10000], Loss: 0.5519447326660156\n",
      "Epoch [3261/10000], Loss: 0.5518114566802979\n",
      "Epoch [3262/10000], Loss: 0.5516781806945801\n",
      "Epoch [3263/10000], Loss: 0.5515451431274414\n",
      "Epoch [3264/10000], Loss: 0.5514122247695923\n",
      "Epoch [3265/10000], Loss: 0.5512793660163879\n",
      "Epoch [3266/10000], Loss: 0.5511466264724731\n",
      "Epoch [3267/10000], Loss: 0.5510140657424927\n",
      "Epoch [3268/10000], Loss: 0.5508816242218018\n",
      "Epoch [3269/10000], Loss: 0.5507492423057556\n",
      "Epoch [3270/10000], Loss: 0.550616979598999\n",
      "Epoch [3271/10000], Loss: 0.5504848957061768\n",
      "Epoch [3272/10000], Loss: 0.5503528118133545\n",
      "Epoch [3273/10000], Loss: 0.5502209067344666\n",
      "Epoch [3274/10000], Loss: 0.5500891208648682\n",
      "Epoch [3275/10000], Loss: 0.5499575138092041\n",
      "Epoch [3276/10000], Loss: 0.54982590675354\n",
      "Epoch [3277/10000], Loss: 0.5496944785118103\n",
      "Epoch [3278/10000], Loss: 0.5495631694793701\n",
      "Epoch [3279/10000], Loss: 0.5494319200515747\n",
      "Epoch [3280/10000], Loss: 0.5493007898330688\n",
      "Epoch [3281/10000], Loss: 0.5491697788238525\n",
      "Epoch [3282/10000], Loss: 0.5490389466285706\n",
      "Epoch [3283/10000], Loss: 0.5489081740379333\n",
      "Epoch [3284/10000], Loss: 0.5487774610519409\n",
      "Epoch [3285/10000], Loss: 0.5486469864845276\n",
      "Epoch [3286/10000], Loss: 0.5485165119171143\n",
      "Epoch [3287/10000], Loss: 0.5483861565589905\n",
      "Epoch [3288/10000], Loss: 0.548255980014801\n",
      "Epoch [3289/10000], Loss: 0.5481258630752563\n",
      "Epoch [3290/10000], Loss: 0.5479958653450012\n",
      "Epoch [3291/10000], Loss: 0.5478660464286804\n",
      "Epoch [3292/10000], Loss: 0.5477362275123596\n",
      "Epoch [3293/10000], Loss: 0.5476065278053284\n",
      "Epoch [3294/10000], Loss: 0.5474770069122314\n",
      "Epoch [3295/10000], Loss: 0.5473475456237793\n",
      "Epoch [3296/10000], Loss: 0.5472182035446167\n",
      "Epoch [3297/10000], Loss: 0.5470889806747437\n",
      "Epoch [3298/10000], Loss: 0.5469598770141602\n",
      "Epoch [3299/10000], Loss: 0.5468308925628662\n",
      "Epoch [3300/10000], Loss: 0.5467020273208618\n",
      "Epoch [3301/10000], Loss: 0.546573281288147\n",
      "Epoch [3302/10000], Loss: 0.5464446544647217\n",
      "Epoch [3303/10000], Loss: 0.5463160276412964\n",
      "Epoch [3304/10000], Loss: 0.5461877584457397\n",
      "Epoch [3305/10000], Loss: 0.5460593700408936\n",
      "Epoch [3306/10000], Loss: 0.5459312200546265\n",
      "Epoch [3307/10000], Loss: 0.5458031892776489\n",
      "Epoch [3308/10000], Loss: 0.5456752777099609\n",
      "Epoch [3309/10000], Loss: 0.5455472469329834\n",
      "Epoch [3310/10000], Loss: 0.5454195141792297\n",
      "Epoch [3311/10000], Loss: 0.5452919006347656\n",
      "Epoch [3312/10000], Loss: 0.5451643466949463\n",
      "Epoch [3313/10000], Loss: 0.5450369119644165\n",
      "Epoch [3314/10000], Loss: 0.544909656047821\n",
      "Epoch [3315/10000], Loss: 0.5447824001312256\n",
      "Epoch [3316/10000], Loss: 0.5446553230285645\n",
      "Epoch [3317/10000], Loss: 0.5445282459259033\n",
      "Epoch [3318/10000], Loss: 0.5444014072418213\n",
      "Epoch [3319/10000], Loss: 0.5442745685577393\n",
      "Epoch [3320/10000], Loss: 0.5441479086875916\n",
      "Epoch [3321/10000], Loss: 0.5440212488174438\n",
      "Epoch [3322/10000], Loss: 0.5438947677612305\n",
      "Epoch [3323/10000], Loss: 0.5437684059143066\n",
      "Epoch [3324/10000], Loss: 0.5436421036720276\n",
      "Epoch [3325/10000], Loss: 0.5435159206390381\n",
      "Epoch [3326/10000], Loss: 0.5433898568153381\n",
      "Epoch [3327/10000], Loss: 0.5432639122009277\n",
      "Epoch [3328/10000], Loss: 0.5431380271911621\n",
      "Epoch [3329/10000], Loss: 0.5430123209953308\n",
      "Epoch [3330/10000], Loss: 0.5428866744041443\n",
      "Epoch [3331/10000], Loss: 0.5427610874176025\n",
      "Epoch [3332/10000], Loss: 0.5426356792449951\n",
      "Epoch [3333/10000], Loss: 0.5425103902816772\n",
      "Epoch [3334/10000], Loss: 0.5423851013183594\n",
      "Epoch [3335/10000], Loss: 0.5422599911689758\n",
      "Epoch [3336/10000], Loss: 0.5421349406242371\n",
      "Epoch [3337/10000], Loss: 0.5420100092887878\n",
      "Epoch [3338/10000], Loss: 0.5418851971626282\n",
      "Epoch [3339/10000], Loss: 0.5417604446411133\n",
      "Epoch [3340/10000], Loss: 0.5416358709335327\n",
      "Epoch [3341/10000], Loss: 0.5415112972259521\n",
      "Epoch [3342/10000], Loss: 0.5413868427276611\n",
      "Epoch [3343/10000], Loss: 0.5412625074386597\n",
      "Epoch [3344/10000], Loss: 0.5411383509635925\n",
      "Epoch [3345/10000], Loss: 0.5410141944885254\n",
      "Epoch [3346/10000], Loss: 0.5408902168273926\n",
      "Epoch [3347/10000], Loss: 0.5407662391662598\n",
      "Epoch [3348/10000], Loss: 0.5406424403190613\n",
      "Epoch [3349/10000], Loss: 0.5405187606811523\n",
      "Epoch [3350/10000], Loss: 0.5403950214385986\n",
      "Epoch [3351/10000], Loss: 0.540271520614624\n",
      "Epoch [3352/10000], Loss: 0.540148138999939\n",
      "Epoch [3353/10000], Loss: 0.5400248765945435\n",
      "Epoch [3354/10000], Loss: 0.539901614189148\n",
      "Epoch [3355/10000], Loss: 0.539778470993042\n",
      "Epoch [3356/10000], Loss: 0.5396554470062256\n",
      "Epoch [3357/10000], Loss: 0.539532482624054\n",
      "Epoch [3358/10000], Loss: 0.5394096374511719\n",
      "Epoch [3359/10000], Loss: 0.5392868518829346\n",
      "Epoch [3360/10000], Loss: 0.539164125919342\n",
      "Epoch [3361/10000], Loss: 0.5390414595603943\n",
      "Epoch [3362/10000], Loss: 0.5389189720153809\n",
      "Epoch [3363/10000], Loss: 0.5387964844703674\n",
      "Epoch [3364/10000], Loss: 0.5386740565299988\n",
      "Epoch [3365/10000], Loss: 0.5385518074035645\n",
      "Epoch [3366/10000], Loss: 0.5384294986724854\n",
      "Epoch [3367/10000], Loss: 0.5383073091506958\n",
      "Epoch [3368/10000], Loss: 0.5381852388381958\n",
      "Epoch [3369/10000], Loss: 0.5380632877349854\n",
      "Epoch [3370/10000], Loss: 0.5379413366317749\n",
      "Epoch [3371/10000], Loss: 0.5378195643424988\n",
      "Epoch [3372/10000], Loss: 0.5376978516578674\n",
      "Epoch [3373/10000], Loss: 0.5375763177871704\n",
      "Epoch [3374/10000], Loss: 0.5374548435211182\n",
      "Epoch [3375/10000], Loss: 0.5373334884643555\n",
      "Epoch [3376/10000], Loss: 0.5372122526168823\n",
      "Epoch [3377/10000], Loss: 0.5370911359786987\n",
      "Epoch [3378/10000], Loss: 0.5369700193405151\n",
      "Epoch [3379/10000], Loss: 0.5368490219116211\n",
      "Epoch [3380/10000], Loss: 0.5367281436920166\n",
      "Epoch [3381/10000], Loss: 0.5366073250770569\n",
      "Epoch [3382/10000], Loss: 0.5364866852760315\n",
      "Epoch [3383/10000], Loss: 0.5363661050796509\n",
      "Epoch [3384/10000], Loss: 0.5362457036972046\n",
      "Epoch [3385/10000], Loss: 0.5361253023147583\n",
      "Epoch [3386/10000], Loss: 0.5360050201416016\n",
      "Epoch [3387/10000], Loss: 0.5358849167823792\n",
      "Epoch [3388/10000], Loss: 0.5357649326324463\n",
      "Epoch [3389/10000], Loss: 0.5356450080871582\n",
      "Epoch [3390/10000], Loss: 0.5355251431465149\n",
      "Epoch [3391/10000], Loss: 0.5354053974151611\n",
      "Epoch [3392/10000], Loss: 0.5352857708930969\n",
      "Epoch [3393/10000], Loss: 0.5351662635803223\n",
      "Epoch [3394/10000], Loss: 0.5350468158721924\n",
      "Epoch [3395/10000], Loss: 0.534927487373352\n",
      "Epoch [3396/10000], Loss: 0.5348082780838013\n",
      "Epoch [3397/10000], Loss: 0.53468918800354\n",
      "Epoch [3398/10000], Loss: 0.5345701575279236\n",
      "Epoch [3399/10000], Loss: 0.5344512462615967\n",
      "Epoch [3400/10000], Loss: 0.5343323945999146\n",
      "Epoch [3401/10000], Loss: 0.534213662147522\n",
      "Epoch [3402/10000], Loss: 0.534095048904419\n",
      "Epoch [3403/10000], Loss: 0.5339764356613159\n",
      "Epoch [3404/10000], Loss: 0.533858060836792\n",
      "Epoch [3405/10000], Loss: 0.5337396860122681\n",
      "Epoch [3406/10000], Loss: 0.5336213707923889\n",
      "Epoch [3407/10000], Loss: 0.5335031747817993\n",
      "Epoch [3408/10000], Loss: 0.533385157585144\n",
      "Epoch [3409/10000], Loss: 0.5332671403884888\n",
      "Epoch [3410/10000], Loss: 0.533149242401123\n",
      "Epoch [3411/10000], Loss: 0.5330313444137573\n",
      "Epoch [3412/10000], Loss: 0.5329136848449707\n",
      "Epoch [3413/10000], Loss: 0.5327960252761841\n",
      "Epoch [3414/10000], Loss: 0.532678484916687\n",
      "Epoch [3415/10000], Loss: 0.5325610637664795\n",
      "Epoch [3416/10000], Loss: 0.532443642616272\n",
      "Epoch [3417/10000], Loss: 0.5323264002799988\n",
      "Epoch [3418/10000], Loss: 0.5322092771530151\n",
      "Epoch [3419/10000], Loss: 0.5320921540260315\n",
      "Epoch [3420/10000], Loss: 0.5319751501083374\n",
      "Epoch [3421/10000], Loss: 0.5318582653999329\n",
      "Epoch [3422/10000], Loss: 0.5317413806915283\n",
      "Epoch [3423/10000], Loss: 0.5316247344017029\n",
      "Epoch [3424/10000], Loss: 0.5315080285072327\n",
      "Epoch [3425/10000], Loss: 0.5313915014266968\n",
      "Epoch [3426/10000], Loss: 0.5312750339508057\n",
      "Epoch [3427/10000], Loss: 0.5311586856842041\n",
      "Epoch [3428/10000], Loss: 0.5310423374176025\n",
      "Epoch [3429/10000], Loss: 0.5309260487556458\n",
      "Epoch [3430/10000], Loss: 0.5308098793029785\n",
      "Epoch [3431/10000], Loss: 0.5306937098503113\n",
      "Epoch [3432/10000], Loss: 0.5305776596069336\n",
      "Epoch [3433/10000], Loss: 0.5304617881774902\n",
      "Epoch [3434/10000], Loss: 0.5303459167480469\n",
      "Epoch [3435/10000], Loss: 0.5302302241325378\n",
      "Epoch [3436/10000], Loss: 0.5301145911216736\n",
      "Epoch [3437/10000], Loss: 0.5299990177154541\n",
      "Epoch [3438/10000], Loss: 0.529883623123169\n",
      "Epoch [3439/10000], Loss: 0.5297682285308838\n",
      "Epoch [3440/10000], Loss: 0.5296529531478882\n",
      "Epoch [3441/10000], Loss: 0.5295377969741821\n",
      "Epoch [3442/10000], Loss: 0.5294227004051208\n",
      "Epoch [3443/10000], Loss: 0.5293076038360596\n",
      "Epoch [3444/10000], Loss: 0.5291928052902222\n",
      "Epoch [3445/10000], Loss: 0.5290778875350952\n",
      "Epoch [3446/10000], Loss: 0.5289631485939026\n",
      "Epoch [3447/10000], Loss: 0.52884840965271\n",
      "Epoch [3448/10000], Loss: 0.5287339091300964\n",
      "Epoch [3449/10000], Loss: 0.5286192893981934\n",
      "Epoch [3450/10000], Loss: 0.5285049080848694\n",
      "Epoch [3451/10000], Loss: 0.528390645980835\n",
      "Epoch [3452/10000], Loss: 0.5282763838768005\n",
      "Epoch [3453/10000], Loss: 0.5281623005867004\n",
      "Epoch [3454/10000], Loss: 0.5280482769012451\n",
      "Epoch [3455/10000], Loss: 0.5279343128204346\n",
      "Epoch [3456/10000], Loss: 0.5278205275535583\n",
      "Epoch [3457/10000], Loss: 0.5277067422866821\n",
      "Epoch [3458/10000], Loss: 0.5275930166244507\n",
      "Epoch [3459/10000], Loss: 0.5274794101715088\n",
      "Epoch [3460/10000], Loss: 0.5273659229278564\n",
      "Epoch [3461/10000], Loss: 0.5272525548934937\n",
      "Epoch [3462/10000], Loss: 0.5271391868591309\n",
      "Epoch [3463/10000], Loss: 0.5270259380340576\n",
      "Epoch [3464/10000], Loss: 0.5269128084182739\n",
      "Epoch [3465/10000], Loss: 0.5267997980117798\n",
      "Epoch [3466/10000], Loss: 0.5266868472099304\n",
      "Epoch [3467/10000], Loss: 0.526573896408081\n",
      "Epoch [3468/10000], Loss: 0.5264611840248108\n",
      "Epoch [3469/10000], Loss: 0.5263484716415405\n",
      "Epoch [3470/10000], Loss: 0.5262359380722046\n",
      "Epoch [3471/10000], Loss: 0.5261234045028687\n",
      "Epoch [3472/10000], Loss: 0.5260109901428223\n",
      "Epoch [3473/10000], Loss: 0.5258985757827759\n",
      "Epoch [3474/10000], Loss: 0.525786280632019\n",
      "Epoch [3475/10000], Loss: 0.5256741046905518\n",
      "Epoch [3476/10000], Loss: 0.5255619883537292\n",
      "Epoch [3477/10000], Loss: 0.5254499912261963\n",
      "Epoch [3478/10000], Loss: 0.5253380537033081\n",
      "Epoch [3479/10000], Loss: 0.5252261757850647\n",
      "Epoch [3480/10000], Loss: 0.5251144170761108\n",
      "Epoch [3481/10000], Loss: 0.5250027179718018\n",
      "Epoch [3482/10000], Loss: 0.5248910188674927\n",
      "Epoch [3483/10000], Loss: 0.5247794389724731\n",
      "Epoch [3484/10000], Loss: 0.5246679782867432\n",
      "Epoch [3485/10000], Loss: 0.5245566368103027\n",
      "Epoch [3486/10000], Loss: 0.5244452953338623\n",
      "Epoch [3487/10000], Loss: 0.5243340730667114\n",
      "Epoch [3488/10000], Loss: 0.5242230296134949\n",
      "Epoch [3489/10000], Loss: 0.5241119861602783\n",
      "Epoch [3490/10000], Loss: 0.5240011215209961\n",
      "Epoch [3491/10000], Loss: 0.5238902568817139\n",
      "Epoch [3492/10000], Loss: 0.5237794518470764\n",
      "Epoch [3493/10000], Loss: 0.5236688256263733\n",
      "Epoch [3494/10000], Loss: 0.5235581994056702\n",
      "Epoch [3495/10000], Loss: 0.5234477519989014\n",
      "Epoch [3496/10000], Loss: 0.5233373641967773\n",
      "Epoch [3497/10000], Loss: 0.5232270359992981\n",
      "Epoch [3498/10000], Loss: 0.5231168270111084\n",
      "Epoch [3499/10000], Loss: 0.5230066776275635\n",
      "Epoch [3500/10000], Loss: 0.5228966474533081\n",
      "Epoch [3501/10000], Loss: 0.5227866768836975\n",
      "Epoch [3502/10000], Loss: 0.5226768255233765\n",
      "Epoch [3503/10000], Loss: 0.5225670337677002\n",
      "Epoch [3504/10000], Loss: 0.5224573612213135\n",
      "Epoch [3505/10000], Loss: 0.5223476886749268\n",
      "Epoch [3506/10000], Loss: 0.5222381949424744\n",
      "Epoch [3507/10000], Loss: 0.5221288204193115\n",
      "Epoch [3508/10000], Loss: 0.5220193862915039\n",
      "Epoch [3509/10000], Loss: 0.5219100713729858\n",
      "Epoch [3510/10000], Loss: 0.5218008756637573\n",
      "Epoch [3511/10000], Loss: 0.5216916799545288\n",
      "Epoch [3512/10000], Loss: 0.5215826034545898\n",
      "Epoch [3513/10000], Loss: 0.5214736461639404\n",
      "Epoch [3514/10000], Loss: 0.5213648080825806\n",
      "Epoch [3515/10000], Loss: 0.5212559700012207\n",
      "Epoch [3516/10000], Loss: 0.5211472511291504\n",
      "Epoch [3517/10000], Loss: 0.5210385918617249\n",
      "Epoch [3518/10000], Loss: 0.5209299325942993\n",
      "Epoch [3519/10000], Loss: 0.5208215117454529\n",
      "Epoch [3520/10000], Loss: 0.5207130312919617\n",
      "Epoch [3521/10000], Loss: 0.52060467004776\n",
      "Epoch [3522/10000], Loss: 0.5204964280128479\n",
      "Epoch [3523/10000], Loss: 0.5203882455825806\n",
      "Epoch [3524/10000], Loss: 0.520280122756958\n",
      "Epoch [3525/10000], Loss: 0.520172119140625\n",
      "Epoch [3526/10000], Loss: 0.5200642347335815\n",
      "Epoch [3527/10000], Loss: 0.5199563503265381\n",
      "Epoch [3528/10000], Loss: 0.5198487043380737\n",
      "Epoch [3529/10000], Loss: 0.5197409987449646\n",
      "Epoch [3530/10000], Loss: 0.5196335315704346\n",
      "Epoch [3531/10000], Loss: 0.5195260047912598\n",
      "Epoch [3532/10000], Loss: 0.5194185376167297\n",
      "Epoch [3533/10000], Loss: 0.519311249256134\n",
      "Epoch [3534/10000], Loss: 0.5192040205001831\n",
      "Epoch [3535/10000], Loss: 0.519096851348877\n",
      "Epoch [3536/10000], Loss: 0.5189897418022156\n",
      "Epoch [3537/10000], Loss: 0.5188827514648438\n",
      "Epoch [3538/10000], Loss: 0.5187758803367615\n",
      "Epoch [3539/10000], Loss: 0.5186688899993896\n",
      "Epoch [3540/10000], Loss: 0.5185621976852417\n",
      "Epoch [3541/10000], Loss: 0.5184555053710938\n",
      "Epoch [3542/10000], Loss: 0.5183489322662354\n",
      "Epoch [3543/10000], Loss: 0.518242359161377\n",
      "Epoch [3544/10000], Loss: 0.5181359648704529\n",
      "Epoch [3545/10000], Loss: 0.5180295705795288\n",
      "Epoch [3546/10000], Loss: 0.5179233551025391\n",
      "Epoch [3547/10000], Loss: 0.5178170800209045\n",
      "Epoch [3548/10000], Loss: 0.5177109241485596\n",
      "Epoch [3549/10000], Loss: 0.5176048874855042\n",
      "Epoch [3550/10000], Loss: 0.5174989700317383\n",
      "Epoch [3551/10000], Loss: 0.5173931121826172\n",
      "Epoch [3552/10000], Loss: 0.5172872543334961\n",
      "Epoch [3553/10000], Loss: 0.5171815752983093\n",
      "Epoch [3554/10000], Loss: 0.5170758962631226\n",
      "Epoch [3555/10000], Loss: 0.5169703960418701\n",
      "Epoch [3556/10000], Loss: 0.5168648958206177\n",
      "Epoch [3557/10000], Loss: 0.5167593955993652\n",
      "Epoch [3558/10000], Loss: 0.5166541337966919\n",
      "Epoch [3559/10000], Loss: 0.5165488719940186\n",
      "Epoch [3560/10000], Loss: 0.5164437294006348\n",
      "Epoch [3561/10000], Loss: 0.516338586807251\n",
      "Epoch [3562/10000], Loss: 0.5162335634231567\n",
      "Epoch [3563/10000], Loss: 0.5161285996437073\n",
      "Epoch [3564/10000], Loss: 0.5160237550735474\n",
      "Epoch [3565/10000], Loss: 0.5159189701080322\n",
      "Epoch [3566/10000], Loss: 0.5158142447471619\n",
      "Epoch [3567/10000], Loss: 0.515709638595581\n",
      "Epoch [3568/10000], Loss: 0.5156049728393555\n",
      "Epoch [3569/10000], Loss: 0.515500545501709\n",
      "Epoch [3570/10000], Loss: 0.5153961181640625\n",
      "Epoch [3571/10000], Loss: 0.5152919292449951\n",
      "Epoch [3572/10000], Loss: 0.5151877403259277\n",
      "Epoch [3573/10000], Loss: 0.5150835514068604\n",
      "Epoch [3574/10000], Loss: 0.5149795413017273\n",
      "Epoch [3575/10000], Loss: 0.5148756504058838\n",
      "Epoch [3576/10000], Loss: 0.5147716999053955\n",
      "Epoch [3577/10000], Loss: 0.5146679878234863\n",
      "Epoch [3578/10000], Loss: 0.5145642757415771\n",
      "Epoch [3579/10000], Loss: 0.5144606828689575\n",
      "Epoch [3580/10000], Loss: 0.5143570303916931\n",
      "Epoch [3581/10000], Loss: 0.5142536163330078\n",
      "Epoch [3582/10000], Loss: 0.5141502618789673\n",
      "Epoch [3583/10000], Loss: 0.5140469074249268\n",
      "Epoch [3584/10000], Loss: 0.5139436721801758\n",
      "Epoch [3585/10000], Loss: 0.5138405561447144\n",
      "Epoch [3586/10000], Loss: 0.5137374997138977\n",
      "Epoch [3587/10000], Loss: 0.513634443283081\n",
      "Epoch [3588/10000], Loss: 0.5135315656661987\n",
      "Epoch [3589/10000], Loss: 0.5134286880493164\n",
      "Epoch [3590/10000], Loss: 0.5133259296417236\n",
      "Epoch [3591/10000], Loss: 0.5132231712341309\n",
      "Epoch [3592/10000], Loss: 0.5131206512451172\n",
      "Epoch [3593/10000], Loss: 0.5130179524421692\n",
      "Epoch [3594/10000], Loss: 0.5129154920578003\n",
      "Epoch [3595/10000], Loss: 0.512813150882721\n",
      "Epoch [3596/10000], Loss: 0.5127108097076416\n",
      "Epoch [3597/10000], Loss: 0.512608528137207\n",
      "Epoch [3598/10000], Loss: 0.5125064253807068\n",
      "Epoch [3599/10000], Loss: 0.5124043226242065\n",
      "Epoch [3600/10000], Loss: 0.5123022198677063\n",
      "Epoch [3601/10000], Loss: 0.5122002959251404\n",
      "Epoch [3602/10000], Loss: 0.5120984315872192\n",
      "Epoch [3603/10000], Loss: 0.5119965076446533\n",
      "Epoch [3604/10000], Loss: 0.5118948221206665\n",
      "Epoch [3605/10000], Loss: 0.5117931365966797\n",
      "Epoch [3606/10000], Loss: 0.5116915702819824\n",
      "Epoch [3607/10000], Loss: 0.5115900039672852\n",
      "Epoch [3608/10000], Loss: 0.5114884972572327\n",
      "Epoch [3609/10000], Loss: 0.5113871097564697\n",
      "Epoch [3610/10000], Loss: 0.5112857818603516\n",
      "Epoch [3611/10000], Loss: 0.511184573173523\n",
      "Epoch [3612/10000], Loss: 0.5110833644866943\n",
      "Epoch [3613/10000], Loss: 0.5109822750091553\n",
      "Epoch [3614/10000], Loss: 0.5108813047409058\n",
      "Epoch [3615/10000], Loss: 0.5107803344726562\n",
      "Epoch [3616/10000], Loss: 0.5106794834136963\n",
      "Epoch [3617/10000], Loss: 0.5105786919593811\n",
      "Epoch [3618/10000], Loss: 0.5104780197143555\n",
      "Epoch [3619/10000], Loss: 0.5103774070739746\n",
      "Epoch [3620/10000], Loss: 0.5102767944335938\n",
      "Epoch [3621/10000], Loss: 0.5101763606071472\n",
      "Epoch [3622/10000], Loss: 0.5100759863853455\n",
      "Epoch [3623/10000], Loss: 0.5099756717681885\n",
      "Epoch [3624/10000], Loss: 0.5098754167556763\n",
      "Epoch [3625/10000], Loss: 0.5097752809524536\n",
      "Epoch [3626/10000], Loss: 0.509675145149231\n",
      "Epoch [3627/10000], Loss: 0.5095751285552979\n",
      "Epoch [3628/10000], Loss: 0.5094751715660095\n",
      "Epoch [3629/10000], Loss: 0.5093753337860107\n",
      "Epoch [3630/10000], Loss: 0.509275496006012\n",
      "Epoch [3631/10000], Loss: 0.5091757774353027\n",
      "Epoch [3632/10000], Loss: 0.5090761184692383\n",
      "Epoch [3633/10000], Loss: 0.5089765787124634\n",
      "Epoch [3634/10000], Loss: 0.508877158164978\n",
      "Epoch [3635/10000], Loss: 0.5087777972221375\n",
      "Epoch [3636/10000], Loss: 0.5086784362792969\n",
      "Epoch [3637/10000], Loss: 0.5085792541503906\n",
      "Epoch [3638/10000], Loss: 0.5084800720214844\n",
      "Epoch [3639/10000], Loss: 0.5083810091018677\n",
      "Epoch [3640/10000], Loss: 0.508281946182251\n",
      "Epoch [3641/10000], Loss: 0.5081830620765686\n",
      "Epoch [3642/10000], Loss: 0.5080842971801758\n",
      "Epoch [3643/10000], Loss: 0.5079854726791382\n",
      "Epoch [3644/10000], Loss: 0.5078868865966797\n",
      "Epoch [3645/10000], Loss: 0.5077883005142212\n",
      "Epoch [3646/10000], Loss: 0.5076898336410522\n",
      "Epoch [3647/10000], Loss: 0.5075913667678833\n",
      "Epoch [3648/10000], Loss: 0.5074930191040039\n",
      "Epoch [3649/10000], Loss: 0.5073947906494141\n",
      "Epoch [3650/10000], Loss: 0.5072965621948242\n",
      "Epoch [3651/10000], Loss: 0.5071985125541687\n",
      "Epoch [3652/10000], Loss: 0.5071004629135132\n",
      "Epoch [3653/10000], Loss: 0.507002592086792\n",
      "Epoch [3654/10000], Loss: 0.5069047212600708\n",
      "Epoch [3655/10000], Loss: 0.5068069100379944\n",
      "Epoch [3656/10000], Loss: 0.5067092180252075\n",
      "Epoch [3657/10000], Loss: 0.5066115856170654\n",
      "Epoch [3658/10000], Loss: 0.5065140724182129\n",
      "Epoch [3659/10000], Loss: 0.5064164996147156\n",
      "Epoch [3660/10000], Loss: 0.5063190460205078\n",
      "Epoch [3661/10000], Loss: 0.5062217712402344\n",
      "Epoch [3662/10000], Loss: 0.5061244964599609\n",
      "Epoch [3663/10000], Loss: 0.5060272812843323\n",
      "Epoch [3664/10000], Loss: 0.5059301853179932\n",
      "Epoch [3665/10000], Loss: 0.5058332085609436\n",
      "Epoch [3666/10000], Loss: 0.505736231803894\n",
      "Epoch [3667/10000], Loss: 0.5056394338607788\n",
      "Epoch [3668/10000], Loss: 0.505542516708374\n",
      "Epoch [3669/10000], Loss: 0.5054458975791931\n",
      "Epoch [3670/10000], Loss: 0.5053492188453674\n",
      "Epoch [3671/10000], Loss: 0.5052525997161865\n",
      "Epoch [3672/10000], Loss: 0.5051561594009399\n",
      "Epoch [3673/10000], Loss: 0.5050597786903381\n",
      "Epoch [3674/10000], Loss: 0.5049633979797363\n",
      "Epoch [3675/10000], Loss: 0.5048670768737793\n",
      "Epoch [3676/10000], Loss: 0.5047709941864014\n",
      "Epoch [3677/10000], Loss: 0.5046749114990234\n",
      "Epoch [3678/10000], Loss: 0.5045788288116455\n",
      "Epoch [3679/10000], Loss: 0.5044828653335571\n",
      "Epoch [3680/10000], Loss: 0.5043870210647583\n",
      "Epoch [3681/10000], Loss: 0.5042911767959595\n",
      "Epoch [3682/10000], Loss: 0.5041954517364502\n",
      "Epoch [3683/10000], Loss: 0.5040998458862305\n",
      "Epoch [3684/10000], Loss: 0.5040042400360107\n",
      "Epoch [3685/10000], Loss: 0.5039087533950806\n",
      "Epoch [3686/10000], Loss: 0.5038132667541504\n",
      "Epoch [3687/10000], Loss: 0.5037178993225098\n",
      "Epoch [3688/10000], Loss: 0.5036226511001587\n",
      "Epoch [3689/10000], Loss: 0.5035275220870972\n",
      "Epoch [3690/10000], Loss: 0.5034322738647461\n",
      "Epoch [3691/10000], Loss: 0.5033372640609741\n",
      "Epoch [3692/10000], Loss: 0.5032422542572021\n",
      "Epoch [3693/10000], Loss: 0.5031473636627197\n",
      "Epoch [3694/10000], Loss: 0.5030525326728821\n",
      "Epoch [3695/10000], Loss: 0.5029577612876892\n",
      "Epoch [3696/10000], Loss: 0.5028631687164307\n",
      "Epoch [3697/10000], Loss: 0.5027685165405273\n",
      "Epoch [3698/10000], Loss: 0.5026739835739136\n",
      "Epoch [3699/10000], Loss: 0.5025795102119446\n",
      "Epoch [3700/10000], Loss: 0.5024851560592651\n",
      "Epoch [3701/10000], Loss: 0.5023908615112305\n",
      "Epoch [3702/10000], Loss: 0.5022966265678406\n",
      "Epoch [3703/10000], Loss: 0.5022025108337402\n",
      "Epoch [3704/10000], Loss: 0.5021083354949951\n",
      "Epoch [3705/10000], Loss: 0.5020143389701843\n",
      "Epoch [3706/10000], Loss: 0.5019204020500183\n",
      "Epoch [3707/10000], Loss: 0.5018265247344971\n",
      "Epoch [3708/10000], Loss: 0.5017327666282654\n",
      "Epoch [3709/10000], Loss: 0.5016390681266785\n",
      "Epoch [3710/10000], Loss: 0.5015454292297363\n",
      "Epoch [3711/10000], Loss: 0.501451849937439\n",
      "Epoch [3712/10000], Loss: 0.5013583302497864\n",
      "Epoch [3713/10000], Loss: 0.5012648701667786\n",
      "Epoch [3714/10000], Loss: 0.5011715888977051\n",
      "Epoch [3715/10000], Loss: 0.5010782480239868\n",
      "Epoch [3716/10000], Loss: 0.5009849667549133\n",
      "Epoch [3717/10000], Loss: 0.5008918642997742\n",
      "Epoch [3718/10000], Loss: 0.5007988214492798\n",
      "Epoch [3719/10000], Loss: 0.5007057785987854\n",
      "Epoch [3720/10000], Loss: 0.5006129145622253\n",
      "Epoch [3721/10000], Loss: 0.5005199909210205\n",
      "Epoch [3722/10000], Loss: 0.50042724609375\n",
      "Epoch [3723/10000], Loss: 0.5003345012664795\n",
      "Epoch [3724/10000], Loss: 0.5002418756484985\n",
      "Epoch [3725/10000], Loss: 0.5001492500305176\n",
      "Epoch [3726/10000], Loss: 0.5000567436218262\n",
      "Epoch [3727/10000], Loss: 0.49996429681777954\n",
      "Epoch [3728/10000], Loss: 0.4998719096183777\n",
      "Epoch [3729/10000], Loss: 0.4997795820236206\n",
      "Epoch [3730/10000], Loss: 0.4996873736381531\n",
      "Epoch [3731/10000], Loss: 0.4995952248573303\n",
      "Epoch [3732/10000], Loss: 0.49950307607650757\n",
      "Epoch [3733/10000], Loss: 0.49941104650497437\n",
      "Epoch [3734/10000], Loss: 0.49931901693344116\n",
      "Epoch [3735/10000], Loss: 0.4992271661758423\n",
      "Epoch [3736/10000], Loss: 0.4991353750228882\n",
      "Epoch [3737/10000], Loss: 0.4990435838699341\n",
      "Epoch [3738/10000], Loss: 0.4989519715309143\n",
      "Epoch [3739/10000], Loss: 0.49886029958724976\n",
      "Epoch [3740/10000], Loss: 0.49876874685287476\n",
      "Epoch [3741/10000], Loss: 0.49867719411849976\n",
      "Epoch [3742/10000], Loss: 0.4985858201980591\n",
      "Epoch [3743/10000], Loss: 0.4984944462776184\n",
      "Epoch [3744/10000], Loss: 0.4984031319618225\n",
      "Epoch [3745/10000], Loss: 0.4983118772506714\n",
      "Epoch [3746/10000], Loss: 0.4982207417488098\n",
      "Epoch [3747/10000], Loss: 0.498129665851593\n",
      "Epoch [3748/10000], Loss: 0.4980385899543762\n",
      "Epoch [3749/10000], Loss: 0.497947633266449\n",
      "Epoch [3750/10000], Loss: 0.4978567361831665\n",
      "Epoch [3751/10000], Loss: 0.4977659583091736\n",
      "Epoch [3752/10000], Loss: 0.49767524003982544\n",
      "Epoch [3753/10000], Loss: 0.4975845217704773\n",
      "Epoch [3754/10000], Loss: 0.4974939227104187\n",
      "Epoch [3755/10000], Loss: 0.4974033236503601\n",
      "Epoch [3756/10000], Loss: 0.49731290340423584\n",
      "Epoch [3757/10000], Loss: 0.4972224831581116\n",
      "Epoch [3758/10000], Loss: 0.49713218212127686\n",
      "Epoch [3759/10000], Loss: 0.49704182147979736\n",
      "Epoch [3760/10000], Loss: 0.4969516396522522\n",
      "Epoch [3761/10000], Loss: 0.49686145782470703\n",
      "Epoch [3762/10000], Loss: 0.4967714548110962\n",
      "Epoch [3763/10000], Loss: 0.4966813921928406\n",
      "Epoch [3764/10000], Loss: 0.4965914487838745\n",
      "Epoch [3765/10000], Loss: 0.4965015649795532\n",
      "Epoch [3766/10000], Loss: 0.4964118003845215\n",
      "Epoch [3767/10000], Loss: 0.4963220953941345\n",
      "Epoch [3768/10000], Loss: 0.4962323307991028\n",
      "Epoch [3769/10000], Loss: 0.49614274501800537\n",
      "Epoch [3770/10000], Loss: 0.49605321884155273\n",
      "Epoch [3771/10000], Loss: 0.4959637522697449\n",
      "Epoch [3772/10000], Loss: 0.4958743453025818\n",
      "Epoch [3773/10000], Loss: 0.4957849979400635\n",
      "Epoch [3774/10000], Loss: 0.4956957697868347\n",
      "Epoch [3775/10000], Loss: 0.49560654163360596\n",
      "Epoch [3776/10000], Loss: 0.495517373085022\n",
      "Epoch [3777/10000], Loss: 0.49542826414108276\n",
      "Epoch [3778/10000], Loss: 0.4953392744064331\n",
      "Epoch [3779/10000], Loss: 0.4952503442764282\n",
      "Epoch [3780/10000], Loss: 0.4951615333557129\n",
      "Epoch [3781/10000], Loss: 0.49507272243499756\n",
      "Epoch [3782/10000], Loss: 0.494983971118927\n",
      "Epoch [3783/10000], Loss: 0.4948952794075012\n",
      "Epoch [3784/10000], Loss: 0.4948066473007202\n",
      "Epoch [3785/10000], Loss: 0.494718074798584\n",
      "Epoch [3786/10000], Loss: 0.4946296215057373\n",
      "Epoch [3787/10000], Loss: 0.4945412278175354\n",
      "Epoch [3788/10000], Loss: 0.4944527745246887\n",
      "Epoch [3789/10000], Loss: 0.49436455965042114\n",
      "Epoch [3790/10000], Loss: 0.494276225566864\n",
      "Epoch [3791/10000], Loss: 0.494188129901886\n",
      "Epoch [3792/10000], Loss: 0.4940999746322632\n",
      "Epoch [3793/10000], Loss: 0.49401193857192993\n",
      "Epoch [3794/10000], Loss: 0.49392402172088623\n",
      "Epoch [3795/10000], Loss: 0.49383604526519775\n",
      "Epoch [3796/10000], Loss: 0.4937483072280884\n",
      "Epoch [3797/10000], Loss: 0.49366050958633423\n",
      "Epoch [3798/10000], Loss: 0.49357277154922485\n",
      "Epoch [3799/10000], Loss: 0.49348515272140503\n",
      "Epoch [3800/10000], Loss: 0.49339759349823\n",
      "Epoch [3801/10000], Loss: 0.49330997467041016\n",
      "Epoch [3802/10000], Loss: 0.49322259426116943\n",
      "Epoch [3803/10000], Loss: 0.49313515424728394\n",
      "Epoch [3804/10000], Loss: 0.493047833442688\n",
      "Epoch [3805/10000], Loss: 0.4929605722427368\n",
      "Epoch [3806/10000], Loss: 0.4928733706474304\n",
      "Epoch [3807/10000], Loss: 0.492786169052124\n",
      "Epoch [3808/10000], Loss: 0.4926990866661072\n",
      "Epoch [3809/10000], Loss: 0.4926121234893799\n",
      "Epoch [3810/10000], Loss: 0.4925251603126526\n",
      "Epoch [3811/10000], Loss: 0.4924381971359253\n",
      "Epoch [3812/10000], Loss: 0.4923514127731323\n",
      "Epoch [3813/10000], Loss: 0.4922645688056946\n",
      "Epoch [3814/10000], Loss: 0.49217790365219116\n",
      "Epoch [3815/10000], Loss: 0.4920911192893982\n",
      "Epoch [3816/10000], Loss: 0.4920046329498291\n",
      "Epoch [3817/10000], Loss: 0.49191808700561523\n",
      "Epoch [3818/10000], Loss: 0.49183160066604614\n",
      "Epoch [3819/10000], Loss: 0.4917451739311218\n",
      "Epoch [3820/10000], Loss: 0.49165886640548706\n",
      "Epoch [3821/10000], Loss: 0.4915725588798523\n",
      "Epoch [3822/10000], Loss: 0.4914863109588623\n",
      "Epoch [3823/10000], Loss: 0.49140018224716187\n",
      "Epoch [3824/10000], Loss: 0.4913140535354614\n",
      "Epoch [3825/10000], Loss: 0.49122798442840576\n",
      "Epoch [3826/10000], Loss: 0.4911419153213501\n",
      "Epoch [3827/10000], Loss: 0.49105602502822876\n",
      "Epoch [3828/10000], Loss: 0.490970253944397\n",
      "Epoch [3829/10000], Loss: 0.4908844232559204\n",
      "Epoch [3830/10000], Loss: 0.4907987117767334\n",
      "Epoch [3831/10000], Loss: 0.4907130002975464\n",
      "Epoch [3832/10000], Loss: 0.49062734842300415\n",
      "Epoch [3833/10000], Loss: 0.4905417561531067\n",
      "Epoch [3834/10000], Loss: 0.4904562830924988\n",
      "Epoch [3835/10000], Loss: 0.49037081003189087\n",
      "Epoch [3836/10000], Loss: 0.4902854561805725\n",
      "Epoch [3837/10000], Loss: 0.49020010232925415\n",
      "Epoch [3838/10000], Loss: 0.49011480808258057\n",
      "Epoch [3839/10000], Loss: 0.49002957344055176\n",
      "Epoch [3840/10000], Loss: 0.4899443984031677\n",
      "Epoch [3841/10000], Loss: 0.48985934257507324\n",
      "Epoch [3842/10000], Loss: 0.48977428674697876\n",
      "Epoch [3843/10000], Loss: 0.48968929052352905\n",
      "Epoch [3844/10000], Loss: 0.4896044135093689\n",
      "Epoch [3845/10000], Loss: 0.48951947689056396\n",
      "Epoch [3846/10000], Loss: 0.4894346594810486\n",
      "Epoch [3847/10000], Loss: 0.489349901676178\n",
      "Epoch [3848/10000], Loss: 0.48926520347595215\n",
      "Epoch [3849/10000], Loss: 0.4891805052757263\n",
      "Epoch [3850/10000], Loss: 0.4890959858894348\n",
      "Epoch [3851/10000], Loss: 0.4890114665031433\n",
      "Epoch [3852/10000], Loss: 0.4889270067214966\n",
      "Epoch [3853/10000], Loss: 0.48884260654449463\n",
      "Epoch [3854/10000], Loss: 0.4887582063674927\n",
      "Epoch [3855/10000], Loss: 0.48867398500442505\n",
      "Epoch [3856/10000], Loss: 0.48858970403671265\n",
      "Epoch [3857/10000], Loss: 0.488505482673645\n",
      "Epoch [3858/10000], Loss: 0.48842138051986694\n",
      "Epoch [3859/10000], Loss: 0.4883372187614441\n",
      "Epoch [3860/10000], Loss: 0.48825323581695557\n",
      "Epoch [3861/10000], Loss: 0.48816925287246704\n",
      "Epoch [3862/10000], Loss: 0.4880853295326233\n",
      "Epoch [3863/10000], Loss: 0.4880014657974243\n",
      "Epoch [3864/10000], Loss: 0.4879177212715149\n",
      "Epoch [3865/10000], Loss: 0.4878339171409607\n",
      "Epoch [3866/10000], Loss: 0.48775023221969604\n",
      "Epoch [3867/10000], Loss: 0.48766660690307617\n",
      "Epoch [3868/10000], Loss: 0.48758310079574585\n",
      "Epoch [3869/10000], Loss: 0.487499475479126\n",
      "Epoch [3870/10000], Loss: 0.4874160885810852\n",
      "Epoch [3871/10000], Loss: 0.48733264207839966\n",
      "Epoch [3872/10000], Loss: 0.4872492551803589\n",
      "Epoch [3873/10000], Loss: 0.48716598749160767\n",
      "Epoch [3874/10000], Loss: 0.48708266019821167\n",
      "Epoch [3875/10000], Loss: 0.4869995713233948\n",
      "Epoch [3876/10000], Loss: 0.4869164228439331\n",
      "Epoch [3877/10000], Loss: 0.4868333339691162\n",
      "Epoch [3878/10000], Loss: 0.4867502450942993\n",
      "Epoch [3879/10000], Loss: 0.48666733503341675\n",
      "Epoch [3880/10000], Loss: 0.4865844249725342\n",
      "Epoch [3881/10000], Loss: 0.4865015745162964\n",
      "Epoch [3882/10000], Loss: 0.48641878366470337\n",
      "Epoch [3883/10000], Loss: 0.4863360524177551\n",
      "Epoch [3884/10000], Loss: 0.4862533211708069\n",
      "Epoch [3885/10000], Loss: 0.4861706495285034\n",
      "Epoch [3886/10000], Loss: 0.4860881567001343\n",
      "Epoch [3887/10000], Loss: 0.48600560426712036\n",
      "Epoch [3888/10000], Loss: 0.4859231114387512\n",
      "Epoch [3889/10000], Loss: 0.48584067821502686\n",
      "Epoch [3890/10000], Loss: 0.48575830459594727\n",
      "Epoch [3891/10000], Loss: 0.4856759309768677\n",
      "Epoch [3892/10000], Loss: 0.48559367656707764\n",
      "Epoch [3893/10000], Loss: 0.4855114817619324\n",
      "Epoch [3894/10000], Loss: 0.4854293465614319\n",
      "Epoch [3895/10000], Loss: 0.4853472113609314\n",
      "Epoch [3896/10000], Loss: 0.48526519536972046\n",
      "Epoch [3897/10000], Loss: 0.48518311977386475\n",
      "Epoch [3898/10000], Loss: 0.4851011633872986\n",
      "Epoch [3899/10000], Loss: 0.4850192666053772\n",
      "Epoch [3900/10000], Loss: 0.48493748903274536\n",
      "Epoch [3901/10000], Loss: 0.4848557114601135\n",
      "Epoch [3902/10000], Loss: 0.48477399349212646\n",
      "Epoch [3903/10000], Loss: 0.4846922755241394\n",
      "Epoch [3904/10000], Loss: 0.4846106767654419\n",
      "Epoch [3905/10000], Loss: 0.48452913761138916\n",
      "Epoch [3906/10000], Loss: 0.48444753885269165\n",
      "Epoch [3907/10000], Loss: 0.4843660593032837\n",
      "Epoch [3908/10000], Loss: 0.4842846989631653\n",
      "Epoch [3909/10000], Loss: 0.48420339822769165\n",
      "Epoch [3910/10000], Loss: 0.48412203788757324\n",
      "Epoch [3911/10000], Loss: 0.4840407371520996\n",
      "Epoch [3912/10000], Loss: 0.48395949602127075\n",
      "Epoch [3913/10000], Loss: 0.48387831449508667\n",
      "Epoch [3914/10000], Loss: 0.48379719257354736\n",
      "Epoch [3915/10000], Loss: 0.48371613025665283\n",
      "Epoch [3916/10000], Loss: 0.4836351275444031\n",
      "Epoch [3917/10000], Loss: 0.4835541844367981\n",
      "Epoch [3918/10000], Loss: 0.48347318172454834\n",
      "Epoch [3919/10000], Loss: 0.4833923578262329\n",
      "Epoch [3920/10000], Loss: 0.48331159353256226\n",
      "Epoch [3921/10000], Loss: 0.4832307696342468\n",
      "Epoch [3922/10000], Loss: 0.48315006494522095\n",
      "Epoch [3923/10000], Loss: 0.48306941986083984\n",
      "Epoch [3924/10000], Loss: 0.4829888343811035\n",
      "Epoch [3925/10000], Loss: 0.4829082489013672\n",
      "Epoch [3926/10000], Loss: 0.48282772302627563\n",
      "Epoch [3927/10000], Loss: 0.48274731636047363\n",
      "Epoch [3928/10000], Loss: 0.4826669692993164\n",
      "Epoch [3929/10000], Loss: 0.4825865626335144\n",
      "Epoch [3930/10000], Loss: 0.4825062155723572\n",
      "Epoch [3931/10000], Loss: 0.4824260473251343\n",
      "Epoch [3932/10000], Loss: 0.4823457598686218\n",
      "Epoch [3933/10000], Loss: 0.4822656512260437\n",
      "Epoch [3934/10000], Loss: 0.4821855425834656\n",
      "Epoch [3935/10000], Loss: 0.4821054935455322\n",
      "Epoch [3936/10000], Loss: 0.4820254445075989\n",
      "Epoch [3937/10000], Loss: 0.4819454550743103\n",
      "Epoch [3938/10000], Loss: 0.4818655848503113\n",
      "Epoch [3939/10000], Loss: 0.4817856550216675\n",
      "Epoch [3940/10000], Loss: 0.48170584440231323\n",
      "Epoch [3941/10000], Loss: 0.48162609338760376\n",
      "Epoch [3942/10000], Loss: 0.4815463423728943\n",
      "Epoch [3943/10000], Loss: 0.48146671056747437\n",
      "Epoch [3944/10000], Loss: 0.48138701915740967\n",
      "Epoch [3945/10000], Loss: 0.4813075065612793\n",
      "Epoch [3946/10000], Loss: 0.48122793436050415\n",
      "Epoch [3947/10000], Loss: 0.4811484217643738\n",
      "Epoch [3948/10000], Loss: 0.4810689687728882\n",
      "Epoch [3949/10000], Loss: 0.48098957538604736\n",
      "Epoch [3950/10000], Loss: 0.4809102416038513\n",
      "Epoch [3951/10000], Loss: 0.48083096742630005\n",
      "Epoch [3952/10000], Loss: 0.480751633644104\n",
      "Epoch [3953/10000], Loss: 0.4806724786758423\n",
      "Epoch [3954/10000], Loss: 0.48059338331222534\n",
      "Epoch [3955/10000], Loss: 0.4805142283439636\n",
      "Epoch [3956/10000], Loss: 0.48043519258499146\n",
      "Epoch [3957/10000], Loss: 0.4803561568260193\n",
      "Epoch [3958/10000], Loss: 0.4802771806716919\n",
      "Epoch [3959/10000], Loss: 0.4801982641220093\n",
      "Epoch [3960/10000], Loss: 0.48011934757232666\n",
      "Epoch [3961/10000], Loss: 0.4800405502319336\n",
      "Epoch [3962/10000], Loss: 0.4799617528915405\n",
      "Epoch [3963/10000], Loss: 0.47988301515579224\n",
      "Epoch [3964/10000], Loss: 0.4798043370246887\n",
      "Epoch [3965/10000], Loss: 0.4797256588935852\n",
      "Epoch [3966/10000], Loss: 0.47964704036712646\n",
      "Epoch [3967/10000], Loss: 0.4795685410499573\n",
      "Epoch [3968/10000], Loss: 0.4794899821281433\n",
      "Epoch [3969/10000], Loss: 0.4794114828109741\n",
      "Epoch [3970/10000], Loss: 0.4793330430984497\n",
      "Epoch [3971/10000], Loss: 0.47925466299057007\n",
      "Epoch [3972/10000], Loss: 0.4791763424873352\n",
      "Epoch [3973/10000], Loss: 0.47909802198410034\n",
      "Epoch [3974/10000], Loss: 0.47901976108551025\n",
      "Epoch [3975/10000], Loss: 0.4789416193962097\n",
      "Epoch [3976/10000], Loss: 0.4788634777069092\n",
      "Epoch [3977/10000], Loss: 0.47878533601760864\n",
      "Epoch [3978/10000], Loss: 0.4787072539329529\n",
      "Epoch [3979/10000], Loss: 0.4786292314529419\n",
      "Epoch [3980/10000], Loss: 0.47855132818222046\n",
      "Epoch [3981/10000], Loss: 0.47847336530685425\n",
      "Epoch [3982/10000], Loss: 0.4783955216407776\n",
      "Epoch [3983/10000], Loss: 0.4783176779747009\n",
      "Epoch [3984/10000], Loss: 0.47823989391326904\n",
      "Epoch [3985/10000], Loss: 0.47816216945648193\n",
      "Epoch [3986/10000], Loss: 0.4780844449996948\n",
      "Epoch [3987/10000], Loss: 0.4780067801475525\n",
      "Epoch [3988/10000], Loss: 0.47792917490005493\n",
      "Epoch [3989/10000], Loss: 0.47785162925720215\n",
      "Epoch [3990/10000], Loss: 0.47777408361434937\n",
      "Epoch [3991/10000], Loss: 0.47769659757614136\n",
      "Epoch [3992/10000], Loss: 0.4776192307472229\n",
      "Epoch [3993/10000], Loss: 0.47754180431365967\n",
      "Epoch [3994/10000], Loss: 0.47746437788009644\n",
      "Epoch [3995/10000], Loss: 0.47738713026046753\n",
      "Epoch [3996/10000], Loss: 0.4773098826408386\n",
      "Epoch [3997/10000], Loss: 0.4772326350212097\n",
      "Epoch [3998/10000], Loss: 0.47715550661087036\n",
      "Epoch [3999/10000], Loss: 0.477078378200531\n",
      "Epoch [4000/10000], Loss: 0.4770013093948364\n",
      "Epoch [4001/10000], Loss: 0.4769243001937866\n",
      "Epoch [4002/10000], Loss: 0.47684723138809204\n",
      "Epoch [4003/10000], Loss: 0.476770281791687\n",
      "Epoch [4004/10000], Loss: 0.476693332195282\n",
      "Epoch [4005/10000], Loss: 0.4766165018081665\n",
      "Epoch [4006/10000], Loss: 0.476539671421051\n",
      "Epoch [4007/10000], Loss: 0.4764629006385803\n",
      "Epoch [4008/10000], Loss: 0.4763861894607544\n",
      "Epoch [4009/10000], Loss: 0.47630947828292847\n",
      "Epoch [4010/10000], Loss: 0.4762328267097473\n",
      "Epoch [4011/10000], Loss: 0.47615617513656616\n",
      "Epoch [4012/10000], Loss: 0.47607964277267456\n",
      "Epoch [4013/10000], Loss: 0.47600311040878296\n",
      "Epoch [4014/10000], Loss: 0.47592657804489136\n",
      "Epoch [4015/10000], Loss: 0.4758501648902893\n",
      "Epoch [4016/10000], Loss: 0.4757736921310425\n",
      "Epoch [4017/10000], Loss: 0.4756973385810852\n",
      "Epoch [4018/10000], Loss: 0.47562098503112793\n",
      "Epoch [4019/10000], Loss: 0.4755447506904602\n",
      "Epoch [4020/10000], Loss: 0.47546839714050293\n",
      "Epoch [4021/10000], Loss: 0.47539228200912476\n",
      "Epoch [4022/10000], Loss: 0.4753161072731018\n",
      "Epoch [4023/10000], Loss: 0.47523993253707886\n",
      "Epoch [4024/10000], Loss: 0.4751638174057007\n",
      "Epoch [4025/10000], Loss: 0.47508782148361206\n",
      "Epoch [4026/10000], Loss: 0.4750118851661682\n",
      "Epoch [4027/10000], Loss: 0.4749358296394348\n",
      "Epoch [4028/10000], Loss: 0.47485989332199097\n",
      "Epoch [4029/10000], Loss: 0.47478407621383667\n",
      "Epoch [4030/10000], Loss: 0.4747082591056824\n",
      "Epoch [4031/10000], Loss: 0.4746323823928833\n",
      "Epoch [4032/10000], Loss: 0.4745566248893738\n",
      "Epoch [4033/10000], Loss: 0.4744809865951538\n",
      "Epoch [4034/10000], Loss: 0.47440528869628906\n",
      "Epoch [4035/10000], Loss: 0.4743296504020691\n",
      "Epoch [4036/10000], Loss: 0.4742540121078491\n",
      "Epoch [4037/10000], Loss: 0.4741784334182739\n",
      "Epoch [4038/10000], Loss: 0.4741029143333435\n",
      "Epoch [4039/10000], Loss: 0.47402751445770264\n",
      "Epoch [4040/10000], Loss: 0.47395211458206177\n",
      "Epoch [4041/10000], Loss: 0.4738766551017761\n",
      "Epoch [4042/10000], Loss: 0.47380131483078003\n",
      "Epoch [4043/10000], Loss: 0.4737260937690735\n",
      "Epoch [4044/10000], Loss: 0.4736507534980774\n",
      "Epoch [4045/10000], Loss: 0.47357553243637085\n",
      "Epoch [4046/10000], Loss: 0.4735003709793091\n",
      "Epoch [4047/10000], Loss: 0.4734252095222473\n",
      "Epoch [4048/10000], Loss: 0.4733501076698303\n",
      "Epoch [4049/10000], Loss: 0.4732750654220581\n",
      "Epoch [4050/10000], Loss: 0.4731999635696411\n",
      "Epoch [4051/10000], Loss: 0.47312498092651367\n",
      "Epoch [4052/10000], Loss: 0.47304999828338623\n",
      "Epoch [4053/10000], Loss: 0.47297513484954834\n",
      "Epoch [4054/10000], Loss: 0.47290027141571045\n",
      "Epoch [4055/10000], Loss: 0.47282540798187256\n",
      "Epoch [4056/10000], Loss: 0.47275060415267944\n",
      "Epoch [4057/10000], Loss: 0.4726759195327759\n",
      "Epoch [4058/10000], Loss: 0.47260111570358276\n",
      "Epoch [4059/10000], Loss: 0.47252655029296875\n",
      "Epoch [4060/10000], Loss: 0.4724518656730652\n",
      "Epoch [4061/10000], Loss: 0.4723772406578064\n",
      "Epoch [4062/10000], Loss: 0.47230273485183716\n",
      "Epoch [4063/10000], Loss: 0.47222816944122314\n",
      "Epoch [4064/10000], Loss: 0.47215378284454346\n",
      "Epoch [4065/10000], Loss: 0.472079336643219\n",
      "Epoch [4066/10000], Loss: 0.4720049500465393\n",
      "Epoch [4067/10000], Loss: 0.4719305634498596\n",
      "Epoch [4068/10000], Loss: 0.47185617685317993\n",
      "Epoch [4069/10000], Loss: 0.4717819094657898\n",
      "Epoch [4070/10000], Loss: 0.47170770168304443\n",
      "Epoch [4071/10000], Loss: 0.4716334939002991\n",
      "Epoch [4072/10000], Loss: 0.4715593457221985\n",
      "Epoch [4073/10000], Loss: 0.4714851975440979\n",
      "Epoch [4074/10000], Loss: 0.47141116857528687\n",
      "Epoch [4075/10000], Loss: 0.47133713960647583\n",
      "Epoch [4076/10000], Loss: 0.47126305103302\n",
      "Epoch [4077/10000], Loss: 0.47118908166885376\n",
      "Epoch [4078/10000], Loss: 0.4711151719093323\n",
      "Epoch [4079/10000], Loss: 0.47104132175445557\n",
      "Epoch [4080/10000], Loss: 0.4709674119949341\n",
      "Epoch [4081/10000], Loss: 0.4708935618400574\n",
      "Epoch [4082/10000], Loss: 0.4708198308944702\n",
      "Epoch [4083/10000], Loss: 0.47074609994888306\n",
      "Epoch [4084/10000], Loss: 0.4706723093986511\n",
      "Epoch [4085/10000], Loss: 0.4705986976623535\n",
      "Epoch [4086/10000], Loss: 0.47052502632141113\n",
      "Epoch [4087/10000], Loss: 0.4704514741897583\n",
      "Epoch [4088/10000], Loss: 0.4703778624534607\n",
      "Epoch [4089/10000], Loss: 0.47030436992645264\n",
      "Epoch [4090/10000], Loss: 0.47023093700408936\n",
      "Epoch [4091/10000], Loss: 0.4701574444770813\n",
      "Epoch [4092/10000], Loss: 0.4700840711593628\n",
      "Epoch [4093/10000], Loss: 0.4700106978416443\n",
      "Epoch [4094/10000], Loss: 0.469937264919281\n",
      "Epoch [4095/10000], Loss: 0.46986401081085205\n",
      "Epoch [4096/10000], Loss: 0.4697907567024231\n",
      "Epoch [4097/10000], Loss: 0.4697175621986389\n",
      "Epoch [4098/10000], Loss: 0.46964436769485474\n",
      "Epoch [4099/10000], Loss: 0.46957117319107056\n",
      "Epoch [4100/10000], Loss: 0.4694980978965759\n",
      "Epoch [4101/10000], Loss: 0.4694250226020813\n",
      "Epoch [4102/10000], Loss: 0.46935200691223145\n",
      "Epoch [4103/10000], Loss: 0.4692789912223816\n",
      "Epoch [4104/10000], Loss: 0.4692060351371765\n",
      "Epoch [4105/10000], Loss: 0.4691331386566162\n",
      "Epoch [4106/10000], Loss: 0.4690602421760559\n",
      "Epoch [4107/10000], Loss: 0.4689874053001404\n",
      "Epoch [4108/10000], Loss: 0.46891462802886963\n",
      "Epoch [4109/10000], Loss: 0.4688418507575989\n",
      "Epoch [4110/10000], Loss: 0.4687691330909729\n",
      "Epoch [4111/10000], Loss: 0.4686964154243469\n",
      "Epoch [4112/10000], Loss: 0.4686237573623657\n",
      "Epoch [4113/10000], Loss: 0.4685511589050293\n",
      "Epoch [4114/10000], Loss: 0.46847856044769287\n",
      "Epoch [4115/10000], Loss: 0.468406081199646\n",
      "Epoch [4116/10000], Loss: 0.46833348274230957\n",
      "Epoch [4117/10000], Loss: 0.46826106309890747\n",
      "Epoch [4118/10000], Loss: 0.4681885838508606\n",
      "Epoch [4119/10000], Loss: 0.46811622381210327\n",
      "Epoch [4120/10000], Loss: 0.46804386377334595\n",
      "Epoch [4121/10000], Loss: 0.4679715037345886\n",
      "Epoch [4122/10000], Loss: 0.4678992033004761\n",
      "Epoch [4123/10000], Loss: 0.4678269624710083\n",
      "Epoch [4124/10000], Loss: 0.4677547812461853\n",
      "Epoch [4125/10000], Loss: 0.4676826000213623\n",
      "Epoch [4126/10000], Loss: 0.4676104784011841\n",
      "Epoch [4127/10000], Loss: 0.4675382971763611\n",
      "Epoch [4128/10000], Loss: 0.46746623516082764\n",
      "Epoch [4129/10000], Loss: 0.4673941731452942\n",
      "Epoch [4130/10000], Loss: 0.4673222303390503\n",
      "Epoch [4131/10000], Loss: 0.4672502875328064\n",
      "Epoch [4132/10000], Loss: 0.4671782851219177\n",
      "Epoch [4133/10000], Loss: 0.4671064615249634\n",
      "Epoch [4134/10000], Loss: 0.46703463792800903\n",
      "Epoch [4135/10000], Loss: 0.4669628143310547\n",
      "Epoch [4136/10000], Loss: 0.46689099073410034\n",
      "Epoch [4137/10000], Loss: 0.46681922674179077\n",
      "Epoch [4138/10000], Loss: 0.46674758195877075\n",
      "Epoch [4139/10000], Loss: 0.46667593717575073\n",
      "Epoch [4140/10000], Loss: 0.4666042923927307\n",
      "Epoch [4141/10000], Loss: 0.4665326476097107\n",
      "Epoch [4142/10000], Loss: 0.4664611220359802\n",
      "Epoch [4143/10000], Loss: 0.46638959646224976\n",
      "Epoch [4144/10000], Loss: 0.4663180708885193\n",
      "Epoch [4145/10000], Loss: 0.46624672412872314\n",
      "Epoch [4146/10000], Loss: 0.4661753177642822\n",
      "Epoch [4147/10000], Loss: 0.4661039113998413\n",
      "Epoch [4148/10000], Loss: 0.46603256464004517\n",
      "Epoch [4149/10000], Loss: 0.4659612774848938\n",
      "Epoch [4150/10000], Loss: 0.46588999032974243\n",
      "Epoch [4151/10000], Loss: 0.46581876277923584\n",
      "Epoch [4152/10000], Loss: 0.46574753522872925\n",
      "Epoch [4153/10000], Loss: 0.4656764268875122\n",
      "Epoch [4154/10000], Loss: 0.46560531854629517\n",
      "Epoch [4155/10000], Loss: 0.4655342698097229\n",
      "Epoch [4156/10000], Loss: 0.46546322107315063\n",
      "Epoch [4157/10000], Loss: 0.46539217233657837\n",
      "Epoch [4158/10000], Loss: 0.46532124280929565\n",
      "Epoch [4159/10000], Loss: 0.46525031328201294\n",
      "Epoch [4160/10000], Loss: 0.465179443359375\n",
      "Epoch [4161/10000], Loss: 0.4651085138320923\n",
      "Epoch [4162/10000], Loss: 0.4650377035140991\n",
      "Epoch [4163/10000], Loss: 0.4649670720100403\n",
      "Epoch [4164/10000], Loss: 0.4648962616920471\n",
      "Epoch [4165/10000], Loss: 0.46482551097869873\n",
      "Epoch [4166/10000], Loss: 0.4647548794746399\n",
      "Epoch [4167/10000], Loss: 0.4646841883659363\n",
      "Epoch [4168/10000], Loss: 0.4646136164665222\n",
      "Epoch [4169/10000], Loss: 0.46454310417175293\n",
      "Epoch [4170/10000], Loss: 0.46447259187698364\n",
      "Epoch [4171/10000], Loss: 0.46440207958221436\n",
      "Epoch [4172/10000], Loss: 0.46433162689208984\n",
      "Epoch [4173/10000], Loss: 0.4642612338066101\n",
      "Epoch [4174/10000], Loss: 0.46419090032577515\n",
      "Epoch [4175/10000], Loss: 0.4641205072402954\n",
      "Epoch [4176/10000], Loss: 0.4640502333641052\n",
      "Epoch [4177/10000], Loss: 0.46397995948791504\n",
      "Epoch [4178/10000], Loss: 0.46390974521636963\n",
      "Epoch [4179/10000], Loss: 0.4638395309448242\n",
      "Epoch [4180/10000], Loss: 0.4637693762779236\n",
      "Epoch [4181/10000], Loss: 0.46369922161102295\n",
      "Epoch [4182/10000], Loss: 0.4636291265487671\n",
      "Epoch [4183/10000], Loss: 0.463559091091156\n",
      "Epoch [4184/10000], Loss: 0.4634891152381897\n",
      "Epoch [4185/10000], Loss: 0.4634190797805786\n",
      "Epoch [4186/10000], Loss: 0.4633491635322571\n",
      "Epoch [4187/10000], Loss: 0.46327924728393555\n",
      "Epoch [4188/10000], Loss: 0.46320945024490356\n",
      "Epoch [4189/10000], Loss: 0.46313953399658203\n",
      "Epoch [4190/10000], Loss: 0.46306973695755005\n",
      "Epoch [4191/10000], Loss: 0.46299993991851807\n",
      "Epoch [4192/10000], Loss: 0.46293026208877563\n",
      "Epoch [4193/10000], Loss: 0.4628605246543884\n",
      "Epoch [4194/10000], Loss: 0.462790846824646\n",
      "Epoch [4195/10000], Loss: 0.46272122859954834\n",
      "Epoch [4196/10000], Loss: 0.46265166997909546\n",
      "Epoch [4197/10000], Loss: 0.4625821113586426\n",
      "Epoch [4198/10000], Loss: 0.4625125527381897\n",
      "Epoch [4199/10000], Loss: 0.46244317293167114\n",
      "Epoch [4200/10000], Loss: 0.46237367391586304\n",
      "Epoch [4201/10000], Loss: 0.4623042345046997\n",
      "Epoch [4202/10000], Loss: 0.4622349143028259\n",
      "Epoch [4203/10000], Loss: 0.46216559410095215\n",
      "Epoch [4204/10000], Loss: 0.46209633350372314\n",
      "Epoch [4205/10000], Loss: 0.46202701330184937\n",
      "Epoch [4206/10000], Loss: 0.46195781230926514\n",
      "Epoch [4207/10000], Loss: 0.4618886709213257\n",
      "Epoch [4208/10000], Loss: 0.46181952953338623\n",
      "Epoch [4209/10000], Loss: 0.4617503881454468\n",
      "Epoch [4210/10000], Loss: 0.4616813063621521\n",
      "Epoch [4211/10000], Loss: 0.4616122841835022\n",
      "Epoch [4212/10000], Loss: 0.4615432620048523\n",
      "Epoch [4213/10000], Loss: 0.46147435903549194\n",
      "Epoch [4214/10000], Loss: 0.4614053964614868\n",
      "Epoch [4215/10000], Loss: 0.46133655309677124\n",
      "Epoch [4216/10000], Loss: 0.4612676501274109\n",
      "Epoch [4217/10000], Loss: 0.4611988067626953\n",
      "Epoch [4218/10000], Loss: 0.4611300826072693\n",
      "Epoch [4219/10000], Loss: 0.4610612392425537\n",
      "Epoch [4220/10000], Loss: 0.46099257469177246\n",
      "Epoch [4221/10000], Loss: 0.460923969745636\n",
      "Epoch [4222/10000], Loss: 0.4608551859855652\n",
      "Epoch [4223/10000], Loss: 0.4607866406440735\n",
      "Epoch [4224/10000], Loss: 0.460718035697937\n",
      "Epoch [4225/10000], Loss: 0.4606495499610901\n",
      "Epoch [4226/10000], Loss: 0.4605810046195984\n",
      "Epoch [4227/10000], Loss: 0.46051257848739624\n",
      "Epoch [4228/10000], Loss: 0.4604441523551941\n",
      "Epoch [4229/10000], Loss: 0.46037572622299194\n",
      "Epoch [4230/10000], Loss: 0.46030741930007935\n",
      "Epoch [4231/10000], Loss: 0.46023911237716675\n",
      "Epoch [4232/10000], Loss: 0.46017080545425415\n",
      "Epoch [4233/10000], Loss: 0.46010255813598633\n",
      "Epoch [4234/10000], Loss: 0.4600343704223633\n",
      "Epoch [4235/10000], Loss: 0.45996618270874023\n",
      "Epoch [4236/10000], Loss: 0.45989805459976196\n",
      "Epoch [4237/10000], Loss: 0.4598299264907837\n",
      "Epoch [4238/10000], Loss: 0.4597618579864502\n",
      "Epoch [4239/10000], Loss: 0.4596938490867615\n",
      "Epoch [4240/10000], Loss: 0.45962584018707275\n",
      "Epoch [4241/10000], Loss: 0.4595578908920288\n",
      "Epoch [4242/10000], Loss: 0.45949000120162964\n",
      "Epoch [4243/10000], Loss: 0.4594220519065857\n",
      "Epoch [4244/10000], Loss: 0.4593542814254761\n",
      "Epoch [4245/10000], Loss: 0.4592864513397217\n",
      "Epoch [4246/10000], Loss: 0.4592186212539673\n",
      "Epoch [4247/10000], Loss: 0.4591509699821472\n",
      "Epoch [4248/10000], Loss: 0.4590831995010376\n",
      "Epoch [4249/10000], Loss: 0.45901554822921753\n",
      "Epoch [4250/10000], Loss: 0.45894795656204224\n",
      "Epoch [4251/10000], Loss: 0.45888030529022217\n",
      "Epoch [4252/10000], Loss: 0.4588127136230469\n",
      "Epoch [4253/10000], Loss: 0.4587453007698059\n",
      "Epoch [4254/10000], Loss: 0.45867782831192017\n",
      "Epoch [4255/10000], Loss: 0.4586103558540344\n",
      "Epoch [4256/10000], Loss: 0.45854294300079346\n",
      "Epoch [4257/10000], Loss: 0.4584755301475525\n",
      "Epoch [4258/10000], Loss: 0.4584082365036011\n",
      "Epoch [4259/10000], Loss: 0.45834094285964966\n",
      "Epoch [4260/10000], Loss: 0.458273708820343\n",
      "Epoch [4261/10000], Loss: 0.4582064747810364\n",
      "Epoch [4262/10000], Loss: 0.4581393003463745\n",
      "Epoch [4263/10000], Loss: 0.45807212591171265\n",
      "Epoch [4264/10000], Loss: 0.45800501108169556\n",
      "Epoch [4265/10000], Loss: 0.45793795585632324\n",
      "Epoch [4266/10000], Loss: 0.4578710198402405\n",
      "Epoch [4267/10000], Loss: 0.4578039050102234\n",
      "Epoch [4268/10000], Loss: 0.45773690938949585\n",
      "Epoch [4269/10000], Loss: 0.45767003297805786\n",
      "Epoch [4270/10000], Loss: 0.4576031565666199\n",
      "Epoch [4271/10000], Loss: 0.4575362801551819\n",
      "Epoch [4272/10000], Loss: 0.45746946334838867\n",
      "Epoch [4273/10000], Loss: 0.45740270614624023\n",
      "Epoch [4274/10000], Loss: 0.457335889339447\n",
      "Epoch [4275/10000], Loss: 0.45726925134658813\n",
      "Epoch [4276/10000], Loss: 0.4572024941444397\n",
      "Epoch [4277/10000], Loss: 0.4571358561515808\n",
      "Epoch [4278/10000], Loss: 0.4570692181587219\n",
      "Epoch [4279/10000], Loss: 0.4570026993751526\n",
      "Epoch [4280/10000], Loss: 0.4569360613822937\n",
      "Epoch [4281/10000], Loss: 0.45686954259872437\n",
      "Epoch [4282/10000], Loss: 0.4568031430244446\n",
      "Epoch [4283/10000], Loss: 0.45673668384552\n",
      "Epoch [4284/10000], Loss: 0.456670343875885\n",
      "Epoch [4285/10000], Loss: 0.4566039443016052\n",
      "Epoch [4286/10000], Loss: 0.4565376043319702\n",
      "Epoch [4287/10000], Loss: 0.45647132396698\n",
      "Epoch [4288/10000], Loss: 0.45640498399734497\n",
      "Epoch [4289/10000], Loss: 0.4563388228416443\n",
      "Epoch [4290/10000], Loss: 0.4562726616859436\n",
      "Epoch [4291/10000], Loss: 0.4562065005302429\n",
      "Epoch [4292/10000], Loss: 0.456140398979187\n",
      "Epoch [4293/10000], Loss: 0.4560742974281311\n",
      "Epoch [4294/10000], Loss: 0.45600825548171997\n",
      "Epoch [4295/10000], Loss: 0.45594215393066406\n",
      "Epoch [4296/10000], Loss: 0.4558762311935425\n",
      "Epoch [4297/10000], Loss: 0.4558102488517761\n",
      "Epoch [4298/10000], Loss: 0.45574432611465454\n",
      "Epoch [4299/10000], Loss: 0.45567846298217773\n",
      "Epoch [4300/10000], Loss: 0.4556125998497009\n",
      "Epoch [4301/10000], Loss: 0.4555467963218689\n",
      "Epoch [4302/10000], Loss: 0.45548099279403687\n",
      "Epoch [4303/10000], Loss: 0.4554153084754944\n",
      "Epoch [4304/10000], Loss: 0.45534956455230713\n",
      "Epoch [4305/10000], Loss: 0.45528388023376465\n",
      "Epoch [4306/10000], Loss: 0.45521825551986694\n",
      "Epoch [4307/10000], Loss: 0.45515263080596924\n",
      "Epoch [4308/10000], Loss: 0.4550870656967163\n",
      "Epoch [4309/10000], Loss: 0.4550215005874634\n",
      "Epoch [4310/10000], Loss: 0.4549559950828552\n",
      "Epoch [4311/10000], Loss: 0.45489054918289185\n",
      "Epoch [4312/10000], Loss: 0.45482510328292847\n",
      "Epoch [4313/10000], Loss: 0.45475971698760986\n",
      "Epoch [4314/10000], Loss: 0.45469433069229126\n",
      "Epoch [4315/10000], Loss: 0.4546290636062622\n",
      "Epoch [4316/10000], Loss: 0.4545637369155884\n",
      "Epoch [4317/10000], Loss: 0.4544984698295593\n",
      "Epoch [4318/10000], Loss: 0.45443326234817505\n",
      "Epoch [4319/10000], Loss: 0.45436805486679077\n",
      "Epoch [4320/10000], Loss: 0.45430290699005127\n",
      "Epoch [4321/10000], Loss: 0.45423781871795654\n",
      "Epoch [4322/10000], Loss: 0.4541727304458618\n",
      "Epoch [4323/10000], Loss: 0.4541076421737671\n",
      "Epoch [4324/10000], Loss: 0.4540426731109619\n",
      "Epoch [4325/10000], Loss: 0.45397764444351196\n",
      "Epoch [4326/10000], Loss: 0.4539126753807068\n",
      "Epoch [4327/10000], Loss: 0.4538477659225464\n",
      "Epoch [4328/10000], Loss: 0.45378297567367554\n",
      "Epoch [4329/10000], Loss: 0.45371806621551514\n",
      "Epoch [4330/10000], Loss: 0.4536532759666443\n",
      "Epoch [4331/10000], Loss: 0.4535885453224182\n",
      "Epoch [4332/10000], Loss: 0.45352375507354736\n",
      "Epoch [4333/10000], Loss: 0.45345908403396606\n",
      "Epoch [4334/10000], Loss: 0.45339441299438477\n",
      "Epoch [4335/10000], Loss: 0.45332980155944824\n",
      "Epoch [4336/10000], Loss: 0.4532652497291565\n",
      "Epoch [4337/10000], Loss: 0.45320063829421997\n",
      "Epoch [4338/10000], Loss: 0.4531360864639282\n",
      "Epoch [4339/10000], Loss: 0.453071653842926\n",
      "Epoch [4340/10000], Loss: 0.45300722122192383\n",
      "Epoch [4341/10000], Loss: 0.45294278860092163\n",
      "Epoch [4342/10000], Loss: 0.45287835597991943\n",
      "Epoch [4343/10000], Loss: 0.45281410217285156\n",
      "Epoch [4344/10000], Loss: 0.4527497887611389\n",
      "Epoch [4345/10000], Loss: 0.45268547534942627\n",
      "Epoch [4346/10000], Loss: 0.4526211619377136\n",
      "Epoch [4347/10000], Loss: 0.4525569677352905\n",
      "Epoch [4348/10000], Loss: 0.45249277353286743\n",
      "Epoch [4349/10000], Loss: 0.4524286389350891\n",
      "Epoch [4350/10000], Loss: 0.452364444732666\n",
      "Epoch [4351/10000], Loss: 0.45230042934417725\n",
      "Epoch [4352/10000], Loss: 0.4522362947463989\n",
      "Epoch [4353/10000], Loss: 0.45217227935791016\n",
      "Epoch [4354/10000], Loss: 0.45210832357406616\n",
      "Epoch [4355/10000], Loss: 0.45204436779022217\n",
      "Epoch [4356/10000], Loss: 0.4519804120063782\n",
      "Epoch [4357/10000], Loss: 0.45191657543182373\n",
      "Epoch [4358/10000], Loss: 0.4518527388572693\n",
      "Epoch [4359/10000], Loss: 0.45178884267807007\n",
      "Epoch [4360/10000], Loss: 0.4517251253128052\n",
      "Epoch [4361/10000], Loss: 0.45166128873825073\n",
      "Epoch [4362/10000], Loss: 0.4515976309776306\n",
      "Epoch [4363/10000], Loss: 0.4515339136123657\n",
      "Epoch [4364/10000], Loss: 0.4514703154563904\n",
      "Epoch [4365/10000], Loss: 0.45140671730041504\n",
      "Epoch [4366/10000], Loss: 0.4513431191444397\n",
      "Epoch [4367/10000], Loss: 0.45127958059310913\n",
      "Epoch [4368/10000], Loss: 0.45121610164642334\n",
      "Epoch [4369/10000], Loss: 0.4511526823043823\n",
      "Epoch [4370/10000], Loss: 0.45108920335769653\n",
      "Epoch [4371/10000], Loss: 0.4510257840156555\n",
      "Epoch [4372/10000], Loss: 0.4509624242782593\n",
      "Epoch [4373/10000], Loss: 0.45089900493621826\n",
      "Epoch [4374/10000], Loss: 0.4508357644081116\n",
      "Epoch [4375/10000], Loss: 0.4507724642753601\n",
      "Epoch [4376/10000], Loss: 0.4507092237472534\n",
      "Epoch [4377/10000], Loss: 0.4506461024284363\n",
      "Epoch [4378/10000], Loss: 0.4505828619003296\n",
      "Epoch [4379/10000], Loss: 0.4505196809768677\n",
      "Epoch [4380/10000], Loss: 0.4504566192626953\n",
      "Epoch [4381/10000], Loss: 0.4503936171531677\n",
      "Epoch [4382/10000], Loss: 0.4503304958343506\n",
      "Epoch [4383/10000], Loss: 0.450267493724823\n",
      "Epoch [4384/10000], Loss: 0.4502044916152954\n",
      "Epoch [4385/10000], Loss: 0.4501415491104126\n",
      "Epoch [4386/10000], Loss: 0.45007866621017456\n",
      "Epoch [4387/10000], Loss: 0.4500157833099365\n",
      "Epoch [4388/10000], Loss: 0.44995296001434326\n",
      "Epoch [4389/10000], Loss: 0.44989013671875\n",
      "Epoch [4390/10000], Loss: 0.44982731342315674\n",
      "Epoch [4391/10000], Loss: 0.449764609336853\n",
      "Epoch [4392/10000], Loss: 0.4497019052505493\n",
      "Epoch [4393/10000], Loss: 0.4496392011642456\n",
      "Epoch [4394/10000], Loss: 0.44957661628723145\n",
      "Epoch [4395/10000], Loss: 0.44951391220092773\n",
      "Epoch [4396/10000], Loss: 0.44945138692855835\n",
      "Epoch [4397/10000], Loss: 0.4493888020515442\n",
      "Epoch [4398/10000], Loss: 0.4493262767791748\n",
      "Epoch [4399/10000], Loss: 0.4492637515068054\n",
      "Epoch [4400/10000], Loss: 0.4492013454437256\n",
      "Epoch [4401/10000], Loss: 0.449138879776001\n",
      "Epoch [4402/10000], Loss: 0.4490765333175659\n",
      "Epoch [4403/10000], Loss: 0.4490141272544861\n",
      "Epoch [4404/10000], Loss: 0.4489518404006958\n",
      "Epoch [4405/10000], Loss: 0.4488895535469055\n",
      "Epoch [4406/10000], Loss: 0.44882726669311523\n",
      "Epoch [4407/10000], Loss: 0.4487650394439697\n",
      "Epoch [4408/10000], Loss: 0.4487028121948242\n",
      "Epoch [4409/10000], Loss: 0.4486406445503235\n",
      "Epoch [4410/10000], Loss: 0.44857847690582275\n",
      "Epoch [4411/10000], Loss: 0.4485163688659668\n",
      "Epoch [4412/10000], Loss: 0.4484543800354004\n",
      "Epoch [4413/10000], Loss: 0.4483923316001892\n",
      "Epoch [4414/10000], Loss: 0.4483303427696228\n",
      "Epoch [4415/10000], Loss: 0.4482683539390564\n",
      "Epoch [4416/10000], Loss: 0.44820636510849\n",
      "Epoch [4417/10000], Loss: 0.44814449548721313\n",
      "Epoch [4418/10000], Loss: 0.4480826258659363\n",
      "Epoch [4419/10000], Loss: 0.4480207562446594\n",
      "Epoch [4420/10000], Loss: 0.44795894622802734\n",
      "Epoch [4421/10000], Loss: 0.44789713621139526\n",
      "Epoch [4422/10000], Loss: 0.44783544540405273\n",
      "Epoch [4423/10000], Loss: 0.44777369499206543\n",
      "Epoch [4424/10000], Loss: 0.4477120041847229\n",
      "Epoch [4425/10000], Loss: 0.44765037298202515\n",
      "Epoch [4426/10000], Loss: 0.4475887417793274\n",
      "Epoch [4427/10000], Loss: 0.44752711057662964\n",
      "Epoch [4428/10000], Loss: 0.44746559858322144\n",
      "Epoch [4429/10000], Loss: 0.44740408658981323\n",
      "Epoch [4430/10000], Loss: 0.44734257459640503\n",
      "Epoch [4431/10000], Loss: 0.4472811222076416\n",
      "Epoch [4432/10000], Loss: 0.4472196698188782\n",
      "Epoch [4433/10000], Loss: 0.4471582770347595\n",
      "Epoch [4434/10000], Loss: 0.44709688425064087\n",
      "Epoch [4435/10000], Loss: 0.447035551071167\n",
      "Epoch [4436/10000], Loss: 0.4469742178916931\n",
      "Epoch [4437/10000], Loss: 0.4469130039215088\n",
      "Epoch [4438/10000], Loss: 0.44685178995132446\n",
      "Epoch [4439/10000], Loss: 0.44679051637649536\n",
      "Epoch [4440/10000], Loss: 0.44672930240631104\n",
      "Epoch [4441/10000], Loss: 0.4466681480407715\n",
      "Epoch [4442/10000], Loss: 0.44660699367523193\n",
      "Epoch [4443/10000], Loss: 0.44654589891433716\n",
      "Epoch [4444/10000], Loss: 0.44648486375808716\n",
      "Epoch [4445/10000], Loss: 0.44642382860183716\n",
      "Epoch [4446/10000], Loss: 0.44636285305023193\n",
      "Epoch [4447/10000], Loss: 0.44630181789398193\n",
      "Epoch [4448/10000], Loss: 0.44624096155166626\n",
      "Epoch [4449/10000], Loss: 0.4461800456047058\n",
      "Epoch [4450/10000], Loss: 0.44611912965774536\n",
      "Epoch [4451/10000], Loss: 0.44605833292007446\n",
      "Epoch [4452/10000], Loss: 0.4459974765777588\n",
      "Epoch [4453/10000], Loss: 0.44593673944473267\n",
      "Epoch [4454/10000], Loss: 0.44587594270706177\n",
      "Epoch [4455/10000], Loss: 0.44581520557403564\n",
      "Epoch [4456/10000], Loss: 0.4457544684410095\n",
      "Epoch [4457/10000], Loss: 0.44569385051727295\n",
      "Epoch [4458/10000], Loss: 0.4456332325935364\n",
      "Epoch [4459/10000], Loss: 0.4455726146697998\n",
      "Epoch [4460/10000], Loss: 0.44551199674606323\n",
      "Epoch [4461/10000], Loss: 0.44545143842697144\n",
      "Epoch [4462/10000], Loss: 0.4453909397125244\n",
      "Epoch [4463/10000], Loss: 0.44533050060272217\n",
      "Epoch [4464/10000], Loss: 0.44526994228363037\n",
      "Epoch [4465/10000], Loss: 0.4452095627784729\n",
      "Epoch [4466/10000], Loss: 0.4451492428779602\n",
      "Epoch [4467/10000], Loss: 0.44508880376815796\n",
      "Epoch [4468/10000], Loss: 0.44502848386764526\n",
      "Epoch [4469/10000], Loss: 0.44496816396713257\n",
      "Epoch [4470/10000], Loss: 0.4449079632759094\n",
      "Epoch [4471/10000], Loss: 0.44484764337539673\n",
      "Epoch [4472/10000], Loss: 0.4447873830795288\n",
      "Epoch [4473/10000], Loss: 0.44472724199295044\n",
      "Epoch [4474/10000], Loss: 0.4446670413017273\n",
      "Epoch [4475/10000], Loss: 0.4446069002151489\n",
      "Epoch [4476/10000], Loss: 0.44454675912857056\n",
      "Epoch [4477/10000], Loss: 0.44448673725128174\n",
      "Epoch [4478/10000], Loss: 0.44442665576934814\n",
      "Epoch [4479/10000], Loss: 0.4443666338920593\n",
      "Epoch [4480/10000], Loss: 0.44430655241012573\n",
      "Epoch [4481/10000], Loss: 0.44424664974212646\n",
      "Epoch [4482/10000], Loss: 0.444186806678772\n",
      "Epoch [4483/10000], Loss: 0.44412678480148315\n",
      "Epoch [4484/10000], Loss: 0.44406694173812866\n",
      "Epoch [4485/10000], Loss: 0.4440070390701294\n",
      "Epoch [4486/10000], Loss: 0.4439472556114197\n",
      "Epoch [4487/10000], Loss: 0.44388753175735474\n",
      "Epoch [4488/10000], Loss: 0.443827748298645\n",
      "Epoch [4489/10000], Loss: 0.4437679648399353\n",
      "Epoch [4490/10000], Loss: 0.44370830059051514\n",
      "Epoch [4491/10000], Loss: 0.4436485767364502\n",
      "Epoch [4492/10000], Loss: 0.4435889720916748\n",
      "Epoch [4493/10000], Loss: 0.44352930784225464\n",
      "Epoch [4494/10000], Loss: 0.44346970319747925\n",
      "Epoch [4495/10000], Loss: 0.4434102177619934\n",
      "Epoch [4496/10000], Loss: 0.443350613117218\n",
      "Epoch [4497/10000], Loss: 0.4432911276817322\n",
      "Epoch [4498/10000], Loss: 0.44323164224624634\n",
      "Epoch [4499/10000], Loss: 0.4431720972061157\n",
      "Epoch [4500/10000], Loss: 0.44311273097991943\n",
      "Epoch [4501/10000], Loss: 0.44305336475372314\n",
      "Epoch [4502/10000], Loss: 0.44299399852752686\n",
      "Epoch [4503/10000], Loss: 0.44293463230133057\n",
      "Epoch [4504/10000], Loss: 0.44287532567977905\n",
      "Epoch [4505/10000], Loss: 0.4428160786628723\n",
      "Epoch [4506/10000], Loss: 0.4427567720413208\n",
      "Epoch [4507/10000], Loss: 0.4426976442337036\n",
      "Epoch [4508/10000], Loss: 0.4426383972167969\n",
      "Epoch [4509/10000], Loss: 0.44257915019989014\n",
      "Epoch [4510/10000], Loss: 0.44252002239227295\n",
      "Epoch [4511/10000], Loss: 0.44246095418930054\n",
      "Epoch [4512/10000], Loss: 0.44240182638168335\n",
      "Epoch [4513/10000], Loss: 0.44234275817871094\n",
      "Epoch [4514/10000], Loss: 0.4422837495803833\n",
      "Epoch [4515/10000], Loss: 0.44222474098205566\n",
      "Epoch [4516/10000], Loss: 0.442165732383728\n",
      "Epoch [4517/10000], Loss: 0.44210678339004517\n",
      "Epoch [4518/10000], Loss: 0.4420478940010071\n",
      "Epoch [4519/10000], Loss: 0.441989004611969\n",
      "Epoch [4520/10000], Loss: 0.4419301152229309\n",
      "Epoch [4521/10000], Loss: 0.4418712258338928\n",
      "Epoch [4522/10000], Loss: 0.4418123960494995\n",
      "Epoch [4523/10000], Loss: 0.441753625869751\n",
      "Epoch [4524/10000], Loss: 0.44169485569000244\n",
      "Epoch [4525/10000], Loss: 0.4416361451148987\n",
      "Epoch [4526/10000], Loss: 0.4415774345397949\n",
      "Epoch [4527/10000], Loss: 0.44151872396469116\n",
      "Epoch [4528/10000], Loss: 0.4414600729942322\n",
      "Epoch [4529/10000], Loss: 0.4414014220237732\n",
      "Epoch [4530/10000], Loss: 0.4413427710533142\n",
      "Epoch [4531/10000], Loss: 0.4412842392921448\n",
      "Epoch [4532/10000], Loss: 0.44122564792633057\n",
      "Epoch [4533/10000], Loss: 0.4411672353744507\n",
      "Epoch [4534/10000], Loss: 0.4411086440086365\n",
      "Epoch [4535/10000], Loss: 0.4410501718521118\n",
      "Epoch [4536/10000], Loss: 0.44099175930023193\n",
      "Epoch [4537/10000], Loss: 0.4409332871437073\n",
      "Epoch [4538/10000], Loss: 0.44087493419647217\n",
      "Epoch [4539/10000], Loss: 0.4408165216445923\n",
      "Epoch [4540/10000], Loss: 0.4407581686973572\n",
      "Epoch [4541/10000], Loss: 0.44069987535476685\n",
      "Epoch [4542/10000], Loss: 0.44064152240753174\n",
      "Epoch [4543/10000], Loss: 0.44058334827423096\n",
      "Epoch [4544/10000], Loss: 0.4405251145362854\n",
      "Epoch [4545/10000], Loss: 0.44046682119369507\n",
      "Epoch [4546/10000], Loss: 0.44040870666503906\n",
      "Epoch [4547/10000], Loss: 0.4403504729270935\n",
      "Epoch [4548/10000], Loss: 0.4402923583984375\n",
      "Epoch [4549/10000], Loss: 0.4402342438697815\n",
      "Epoch [4550/10000], Loss: 0.4401761293411255\n",
      "Epoch [4551/10000], Loss: 0.44011813402175903\n",
      "Epoch [4552/10000], Loss: 0.4400600790977478\n",
      "Epoch [4553/10000], Loss: 0.4400020241737366\n",
      "Epoch [4554/10000], Loss: 0.4399440288543701\n",
      "Epoch [4555/10000], Loss: 0.4398861527442932\n",
      "Epoch [4556/10000], Loss: 0.43982815742492676\n",
      "Epoch [4557/10000], Loss: 0.4397702217102051\n",
      "Epoch [4558/10000], Loss: 0.4397123456001282\n",
      "Epoch [4559/10000], Loss: 0.43965446949005127\n",
      "Epoch [4560/10000], Loss: 0.43959665298461914\n",
      "Epoch [4561/10000], Loss: 0.439538836479187\n",
      "Epoch [4562/10000], Loss: 0.4394810199737549\n",
      "Epoch [4563/10000], Loss: 0.43942326307296753\n",
      "Epoch [4564/10000], Loss: 0.43936556577682495\n",
      "Epoch [4565/10000], Loss: 0.4393078684806824\n",
      "Epoch [4566/10000], Loss: 0.4392501711845398\n",
      "Epoch [4567/10000], Loss: 0.439192533493042\n",
      "Epoch [4568/10000], Loss: 0.4391348958015442\n",
      "Epoch [4569/10000], Loss: 0.4390772581100464\n",
      "Epoch [4570/10000], Loss: 0.43901968002319336\n",
      "Epoch [4571/10000], Loss: 0.43896210193634033\n",
      "Epoch [4572/10000], Loss: 0.43890446424484253\n",
      "Epoch [4573/10000], Loss: 0.4388469457626343\n",
      "Epoch [4574/10000], Loss: 0.4387894868850708\n",
      "Epoch [4575/10000], Loss: 0.43873196840286255\n",
      "Epoch [4576/10000], Loss: 0.4386746287345886\n",
      "Epoch [4577/10000], Loss: 0.4386172294616699\n",
      "Epoch [4578/10000], Loss: 0.438559889793396\n",
      "Epoch [4579/10000], Loss: 0.4385024905204773\n",
      "Epoch [4580/10000], Loss: 0.4384452700614929\n",
      "Epoch [4581/10000], Loss: 0.43838798999786377\n",
      "Epoch [4582/10000], Loss: 0.4383307099342346\n",
      "Epoch [4583/10000], Loss: 0.43827348947525024\n",
      "Epoch [4584/10000], Loss: 0.43821626901626587\n",
      "Epoch [4585/10000], Loss: 0.43815910816192627\n",
      "Epoch [4586/10000], Loss: 0.4381018877029419\n",
      "Epoch [4587/10000], Loss: 0.4380447268486023\n",
      "Epoch [4588/10000], Loss: 0.43798762559890747\n",
      "Epoch [4589/10000], Loss: 0.43793052434921265\n",
      "Epoch [4590/10000], Loss: 0.4378734827041626\n",
      "Epoch [4591/10000], Loss: 0.4378165006637573\n",
      "Epoch [4592/10000], Loss: 0.4377594590187073\n",
      "Epoch [4593/10000], Loss: 0.43770235776901245\n",
      "Epoch [4594/10000], Loss: 0.43764549493789673\n",
      "Epoch [4595/10000], Loss: 0.43758857250213623\n",
      "Epoch [4596/10000], Loss: 0.43753165006637573\n",
      "Epoch [4597/10000], Loss: 0.43747472763061523\n",
      "Epoch [4598/10000], Loss: 0.43741780519485474\n",
      "Epoch [4599/10000], Loss: 0.43736106157302856\n",
      "Epoch [4600/10000], Loss: 0.43730419874191284\n",
      "Epoch [4601/10000], Loss: 0.4372473359107971\n",
      "Epoch [4602/10000], Loss: 0.4371906518936157\n",
      "Epoch [4603/10000], Loss: 0.4371338486671448\n",
      "Epoch [4604/10000], Loss: 0.4370771646499634\n",
      "Epoch [4605/10000], Loss: 0.43702036142349243\n",
      "Epoch [4606/10000], Loss: 0.43696367740631104\n",
      "Epoch [4607/10000], Loss: 0.4369070529937744\n",
      "Epoch [4608/10000], Loss: 0.436850368976593\n",
      "Epoch [4609/10000], Loss: 0.43679380416870117\n",
      "Epoch [4610/10000], Loss: 0.43673717975616455\n",
      "Epoch [4611/10000], Loss: 0.4366806149482727\n",
      "Epoch [4612/10000], Loss: 0.43662405014038086\n",
      "Epoch [4613/10000], Loss: 0.43656760454177856\n",
      "Epoch [4614/10000], Loss: 0.43651098012924194\n",
      "Epoch [4615/10000], Loss: 0.43645453453063965\n",
      "Epoch [4616/10000], Loss: 0.43639808893203735\n",
      "Epoch [4617/10000], Loss: 0.43634164333343506\n",
      "Epoch [4618/10000], Loss: 0.436285138130188\n",
      "Epoch [4619/10000], Loss: 0.43622881174087524\n",
      "Epoch [4620/10000], Loss: 0.4361724257469177\n",
      "Epoch [4621/10000], Loss: 0.4361160397529602\n",
      "Epoch [4622/10000], Loss: 0.43605977296829224\n",
      "Epoch [4623/10000], Loss: 0.4360034465789795\n",
      "Epoch [4624/10000], Loss: 0.43594712018966675\n",
      "Epoch [4625/10000], Loss: 0.4358908534049988\n",
      "Epoch [4626/10000], Loss: 0.4358346462249756\n",
      "Epoch [4627/10000], Loss: 0.4357784390449524\n",
      "Epoch [4628/10000], Loss: 0.4357222318649292\n",
      "Epoch [4629/10000], Loss: 0.435666024684906\n",
      "Epoch [4630/10000], Loss: 0.43560993671417236\n",
      "Epoch [4631/10000], Loss: 0.43555378913879395\n",
      "Epoch [4632/10000], Loss: 0.4354977011680603\n",
      "Epoch [4633/10000], Loss: 0.4354415535926819\n",
      "Epoch [4634/10000], Loss: 0.435385525226593\n",
      "Epoch [4635/10000], Loss: 0.43532949686050415\n",
      "Epoch [4636/10000], Loss: 0.43527352809906006\n",
      "Epoch [4637/10000], Loss: 0.4352174997329712\n",
      "Epoch [4638/10000], Loss: 0.4351615309715271\n",
      "Epoch [4639/10000], Loss: 0.435105562210083\n",
      "Epoch [4640/10000], Loss: 0.4350496530532837\n",
      "Epoch [4641/10000], Loss: 0.43499380350112915\n",
      "Epoch [4642/10000], Loss: 0.4349379539489746\n",
      "Epoch [4643/10000], Loss: 0.4348820447921753\n",
      "Epoch [4644/10000], Loss: 0.43482619524002075\n",
      "Epoch [4645/10000], Loss: 0.4347703456878662\n",
      "Epoch [4646/10000], Loss: 0.43471449613571167\n",
      "Epoch [4647/10000], Loss: 0.4346587657928467\n",
      "Epoch [4648/10000], Loss: 0.4346030354499817\n",
      "Epoch [4649/10000], Loss: 0.4345473051071167\n",
      "Epoch [4650/10000], Loss: 0.4344915747642517\n",
      "Epoch [4651/10000], Loss: 0.4344358444213867\n",
      "Epoch [4652/10000], Loss: 0.4343801736831665\n",
      "Epoch [4653/10000], Loss: 0.43432456254959106\n",
      "Epoch [4654/10000], Loss: 0.43426889181137085\n",
      "Epoch [4655/10000], Loss: 0.4342132806777954\n",
      "Epoch [4656/10000], Loss: 0.43415772914886475\n",
      "Epoch [4657/10000], Loss: 0.4341021180152893\n",
      "Epoch [4658/10000], Loss: 0.43404656648635864\n",
      "Epoch [4659/10000], Loss: 0.43399107456207275\n",
      "Epoch [4660/10000], Loss: 0.4339355230331421\n",
      "Epoch [4661/10000], Loss: 0.4338800311088562\n",
      "Epoch [4662/10000], Loss: 0.4338245987892151\n",
      "Epoch [4663/10000], Loss: 0.4337691068649292\n",
      "Epoch [4664/10000], Loss: 0.43371373414993286\n",
      "Epoch [4665/10000], Loss: 0.43365830183029175\n",
      "Epoch [4666/10000], Loss: 0.4336029291152954\n",
      "Epoch [4667/10000], Loss: 0.43354761600494385\n",
      "Epoch [4668/10000], Loss: 0.4334922432899475\n",
      "Epoch [4669/10000], Loss: 0.43343693017959595\n",
      "Epoch [4670/10000], Loss: 0.4333816170692444\n",
      "Epoch [4671/10000], Loss: 0.4333263635635376\n",
      "Epoch [4672/10000], Loss: 0.43327099084854126\n",
      "Epoch [4673/10000], Loss: 0.43321579694747925\n",
      "Epoch [4674/10000], Loss: 0.43316054344177246\n",
      "Epoch [4675/10000], Loss: 0.4331054091453552\n",
      "Epoch [4676/10000], Loss: 0.4330502152442932\n",
      "Epoch [4677/10000], Loss: 0.4329950213432312\n",
      "Epoch [4678/10000], Loss: 0.4329398274421692\n",
      "Epoch [4679/10000], Loss: 0.43288475275039673\n",
      "Epoch [4680/10000], Loss: 0.43282967805862427\n",
      "Epoch [4681/10000], Loss: 0.43277454376220703\n",
      "Epoch [4682/10000], Loss: 0.43271952867507935\n",
      "Epoch [4683/10000], Loss: 0.4326644539833069\n",
      "Epoch [4684/10000], Loss: 0.4326094388961792\n",
      "Epoch [4685/10000], Loss: 0.4325544238090515\n",
      "Epoch [4686/10000], Loss: 0.4324994683265686\n",
      "Epoch [4687/10000], Loss: 0.4324444532394409\n",
      "Epoch [4688/10000], Loss: 0.4323895573616028\n",
      "Epoch [4689/10000], Loss: 0.4323346018791199\n",
      "Epoch [4690/10000], Loss: 0.43227970600128174\n",
      "Epoch [4691/10000], Loss: 0.4322248101234436\n",
      "Epoch [4692/10000], Loss: 0.43216991424560547\n",
      "Epoch [4693/10000], Loss: 0.4321151375770569\n",
      "Epoch [4694/10000], Loss: 0.4320603013038635\n",
      "Epoch [4695/10000], Loss: 0.43200546503067017\n",
      "Epoch [4696/10000], Loss: 0.4319506883621216\n",
      "Epoch [4697/10000], Loss: 0.431895911693573\n",
      "Epoch [4698/10000], Loss: 0.43184107542037964\n",
      "Epoch [4699/10000], Loss: 0.4317864179611206\n",
      "Epoch [4700/10000], Loss: 0.431731641292572\n",
      "Epoch [4701/10000], Loss: 0.431676983833313\n",
      "Epoch [4702/10000], Loss: 0.43162232637405396\n",
      "Epoch [4703/10000], Loss: 0.4315676689147949\n",
      "Epoch [4704/10000], Loss: 0.4315129518508911\n",
      "Epoch [4705/10000], Loss: 0.43145835399627686\n",
      "Epoch [4706/10000], Loss: 0.4314037561416626\n",
      "Epoch [4707/10000], Loss: 0.43134915828704834\n",
      "Epoch [4708/10000], Loss: 0.43129462003707886\n",
      "Epoch [4709/10000], Loss: 0.4312400817871094\n",
      "Epoch [4710/10000], Loss: 0.4311854839324951\n",
      "Epoch [4711/10000], Loss: 0.4311310052871704\n",
      "Epoch [4712/10000], Loss: 0.4310764670372009\n",
      "Epoch [4713/10000], Loss: 0.431022047996521\n",
      "Epoch [4714/10000], Loss: 0.4309675097465515\n",
      "Epoch [4715/10000], Loss: 0.43091315031051636\n",
      "Epoch [4716/10000], Loss: 0.4308587312698364\n",
      "Epoch [4717/10000], Loss: 0.4308043122291565\n",
      "Epoch [4718/10000], Loss: 0.43074995279312134\n",
      "Epoch [4719/10000], Loss: 0.4306955933570862\n",
      "Epoch [4720/10000], Loss: 0.430641233921051\n",
      "Epoch [4721/10000], Loss: 0.43058687448501587\n",
      "Epoch [4722/10000], Loss: 0.4305325746536255\n",
      "Epoch [4723/10000], Loss: 0.4304783344268799\n",
      "Epoch [4724/10000], Loss: 0.4304240345954895\n",
      "Epoch [4725/10000], Loss: 0.4303697943687439\n",
      "Epoch [4726/10000], Loss: 0.4303154945373535\n",
      "Epoch [4727/10000], Loss: 0.4302613139152527\n",
      "Epoch [4728/10000], Loss: 0.4302070736885071\n",
      "Epoch [4729/10000], Loss: 0.430152952671051\n",
      "Epoch [4730/10000], Loss: 0.4300987124443054\n",
      "Epoch [4731/10000], Loss: 0.43004459142684937\n",
      "Epoch [4732/10000], Loss: 0.42999041080474854\n",
      "Epoch [4733/10000], Loss: 0.42993634939193726\n",
      "Epoch [4734/10000], Loss: 0.4298822283744812\n",
      "Epoch [4735/10000], Loss: 0.4298281669616699\n",
      "Epoch [4736/10000], Loss: 0.42977410554885864\n",
      "Epoch [4737/10000], Loss: 0.42972004413604736\n",
      "Epoch [4738/10000], Loss: 0.4296659827232361\n",
      "Epoch [4739/10000], Loss: 0.42961204051971436\n",
      "Epoch [4740/10000], Loss: 0.4295579791069031\n",
      "Epoch [4741/10000], Loss: 0.4295039772987366\n",
      "Epoch [4742/10000], Loss: 0.42945003509521484\n",
      "Epoch [4743/10000], Loss: 0.4293960928916931\n",
      "Epoch [4744/10000], Loss: 0.42934221029281616\n",
      "Epoch [4745/10000], Loss: 0.4292883276939392\n",
      "Epoch [4746/10000], Loss: 0.4292343854904175\n",
      "Epoch [4747/10000], Loss: 0.4291805028915405\n",
      "Epoch [4748/10000], Loss: 0.4291266202926636\n",
      "Epoch [4749/10000], Loss: 0.4290727972984314\n",
      "Epoch [4750/10000], Loss: 0.429019033908844\n",
      "Epoch [4751/10000], Loss: 0.42896515130996704\n",
      "Epoch [4752/10000], Loss: 0.42891138792037964\n",
      "Epoch [4753/10000], Loss: 0.42885762453079224\n",
      "Epoch [4754/10000], Loss: 0.42880386114120483\n",
      "Epoch [4755/10000], Loss: 0.42875009775161743\n",
      "Epoch [4756/10000], Loss: 0.4286964535713196\n",
      "Epoch [4757/10000], Loss: 0.4286426305770874\n",
      "Epoch [4758/10000], Loss: 0.42858898639678955\n",
      "Epoch [4759/10000], Loss: 0.4285353422164917\n",
      "Epoch [4760/10000], Loss: 0.4284816384315491\n",
      "Epoch [4761/10000], Loss: 0.4284279942512512\n",
      "Epoch [4762/10000], Loss: 0.42837440967559814\n",
      "Epoch [4763/10000], Loss: 0.4283207654953003\n",
      "Epoch [4764/10000], Loss: 0.4282671809196472\n",
      "Epoch [4765/10000], Loss: 0.42821359634399414\n",
      "Epoch [4766/10000], Loss: 0.42816001176834106\n",
      "Epoch [4767/10000], Loss: 0.42810648679733276\n",
      "Epoch [4768/10000], Loss: 0.42805296182632446\n",
      "Epoch [4769/10000], Loss: 0.42799949645996094\n",
      "Epoch [4770/10000], Loss: 0.42794597148895264\n",
      "Epoch [4771/10000], Loss: 0.4278925061225891\n",
      "Epoch [4772/10000], Loss: 0.4278389811515808\n",
      "Epoch [4773/10000], Loss: 0.42778557538986206\n",
      "Epoch [4774/10000], Loss: 0.42773211002349854\n",
      "Epoch [4775/10000], Loss: 0.4276787042617798\n",
      "Epoch [4776/10000], Loss: 0.4276253581047058\n",
      "Epoch [4777/10000], Loss: 0.42757195234298706\n",
      "Epoch [4778/10000], Loss: 0.42751866579055786\n",
      "Epoch [4779/10000], Loss: 0.42746514081954956\n",
      "Epoch [4780/10000], Loss: 0.4274119734764099\n",
      "Epoch [4781/10000], Loss: 0.42735862731933594\n",
      "Epoch [4782/10000], Loss: 0.42730534076690674\n",
      "Epoch [4783/10000], Loss: 0.42725199460983276\n",
      "Epoch [4784/10000], Loss: 0.42719876766204834\n",
      "Epoch [4785/10000], Loss: 0.42714548110961914\n",
      "Epoch [4786/10000], Loss: 0.42709219455718994\n",
      "Epoch [4787/10000], Loss: 0.4270390272140503\n",
      "Epoch [4788/10000], Loss: 0.42698585987091064\n",
      "Epoch [4789/10000], Loss: 0.426932692527771\n",
      "Epoch [4790/10000], Loss: 0.4268794655799866\n",
      "Epoch [4791/10000], Loss: 0.4268262982368469\n",
      "Epoch [4792/10000], Loss: 0.42677319049835205\n",
      "Epoch [4793/10000], Loss: 0.4267200827598572\n",
      "Epoch [4794/10000], Loss: 0.42666685581207275\n",
      "Epoch [4795/10000], Loss: 0.42661380767822266\n",
      "Epoch [4796/10000], Loss: 0.4265606999397278\n",
      "Epoch [4797/10000], Loss: 0.4265076518058777\n",
      "Epoch [4798/10000], Loss: 0.4264546036720276\n",
      "Epoch [4799/10000], Loss: 0.4264015555381775\n",
      "Epoch [4800/10000], Loss: 0.42634856700897217\n",
      "Epoch [4801/10000], Loss: 0.42629557847976685\n",
      "Epoch [4802/10000], Loss: 0.42624253034591675\n",
      "Epoch [4803/10000], Loss: 0.4261896014213562\n",
      "Epoch [4804/10000], Loss: 0.4261366128921509\n",
      "Epoch [4805/10000], Loss: 0.42608368396759033\n",
      "Epoch [4806/10000], Loss: 0.4260307550430298\n",
      "Epoch [4807/10000], Loss: 0.42597782611846924\n",
      "Epoch [4808/10000], Loss: 0.42592495679855347\n",
      "Epoch [4809/10000], Loss: 0.42587214708328247\n",
      "Epoch [4810/10000], Loss: 0.4258192181587219\n",
      "Epoch [4811/10000], Loss: 0.42576634883880615\n",
      "Epoch [4812/10000], Loss: 0.4257134795188904\n",
      "Epoch [4813/10000], Loss: 0.4256606698036194\n",
      "Epoch [4814/10000], Loss: 0.4256078600883484\n",
      "Epoch [4815/10000], Loss: 0.42555510997772217\n",
      "Epoch [4816/10000], Loss: 0.42550230026245117\n",
      "Epoch [4817/10000], Loss: 0.4254494905471802\n",
      "Epoch [4818/10000], Loss: 0.42539680004119873\n",
      "Epoch [4819/10000], Loss: 0.4253440499305725\n",
      "Epoch [4820/10000], Loss: 0.42529135942459106\n",
      "Epoch [4821/10000], Loss: 0.42523854970932007\n",
      "Epoch [4822/10000], Loss: 0.4251858592033386\n",
      "Epoch [4823/10000], Loss: 0.42513322830200195\n",
      "Epoch [4824/10000], Loss: 0.4250805974006653\n",
      "Epoch [4825/10000], Loss: 0.42502790689468384\n",
      "Epoch [4826/10000], Loss: 0.42497527599334717\n",
      "Epoch [4827/10000], Loss: 0.4249225854873657\n",
      "Epoch [4828/10000], Loss: 0.42486995458602905\n",
      "Epoch [4829/10000], Loss: 0.42481738328933716\n",
      "Epoch [4830/10000], Loss: 0.4247647523880005\n",
      "Epoch [4831/10000], Loss: 0.42471224069595337\n",
      "Epoch [4832/10000], Loss: 0.42465972900390625\n",
      "Epoch [4833/10000], Loss: 0.42460715770721436\n",
      "Epoch [4834/10000], Loss: 0.424554705619812\n",
      "Epoch [4835/10000], Loss: 0.4245021939277649\n",
      "Epoch [4836/10000], Loss: 0.42444974184036255\n",
      "Epoch [4837/10000], Loss: 0.4243972897529602\n",
      "Epoch [4838/10000], Loss: 0.42434483766555786\n",
      "Epoch [4839/10000], Loss: 0.42429226636886597\n",
      "Epoch [4840/10000], Loss: 0.4242398142814636\n",
      "Epoch [4841/10000], Loss: 0.4241875410079956\n",
      "Epoch [4842/10000], Loss: 0.42413508892059326\n",
      "Epoch [4843/10000], Loss: 0.4240826964378357\n",
      "Epoch [4844/10000], Loss: 0.4240303039550781\n",
      "Epoch [4845/10000], Loss: 0.42397791147232056\n",
      "Epoch [4846/10000], Loss: 0.4239256978034973\n",
      "Epoch [4847/10000], Loss: 0.42387330532073975\n",
      "Epoch [4848/10000], Loss: 0.42382097244262695\n",
      "Epoch [4849/10000], Loss: 0.42376869916915894\n",
      "Epoch [4850/10000], Loss: 0.4237164258956909\n",
      "Epoch [4851/10000], Loss: 0.4236640930175781\n",
      "Epoch [4852/10000], Loss: 0.4236118197441101\n",
      "Epoch [4853/10000], Loss: 0.42355966567993164\n",
      "Epoch [4854/10000], Loss: 0.42350733280181885\n",
      "Epoch [4855/10000], Loss: 0.4234551787376404\n",
      "Epoch [4856/10000], Loss: 0.42340296506881714\n",
      "Epoch [4857/10000], Loss: 0.42335081100463867\n",
      "Epoch [4858/10000], Loss: 0.4232986569404602\n",
      "Epoch [4859/10000], Loss: 0.42324644327163696\n",
      "Epoch [4860/10000], Loss: 0.42319434881210327\n",
      "Epoch [4861/10000], Loss: 0.42314213514328003\n",
      "Epoch [4862/10000], Loss: 0.42309004068374634\n",
      "Epoch [4863/10000], Loss: 0.4230380058288574\n",
      "Epoch [4864/10000], Loss: 0.42298591136932373\n",
      "Epoch [4865/10000], Loss: 0.4229338765144348\n",
      "Epoch [4866/10000], Loss: 0.42288172245025635\n",
      "Epoch [4867/10000], Loss: 0.42282968759536743\n",
      "Epoch [4868/10000], Loss: 0.4227777123451233\n",
      "Epoch [4869/10000], Loss: 0.4227256178855896\n",
      "Epoch [4870/10000], Loss: 0.42267364263534546\n",
      "Epoch [4871/10000], Loss: 0.4226216673851013\n",
      "Epoch [4872/10000], Loss: 0.4225696325302124\n",
      "Epoch [4873/10000], Loss: 0.42251771688461304\n",
      "Epoch [4874/10000], Loss: 0.4224657416343689\n",
      "Epoch [4875/10000], Loss: 0.42241376638412476\n",
      "Epoch [4876/10000], Loss: 0.4223618507385254\n",
      "Epoch [4877/10000], Loss: 0.422309935092926\n",
      "Epoch [4878/10000], Loss: 0.42225807905197144\n",
      "Epoch [4879/10000], Loss: 0.42220622301101685\n",
      "Epoch [4880/10000], Loss: 0.4221543073654175\n",
      "Epoch [4881/10000], Loss: 0.4221024513244629\n",
      "Epoch [4882/10000], Loss: 0.4220505952835083\n",
      "Epoch [4883/10000], Loss: 0.4219987988471985\n",
      "Epoch [4884/10000], Loss: 0.42194700241088867\n",
      "Epoch [4885/10000], Loss: 0.4218951463699341\n",
      "Epoch [4886/10000], Loss: 0.42184340953826904\n",
      "Epoch [4887/10000], Loss: 0.42179161310195923\n",
      "Epoch [4888/10000], Loss: 0.4217398762702942\n",
      "Epoch [4889/10000], Loss: 0.4216880798339844\n",
      "Epoch [4890/10000], Loss: 0.4216364026069641\n",
      "Epoch [4891/10000], Loss: 0.4215846657752991\n",
      "Epoch [4892/10000], Loss: 0.42153292894363403\n",
      "Epoch [4893/10000], Loss: 0.421481192111969\n",
      "Epoch [4894/10000], Loss: 0.4214295744895935\n",
      "Epoch [4895/10000], Loss: 0.421377956867218\n",
      "Epoch [4896/10000], Loss: 0.42132627964019775\n",
      "Epoch [4897/10000], Loss: 0.42127466201782227\n",
      "Epoch [4898/10000], Loss: 0.4212230443954468\n",
      "Epoch [4899/10000], Loss: 0.42117148637771606\n",
      "Epoch [4900/10000], Loss: 0.4211198091506958\n",
      "Epoch [4901/10000], Loss: 0.4210682511329651\n",
      "Epoch [4902/10000], Loss: 0.4210166931152344\n",
      "Epoch [4903/10000], Loss: 0.42096519470214844\n",
      "Epoch [4904/10000], Loss: 0.42091357707977295\n",
      "Epoch [4905/10000], Loss: 0.420862078666687\n",
      "Epoch [4906/10000], Loss: 0.4208105802536011\n",
      "Epoch [4907/10000], Loss: 0.42075908184051514\n",
      "Epoch [4908/10000], Loss: 0.420707643032074\n",
      "Epoch [4909/10000], Loss: 0.42065614461898804\n",
      "Epoch [4910/10000], Loss: 0.4206046462059021\n",
      "Epoch [4911/10000], Loss: 0.4205533266067505\n",
      "Epoch [4912/10000], Loss: 0.42050182819366455\n",
      "Epoch [4913/10000], Loss: 0.42045050859451294\n",
      "Epoch [4914/10000], Loss: 0.4203990697860718\n",
      "Epoch [4915/10000], Loss: 0.4203476309776306\n",
      "Epoch [4916/10000], Loss: 0.42029625177383423\n",
      "Epoch [4917/10000], Loss: 0.4202449917793274\n",
      "Epoch [4918/10000], Loss: 0.420193612575531\n",
      "Epoch [4919/10000], Loss: 0.4201422929763794\n",
      "Epoch [4920/10000], Loss: 0.4200909733772278\n",
      "Epoch [4921/10000], Loss: 0.42003971338272095\n",
      "Epoch [4922/10000], Loss: 0.4199884533882141\n",
      "Epoch [4923/10000], Loss: 0.4199371337890625\n",
      "Epoch [4924/10000], Loss: 0.41988593339920044\n",
      "Epoch [4925/10000], Loss: 0.41983461380004883\n",
      "Epoch [4926/10000], Loss: 0.41978347301483154\n",
      "Epoch [4927/10000], Loss: 0.4197322726249695\n",
      "Epoch [4928/10000], Loss: 0.4196810722351074\n",
      "Epoch [4929/10000], Loss: 0.41962987184524536\n",
      "Epoch [4930/10000], Loss: 0.4195786714553833\n",
      "Epoch [4931/10000], Loss: 0.419527530670166\n",
      "Epoch [4932/10000], Loss: 0.4194764494895935\n",
      "Epoch [4933/10000], Loss: 0.4194253087043762\n",
      "Epoch [4934/10000], Loss: 0.41937416791915894\n",
      "Epoch [4935/10000], Loss: 0.41932302713394165\n",
      "Epoch [4936/10000], Loss: 0.41927194595336914\n",
      "Epoch [4937/10000], Loss: 0.4192209243774414\n",
      "Epoch [4938/10000], Loss: 0.4191698431968689\n",
      "Epoch [4939/10000], Loss: 0.41911882162094116\n",
      "Epoch [4940/10000], Loss: 0.4190678000450134\n",
      "Epoch [4941/10000], Loss: 0.41901683807373047\n",
      "Epoch [4942/10000], Loss: 0.41896581649780273\n",
      "Epoch [4943/10000], Loss: 0.418914794921875\n",
      "Epoch [4944/10000], Loss: 0.41886383295059204\n",
      "Epoch [4945/10000], Loss: 0.4188128709793091\n",
      "Epoch [4946/10000], Loss: 0.4187620282173157\n",
      "Epoch [4947/10000], Loss: 0.4187110662460327\n",
      "Epoch [4948/10000], Loss: 0.41866016387939453\n",
      "Epoch [4949/10000], Loss: 0.41860926151275635\n",
      "Epoch [4950/10000], Loss: 0.41855841875076294\n",
      "Epoch [4951/10000], Loss: 0.41850757598876953\n",
      "Epoch [4952/10000], Loss: 0.41845667362213135\n",
      "Epoch [4953/10000], Loss: 0.4184058904647827\n",
      "Epoch [4954/10000], Loss: 0.4183550477027893\n",
      "Epoch [4955/10000], Loss: 0.4183042645454407\n",
      "Epoch [4956/10000], Loss: 0.41825348138809204\n",
      "Epoch [4957/10000], Loss: 0.4182026982307434\n",
      "Epoch [4958/10000], Loss: 0.41815197467803955\n",
      "Epoch [4959/10000], Loss: 0.4181012511253357\n",
      "Epoch [4960/10000], Loss: 0.41805046796798706\n",
      "Epoch [4961/10000], Loss: 0.4179997444152832\n",
      "Epoch [4962/10000], Loss: 0.4179491400718689\n",
      "Epoch [4963/10000], Loss: 0.41789835691452026\n",
      "Epoch [4964/10000], Loss: 0.41784775257110596\n",
      "Epoch [4965/10000], Loss: 0.4177970886230469\n",
      "Epoch [4966/10000], Loss: 0.41774648427963257\n",
      "Epoch [4967/10000], Loss: 0.41769587993621826\n",
      "Epoch [4968/10000], Loss: 0.41764533519744873\n",
      "Epoch [4969/10000], Loss: 0.4175947308540344\n",
      "Epoch [4970/10000], Loss: 0.4175441861152649\n",
      "Epoch [4971/10000], Loss: 0.41749364137649536\n",
      "Epoch [4972/10000], Loss: 0.4174429774284363\n",
      "Epoch [4973/10000], Loss: 0.41739243268966675\n",
      "Epoch [4974/10000], Loss: 0.41734200716018677\n",
      "Epoch [4975/10000], Loss: 0.417291522026062\n",
      "Epoch [4976/10000], Loss: 0.4172409772872925\n",
      "Epoch [4977/10000], Loss: 0.4171905517578125\n",
      "Epoch [4978/10000], Loss: 0.4171401262283325\n",
      "Epoch [4979/10000], Loss: 0.4170897603034973\n",
      "Epoch [4980/10000], Loss: 0.41703927516937256\n",
      "Epoch [4981/10000], Loss: 0.41698890924453735\n",
      "Epoch [4982/10000], Loss: 0.4169384837150574\n",
      "Epoch [4983/10000], Loss: 0.41688817739486694\n",
      "Epoch [4984/10000], Loss: 0.41683781147003174\n",
      "Epoch [4985/10000], Loss: 0.41678744554519653\n",
      "Epoch [4986/10000], Loss: 0.4167371392250061\n",
      "Epoch [4987/10000], Loss: 0.4166868329048157\n",
      "Epoch [4988/10000], Loss: 0.41663658618927\n",
      "Epoch [4989/10000], Loss: 0.41658633947372437\n",
      "Epoch [4990/10000], Loss: 0.41653597354888916\n",
      "Epoch [4991/10000], Loss: 0.4164857864379883\n",
      "Epoch [4992/10000], Loss: 0.4164355993270874\n",
      "Epoch [4993/10000], Loss: 0.41638535261154175\n",
      "Epoch [4994/10000], Loss: 0.41633516550064087\n",
      "Epoch [4995/10000], Loss: 0.41628503799438477\n",
      "Epoch [4996/10000], Loss: 0.41623491048812866\n",
      "Epoch [4997/10000], Loss: 0.41618478298187256\n",
      "Epoch [4998/10000], Loss: 0.41613471508026123\n",
      "Epoch [4999/10000], Loss: 0.4160846471786499\n",
      "Epoch [5000/10000], Loss: 0.4160345792770386\n",
      "Epoch [5001/10000], Loss: 0.41598451137542725\n",
      "Epoch [5002/10000], Loss: 0.4159345030784607\n",
      "Epoch [5003/10000], Loss: 0.41588443517684937\n",
      "Epoch [5004/10000], Loss: 0.4158344268798828\n",
      "Epoch [5005/10000], Loss: 0.41578441858291626\n",
      "Epoch [5006/10000], Loss: 0.41573458909988403\n",
      "Epoch [5007/10000], Loss: 0.4156845808029175\n",
      "Epoch [5008/10000], Loss: 0.4156346321105957\n",
      "Epoch [5009/10000], Loss: 0.4155846834182739\n",
      "Epoch [5010/10000], Loss: 0.4155347943305969\n",
      "Epoch [5011/10000], Loss: 0.4154849648475647\n",
      "Epoch [5012/10000], Loss: 0.4154350161552429\n",
      "Epoch [5013/10000], Loss: 0.4153851866722107\n",
      "Epoch [5014/10000], Loss: 0.41533535718917847\n",
      "Epoch [5015/10000], Loss: 0.4152856469154358\n",
      "Epoch [5016/10000], Loss: 0.41523581743240356\n",
      "Epoch [5017/10000], Loss: 0.4151860475540161\n",
      "Epoch [5018/10000], Loss: 0.41513627767562866\n",
      "Epoch [5019/10000], Loss: 0.415086567401886\n",
      "Epoch [5020/10000], Loss: 0.41503679752349854\n",
      "Epoch [5021/10000], Loss: 0.41498714685440063\n",
      "Epoch [5022/10000], Loss: 0.41493749618530273\n",
      "Epoch [5023/10000], Loss: 0.41488778591156006\n",
      "Epoch [5024/10000], Loss: 0.41483813524246216\n",
      "Epoch [5025/10000], Loss: 0.41478854417800903\n",
      "Epoch [5026/10000], Loss: 0.41473883390426636\n",
      "Epoch [5027/10000], Loss: 0.4146893620491028\n",
      "Epoch [5028/10000], Loss: 0.4146396517753601\n",
      "Epoch [5029/10000], Loss: 0.414590060710907\n",
      "Epoch [5030/10000], Loss: 0.4145406484603882\n",
      "Epoch [5031/10000], Loss: 0.41449105739593506\n",
      "Epoch [5032/10000], Loss: 0.4144415855407715\n",
      "Epoch [5033/10000], Loss: 0.4143921136856079\n",
      "Epoch [5034/10000], Loss: 0.41434258222579956\n",
      "Epoch [5035/10000], Loss: 0.41429322957992554\n",
      "Epoch [5036/10000], Loss: 0.41424375772476196\n",
      "Epoch [5037/10000], Loss: 0.41419440507888794\n",
      "Epoch [5038/10000], Loss: 0.41414499282836914\n",
      "Epoch [5039/10000], Loss: 0.4140956997871399\n",
      "Epoch [5040/10000], Loss: 0.4140462875366211\n",
      "Epoch [5041/10000], Loss: 0.41399699449539185\n",
      "Epoch [5042/10000], Loss: 0.4139476418495178\n",
      "Epoch [5043/10000], Loss: 0.4138983488082886\n",
      "Epoch [5044/10000], Loss: 0.4138490557670593\n",
      "Epoch [5045/10000], Loss: 0.41379982233047485\n",
      "Epoch [5046/10000], Loss: 0.41375064849853516\n",
      "Epoch [5047/10000], Loss: 0.41370147466659546\n",
      "Epoch [5048/10000], Loss: 0.41365230083465576\n",
      "Epoch [5049/10000], Loss: 0.41360312700271606\n",
      "Epoch [5050/10000], Loss: 0.41355401277542114\n",
      "Epoch [5051/10000], Loss: 0.41350477933883667\n",
      "Epoch [5052/10000], Loss: 0.41345566511154175\n",
      "Epoch [5053/10000], Loss: 0.4134066700935364\n",
      "Epoch [5054/10000], Loss: 0.41335761547088623\n",
      "Epoch [5055/10000], Loss: 0.4133085608482361\n",
      "Epoch [5056/10000], Loss: 0.4132595658302307\n",
      "Epoch [5057/10000], Loss: 0.41321051120758057\n",
      "Epoch [5058/10000], Loss: 0.41316157579421997\n",
      "Epoch [5059/10000], Loss: 0.4131125807762146\n",
      "Epoch [5060/10000], Loss: 0.413063645362854\n",
      "Epoch [5061/10000], Loss: 0.4130147695541382\n",
      "Epoch [5062/10000], Loss: 0.4129657745361328\n",
      "Epoch [5063/10000], Loss: 0.41291695833206177\n",
      "Epoch [5064/10000], Loss: 0.41286808252334595\n",
      "Epoch [5065/10000], Loss: 0.4128193259239197\n",
      "Epoch [5066/10000], Loss: 0.4127703905105591\n",
      "Epoch [5067/10000], Loss: 0.4127216935157776\n",
      "Epoch [5068/10000], Loss: 0.41267287731170654\n",
      "Epoch [5069/10000], Loss: 0.4126240611076355\n",
      "Epoch [5070/10000], Loss: 0.4125754237174988\n",
      "Epoch [5071/10000], Loss: 0.4125266671180725\n",
      "Epoch [5072/10000], Loss: 0.412477970123291\n",
      "Epoch [5073/10000], Loss: 0.4124293327331543\n",
      "Epoch [5074/10000], Loss: 0.4123806357383728\n",
      "Epoch [5075/10000], Loss: 0.41233205795288086\n",
      "Epoch [5076/10000], Loss: 0.4122834801673889\n",
      "Epoch [5077/10000], Loss: 0.4122348427772522\n",
      "Epoch [5078/10000], Loss: 0.41218632459640503\n",
      "Epoch [5079/10000], Loss: 0.41213786602020264\n",
      "Epoch [5080/10000], Loss: 0.41208934783935547\n",
      "Epoch [5081/10000], Loss: 0.4120408296585083\n",
      "Epoch [5082/10000], Loss: 0.4119923710823059\n",
      "Epoch [5083/10000], Loss: 0.4119439721107483\n",
      "Epoch [5084/10000], Loss: 0.4118954539299011\n",
      "Epoch [5085/10000], Loss: 0.4118471145629883\n",
      "Epoch [5086/10000], Loss: 0.41179877519607544\n",
      "Epoch [5087/10000], Loss: 0.4117503762245178\n",
      "Epoch [5088/10000], Loss: 0.411702036857605\n",
      "Epoch [5089/10000], Loss: 0.41165369749069214\n",
      "Epoch [5090/10000], Loss: 0.4116054177284241\n",
      "Epoch [5091/10000], Loss: 0.4115571975708008\n",
      "Epoch [5092/10000], Loss: 0.4115089774131775\n",
      "Epoch [5093/10000], Loss: 0.4114607572555542\n",
      "Epoch [5094/10000], Loss: 0.4114125967025757\n",
      "Epoch [5095/10000], Loss: 0.41136443614959717\n",
      "Epoch [5096/10000], Loss: 0.41131627559661865\n",
      "Epoch [5097/10000], Loss: 0.4112682342529297\n",
      "Epoch [5098/10000], Loss: 0.41122007369995117\n",
      "Epoch [5099/10000], Loss: 0.4111720323562622\n",
      "Epoch [5100/10000], Loss: 0.41112393140792847\n",
      "Epoch [5101/10000], Loss: 0.41107600927352905\n",
      "Epoch [5102/10000], Loss: 0.4110279679298401\n",
      "Epoch [5103/10000], Loss: 0.4109800457954407\n",
      "Epoch [5104/10000], Loss: 0.41093212366104126\n",
      "Epoch [5105/10000], Loss: 0.41088420152664185\n",
      "Epoch [5106/10000], Loss: 0.41083621978759766\n",
      "Epoch [5107/10000], Loss: 0.4107884168624878\n",
      "Epoch [5108/10000], Loss: 0.4107404947280884\n",
      "Epoch [5109/10000], Loss: 0.4106926918029785\n",
      "Epoch [5110/10000], Loss: 0.41064488887786865\n",
      "Epoch [5111/10000], Loss: 0.41059714555740356\n",
      "Epoch [5112/10000], Loss: 0.41054946184158325\n",
      "Epoch [5113/10000], Loss: 0.4105016589164734\n",
      "Epoch [5114/10000], Loss: 0.4104540944099426\n",
      "Epoch [5115/10000], Loss: 0.410406231880188\n",
      "Epoch [5116/10000], Loss: 0.41035860776901245\n",
      "Epoch [5117/10000], Loss: 0.4103110432624817\n",
      "Epoch [5118/10000], Loss: 0.41026341915130615\n",
      "Epoch [5119/10000], Loss: 0.4102158546447754\n",
      "Epoch [5120/10000], Loss: 0.41016829013824463\n",
      "Epoch [5121/10000], Loss: 0.41012078523635864\n",
      "Epoch [5122/10000], Loss: 0.41007328033447266\n",
      "Epoch [5123/10000], Loss: 0.41002577543258667\n",
      "Epoch [5124/10000], Loss: 0.40997833013534546\n",
      "Epoch [5125/10000], Loss: 0.4099308252334595\n",
      "Epoch [5126/10000], Loss: 0.4098834991455078\n",
      "Epoch [5127/10000], Loss: 0.40983617305755615\n",
      "Epoch [5128/10000], Loss: 0.4097887873649597\n",
      "Epoch [5129/10000], Loss: 0.40974146127700806\n",
      "Epoch [5130/10000], Loss: 0.40969425439834595\n",
      "Epoch [5131/10000], Loss: 0.4096469283103943\n",
      "Epoch [5132/10000], Loss: 0.4095997214317322\n",
      "Epoch [5133/10000], Loss: 0.4095524549484253\n",
      "Epoch [5134/10000], Loss: 0.40950530767440796\n",
      "Epoch [5135/10000], Loss: 0.40945810079574585\n",
      "Epoch [5136/10000], Loss: 0.4094109535217285\n",
      "Epoch [5137/10000], Loss: 0.4093638062477112\n",
      "Epoch [5138/10000], Loss: 0.4093167185783386\n",
      "Epoch [5139/10000], Loss: 0.40926969051361084\n",
      "Epoch [5140/10000], Loss: 0.40922266244888306\n",
      "Epoch [5141/10000], Loss: 0.40917569398880005\n",
      "Epoch [5142/10000], Loss: 0.40912866592407227\n",
      "Epoch [5143/10000], Loss: 0.40908175706863403\n",
      "Epoch [5144/10000], Loss: 0.409034788608551\n",
      "Epoch [5145/10000], Loss: 0.40898793935775757\n",
      "Epoch [5146/10000], Loss: 0.4089410901069641\n",
      "Epoch [5147/10000], Loss: 0.4088941216468811\n",
      "Epoch [5148/10000], Loss: 0.40884727239608765\n",
      "Epoch [5149/10000], Loss: 0.40880054235458374\n",
      "Epoch [5150/10000], Loss: 0.40875381231307983\n",
      "Epoch [5151/10000], Loss: 0.4087070822715759\n",
      "Epoch [5152/10000], Loss: 0.408660352230072\n",
      "Epoch [5153/10000], Loss: 0.4086136221885681\n",
      "Epoch [5154/10000], Loss: 0.408566951751709\n",
      "Epoch [5155/10000], Loss: 0.40852028131484985\n",
      "Epoch [5156/10000], Loss: 0.4084736704826355\n",
      "Epoch [5157/10000], Loss: 0.4084271192550659\n",
      "Epoch [5158/10000], Loss: 0.40838050842285156\n",
      "Epoch [5159/10000], Loss: 0.40833407640457153\n",
      "Epoch [5160/10000], Loss: 0.4082874655723572\n",
      "Epoch [5161/10000], Loss: 0.4082409739494324\n",
      "Epoch [5162/10000], Loss: 0.4081946015357971\n",
      "Epoch [5163/10000], Loss: 0.4081481695175171\n",
      "Epoch [5164/10000], Loss: 0.40810173749923706\n",
      "Epoch [5165/10000], Loss: 0.4080553650856018\n",
      "Epoch [5166/10000], Loss: 0.4080091118812561\n",
      "Epoch [5167/10000], Loss: 0.40796273946762085\n",
      "Epoch [5168/10000], Loss: 0.4079165458679199\n",
      "Epoch [5169/10000], Loss: 0.40787023305892944\n",
      "Epoch [5170/10000], Loss: 0.4078240394592285\n",
      "Epoch [5171/10000], Loss: 0.4077778458595276\n",
      "Epoch [5172/10000], Loss: 0.40773165225982666\n",
      "Epoch [5173/10000], Loss: 0.40768545866012573\n",
      "Epoch [5174/10000], Loss: 0.40763938426971436\n",
      "Epoch [5175/10000], Loss: 0.407593309879303\n",
      "Epoch [5176/10000], Loss: 0.4075472950935364\n",
      "Epoch [5177/10000], Loss: 0.407501220703125\n",
      "Epoch [5178/10000], Loss: 0.4074552655220032\n",
      "Epoch [5179/10000], Loss: 0.4074092507362366\n",
      "Epoch [5180/10000], Loss: 0.4073633551597595\n",
      "Epoch [5181/10000], Loss: 0.4073173403739929\n",
      "Epoch [5182/10000], Loss: 0.40727150440216064\n",
      "Epoch [5183/10000], Loss: 0.4072256088256836\n",
      "Epoch [5184/10000], Loss: 0.4071798324584961\n",
      "Epoch [5185/10000], Loss: 0.4071339964866638\n",
      "Epoch [5186/10000], Loss: 0.4070882201194763\n",
      "Epoch [5187/10000], Loss: 0.4070424437522888\n",
      "Epoch [5188/10000], Loss: 0.4069967269897461\n",
      "Epoch [5189/10000], Loss: 0.40695101022720337\n",
      "Epoch [5190/10000], Loss: 0.4069053530693054\n",
      "Epoch [5191/10000], Loss: 0.40685975551605225\n",
      "Epoch [5192/10000], Loss: 0.4068141579627991\n",
      "Epoch [5193/10000], Loss: 0.4067686200141907\n",
      "Epoch [5194/10000], Loss: 0.4067230820655823\n",
      "Epoch [5195/10000], Loss: 0.40667760372161865\n",
      "Epoch [5196/10000], Loss: 0.40663206577301025\n",
      "Epoch [5197/10000], Loss: 0.40658658742904663\n",
      "Epoch [5198/10000], Loss: 0.40654122829437256\n",
      "Epoch [5199/10000], Loss: 0.40649574995040894\n",
      "Epoch [5200/10000], Loss: 0.40645039081573486\n",
      "Epoch [5201/10000], Loss: 0.40640509128570557\n",
      "Epoch [5202/10000], Loss: 0.4063596725463867\n",
      "Epoch [5203/10000], Loss: 0.406314492225647\n",
      "Epoch [5204/10000], Loss: 0.4062691926956177\n",
      "Epoch [5205/10000], Loss: 0.40622395277023315\n",
      "Epoch [5206/10000], Loss: 0.4061788320541382\n",
      "Epoch [5207/10000], Loss: 0.40613359212875366\n",
      "Epoch [5208/10000], Loss: 0.4060884714126587\n",
      "Epoch [5209/10000], Loss: 0.4060433506965637\n",
      "Epoch [5210/10000], Loss: 0.4059983491897583\n",
      "Epoch [5211/10000], Loss: 0.4059532880783081\n",
      "Epoch [5212/10000], Loss: 0.4059082269668579\n",
      "Epoch [5213/10000], Loss: 0.4058632254600525\n",
      "Epoch [5214/10000], Loss: 0.40581828355789185\n",
      "Epoch [5215/10000], Loss: 0.4057732820510864\n",
      "Epoch [5216/10000], Loss: 0.40572839975357056\n",
      "Epoch [5217/10000], Loss: 0.4056835174560547\n",
      "Epoch [5218/10000], Loss: 0.4056386947631836\n",
      "Epoch [5219/10000], Loss: 0.4055938720703125\n",
      "Epoch [5220/10000], Loss: 0.4055491089820862\n",
      "Epoch [5221/10000], Loss: 0.40550434589385986\n",
      "Epoch [5222/10000], Loss: 0.40545952320098877\n",
      "Epoch [5223/10000], Loss: 0.4054149389266968\n",
      "Epoch [5224/10000], Loss: 0.40537023544311523\n",
      "Epoch [5225/10000], Loss: 0.4053255319595337\n",
      "Epoch [5226/10000], Loss: 0.4052810072898865\n",
      "Epoch [5227/10000], Loss: 0.4052363634109497\n",
      "Epoch [5228/10000], Loss: 0.4051917791366577\n",
      "Epoch [5229/10000], Loss: 0.4051472544670105\n",
      "Epoch [5230/10000], Loss: 0.40510278940200806\n",
      "Epoch [5231/10000], Loss: 0.4050583243370056\n",
      "Epoch [5232/10000], Loss: 0.40501391887664795\n",
      "Epoch [5233/10000], Loss: 0.40496957302093506\n",
      "Epoch [5234/10000], Loss: 0.40492522716522217\n",
      "Epoch [5235/10000], Loss: 0.4048808217048645\n",
      "Epoch [5236/10000], Loss: 0.4048365354537964\n",
      "Epoch [5237/10000], Loss: 0.4047921895980835\n",
      "Epoch [5238/10000], Loss: 0.40474796295166016\n",
      "Epoch [5239/10000], Loss: 0.4047037363052368\n",
      "Epoch [5240/10000], Loss: 0.40465956926345825\n",
      "Epoch [5241/10000], Loss: 0.4046153426170349\n",
      "Epoch [5242/10000], Loss: 0.40457117557525635\n",
      "Epoch [5243/10000], Loss: 0.40452712774276733\n",
      "Epoch [5244/10000], Loss: 0.40448302030563354\n",
      "Epoch [5245/10000], Loss: 0.40443897247314453\n",
      "Epoch [5246/10000], Loss: 0.40439504384994507\n",
      "Epoch [5247/10000], Loss: 0.40435105562210083\n",
      "Epoch [5248/10000], Loss: 0.4043070673942566\n",
      "Epoch [5249/10000], Loss: 0.40426307916641235\n",
      "Epoch [5250/10000], Loss: 0.40421926975250244\n",
      "Epoch [5251/10000], Loss: 0.40417540073394775\n",
      "Epoch [5252/10000], Loss: 0.40413159132003784\n",
      "Epoch [5253/10000], Loss: 0.40408772230148315\n",
      "Epoch [5254/10000], Loss: 0.4040440320968628\n",
      "Epoch [5255/10000], Loss: 0.40400028228759766\n",
      "Epoch [5256/10000], Loss: 0.4039565324783325\n",
      "Epoch [5257/10000], Loss: 0.40391290187835693\n",
      "Epoch [5258/10000], Loss: 0.4038692116737366\n",
      "Epoch [5259/10000], Loss: 0.40382564067840576\n",
      "Epoch [5260/10000], Loss: 0.4037820100784302\n",
      "Epoch [5261/10000], Loss: 0.40373843908309937\n",
      "Epoch [5262/10000], Loss: 0.40369492769241333\n",
      "Epoch [5263/10000], Loss: 0.4036514163017273\n",
      "Epoch [5264/10000], Loss: 0.40360796451568604\n",
      "Epoch [5265/10000], Loss: 0.4035645127296448\n",
      "Epoch [5266/10000], Loss: 0.4035211205482483\n",
      "Epoch [5267/10000], Loss: 0.4034777283668518\n",
      "Epoch [5268/10000], Loss: 0.4034343957901001\n",
      "Epoch [5269/10000], Loss: 0.4033910632133484\n",
      "Epoch [5270/10000], Loss: 0.40334784984588623\n",
      "Epoch [5271/10000], Loss: 0.4033045768737793\n",
      "Epoch [5272/10000], Loss: 0.40326130390167236\n",
      "Epoch [5273/10000], Loss: 0.403218150138855\n",
      "Epoch [5274/10000], Loss: 0.4031749367713928\n",
      "Epoch [5275/10000], Loss: 0.40313178300857544\n",
      "Epoch [5276/10000], Loss: 0.40308868885040283\n",
      "Epoch [5277/10000], Loss: 0.4030455946922302\n",
      "Epoch [5278/10000], Loss: 0.40300261974334717\n",
      "Epoch [5279/10000], Loss: 0.40295952558517456\n",
      "Epoch [5280/10000], Loss: 0.4029166102409363\n",
      "Epoch [5281/10000], Loss: 0.4028736352920532\n",
      "Epoch [5282/10000], Loss: 0.40283071994781494\n",
      "Epoch [5283/10000], Loss: 0.40278780460357666\n",
      "Epoch [5284/10000], Loss: 0.4027448892593384\n",
      "Epoch [5285/10000], Loss: 0.4027021527290344\n",
      "Epoch [5286/10000], Loss: 0.4026592969894409\n",
      "Epoch [5287/10000], Loss: 0.40261656045913696\n",
      "Epoch [5288/10000], Loss: 0.40257376432418823\n",
      "Epoch [5289/10000], Loss: 0.40253108739852905\n",
      "Epoch [5290/10000], Loss: 0.4024883508682251\n",
      "Epoch [5291/10000], Loss: 0.4024457335472107\n",
      "Epoch [5292/10000], Loss: 0.40240317583084106\n",
      "Epoch [5293/10000], Loss: 0.40236055850982666\n",
      "Epoch [5294/10000], Loss: 0.40231794118881226\n",
      "Epoch [5295/10000], Loss: 0.4022755026817322\n",
      "Epoch [5296/10000], Loss: 0.4022330045700073\n",
      "Epoch [5297/10000], Loss: 0.40219050645828247\n",
      "Epoch [5298/10000], Loss: 0.40214812755584717\n",
      "Epoch [5299/10000], Loss: 0.4021056294441223\n",
      "Epoch [5300/10000], Loss: 0.402063250541687\n",
      "Epoch [5301/10000], Loss: 0.4020209312438965\n",
      "Epoch [5302/10000], Loss: 0.40197867155075073\n",
      "Epoch [5303/10000], Loss: 0.4019363522529602\n",
      "Epoch [5304/10000], Loss: 0.40189415216445923\n",
      "Epoch [5305/10000], Loss: 0.40185195207595825\n",
      "Epoch [5306/10000], Loss: 0.4018096923828125\n",
      "Epoch [5307/10000], Loss: 0.4017675518989563\n",
      "Epoch [5308/10000], Loss: 0.4017254114151001\n",
      "Epoch [5309/10000], Loss: 0.4016832709312439\n",
      "Epoch [5310/10000], Loss: 0.40164124965667725\n",
      "Epoch [5311/10000], Loss: 0.4015992283821106\n",
      "Epoch [5312/10000], Loss: 0.40155714750289917\n",
      "Epoch [5313/10000], Loss: 0.4015151858329773\n",
      "Epoch [5314/10000], Loss: 0.4014732241630554\n",
      "Epoch [5315/10000], Loss: 0.40143126249313354\n",
      "Epoch [5316/10000], Loss: 0.401389479637146\n",
      "Epoch [5317/10000], Loss: 0.4013475775718689\n",
      "Epoch [5318/10000], Loss: 0.4013057351112366\n",
      "Epoch [5319/10000], Loss: 0.4012640118598938\n",
      "Epoch [5320/10000], Loss: 0.4012221693992615\n",
      "Epoch [5321/10000], Loss: 0.4011803865432739\n",
      "Epoch [5322/10000], Loss: 0.4011387825012207\n",
      "Epoch [5323/10000], Loss: 0.4010971188545227\n",
      "Epoch [5324/10000], Loss: 0.40105539560317993\n",
      "Epoch [5325/10000], Loss: 0.4010138511657715\n",
      "Epoch [5326/10000], Loss: 0.40097224712371826\n",
      "Epoch [5327/10000], Loss: 0.4009307026863098\n",
      "Epoch [5328/10000], Loss: 0.4008890986442566\n",
      "Epoch [5329/10000], Loss: 0.4008476138114929\n",
      "Epoch [5330/10000], Loss: 0.40080612897872925\n",
      "Epoch [5331/10000], Loss: 0.4007647633552551\n",
      "Epoch [5332/10000], Loss: 0.40072333812713623\n",
      "Epoch [5333/10000], Loss: 0.40068191289901733\n",
      "Epoch [5334/10000], Loss: 0.400640606880188\n",
      "Epoch [5335/10000], Loss: 0.4005993604660034\n",
      "Epoch [5336/10000], Loss: 0.4005580544471741\n",
      "Epoch [5337/10000], Loss: 0.4005168080329895\n",
      "Epoch [5338/10000], Loss: 0.40047556161880493\n",
      "Epoch [5339/10000], Loss: 0.40043437480926514\n",
      "Epoch [5340/10000], Loss: 0.40039312839508057\n",
      "Epoch [5341/10000], Loss: 0.4003520607948303\n",
      "Epoch [5342/10000], Loss: 0.4003109931945801\n",
      "Epoch [5343/10000], Loss: 0.40026992559432983\n",
      "Epoch [5344/10000], Loss: 0.4002288579940796\n",
      "Epoch [5345/10000], Loss: 0.40018779039382935\n",
      "Epoch [5346/10000], Loss: 0.40014684200286865\n",
      "Epoch [5347/10000], Loss: 0.40010589361190796\n",
      "Epoch [5348/10000], Loss: 0.40006494522094727\n",
      "Epoch [5349/10000], Loss: 0.40002405643463135\n",
      "Epoch [5350/10000], Loss: 0.3999832272529602\n",
      "Epoch [5351/10000], Loss: 0.3999423384666443\n",
      "Epoch [5352/10000], Loss: 0.3999015688896179\n",
      "Epoch [5353/10000], Loss: 0.39986079931259155\n",
      "Epoch [5354/10000], Loss: 0.3998200297355652\n",
      "Epoch [5355/10000], Loss: 0.3997793197631836\n",
      "Epoch [5356/10000], Loss: 0.399738609790802\n",
      "Epoch [5357/10000], Loss: 0.3996979594230652\n",
      "Epoch [5358/10000], Loss: 0.39965730905532837\n",
      "Epoch [5359/10000], Loss: 0.39961671829223633\n",
      "Epoch [5360/10000], Loss: 0.3995761275291443\n",
      "Epoch [5361/10000], Loss: 0.3995356559753418\n",
      "Epoch [5362/10000], Loss: 0.39949512481689453\n",
      "Epoch [5363/10000], Loss: 0.39945459365844727\n",
      "Epoch [5364/10000], Loss: 0.39941418170928955\n",
      "Epoch [5365/10000], Loss: 0.3993738293647766\n",
      "Epoch [5366/10000], Loss: 0.3993333578109741\n",
      "Epoch [5367/10000], Loss: 0.3992930054664612\n",
      "Epoch [5368/10000], Loss: 0.399252712726593\n",
      "Epoch [5369/10000], Loss: 0.3992123603820801\n",
      "Epoch [5370/10000], Loss: 0.3991721272468567\n",
      "Epoch [5371/10000], Loss: 0.39913177490234375\n",
      "Epoch [5372/10000], Loss: 0.3990917205810547\n",
      "Epoch [5373/10000], Loss: 0.3990514874458313\n",
      "Epoch [5374/10000], Loss: 0.3990112543106079\n",
      "Epoch [5375/10000], Loss: 0.39897119998931885\n",
      "Epoch [5376/10000], Loss: 0.398931086063385\n",
      "Epoch [5377/10000], Loss: 0.39889097213745117\n",
      "Epoch [5378/10000], Loss: 0.3988509178161621\n",
      "Epoch [5379/10000], Loss: 0.3988109827041626\n",
      "Epoch [5380/10000], Loss: 0.39877086877822876\n",
      "Epoch [5381/10000], Loss: 0.3987310528755188\n",
      "Epoch [5382/10000], Loss: 0.3986911177635193\n",
      "Epoch [5383/10000], Loss: 0.398651123046875\n",
      "Epoch [5384/10000], Loss: 0.39861124753952026\n",
      "Epoch [5385/10000], Loss: 0.3985714316368103\n",
      "Epoch [5386/10000], Loss: 0.39853161573410034\n",
      "Epoch [5387/10000], Loss: 0.39849185943603516\n",
      "Epoch [5388/10000], Loss: 0.39845216274261475\n",
      "Epoch [5389/10000], Loss: 0.39841240644454956\n",
      "Epoch [5390/10000], Loss: 0.3983726501464844\n",
      "Epoch [5391/10000], Loss: 0.39833301305770874\n",
      "Epoch [5392/10000], Loss: 0.3982934355735779\n",
      "Epoch [5393/10000], Loss: 0.39825379848480225\n",
      "Epoch [5394/10000], Loss: 0.3982141613960266\n",
      "Epoch [5395/10000], Loss: 0.3981746435165405\n",
      "Epoch [5396/10000], Loss: 0.39813512563705444\n",
      "Epoch [5397/10000], Loss: 0.39809560775756836\n",
      "Epoch [5398/10000], Loss: 0.3980560898780823\n",
      "Epoch [5399/10000], Loss: 0.39801663160324097\n",
      "Epoch [5400/10000], Loss: 0.3979772925376892\n",
      "Epoch [5401/10000], Loss: 0.3979378938674927\n",
      "Epoch [5402/10000], Loss: 0.3978985548019409\n",
      "Epoch [5403/10000], Loss: 0.39785921573638916\n",
      "Epoch [5404/10000], Loss: 0.3978199362754822\n",
      "Epoch [5405/10000], Loss: 0.39778071641921997\n",
      "Epoch [5406/10000], Loss: 0.397741436958313\n",
      "Epoch [5407/10000], Loss: 0.3977022171020508\n",
      "Epoch [5408/10000], Loss: 0.39766305685043335\n",
      "Epoch [5409/10000], Loss: 0.3976238965988159\n",
      "Epoch [5410/10000], Loss: 0.3975847363471985\n",
      "Epoch [5411/10000], Loss: 0.39754563570022583\n",
      "Epoch [5412/10000], Loss: 0.39750659465789795\n",
      "Epoch [5413/10000], Loss: 0.3974674940109253\n",
      "Epoch [5414/10000], Loss: 0.39742857217788696\n",
      "Epoch [5415/10000], Loss: 0.3973894715309143\n",
      "Epoch [5416/10000], Loss: 0.3973504900932312\n",
      "Epoch [5417/10000], Loss: 0.39731162786483765\n",
      "Epoch [5418/10000], Loss: 0.3972727060317993\n",
      "Epoch [5419/10000], Loss: 0.39723384380340576\n",
      "Epoch [5420/10000], Loss: 0.3971949815750122\n",
      "Epoch [5421/10000], Loss: 0.3971562385559082\n",
      "Epoch [5422/10000], Loss: 0.3971174359321594\n",
      "Epoch [5423/10000], Loss: 0.39707863330841064\n",
      "Epoch [5424/10000], Loss: 0.3970399498939514\n",
      "Epoch [5425/10000], Loss: 0.3970012068748474\n",
      "Epoch [5426/10000], Loss: 0.39696258306503296\n",
      "Epoch [5427/10000], Loss: 0.39692389965057373\n",
      "Epoch [5428/10000], Loss: 0.39688533544540405\n",
      "Epoch [5429/10000], Loss: 0.3968467712402344\n",
      "Epoch [5430/10000], Loss: 0.3968082070350647\n",
      "Epoch [5431/10000], Loss: 0.396769642829895\n",
      "Epoch [5432/10000], Loss: 0.3967311978340149\n",
      "Epoch [5433/10000], Loss: 0.39669269323349\n",
      "Epoch [5434/10000], Loss: 0.39665424823760986\n",
      "Epoch [5435/10000], Loss: 0.39661580324172974\n",
      "Epoch [5436/10000], Loss: 0.3965774178504944\n",
      "Epoch [5437/10000], Loss: 0.3965390920639038\n",
      "Epoch [5438/10000], Loss: 0.39650070667266846\n",
      "Epoch [5439/10000], Loss: 0.3964623808860779\n",
      "Epoch [5440/10000], Loss: 0.39642417430877686\n",
      "Epoch [5441/10000], Loss: 0.3963858485221863\n",
      "Epoch [5442/10000], Loss: 0.3963475823402405\n",
      "Epoch [5443/10000], Loss: 0.39630937576293945\n",
      "Epoch [5444/10000], Loss: 0.3962712287902832\n",
      "Epoch [5445/10000], Loss: 0.3962330222129822\n",
      "Epoch [5446/10000], Loss: 0.3961949944496155\n",
      "Epoch [5447/10000], Loss: 0.39615684747695923\n",
      "Epoch [5448/10000], Loss: 0.39611876010894775\n",
      "Epoch [5449/10000], Loss: 0.39608079195022583\n",
      "Epoch [5450/10000], Loss: 0.39604276418685913\n",
      "Epoch [5451/10000], Loss: 0.39600467681884766\n",
      "Epoch [5452/10000], Loss: 0.3959668278694153\n",
      "Epoch [5453/10000], Loss: 0.39592885971069336\n",
      "Epoch [5454/10000], Loss: 0.395891010761261\n",
      "Epoch [5455/10000], Loss: 0.39585304260253906\n",
      "Epoch [5456/10000], Loss: 0.39581525325775146\n",
      "Epoch [5457/10000], Loss: 0.3957773447036743\n",
      "Epoch [5458/10000], Loss: 0.3957395553588867\n",
      "Epoch [5459/10000], Loss: 0.3957017660140991\n",
      "Epoch [5460/10000], Loss: 0.3956640362739563\n",
      "Epoch [5461/10000], Loss: 0.39562636613845825\n",
      "Epoch [5462/10000], Loss: 0.39558857679367065\n",
      "Epoch [5463/10000], Loss: 0.39555102586746216\n",
      "Epoch [5464/10000], Loss: 0.3955133557319641\n",
      "Epoch [5465/10000], Loss: 0.3954758048057556\n",
      "Epoch [5466/10000], Loss: 0.39543813467025757\n",
      "Epoch [5467/10000], Loss: 0.3954005241394043\n",
      "Epoch [5468/10000], Loss: 0.3953629732131958\n",
      "Epoch [5469/10000], Loss: 0.39532554149627686\n",
      "Epoch [5470/10000], Loss: 0.39528799057006836\n",
      "Epoch [5471/10000], Loss: 0.3952506184577942\n",
      "Epoch [5472/10000], Loss: 0.39521312713623047\n",
      "Epoch [5473/10000], Loss: 0.3951757550239563\n",
      "Epoch [5474/10000], Loss: 0.39513832330703735\n",
      "Epoch [5475/10000], Loss: 0.39510101079940796\n",
      "Epoch [5476/10000], Loss: 0.39506369829177856\n",
      "Epoch [5477/10000], Loss: 0.39502638578414917\n",
      "Epoch [5478/10000], Loss: 0.3949891924858093\n",
      "Epoch [5479/10000], Loss: 0.39495187997817993\n",
      "Epoch [5480/10000], Loss: 0.3949146866798401\n",
      "Epoch [5481/10000], Loss: 0.39487743377685547\n",
      "Epoch [5482/10000], Loss: 0.3948403000831604\n",
      "Epoch [5483/10000], Loss: 0.39480316638946533\n",
      "Epoch [5484/10000], Loss: 0.39476603269577026\n",
      "Epoch [5485/10000], Loss: 0.39472895860671997\n",
      "Epoch [5486/10000], Loss: 0.39469194412231445\n",
      "Epoch [5487/10000], Loss: 0.39465487003326416\n",
      "Epoch [5488/10000], Loss: 0.3946179151535034\n",
      "Epoch [5489/10000], Loss: 0.3945808410644531\n",
      "Epoch [5490/10000], Loss: 0.39454400539398193\n",
      "Epoch [5491/10000], Loss: 0.3945069909095764\n",
      "Epoch [5492/10000], Loss: 0.3944701552391052\n",
      "Epoch [5493/10000], Loss: 0.3944332003593445\n",
      "Epoch [5494/10000], Loss: 0.3943963646888733\n",
      "Epoch [5495/10000], Loss: 0.3943595886230469\n",
      "Epoch [5496/10000], Loss: 0.3943227529525757\n",
      "Epoch [5497/10000], Loss: 0.39428597688674927\n",
      "Epoch [5498/10000], Loss: 0.3942492604255676\n",
      "Epoch [5499/10000], Loss: 0.39421242475509644\n",
      "Epoch [5500/10000], Loss: 0.3941757082939148\n",
      "Epoch [5501/10000], Loss: 0.3941391706466675\n",
      "Epoch [5502/10000], Loss: 0.39410245418548584\n",
      "Epoch [5503/10000], Loss: 0.394065797328949\n",
      "Epoch [5504/10000], Loss: 0.3940292000770569\n",
      "Epoch [5505/10000], Loss: 0.3939926028251648\n",
      "Epoch [5506/10000], Loss: 0.3939560651779175\n",
      "Epoch [5507/10000], Loss: 0.39391952753067017\n",
      "Epoch [5508/10000], Loss: 0.3938830494880676\n",
      "Epoch [5509/10000], Loss: 0.39384663105010986\n",
      "Epoch [5510/10000], Loss: 0.3938101530075073\n",
      "Epoch [5511/10000], Loss: 0.3937736749649048\n",
      "Epoch [5512/10000], Loss: 0.3937373161315918\n",
      "Epoch [5513/10000], Loss: 0.3937009572982788\n",
      "Epoch [5514/10000], Loss: 0.39366453886032104\n",
      "Epoch [5515/10000], Loss: 0.39362823963165283\n",
      "Epoch [5516/10000], Loss: 0.3935919404029846\n",
      "Epoch [5517/10000], Loss: 0.3935557007789612\n",
      "Epoch [5518/10000], Loss: 0.39351940155029297\n",
      "Epoch [5519/10000], Loss: 0.39348316192626953\n",
      "Epoch [5520/10000], Loss: 0.39344698190689087\n",
      "Epoch [5521/10000], Loss: 0.39341074228286743\n",
      "Epoch [5522/10000], Loss: 0.3933747410774231\n",
      "Epoch [5523/10000], Loss: 0.39333856105804443\n",
      "Epoch [5524/10000], Loss: 0.39330238103866577\n",
      "Epoch [5525/10000], Loss: 0.39326632022857666\n",
      "Epoch [5526/10000], Loss: 0.3932303190231323\n",
      "Epoch [5527/10000], Loss: 0.3931942582130432\n",
      "Epoch [5528/10000], Loss: 0.39315831661224365\n",
      "Epoch [5529/10000], Loss: 0.39312225580215454\n",
      "Epoch [5530/10000], Loss: 0.393086314201355\n",
      "Epoch [5531/10000], Loss: 0.3930503726005554\n",
      "Epoch [5532/10000], Loss: 0.39301449060440063\n",
      "Epoch [5533/10000], Loss: 0.39297860860824585\n",
      "Epoch [5534/10000], Loss: 0.39294278621673584\n",
      "Epoch [5535/10000], Loss: 0.39290696382522583\n",
      "Epoch [5536/10000], Loss: 0.39287108182907104\n",
      "Epoch [5537/10000], Loss: 0.39283525943756104\n",
      "Epoch [5538/10000], Loss: 0.3927996754646301\n",
      "Epoch [5539/10000], Loss: 0.39276379346847534\n",
      "Epoch [5540/10000], Loss: 0.39272814989089966\n",
      "Epoch [5541/10000], Loss: 0.3926924467086792\n",
      "Epoch [5542/10000], Loss: 0.3926568031311035\n",
      "Epoch [5543/10000], Loss: 0.39262115955352783\n",
      "Epoch [5544/10000], Loss: 0.3925855755805969\n",
      "Epoch [5545/10000], Loss: 0.39254993200302124\n",
      "Epoch [5546/10000], Loss: 0.39251434803009033\n",
      "Epoch [5547/10000], Loss: 0.3924788236618042\n",
      "Epoch [5548/10000], Loss: 0.39244329929351807\n",
      "Epoch [5549/10000], Loss: 0.3924078345298767\n",
      "Epoch [5550/10000], Loss: 0.3923723101615906\n",
      "Epoch [5551/10000], Loss: 0.392336905002594\n",
      "Epoch [5552/10000], Loss: 0.3923014998435974\n",
      "Epoch [5553/10000], Loss: 0.3922661542892456\n",
      "Epoch [5554/10000], Loss: 0.39223068952560425\n",
      "Epoch [5555/10000], Loss: 0.3921954035758972\n",
      "Epoch [5556/10000], Loss: 0.3921600580215454\n",
      "Epoch [5557/10000], Loss: 0.3921247720718384\n",
      "Epoch [5558/10000], Loss: 0.39208948612213135\n",
      "Epoch [5559/10000], Loss: 0.3920542597770691\n",
      "Epoch [5560/10000], Loss: 0.39201897382736206\n",
      "Epoch [5561/10000], Loss: 0.3919838070869446\n",
      "Epoch [5562/10000], Loss: 0.3919486403465271\n",
      "Epoch [5563/10000], Loss: 0.3919134736061096\n",
      "Epoch [5564/10000], Loss: 0.39187830686569214\n",
      "Epoch [5565/10000], Loss: 0.391843318939209\n",
      "Epoch [5566/10000], Loss: 0.3918081521987915\n",
      "Epoch [5567/10000], Loss: 0.3917731046676636\n",
      "Epoch [5568/10000], Loss: 0.39173805713653564\n",
      "Epoch [5569/10000], Loss: 0.3917030692100525\n",
      "Epoch [5570/10000], Loss: 0.39166802167892456\n",
      "Epoch [5571/10000], Loss: 0.3916330933570862\n",
      "Epoch [5572/10000], Loss: 0.3915982246398926\n",
      "Epoch [5573/10000], Loss: 0.3915632963180542\n",
      "Epoch [5574/10000], Loss: 0.3915284276008606\n",
      "Epoch [5575/10000], Loss: 0.3914934992790222\n",
      "Epoch [5576/10000], Loss: 0.3914586305618286\n",
      "Epoch [5577/10000], Loss: 0.3914238214492798\n",
      "Epoch [5578/10000], Loss: 0.39138907194137573\n",
      "Epoch [5579/10000], Loss: 0.3913542628288269\n",
      "Epoch [5580/10000], Loss: 0.3913195729255676\n",
      "Epoch [5581/10000], Loss: 0.3912848234176636\n",
      "Epoch [5582/10000], Loss: 0.3912500739097595\n",
      "Epoch [5583/10000], Loss: 0.391215443611145\n",
      "Epoch [5584/10000], Loss: 0.3911808133125305\n",
      "Epoch [5585/10000], Loss: 0.39114612340927124\n",
      "Epoch [5586/10000], Loss: 0.39111143350601196\n",
      "Epoch [5587/10000], Loss: 0.39107686281204224\n",
      "Epoch [5588/10000], Loss: 0.3910422921180725\n",
      "Epoch [5589/10000], Loss: 0.39100778102874756\n",
      "Epoch [5590/10000], Loss: 0.39097321033477783\n",
      "Epoch [5591/10000], Loss: 0.39093875885009766\n",
      "Epoch [5592/10000], Loss: 0.3909043073654175\n",
      "Epoch [5593/10000], Loss: 0.3908699154853821\n",
      "Epoch [5594/10000], Loss: 0.39083540439605713\n",
      "Epoch [5595/10000], Loss: 0.39080101251602173\n",
      "Epoch [5596/10000], Loss: 0.39076662063598633\n",
      "Epoch [5597/10000], Loss: 0.3907322883605957\n",
      "Epoch [5598/10000], Loss: 0.3906979560852051\n",
      "Epoch [5599/10000], Loss: 0.39066368341445923\n",
      "Epoch [5600/10000], Loss: 0.39062929153442383\n",
      "Epoch [5601/10000], Loss: 0.39059507846832275\n",
      "Epoch [5602/10000], Loss: 0.3905608057975769\n",
      "Epoch [5603/10000], Loss: 0.3905266523361206\n",
      "Epoch [5604/10000], Loss: 0.39049237966537476\n",
      "Epoch [5605/10000], Loss: 0.39045822620391846\n",
      "Epoch [5606/10000], Loss: 0.39042413234710693\n",
      "Epoch [5607/10000], Loss: 0.3903900384902954\n",
      "Epoch [5608/10000], Loss: 0.3903558850288391\n",
      "Epoch [5609/10000], Loss: 0.39032185077667236\n",
      "Epoch [5610/10000], Loss: 0.39028775691986084\n",
      "Epoch [5611/10000], Loss: 0.3902537226676941\n",
      "Epoch [5612/10000], Loss: 0.39021968841552734\n",
      "Epoch [5613/10000], Loss: 0.39018571376800537\n",
      "Epoch [5614/10000], Loss: 0.3901516795158386\n",
      "Epoch [5615/10000], Loss: 0.3901177644729614\n",
      "Epoch [5616/10000], Loss: 0.39008384943008423\n",
      "Epoch [5617/10000], Loss: 0.3900499939918518\n",
      "Epoch [5618/10000], Loss: 0.3900161385536194\n",
      "Epoch [5619/10000], Loss: 0.3899822235107422\n",
      "Epoch [5620/10000], Loss: 0.38994836807250977\n",
      "Epoch [5621/10000], Loss: 0.3899145722389221\n",
      "Epoch [5622/10000], Loss: 0.3898807764053345\n",
      "Epoch [5623/10000], Loss: 0.3898469805717468\n",
      "Epoch [5624/10000], Loss: 0.38981330394744873\n",
      "Epoch [5625/10000], Loss: 0.3897795081138611\n",
      "Epoch [5626/10000], Loss: 0.389745831489563\n",
      "Epoch [5627/10000], Loss: 0.38971221446990967\n",
      "Epoch [5628/10000], Loss: 0.3896785378456116\n",
      "Epoch [5629/10000], Loss: 0.38964492082595825\n",
      "Epoch [5630/10000], Loss: 0.38961124420166016\n",
      "Epoch [5631/10000], Loss: 0.3895776867866516\n",
      "Epoch [5632/10000], Loss: 0.38954412937164307\n",
      "Epoch [5633/10000], Loss: 0.3895105719566345\n",
      "Epoch [5634/10000], Loss: 0.3894771337509155\n",
      "Epoch [5635/10000], Loss: 0.3894435167312622\n",
      "Epoch [5636/10000], Loss: 0.389410138130188\n",
      "Epoch [5637/10000], Loss: 0.389376699924469\n",
      "Epoch [5638/10000], Loss: 0.3893432021141052\n",
      "Epoch [5639/10000], Loss: 0.389309823513031\n",
      "Epoch [5640/10000], Loss: 0.3892764449119568\n",
      "Epoch [5641/10000], Loss: 0.38924306631088257\n",
      "Epoch [5642/10000], Loss: 0.38920968770980835\n",
      "Epoch [5643/10000], Loss: 0.3891764283180237\n",
      "Epoch [5644/10000], Loss: 0.38914304971694946\n",
      "Epoch [5645/10000], Loss: 0.3891097903251648\n",
      "Epoch [5646/10000], Loss: 0.3890765905380249\n",
      "Epoch [5647/10000], Loss: 0.38904333114624023\n",
      "Epoch [5648/10000], Loss: 0.38901007175445557\n",
      "Epoch [5649/10000], Loss: 0.38897693157196045\n",
      "Epoch [5650/10000], Loss: 0.38894379138946533\n",
      "Epoch [5651/10000], Loss: 0.3889106512069702\n",
      "Epoch [5652/10000], Loss: 0.3888775110244751\n",
      "Epoch [5653/10000], Loss: 0.38884443044662476\n",
      "Epoch [5654/10000], Loss: 0.3888114094734192\n",
      "Epoch [5655/10000], Loss: 0.3887782692909241\n",
      "Epoch [5656/10000], Loss: 0.3887452483177185\n",
      "Epoch [5657/10000], Loss: 0.38871222734451294\n",
      "Epoch [5658/10000], Loss: 0.3886792063713074\n",
      "Epoch [5659/10000], Loss: 0.3886462450027466\n",
      "Epoch [5660/10000], Loss: 0.3886132836341858\n",
      "Epoch [5661/10000], Loss: 0.3885803818702698\n",
      "Epoch [5662/10000], Loss: 0.38854748010635376\n",
      "Epoch [5663/10000], Loss: 0.38851457834243774\n",
      "Epoch [5664/10000], Loss: 0.3884817361831665\n",
      "Epoch [5665/10000], Loss: 0.3884488344192505\n",
      "Epoch [5666/10000], Loss: 0.3884161114692688\n",
      "Epoch [5667/10000], Loss: 0.38838326930999756\n",
      "Epoch [5668/10000], Loss: 0.38835054636001587\n",
      "Epoch [5669/10000], Loss: 0.3883177638053894\n",
      "Epoch [5670/10000], Loss: 0.3882850408554077\n",
      "Epoch [5671/10000], Loss: 0.38825225830078125\n",
      "Epoch [5672/10000], Loss: 0.38821953535079956\n",
      "Epoch [5673/10000], Loss: 0.3881869912147522\n",
      "Epoch [5674/10000], Loss: 0.3881543278694153\n",
      "Epoch [5675/10000], Loss: 0.38812166452407837\n",
      "Epoch [5676/10000], Loss: 0.38808906078338623\n",
      "Epoch [5677/10000], Loss: 0.3880564570426941\n",
      "Epoch [5678/10000], Loss: 0.3880239725112915\n",
      "Epoch [5679/10000], Loss: 0.38799136877059937\n",
      "Epoch [5680/10000], Loss: 0.38795894384384155\n",
      "Epoch [5681/10000], Loss: 0.38792645931243896\n",
      "Epoch [5682/10000], Loss: 0.3878939747810364\n",
      "Epoch [5683/10000], Loss: 0.387861430644989\n",
      "Epoch [5684/10000], Loss: 0.38782912492752075\n",
      "Epoch [5685/10000], Loss: 0.3877965807914734\n",
      "Epoch [5686/10000], Loss: 0.3877642750740051\n",
      "Epoch [5687/10000], Loss: 0.3877319097518921\n",
      "Epoch [5688/10000], Loss: 0.38769960403442383\n",
      "Epoch [5689/10000], Loss: 0.3876672387123108\n",
      "Epoch [5690/10000], Loss: 0.38763493299484253\n",
      "Epoch [5691/10000], Loss: 0.38760268688201904\n",
      "Epoch [5692/10000], Loss: 0.3875703811645508\n",
      "Epoch [5693/10000], Loss: 0.3875380754470825\n",
      "Epoch [5694/10000], Loss: 0.3875059485435486\n",
      "Epoch [5695/10000], Loss: 0.3874737024307251\n",
      "Epoch [5696/10000], Loss: 0.3874415159225464\n",
      "Epoch [5697/10000], Loss: 0.38740938901901245\n",
      "Epoch [5698/10000], Loss: 0.3873772621154785\n",
      "Epoch [5699/10000], Loss: 0.38734519481658936\n",
      "Epoch [5700/10000], Loss: 0.38731300830841064\n",
      "Epoch [5701/10000], Loss: 0.38728100061416626\n",
      "Epoch [5702/10000], Loss: 0.3872489929199219\n",
      "Epoch [5703/10000], Loss: 0.3872169256210327\n",
      "Epoch [5704/10000], Loss: 0.3871849775314331\n",
      "Epoch [5705/10000], Loss: 0.3871529698371887\n",
      "Epoch [5706/10000], Loss: 0.38712090253829956\n",
      "Epoch [5707/10000], Loss: 0.3870888948440552\n",
      "Epoch [5708/10000], Loss: 0.38705700635910034\n",
      "Epoch [5709/10000], Loss: 0.3870251774787903\n",
      "Epoch [5710/10000], Loss: 0.38699328899383545\n",
      "Epoch [5711/10000], Loss: 0.3869614601135254\n",
      "Epoch [5712/10000], Loss: 0.38692957162857056\n",
      "Epoch [5713/10000], Loss: 0.3868977427482605\n",
      "Epoch [5714/10000], Loss: 0.3868659734725952\n",
      "Epoch [5715/10000], Loss: 0.38683414459228516\n",
      "Epoch [5716/10000], Loss: 0.3868024945259094\n",
      "Epoch [5717/10000], Loss: 0.38677066564559937\n",
      "Epoch [5718/10000], Loss: 0.38673895597457886\n",
      "Epoch [5719/10000], Loss: 0.38670724630355835\n",
      "Epoch [5720/10000], Loss: 0.3866756558418274\n",
      "Epoch [5721/10000], Loss: 0.3866438865661621\n",
      "Epoch [5722/10000], Loss: 0.38661229610443115\n",
      "Epoch [5723/10000], Loss: 0.3865806460380554\n",
      "Epoch [5724/10000], Loss: 0.38654911518096924\n",
      "Epoch [5725/10000], Loss: 0.3865175247192383\n",
      "Epoch [5726/10000], Loss: 0.3864859938621521\n",
      "Epoch [5727/10000], Loss: 0.3864544630050659\n",
      "Epoch [5728/10000], Loss: 0.3864229917526245\n",
      "Epoch [5729/10000], Loss: 0.38639146089553833\n",
      "Epoch [5730/10000], Loss: 0.3863599896430969\n",
      "Epoch [5731/10000], Loss: 0.3863285183906555\n",
      "Epoch [5732/10000], Loss: 0.3862971067428589\n",
      "Epoch [5733/10000], Loss: 0.38626569509506226\n",
      "Epoch [5734/10000], Loss: 0.3862343430519104\n",
      "Epoch [5735/10000], Loss: 0.38620299100875854\n",
      "Epoch [5736/10000], Loss: 0.3861716389656067\n",
      "Epoch [5737/10000], Loss: 0.38614028692245483\n",
      "Epoch [5738/10000], Loss: 0.386108934879303\n",
      "Epoch [5739/10000], Loss: 0.38607776165008545\n",
      "Epoch [5740/10000], Loss: 0.38604646921157837\n",
      "Epoch [5741/10000], Loss: 0.38601523637771606\n",
      "Epoch [5742/10000], Loss: 0.38598400354385376\n",
      "Epoch [5743/10000], Loss: 0.38595283031463623\n",
      "Epoch [5744/10000], Loss: 0.3859216570854187\n",
      "Epoch [5745/10000], Loss: 0.38589048385620117\n",
      "Epoch [5746/10000], Loss: 0.38585931062698364\n",
      "Epoch [5747/10000], Loss: 0.3858281970024109\n",
      "Epoch [5748/10000], Loss: 0.3857971429824829\n",
      "Epoch [5749/10000], Loss: 0.38576608896255493\n",
      "Epoch [5750/10000], Loss: 0.38573503494262695\n",
      "Epoch [5751/10000], Loss: 0.38570404052734375\n",
      "Epoch [5752/10000], Loss: 0.38567298650741577\n",
      "Epoch [5753/10000], Loss: 0.38564199209213257\n",
      "Epoch [5754/10000], Loss: 0.38561099767684937\n",
      "Epoch [5755/10000], Loss: 0.38558006286621094\n",
      "Epoch [5756/10000], Loss: 0.3855491280555725\n",
      "Epoch [5757/10000], Loss: 0.38551825284957886\n",
      "Epoch [5758/10000], Loss: 0.38548725843429565\n",
      "Epoch [5759/10000], Loss: 0.3854564428329468\n",
      "Epoch [5760/10000], Loss: 0.3854256272315979\n",
      "Epoch [5761/10000], Loss: 0.3853948712348938\n",
      "Epoch [5762/10000], Loss: 0.38536399602890015\n",
      "Epoch [5763/10000], Loss: 0.38533318042755127\n",
      "Epoch [5764/10000], Loss: 0.38530242443084717\n",
      "Epoch [5765/10000], Loss: 0.38527166843414307\n",
      "Epoch [5766/10000], Loss: 0.38524091243743896\n",
      "Epoch [5767/10000], Loss: 0.38521021604537964\n",
      "Epoch [5768/10000], Loss: 0.3851795196533203\n",
      "Epoch [5769/10000], Loss: 0.38514888286590576\n",
      "Epoch [5770/10000], Loss: 0.38511818647384644\n",
      "Epoch [5771/10000], Loss: 0.3850875496864319\n",
      "Epoch [5772/10000], Loss: 0.3850569725036621\n",
      "Epoch [5773/10000], Loss: 0.38502639532089233\n",
      "Epoch [5774/10000], Loss: 0.38499581813812256\n",
      "Epoch [5775/10000], Loss: 0.38496530055999756\n",
      "Epoch [5776/10000], Loss: 0.38493478298187256\n",
      "Epoch [5777/10000], Loss: 0.38490426540374756\n",
      "Epoch [5778/10000], Loss: 0.38487374782562256\n",
      "Epoch [5779/10000], Loss: 0.38484328985214233\n",
      "Epoch [5780/10000], Loss: 0.3848128914833069\n",
      "Epoch [5781/10000], Loss: 0.38478243350982666\n",
      "Epoch [5782/10000], Loss: 0.38475197553634644\n",
      "Epoch [5783/10000], Loss: 0.38472169637680054\n",
      "Epoch [5784/10000], Loss: 0.3846912384033203\n",
      "Epoch [5785/10000], Loss: 0.3846610188484192\n",
      "Epoch [5786/10000], Loss: 0.3846306800842285\n",
      "Epoch [5787/10000], Loss: 0.38460028171539307\n",
      "Epoch [5788/10000], Loss: 0.38457006216049194\n",
      "Epoch [5789/10000], Loss: 0.38453978300094604\n",
      "Epoch [5790/10000], Loss: 0.3845095634460449\n",
      "Epoch [5791/10000], Loss: 0.3844793438911438\n",
      "Epoch [5792/10000], Loss: 0.3844490647315979\n",
      "Epoch [5793/10000], Loss: 0.38441890478134155\n",
      "Epoch [5794/10000], Loss: 0.38438880443573\n",
      "Epoch [5795/10000], Loss: 0.3843587040901184\n",
      "Epoch [5796/10000], Loss: 0.3843284845352173\n",
      "Epoch [5797/10000], Loss: 0.38429832458496094\n",
      "Epoch [5798/10000], Loss: 0.3842683434486389\n",
      "Epoch [5799/10000], Loss: 0.38423824310302734\n",
      "Epoch [5800/10000], Loss: 0.3842082619667053\n",
      "Epoch [5801/10000], Loss: 0.38417816162109375\n",
      "Epoch [5802/10000], Loss: 0.38414818048477173\n",
      "Epoch [5803/10000], Loss: 0.3841181993484497\n",
      "Epoch [5804/10000], Loss: 0.3840882182121277\n",
      "Epoch [5805/10000], Loss: 0.3840583562850952\n",
      "Epoch [5806/10000], Loss: 0.3840283751487732\n",
      "Epoch [5807/10000], Loss: 0.38399845361709595\n",
      "Epoch [5808/10000], Loss: 0.38396865129470825\n",
      "Epoch [5809/10000], Loss: 0.383938729763031\n",
      "Epoch [5810/10000], Loss: 0.38390880823135376\n",
      "Epoch [5811/10000], Loss: 0.38387900590896606\n",
      "Epoch [5812/10000], Loss: 0.38384920358657837\n",
      "Epoch [5813/10000], Loss: 0.38381946086883545\n",
      "Epoch [5814/10000], Loss: 0.383789598941803\n",
      "Epoch [5815/10000], Loss: 0.38375991582870483\n",
      "Epoch [5816/10000], Loss: 0.38373011350631714\n",
      "Epoch [5817/10000], Loss: 0.383700430393219\n",
      "Epoch [5818/10000], Loss: 0.3836708068847656\n",
      "Epoch [5819/10000], Loss: 0.3836410641670227\n",
      "Epoch [5820/10000], Loss: 0.38361138105392456\n",
      "Epoch [5821/10000], Loss: 0.3835817575454712\n",
      "Epoch [5822/10000], Loss: 0.3835521340370178\n",
      "Epoch [5823/10000], Loss: 0.38352251052856445\n",
      "Epoch [5824/10000], Loss: 0.38349294662475586\n",
      "Epoch [5825/10000], Loss: 0.38346344232559204\n",
      "Epoch [5826/10000], Loss: 0.38343381881713867\n",
      "Epoch [5827/10000], Loss: 0.38340437412261963\n",
      "Epoch [5828/10000], Loss: 0.3833748698234558\n",
      "Epoch [5829/10000], Loss: 0.3833453059196472\n",
      "Epoch [5830/10000], Loss: 0.38331592082977295\n",
      "Epoch [5831/10000], Loss: 0.3832864761352539\n",
      "Epoch [5832/10000], Loss: 0.38325703144073486\n",
      "Epoch [5833/10000], Loss: 0.3832276463508606\n",
      "Epoch [5834/10000], Loss: 0.38319826126098633\n",
      "Epoch [5835/10000], Loss: 0.3831688165664673\n",
      "Epoch [5836/10000], Loss: 0.38313955068588257\n",
      "Epoch [5837/10000], Loss: 0.3831101655960083\n",
      "Epoch [5838/10000], Loss: 0.3830808401107788\n",
      "Epoch [5839/10000], Loss: 0.3830515742301941\n",
      "Epoch [5840/10000], Loss: 0.3830223083496094\n",
      "Epoch [5841/10000], Loss: 0.38299304246902466\n",
      "Epoch [5842/10000], Loss: 0.3829638361930847\n",
      "Epoch [5843/10000], Loss: 0.3829345703125\n",
      "Epoch [5844/10000], Loss: 0.38290536403656006\n",
      "Epoch [5845/10000], Loss: 0.38287627696990967\n",
      "Epoch [5846/10000], Loss: 0.38284701108932495\n",
      "Epoch [5847/10000], Loss: 0.38281792402267456\n",
      "Epoch [5848/10000], Loss: 0.3827887773513794\n",
      "Epoch [5849/10000], Loss: 0.38275963068008423\n",
      "Epoch [5850/10000], Loss: 0.3827306628227234\n",
      "Epoch [5851/10000], Loss: 0.3827015161514282\n",
      "Epoch [5852/10000], Loss: 0.3826724886894226\n",
      "Epoch [5853/10000], Loss: 0.3826434016227722\n",
      "Epoch [5854/10000], Loss: 0.38261449337005615\n",
      "Epoch [5855/10000], Loss: 0.38258546590805054\n",
      "Epoch [5856/10000], Loss: 0.3825564980506897\n",
      "Epoch [5857/10000], Loss: 0.3825274705886841\n",
      "Epoch [5858/10000], Loss: 0.3824986219406128\n",
      "Epoch [5859/10000], Loss: 0.38246965408325195\n",
      "Epoch [5860/10000], Loss: 0.3824407458305359\n",
      "Epoch [5861/10000], Loss: 0.3824118971824646\n",
      "Epoch [5862/10000], Loss: 0.38238298892974854\n",
      "Epoch [5863/10000], Loss: 0.382354199886322\n",
      "Epoch [5864/10000], Loss: 0.38232529163360596\n",
      "Epoch [5865/10000], Loss: 0.3822965621948242\n",
      "Epoch [5866/10000], Loss: 0.3822677731513977\n",
      "Epoch [5867/10000], Loss: 0.3822389245033264\n",
      "Epoch [5868/10000], Loss: 0.38221025466918945\n",
      "Epoch [5869/10000], Loss: 0.38218146562576294\n",
      "Epoch [5870/10000], Loss: 0.3821527361869812\n",
      "Epoch [5871/10000], Loss: 0.38212406635284424\n",
      "Epoch [5872/10000], Loss: 0.3820953965187073\n",
      "Epoch [5873/10000], Loss: 0.3820667266845703\n",
      "Epoch [5874/10000], Loss: 0.38203805685043335\n",
      "Epoch [5875/10000], Loss: 0.3820093870162964\n",
      "Epoch [5876/10000], Loss: 0.381980836391449\n",
      "Epoch [5877/10000], Loss: 0.3819522261619568\n",
      "Epoch [5878/10000], Loss: 0.3819236755371094\n",
      "Epoch [5879/10000], Loss: 0.38189512491226196\n",
      "Epoch [5880/10000], Loss: 0.3818665146827698\n",
      "Epoch [5881/10000], Loss: 0.3818381428718567\n",
      "Epoch [5882/10000], Loss: 0.38180965185165405\n",
      "Epoch [5883/10000], Loss: 0.38178110122680664\n",
      "Epoch [5884/10000], Loss: 0.381752610206604\n",
      "Epoch [5885/10000], Loss: 0.3817242383956909\n",
      "Epoch [5886/10000], Loss: 0.3816957473754883\n",
      "Epoch [5887/10000], Loss: 0.3816673159599304\n",
      "Epoch [5888/10000], Loss: 0.3816390037536621\n",
      "Epoch [5889/10000], Loss: 0.3816105127334595\n",
      "Epoch [5890/10000], Loss: 0.38158220052719116\n",
      "Epoch [5891/10000], Loss: 0.3815539479255676\n",
      "Epoch [5892/10000], Loss: 0.38152557611465454\n",
      "Epoch [5893/10000], Loss: 0.38149726390838623\n",
      "Epoch [5894/10000], Loss: 0.3814689517021179\n",
      "Epoch [5895/10000], Loss: 0.38144075870513916\n",
      "Epoch [5896/10000], Loss: 0.3814125061035156\n",
      "Epoch [5897/10000], Loss: 0.38138431310653687\n",
      "Epoch [5898/10000], Loss: 0.3813561201095581\n",
      "Epoch [5899/10000], Loss: 0.3813278079032898\n",
      "Epoch [5900/10000], Loss: 0.3812996745109558\n",
      "Epoch [5901/10000], Loss: 0.38127148151397705\n",
      "Epoch [5902/10000], Loss: 0.38124340772628784\n",
      "Epoch [5903/10000], Loss: 0.38121527433395386\n",
      "Epoch [5904/10000], Loss: 0.3811871409416199\n",
      "Epoch [5905/10000], Loss: 0.38115912675857544\n",
      "Epoch [5906/10000], Loss: 0.38113105297088623\n",
      "Epoch [5907/10000], Loss: 0.3811030387878418\n",
      "Epoch [5908/10000], Loss: 0.3810749053955078\n",
      "Epoch [5909/10000], Loss: 0.38104695081710815\n",
      "Epoch [5910/10000], Loss: 0.3810189366340637\n",
      "Epoch [5911/10000], Loss: 0.38099098205566406\n",
      "Epoch [5912/10000], Loss: 0.38096296787261963\n",
      "Epoch [5913/10000], Loss: 0.38093507289886475\n",
      "Epoch [5914/10000], Loss: 0.38090717792510986\n",
      "Epoch [5915/10000], Loss: 0.3808792233467102\n",
      "Epoch [5916/10000], Loss: 0.38085126876831055\n",
      "Epoch [5917/10000], Loss: 0.3808234930038452\n",
      "Epoch [5918/10000], Loss: 0.38079559803009033\n",
      "Epoch [5919/10000], Loss: 0.380767822265625\n",
      "Epoch [5920/10000], Loss: 0.3807399868965149\n",
      "Epoch [5921/10000], Loss: 0.3807121515274048\n",
      "Epoch [5922/10000], Loss: 0.3806843161582947\n",
      "Epoch [5923/10000], Loss: 0.3806565999984741\n",
      "Epoch [5924/10000], Loss: 0.38062888383865356\n",
      "Epoch [5925/10000], Loss: 0.3806012272834778\n",
      "Epoch [5926/10000], Loss: 0.38057345151901245\n",
      "Epoch [5927/10000], Loss: 0.3805456757545471\n",
      "Epoch [5928/10000], Loss: 0.3805180788040161\n",
      "Epoch [5929/10000], Loss: 0.3804903030395508\n",
      "Epoch [5930/10000], Loss: 0.3804627060890198\n",
      "Epoch [5931/10000], Loss: 0.380435049533844\n",
      "Epoch [5932/10000], Loss: 0.380407452583313\n",
      "Epoch [5933/10000], Loss: 0.380379855632782\n",
      "Epoch [5934/10000], Loss: 0.3803521990776062\n",
      "Epoch [5935/10000], Loss: 0.3803247809410095\n",
      "Epoch [5936/10000], Loss: 0.3802971839904785\n",
      "Epoch [5937/10000], Loss: 0.3802696466445923\n",
      "Epoch [5938/10000], Loss: 0.38024216890335083\n",
      "Epoch [5939/10000], Loss: 0.3802146315574646\n",
      "Epoch [5940/10000], Loss: 0.38018715381622314\n",
      "Epoch [5941/10000], Loss: 0.38015973567962646\n",
      "Epoch [5942/10000], Loss: 0.380132257938385\n",
      "Epoch [5943/10000], Loss: 0.38010483980178833\n",
      "Epoch [5944/10000], Loss: 0.38007742166519165\n",
      "Epoch [5945/10000], Loss: 0.38005006313323975\n",
      "Epoch [5946/10000], Loss: 0.38002270460128784\n",
      "Epoch [5947/10000], Loss: 0.37999528646469116\n",
      "Epoch [5948/10000], Loss: 0.37996798753738403\n",
      "Epoch [5949/10000], Loss: 0.37994062900543213\n",
      "Epoch [5950/10000], Loss: 0.379913330078125\n",
      "Epoch [5951/10000], Loss: 0.37988603115081787\n",
      "Epoch [5952/10000], Loss: 0.3798587918281555\n",
      "Epoch [5953/10000], Loss: 0.3798314929008484\n",
      "Epoch [5954/10000], Loss: 0.37980425357818604\n",
      "Epoch [5955/10000], Loss: 0.3797770142555237\n",
      "Epoch [5956/10000], Loss: 0.3797498941421509\n",
      "Epoch [5957/10000], Loss: 0.3797226548194885\n",
      "Epoch [5958/10000], Loss: 0.37969547510147095\n",
      "Epoch [5959/10000], Loss: 0.3796682357788086\n",
      "Epoch [5960/10000], Loss: 0.3796411156654358\n",
      "Epoch [5961/10000], Loss: 0.379613995552063\n",
      "Epoch [5962/10000], Loss: 0.3795868754386902\n",
      "Epoch [5963/10000], Loss: 0.37955987453460693\n",
      "Epoch [5964/10000], Loss: 0.37953275442123413\n",
      "Epoch [5965/10000], Loss: 0.3795056939125061\n",
      "Epoch [5966/10000], Loss: 0.3794787526130676\n",
      "Epoch [5967/10000], Loss: 0.3794516324996948\n",
      "Epoch [5968/10000], Loss: 0.3794246315956116\n",
      "Epoch [5969/10000], Loss: 0.3793976902961731\n",
      "Epoch [5970/10000], Loss: 0.37937068939208984\n",
      "Epoch [5971/10000], Loss: 0.3793436884880066\n",
      "Epoch [5972/10000], Loss: 0.3793167471885681\n",
      "Epoch [5973/10000], Loss: 0.37928974628448486\n",
      "Epoch [5974/10000], Loss: 0.37926292419433594\n",
      "Epoch [5975/10000], Loss: 0.379236102104187\n",
      "Epoch [5976/10000], Loss: 0.37920916080474854\n",
      "Epoch [5977/10000], Loss: 0.37918227910995483\n",
      "Epoch [5978/10000], Loss: 0.3791554570198059\n",
      "Epoch [5979/10000], Loss: 0.379128634929657\n",
      "Epoch [5980/10000], Loss: 0.37910181283950806\n",
      "Epoch [5981/10000], Loss: 0.37907499074935913\n",
      "Epoch [5982/10000], Loss: 0.379048228263855\n",
      "Epoch [5983/10000], Loss: 0.37902146577835083\n",
      "Epoch [5984/10000], Loss: 0.37899476289749146\n",
      "Epoch [5985/10000], Loss: 0.37896794080734253\n",
      "Epoch [5986/10000], Loss: 0.37894123792648315\n",
      "Epoch [5987/10000], Loss: 0.3789145350456238\n",
      "Epoch [5988/10000], Loss: 0.3788878321647644\n",
      "Epoch [5989/10000], Loss: 0.37886112928390503\n",
      "Epoch [5990/10000], Loss: 0.3788345456123352\n",
      "Epoch [5991/10000], Loss: 0.3788079023361206\n",
      "Epoch [5992/10000], Loss: 0.378781259059906\n",
      "Epoch [5993/10000], Loss: 0.3787546753883362\n",
      "Epoch [5994/10000], Loss: 0.3787280321121216\n",
      "Epoch [5995/10000], Loss: 0.37870150804519653\n",
      "Epoch [5996/10000], Loss: 0.3786749839782715\n",
      "Epoch [5997/10000], Loss: 0.37864840030670166\n",
      "Epoch [5998/10000], Loss: 0.3786219358444214\n",
      "Epoch [5999/10000], Loss: 0.37859535217285156\n",
      "Epoch [6000/10000], Loss: 0.37856876850128174\n",
      "Epoch [6001/10000], Loss: 0.37854236364364624\n",
      "Epoch [6002/10000], Loss: 0.37851589918136597\n",
      "Epoch [6003/10000], Loss: 0.3784894347190857\n",
      "Epoch [6004/10000], Loss: 0.37846308946609497\n",
      "Epoch [6005/10000], Loss: 0.3784366250038147\n",
      "Epoch [6006/10000], Loss: 0.378410279750824\n",
      "Epoch [6007/10000], Loss: 0.3783838152885437\n",
      "Epoch [6008/10000], Loss: 0.37835752964019775\n",
      "Epoch [6009/10000], Loss: 0.37833112478256226\n",
      "Epoch [6010/10000], Loss: 0.3783048391342163\n",
      "Epoch [6011/10000], Loss: 0.3782784342765808\n",
      "Epoch [6012/10000], Loss: 0.37825214862823486\n",
      "Epoch [6013/10000], Loss: 0.37822580337524414\n",
      "Epoch [6014/10000], Loss: 0.37819957733154297\n",
      "Epoch [6015/10000], Loss: 0.378173291683197\n",
      "Epoch [6016/10000], Loss: 0.37814706563949585\n",
      "Epoch [6017/10000], Loss: 0.3781208395957947\n",
      "Epoch [6018/10000], Loss: 0.3780946135520935\n",
      "Epoch [6019/10000], Loss: 0.3780684471130371\n",
      "Epoch [6020/10000], Loss: 0.37804222106933594\n",
      "Epoch [6021/10000], Loss: 0.3780161142349243\n",
      "Epoch [6022/10000], Loss: 0.37798988819122314\n",
      "Epoch [6023/10000], Loss: 0.37796372175216675\n",
      "Epoch [6024/10000], Loss: 0.3779377341270447\n",
      "Epoch [6025/10000], Loss: 0.3779115676879883\n",
      "Epoch [6026/10000], Loss: 0.37788552045822144\n",
      "Epoch [6027/10000], Loss: 0.3778594136238098\n",
      "Epoch [6028/10000], Loss: 0.37783336639404297\n",
      "Epoch [6029/10000], Loss: 0.3778073191642761\n",
      "Epoch [6030/10000], Loss: 0.3777812719345093\n",
      "Epoch [6031/10000], Loss: 0.37775522470474243\n",
      "Epoch [6032/10000], Loss: 0.37772929668426514\n",
      "Epoch [6033/10000], Loss: 0.37770330905914307\n",
      "Epoch [6034/10000], Loss: 0.377677321434021\n",
      "Epoch [6035/10000], Loss: 0.3776513338088989\n",
      "Epoch [6036/10000], Loss: 0.3776254653930664\n",
      "Epoch [6037/10000], Loss: 0.3775995373725891\n",
      "Epoch [6038/10000], Loss: 0.37757354974746704\n",
      "Epoch [6039/10000], Loss: 0.3775477409362793\n",
      "Epoch [6040/10000], Loss: 0.377521812915802\n",
      "Epoch [6041/10000], Loss: 0.3774958848953247\n",
      "Epoch [6042/10000], Loss: 0.37747007608413696\n",
      "Epoch [6043/10000], Loss: 0.3774442672729492\n",
      "Epoch [6044/10000], Loss: 0.3774184584617615\n",
      "Epoch [6045/10000], Loss: 0.3773927092552185\n",
      "Epoch [6046/10000], Loss: 0.37736690044403076\n",
      "Epoch [6047/10000], Loss: 0.377341091632843\n",
      "Epoch [6048/10000], Loss: 0.37731534242630005\n",
      "Epoch [6049/10000], Loss: 0.3772895932197571\n",
      "Epoch [6050/10000], Loss: 0.3772639036178589\n",
      "Epoch [6051/10000], Loss: 0.3772382140159607\n",
      "Epoch [6052/10000], Loss: 0.3772125244140625\n",
      "Epoch [6053/10000], Loss: 0.37718677520751953\n",
      "Epoch [6054/10000], Loss: 0.37716108560562134\n",
      "Epoch [6055/10000], Loss: 0.3771354556083679\n",
      "Epoch [6056/10000], Loss: 0.3771098256111145\n",
      "Epoch [6057/10000], Loss: 0.3770841360092163\n",
      "Epoch [6058/10000], Loss: 0.37705856561660767\n",
      "Epoch [6059/10000], Loss: 0.37703293561935425\n",
      "Epoch [6060/10000], Loss: 0.3770073652267456\n",
      "Epoch [6061/10000], Loss: 0.37698179483413696\n",
      "Epoch [6062/10000], Loss: 0.3769562840461731\n",
      "Epoch [6063/10000], Loss: 0.37693071365356445\n",
      "Epoch [6064/10000], Loss: 0.3769051432609558\n",
      "Epoch [6065/10000], Loss: 0.3768796920776367\n",
      "Epoch [6066/10000], Loss: 0.3768541216850281\n",
      "Epoch [6067/10000], Loss: 0.37682873010635376\n",
      "Epoch [6068/10000], Loss: 0.37680327892303467\n",
      "Epoch [6069/10000], Loss: 0.3767777681350708\n",
      "Epoch [6070/10000], Loss: 0.3767523765563965\n",
      "Epoch [6071/10000], Loss: 0.3767269253730774\n",
      "Epoch [6072/10000], Loss: 0.3767014741897583\n",
      "Epoch [6073/10000], Loss: 0.376676082611084\n",
      "Epoch [6074/10000], Loss: 0.37665075063705444\n",
      "Epoch [6075/10000], Loss: 0.3766253590583801\n",
      "Epoch [6076/10000], Loss: 0.3765999674797058\n",
      "Epoch [6077/10000], Loss: 0.37657463550567627\n",
      "Epoch [6078/10000], Loss: 0.3765493631362915\n",
      "Epoch [6079/10000], Loss: 0.37652403116226196\n",
      "Epoch [6080/10000], Loss: 0.3764987587928772\n",
      "Epoch [6081/10000], Loss: 0.37647348642349243\n",
      "Epoch [6082/10000], Loss: 0.3764481544494629\n",
      "Epoch [6083/10000], Loss: 0.3764229416847229\n",
      "Epoch [6084/10000], Loss: 0.37639766931533813\n",
      "Epoch [6085/10000], Loss: 0.37637245655059814\n",
      "Epoch [6086/10000], Loss: 0.37634724378585815\n",
      "Epoch [6087/10000], Loss: 0.37632209062576294\n",
      "Epoch [6088/10000], Loss: 0.3762969374656677\n",
      "Epoch [6089/10000], Loss: 0.37627172470092773\n",
      "Epoch [6090/10000], Loss: 0.3762465715408325\n",
      "Epoch [6091/10000], Loss: 0.3762214779853821\n",
      "Epoch [6092/10000], Loss: 0.37619632482528687\n",
      "Epoch [6093/10000], Loss: 0.37617117166519165\n",
      "Epoch [6094/10000], Loss: 0.376146137714386\n",
      "Epoch [6095/10000], Loss: 0.37612104415893555\n",
      "Epoch [6096/10000], Loss: 0.37609589099884033\n",
      "Epoch [6097/10000], Loss: 0.3760709762573242\n",
      "Epoch [6098/10000], Loss: 0.376045823097229\n",
      "Epoch [6099/10000], Loss: 0.3760208487510681\n",
      "Epoch [6100/10000], Loss: 0.37599581480026245\n",
      "Epoch [6101/10000], Loss: 0.3759707808494568\n",
      "Epoch [6102/10000], Loss: 0.3759457468986511\n",
      "Epoch [6103/10000], Loss: 0.375920832157135\n",
      "Epoch [6104/10000], Loss: 0.3758959174156189\n",
      "Epoch [6105/10000], Loss: 0.3758710026741028\n",
      "Epoch [6106/10000], Loss: 0.3758460283279419\n",
      "Epoch [6107/10000], Loss: 0.3758211135864258\n",
      "Epoch [6108/10000], Loss: 0.37579625844955444\n",
      "Epoch [6109/10000], Loss: 0.37577134370803833\n",
      "Epoch [6110/10000], Loss: 0.37574654817581177\n",
      "Epoch [6111/10000], Loss: 0.37572163343429565\n",
      "Epoch [6112/10000], Loss: 0.3756967782974243\n",
      "Epoch [6113/10000], Loss: 0.375671923160553\n",
      "Epoch [6114/10000], Loss: 0.3756471276283264\n",
      "Epoch [6115/10000], Loss: 0.37562233209609985\n",
      "Epoch [6116/10000], Loss: 0.3755974769592285\n",
      "Epoch [6117/10000], Loss: 0.37557274103164673\n",
      "Epoch [6118/10000], Loss: 0.37554800510406494\n",
      "Epoch [6119/10000], Loss: 0.3755232095718384\n",
      "Epoch [6120/10000], Loss: 0.3754984736442566\n",
      "Epoch [6121/10000], Loss: 0.3754737973213196\n",
      "Epoch [6122/10000], Loss: 0.3754490613937378\n",
      "Epoch [6123/10000], Loss: 0.3754243850708008\n",
      "Epoch [6124/10000], Loss: 0.375399649143219\n",
      "Epoch [6125/10000], Loss: 0.3753751516342163\n",
      "Epoch [6126/10000], Loss: 0.3753504157066345\n",
      "Epoch [6127/10000], Loss: 0.3753257393836975\n",
      "Epoch [6128/10000], Loss: 0.37530118227005005\n",
      "Epoch [6129/10000], Loss: 0.37527650594711304\n",
      "Epoch [6130/10000], Loss: 0.3752519488334656\n",
      "Epoch [6131/10000], Loss: 0.37522733211517334\n",
      "Epoch [6132/10000], Loss: 0.3752027750015259\n",
      "Epoch [6133/10000], Loss: 0.3751782178878784\n",
      "Epoch [6134/10000], Loss: 0.37515372037887573\n",
      "Epoch [6135/10000], Loss: 0.37512916326522827\n",
      "Epoch [6136/10000], Loss: 0.3751046061515808\n",
      "Epoch [6137/10000], Loss: 0.3750801086425781\n",
      "Epoch [6138/10000], Loss: 0.3750556707382202\n",
      "Epoch [6139/10000], Loss: 0.37503111362457275\n",
      "Epoch [6140/10000], Loss: 0.37500667572021484\n",
      "Epoch [6141/10000], Loss: 0.3749822974205017\n",
      "Epoch [6142/10000], Loss: 0.37495774030685425\n",
      "Epoch [6143/10000], Loss: 0.3749333620071411\n",
      "Epoch [6144/10000], Loss: 0.374908983707428\n",
      "Epoch [6145/10000], Loss: 0.37488454580307007\n",
      "Epoch [6146/10000], Loss: 0.3748602271080017\n",
      "Epoch [6147/10000], Loss: 0.3748357892036438\n",
      "Epoch [6148/10000], Loss: 0.3748113512992859\n",
      "Epoch [6149/10000], Loss: 0.37478703260421753\n",
      "Epoch [6150/10000], Loss: 0.37476271390914917\n",
      "Epoch [6151/10000], Loss: 0.3747383952140808\n",
      "Epoch [6152/10000], Loss: 0.37471407651901245\n",
      "Epoch [6153/10000], Loss: 0.3746897578239441\n",
      "Epoch [6154/10000], Loss: 0.37466561794281006\n",
      "Epoch [6155/10000], Loss: 0.3746412992477417\n",
      "Epoch [6156/10000], Loss: 0.3746170401573181\n",
      "Epoch [6157/10000], Loss: 0.37459272146224976\n",
      "Epoch [6158/10000], Loss: 0.37456852197647095\n",
      "Epoch [6159/10000], Loss: 0.37454432249069214\n",
      "Epoch [6160/10000], Loss: 0.37452012300491333\n",
      "Epoch [6161/10000], Loss: 0.3744959235191345\n",
      "Epoch [6162/10000], Loss: 0.3744717240333557\n",
      "Epoch [6163/10000], Loss: 0.37444764375686646\n",
      "Epoch [6164/10000], Loss: 0.37442344427108765\n",
      "Epoch [6165/10000], Loss: 0.3743993043899536\n",
      "Epoch [6166/10000], Loss: 0.3743751645088196\n",
      "Epoch [6167/10000], Loss: 0.3743510842323303\n",
      "Epoch [6168/10000], Loss: 0.37432700395584106\n",
      "Epoch [6169/10000], Loss: 0.37430286407470703\n",
      "Epoch [6170/10000], Loss: 0.3742789030075073\n",
      "Epoch [6171/10000], Loss: 0.3742547035217285\n",
      "Epoch [6172/10000], Loss: 0.37423068284988403\n",
      "Epoch [6173/10000], Loss: 0.3742066025733948\n",
      "Epoch [6174/10000], Loss: 0.37418264150619507\n",
      "Epoch [6175/10000], Loss: 0.3741585612297058\n",
      "Epoch [6176/10000], Loss: 0.3741346597671509\n",
      "Epoch [6177/10000], Loss: 0.3741106390953064\n",
      "Epoch [6178/10000], Loss: 0.3740866780281067\n",
      "Epoch [6179/10000], Loss: 0.3740626573562622\n",
      "Epoch [6180/10000], Loss: 0.3740387558937073\n",
      "Epoch [6181/10000], Loss: 0.37401479482650757\n",
      "Epoch [6182/10000], Loss: 0.3739909529685974\n",
      "Epoch [6183/10000], Loss: 0.3739670515060425\n",
      "Epoch [6184/10000], Loss: 0.373943030834198\n",
      "Epoch [6185/10000], Loss: 0.37391912937164307\n",
      "Epoch [6186/10000], Loss: 0.3738953471183777\n",
      "Epoch [6187/10000], Loss: 0.37387144565582275\n",
      "Epoch [6188/10000], Loss: 0.3738476037979126\n",
      "Epoch [6189/10000], Loss: 0.3738238215446472\n",
      "Epoch [6190/10000], Loss: 0.37379997968673706\n",
      "Epoch [6191/10000], Loss: 0.3737761378288269\n",
      "Epoch [6192/10000], Loss: 0.37375229597091675\n",
      "Epoch [6193/10000], Loss: 0.3737286329269409\n",
      "Epoch [6194/10000], Loss: 0.37370479106903076\n",
      "Epoch [6195/10000], Loss: 0.37368106842041016\n",
      "Epoch [6196/10000], Loss: 0.3736572265625\n",
      "Epoch [6197/10000], Loss: 0.3736335039138794\n",
      "Epoch [6198/10000], Loss: 0.37360984086990356\n",
      "Epoch [6199/10000], Loss: 0.37358611822128296\n",
      "Epoch [6200/10000], Loss: 0.37356239557266235\n",
      "Epoch [6201/10000], Loss: 0.3735387921333313\n",
      "Epoch [6202/10000], Loss: 0.3735150694847107\n",
      "Epoch [6203/10000], Loss: 0.3734913468360901\n",
      "Epoch [6204/10000], Loss: 0.37346774339675903\n",
      "Epoch [6205/10000], Loss: 0.373444139957428\n",
      "Epoch [6206/10000], Loss: 0.3734204173088074\n",
      "Epoch [6207/10000], Loss: 0.3733968734741211\n",
      "Epoch [6208/10000], Loss: 0.3733733296394348\n",
      "Epoch [6209/10000], Loss: 0.373349666595459\n",
      "Epoch [6210/10000], Loss: 0.3733261227607727\n",
      "Epoch [6211/10000], Loss: 0.37330251932144165\n",
      "Epoch [6212/10000], Loss: 0.37327903509140015\n",
      "Epoch [6213/10000], Loss: 0.3732554316520691\n",
      "Epoch [6214/10000], Loss: 0.3732319474220276\n",
      "Epoch [6215/10000], Loss: 0.3732084035873413\n",
      "Epoch [6216/10000], Loss: 0.37318485975265503\n",
      "Epoch [6217/10000], Loss: 0.3731613755226135\n",
      "Epoch [6218/10000], Loss: 0.3731379508972168\n",
      "Epoch [6219/10000], Loss: 0.3731144666671753\n",
      "Epoch [6220/10000], Loss: 0.373090922832489\n",
      "Epoch [6221/10000], Loss: 0.3730676770210266\n",
      "Epoch [6222/10000], Loss: 0.37304413318634033\n",
      "Epoch [6223/10000], Loss: 0.3730207681655884\n",
      "Epoch [6224/10000], Loss: 0.3729972839355469\n",
      "Epoch [6225/10000], Loss: 0.37297385931015015\n",
      "Epoch [6226/10000], Loss: 0.37295055389404297\n",
      "Epoch [6227/10000], Loss: 0.37292712926864624\n",
      "Epoch [6228/10000], Loss: 0.3729037642478943\n",
      "Epoch [6229/10000], Loss: 0.37288039922714233\n",
      "Epoch [6230/10000], Loss: 0.37285709381103516\n",
      "Epoch [6231/10000], Loss: 0.3728337287902832\n",
      "Epoch [6232/10000], Loss: 0.3728104829788208\n",
      "Epoch [6233/10000], Loss: 0.3727870583534241\n",
      "Epoch [6234/10000], Loss: 0.3727639317512512\n",
      "Epoch [6235/10000], Loss: 0.37274062633514404\n",
      "Epoch [6236/10000], Loss: 0.3727172613143921\n",
      "Epoch [6237/10000], Loss: 0.37269407510757446\n",
      "Epoch [6238/10000], Loss: 0.37267082929611206\n",
      "Epoch [6239/10000], Loss: 0.37264764308929443\n",
      "Epoch [6240/10000], Loss: 0.37262439727783203\n",
      "Epoch [6241/10000], Loss: 0.3726012110710144\n",
      "Epoch [6242/10000], Loss: 0.372577965259552\n",
      "Epoch [6243/10000], Loss: 0.37255483865737915\n",
      "Epoch [6244/10000], Loss: 0.3725317120552063\n",
      "Epoch [6245/10000], Loss: 0.3725084662437439\n",
      "Epoch [6246/10000], Loss: 0.37248528003692627\n",
      "Epoch [6247/10000], Loss: 0.3724622130393982\n",
      "Epoch [6248/10000], Loss: 0.37243902683258057\n",
      "Epoch [6249/10000], Loss: 0.3724159598350525\n",
      "Epoch [6250/10000], Loss: 0.37239283323287964\n",
      "Epoch [6251/10000], Loss: 0.37236976623535156\n",
      "Epoch [6252/10000], Loss: 0.3723466992378235\n",
      "Epoch [6253/10000], Loss: 0.3723236322402954\n",
      "Epoch [6254/10000], Loss: 0.37230056524276733\n",
      "Epoch [6255/10000], Loss: 0.37227755784988403\n",
      "Epoch [6256/10000], Loss: 0.37225455045700073\n",
      "Epoch [6257/10000], Loss: 0.37223148345947266\n",
      "Epoch [6258/10000], Loss: 0.3722084164619446\n",
      "Epoch [6259/10000], Loss: 0.3721854090690613\n",
      "Epoch [6260/10000], Loss: 0.37216246128082275\n",
      "Epoch [6261/10000], Loss: 0.37213951349258423\n",
      "Epoch [6262/10000], Loss: 0.3721166253089905\n",
      "Epoch [6263/10000], Loss: 0.3720934987068176\n",
      "Epoch [6264/10000], Loss: 0.37207067012786865\n",
      "Epoch [6265/10000], Loss: 0.3720477223396301\n",
      "Epoch [6266/10000], Loss: 0.3720248341560364\n",
      "Epoch [6267/10000], Loss: 0.3720018267631531\n",
      "Epoch [6268/10000], Loss: 0.3719790577888489\n",
      "Epoch [6269/10000], Loss: 0.3719561696052551\n",
      "Epoch [6270/10000], Loss: 0.3719332814216614\n",
      "Epoch [6271/10000], Loss: 0.3719103932380676\n",
      "Epoch [6272/10000], Loss: 0.37188756465911865\n",
      "Epoch [6273/10000], Loss: 0.3718647360801697\n",
      "Epoch [6274/10000], Loss: 0.3718419075012207\n",
      "Epoch [6275/10000], Loss: 0.37181907892227173\n",
      "Epoch [6276/10000], Loss: 0.37179625034332275\n",
      "Epoch [6277/10000], Loss: 0.37177354097366333\n",
      "Epoch [6278/10000], Loss: 0.37175071239471436\n",
      "Epoch [6279/10000], Loss: 0.3717278838157654\n",
      "Epoch [6280/10000], Loss: 0.3717051148414612\n",
      "Epoch [6281/10000], Loss: 0.3716825246810913\n",
      "Epoch [6282/10000], Loss: 0.37165963649749756\n",
      "Epoch [6283/10000], Loss: 0.37163692712783813\n",
      "Epoch [6284/10000], Loss: 0.37161415815353394\n",
      "Epoch [6285/10000], Loss: 0.3715915083885193\n",
      "Epoch [6286/10000], Loss: 0.3715687394142151\n",
      "Epoch [6287/10000], Loss: 0.3715461492538452\n",
      "Epoch [6288/10000], Loss: 0.3715234398841858\n",
      "Epoch [6289/10000], Loss: 0.3715008497238159\n",
      "Epoch [6290/10000], Loss: 0.37147819995880127\n",
      "Epoch [6291/10000], Loss: 0.3714555501937866\n",
      "Epoch [6292/10000], Loss: 0.371432900428772\n",
      "Epoch [6293/10000], Loss: 0.3714103102684021\n",
      "Epoch [6294/10000], Loss: 0.3713876008987427\n",
      "Epoch [6295/10000], Loss: 0.3713650703430176\n",
      "Epoch [6296/10000], Loss: 0.3713424801826477\n",
      "Epoch [6297/10000], Loss: 0.37131989002227783\n",
      "Epoch [6298/10000], Loss: 0.37129729986190796\n",
      "Epoch [6299/10000], Loss: 0.37127482891082764\n",
      "Epoch [6300/10000], Loss: 0.37125223875045776\n",
      "Epoch [6301/10000], Loss: 0.37122970819473267\n",
      "Epoch [6302/10000], Loss: 0.37120717763900757\n",
      "Epoch [6303/10000], Loss: 0.37118470668792725\n",
      "Epoch [6304/10000], Loss: 0.3711621165275574\n",
      "Epoch [6305/10000], Loss: 0.3711397051811218\n",
      "Epoch [6306/10000], Loss: 0.3711172342300415\n",
      "Epoch [6307/10000], Loss: 0.3710947036743164\n",
      "Epoch [6308/10000], Loss: 0.3710722327232361\n",
      "Epoch [6309/10000], Loss: 0.3710498809814453\n",
      "Epoch [6310/10000], Loss: 0.371027410030365\n",
      "Epoch [6311/10000], Loss: 0.37100499868392944\n",
      "Epoch [6312/10000], Loss: 0.3709825873374939\n",
      "Epoch [6313/10000], Loss: 0.37096017599105835\n",
      "Epoch [6314/10000], Loss: 0.370937705039978\n",
      "Epoch [6315/10000], Loss: 0.37091535329818726\n",
      "Epoch [6316/10000], Loss: 0.3708930015563965\n",
      "Epoch [6317/10000], Loss: 0.3708706498146057\n",
      "Epoch [6318/10000], Loss: 0.37084823846817017\n",
      "Epoch [6319/10000], Loss: 0.3708258867263794\n",
      "Epoch [6320/10000], Loss: 0.3708036541938782\n",
      "Epoch [6321/10000], Loss: 0.3707813024520874\n",
      "Epoch [6322/10000], Loss: 0.3707590103149414\n",
      "Epoch [6323/10000], Loss: 0.37073665857315063\n",
      "Epoch [6324/10000], Loss: 0.37071436643600464\n",
      "Epoch [6325/10000], Loss: 0.3706921339035034\n",
      "Epoch [6326/10000], Loss: 0.3706699013710022\n",
      "Epoch [6327/10000], Loss: 0.3706476092338562\n",
      "Epoch [6328/10000], Loss: 0.370625376701355\n",
      "Epoch [6329/10000], Loss: 0.37060314416885376\n",
      "Epoch [6330/10000], Loss: 0.37058085203170776\n",
      "Epoch [6331/10000], Loss: 0.3705586791038513\n",
      "Epoch [6332/10000], Loss: 0.3705365061759949\n",
      "Epoch [6333/10000], Loss: 0.3705143332481384\n",
      "Epoch [6334/10000], Loss: 0.37049204111099243\n",
      "Epoch [6335/10000], Loss: 0.37046992778778076\n",
      "Epoch [6336/10000], Loss: 0.3704478144645691\n",
      "Epoch [6337/10000], Loss: 0.37042564153671265\n",
      "Epoch [6338/10000], Loss: 0.3704034686088562\n",
      "Epoch [6339/10000], Loss: 0.37038135528564453\n",
      "Epoch [6340/10000], Loss: 0.37035930156707764\n",
      "Epoch [6341/10000], Loss: 0.3703371286392212\n",
      "Epoch [6342/10000], Loss: 0.3703150749206543\n",
      "Epoch [6343/10000], Loss: 0.3702929615974426\n",
      "Epoch [6344/10000], Loss: 0.37027090787887573\n",
      "Epoch [6345/10000], Loss: 0.3702487349510193\n",
      "Epoch [6346/10000], Loss: 0.37022674083709717\n",
      "Epoch [6347/10000], Loss: 0.37020474672317505\n",
      "Epoch [6348/10000], Loss: 0.3701826333999634\n",
      "Epoch [6349/10000], Loss: 0.37016063928604126\n",
      "Epoch [6350/10000], Loss: 0.37013858556747437\n",
      "Epoch [6351/10000], Loss: 0.37011659145355225\n",
      "Epoch [6352/10000], Loss: 0.3700946569442749\n",
      "Epoch [6353/10000], Loss: 0.37007254362106323\n",
      "Epoch [6354/10000], Loss: 0.3700506091117859\n",
      "Epoch [6355/10000], Loss: 0.37002861499786377\n",
      "Epoch [6356/10000], Loss: 0.3700066804885864\n",
      "Epoch [6357/10000], Loss: 0.3699847459793091\n",
      "Epoch [6358/10000], Loss: 0.36996281147003174\n",
      "Epoch [6359/10000], Loss: 0.3699408769607544\n",
      "Epoch [6360/10000], Loss: 0.3699190020561218\n",
      "Epoch [6361/10000], Loss: 0.3698970675468445\n",
      "Epoch [6362/10000], Loss: 0.3698752522468567\n",
      "Epoch [6363/10000], Loss: 0.36985331773757935\n",
      "Epoch [6364/10000], Loss: 0.36983150243759155\n",
      "Epoch [6365/10000], Loss: 0.3698095679283142\n",
      "Epoch [6366/10000], Loss: 0.3697877526283264\n",
      "Epoch [6367/10000], Loss: 0.36976587772369385\n",
      "Epoch [6368/10000], Loss: 0.3697440028190613\n",
      "Epoch [6369/10000], Loss: 0.36972224712371826\n",
      "Epoch [6370/10000], Loss: 0.3697003722190857\n",
      "Epoch [6371/10000], Loss: 0.3696785569190979\n",
      "Epoch [6372/10000], Loss: 0.3696567416191101\n",
      "Epoch [6373/10000], Loss: 0.36963504552841187\n",
      "Epoch [6374/10000], Loss: 0.3696132302284241\n",
      "Epoch [6375/10000], Loss: 0.3695914149284363\n",
      "Epoch [6376/10000], Loss: 0.36956971883773804\n",
      "Epoch [6377/10000], Loss: 0.3695480227470398\n",
      "Epoch [6378/10000], Loss: 0.369526207447052\n",
      "Epoch [6379/10000], Loss: 0.36950451135635376\n",
      "Epoch [6380/10000], Loss: 0.3694828152656555\n",
      "Epoch [6381/10000], Loss: 0.3694611191749573\n",
      "Epoch [6382/10000], Loss: 0.36943942308425903\n",
      "Epoch [6383/10000], Loss: 0.3694177269935608\n",
      "Epoch [6384/10000], Loss: 0.3693960905075073\n",
      "Epoch [6385/10000], Loss: 0.3693743944168091\n",
      "Epoch [6386/10000], Loss: 0.36935269832611084\n",
      "Epoch [6387/10000], Loss: 0.3693310618400574\n",
      "Epoch [6388/10000], Loss: 0.3693094253540039\n",
      "Epoch [6389/10000], Loss: 0.3692878484725952\n",
      "Epoch [6390/10000], Loss: 0.369266152381897\n",
      "Epoch [6391/10000], Loss: 0.3692445755004883\n",
      "Epoch [6392/10000], Loss: 0.3692229986190796\n",
      "Epoch [6393/10000], Loss: 0.36920130252838135\n",
      "Epoch [6394/10000], Loss: 0.3691798448562622\n",
      "Epoch [6395/10000], Loss: 0.3691582679748535\n",
      "Epoch [6396/10000], Loss: 0.3691366910934448\n",
      "Epoch [6397/10000], Loss: 0.3691152334213257\n",
      "Epoch [6398/10000], Loss: 0.3690935969352722\n",
      "Epoch [6399/10000], Loss: 0.3690720796585083\n",
      "Epoch [6400/10000], Loss: 0.3690505623817444\n",
      "Epoch [6401/10000], Loss: 0.36902904510498047\n",
      "Epoch [6402/10000], Loss: 0.36900752782821655\n",
      "Epoch [6403/10000], Loss: 0.3689860701560974\n",
      "Epoch [6404/10000], Loss: 0.36896461248397827\n",
      "Epoch [6405/10000], Loss: 0.3689430356025696\n",
      "Epoch [6406/10000], Loss: 0.36892157793045044\n",
      "Epoch [6407/10000], Loss: 0.3689001202583313\n",
      "Epoch [6408/10000], Loss: 0.36887866258621216\n",
      "Epoch [6409/10000], Loss: 0.3688572645187378\n",
      "Epoch [6410/10000], Loss: 0.3688358664512634\n",
      "Epoch [6411/10000], Loss: 0.3688144087791443\n",
      "Epoch [6412/10000], Loss: 0.3687930107116699\n",
      "Epoch [6413/10000], Loss: 0.36877161264419556\n",
      "Epoch [6414/10000], Loss: 0.3687502145767212\n",
      "Epoch [6415/10000], Loss: 0.3687289357185364\n",
      "Epoch [6416/10000], Loss: 0.368707537651062\n",
      "Epoch [6417/10000], Loss: 0.36868613958358765\n",
      "Epoch [6418/10000], Loss: 0.36866480112075806\n",
      "Epoch [6419/10000], Loss: 0.3686434030532837\n",
      "Epoch [6420/10000], Loss: 0.3686221241950989\n",
      "Epoch [6421/10000], Loss: 0.3686007261276245\n",
      "Epoch [6422/10000], Loss: 0.3685795068740845\n",
      "Epoch [6423/10000], Loss: 0.36855804920196533\n",
      "Epoch [6424/10000], Loss: 0.36853694915771484\n",
      "Epoch [6425/10000], Loss: 0.3685155510902405\n",
      "Epoch [6426/10000], Loss: 0.36849433183670044\n",
      "Epoch [6427/10000], Loss: 0.3684730529785156\n",
      "Epoch [6428/10000], Loss: 0.36845171451568604\n",
      "Epoch [6429/10000], Loss: 0.368430495262146\n",
      "Epoch [6430/10000], Loss: 0.36840927600860596\n",
      "Epoch [6431/10000], Loss: 0.3683881163597107\n",
      "Epoch [6432/10000], Loss: 0.3683667778968811\n",
      "Epoch [6433/10000], Loss: 0.3683456778526306\n",
      "Epoch [6434/10000], Loss: 0.3683244585990906\n",
      "Epoch [6435/10000], Loss: 0.36830323934555054\n",
      "Epoch [6436/10000], Loss: 0.36828213930130005\n",
      "Epoch [6437/10000], Loss: 0.36826092004776\n",
      "Epoch [6438/10000], Loss: 0.36823976039886475\n",
      "Epoch [6439/10000], Loss: 0.3682185411453247\n",
      "Epoch [6440/10000], Loss: 0.3681974411010742\n",
      "Epoch [6441/10000], Loss: 0.36817628145217896\n",
      "Epoch [6442/10000], Loss: 0.3681551218032837\n",
      "Epoch [6443/10000], Loss: 0.3681340217590332\n",
      "Epoch [6444/10000], Loss: 0.3681129813194275\n",
      "Epoch [6445/10000], Loss: 0.36809176206588745\n",
      "Epoch [6446/10000], Loss: 0.36807072162628174\n",
      "Epoch [6447/10000], Loss: 0.36804962158203125\n",
      "Epoch [6448/10000], Loss: 0.36802858114242554\n",
      "Epoch [6449/10000], Loss: 0.3680075407028198\n",
      "Epoch [6450/10000], Loss: 0.36798638105392456\n",
      "Epoch [6451/10000], Loss: 0.3679654002189636\n",
      "Epoch [6452/10000], Loss: 0.36794430017471313\n",
      "Epoch [6453/10000], Loss: 0.3679233193397522\n",
      "Epoch [6454/10000], Loss: 0.36790233850479126\n",
      "Epoch [6455/10000], Loss: 0.3678813576698303\n",
      "Epoch [6456/10000], Loss: 0.3678603768348694\n",
      "Epoch [6457/10000], Loss: 0.36783939599990845\n",
      "Epoch [6458/10000], Loss: 0.36781835556030273\n",
      "Epoch [6459/10000], Loss: 0.3677973747253418\n",
      "Epoch [6460/10000], Loss: 0.36777639389038086\n",
      "Epoch [6461/10000], Loss: 0.3677554726600647\n",
      "Epoch [6462/10000], Loss: 0.36773449182510376\n",
      "Epoch [6463/10000], Loss: 0.3677136301994324\n",
      "Epoch [6464/10000], Loss: 0.367692768573761\n",
      "Epoch [6465/10000], Loss: 0.36767178773880005\n",
      "Epoch [6466/10000], Loss: 0.3676508665084839\n",
      "Epoch [6467/10000], Loss: 0.3676300048828125\n",
      "Epoch [6468/10000], Loss: 0.36760908365249634\n",
      "Epoch [6469/10000], Loss: 0.36758822202682495\n",
      "Epoch [6470/10000], Loss: 0.36756742000579834\n",
      "Epoch [6471/10000], Loss: 0.3675464987754822\n",
      "Epoch [6472/10000], Loss: 0.36752569675445557\n",
      "Epoch [6473/10000], Loss: 0.3675047755241394\n",
      "Epoch [6474/10000], Loss: 0.3674839735031128\n",
      "Epoch [6475/10000], Loss: 0.3674631118774414\n",
      "Epoch [6476/10000], Loss: 0.3674423098564148\n",
      "Epoch [6477/10000], Loss: 0.3674215078353882\n",
      "Epoch [6478/10000], Loss: 0.3674007058143616\n",
      "Epoch [6479/10000], Loss: 0.36737996339797974\n",
      "Epoch [6480/10000], Loss: 0.36735910177230835\n",
      "Epoch [6481/10000], Loss: 0.3673383593559265\n",
      "Epoch [6482/10000], Loss: 0.3673176169395447\n",
      "Epoch [6483/10000], Loss: 0.36729681491851807\n",
      "Epoch [6484/10000], Loss: 0.367276132106781\n",
      "Epoch [6485/10000], Loss: 0.36725538969039917\n",
      "Epoch [6486/10000], Loss: 0.36723464727401733\n",
      "Epoch [6487/10000], Loss: 0.3672139048576355\n",
      "Epoch [6488/10000], Loss: 0.36719322204589844\n",
      "Epoch [6489/10000], Loss: 0.3671725392341614\n",
      "Epoch [6490/10000], Loss: 0.3671518564224243\n",
      "Epoch [6491/10000], Loss: 0.36713117361068726\n",
      "Epoch [6492/10000], Loss: 0.36711055040359497\n",
      "Epoch [6493/10000], Loss: 0.3670898675918579\n",
      "Epoch [6494/10000], Loss: 0.36706918478012085\n",
      "Epoch [6495/10000], Loss: 0.36704856157302856\n",
      "Epoch [6496/10000], Loss: 0.3670278787612915\n",
      "Epoch [6497/10000], Loss: 0.3670072555541992\n",
      "Epoch [6498/10000], Loss: 0.3669866919517517\n",
      "Epoch [6499/10000], Loss: 0.3669660687446594\n",
      "Epoch [6500/10000], Loss: 0.36694544553756714\n",
      "Epoch [6501/10000], Loss: 0.36692488193511963\n",
      "Epoch [6502/10000], Loss: 0.3669043183326721\n",
      "Epoch [6503/10000], Loss: 0.36688369512557983\n",
      "Epoch [6504/10000], Loss: 0.3668631911277771\n",
      "Epoch [6505/10000], Loss: 0.3668425679206848\n",
      "Epoch [6506/10000], Loss: 0.3668220639228821\n",
      "Epoch [6507/10000], Loss: 0.36680155992507935\n",
      "Epoch [6508/10000], Loss: 0.3667810559272766\n",
      "Epoch [6509/10000], Loss: 0.3667604327201843\n",
      "Epoch [6510/10000], Loss: 0.3667399287223816\n",
      "Epoch [6511/10000], Loss: 0.36671942472457886\n",
      "Epoch [6512/10000], Loss: 0.3666989207267761\n",
      "Epoch [6513/10000], Loss: 0.3666783571243286\n",
      "Epoch [6514/10000], Loss: 0.3666580319404602\n",
      "Epoch [6515/10000], Loss: 0.36663752794265747\n",
      "Epoch [6516/10000], Loss: 0.36661702394485474\n",
      "Epoch [6517/10000], Loss: 0.3665965795516968\n",
      "Epoch [6518/10000], Loss: 0.3665761947631836\n",
      "Epoch [6519/10000], Loss: 0.36655575037002563\n",
      "Epoch [6520/10000], Loss: 0.3665352463722229\n",
      "Epoch [6521/10000], Loss: 0.3665148615837097\n",
      "Epoch [6522/10000], Loss: 0.36649447679519653\n",
      "Epoch [6523/10000], Loss: 0.36647409200668335\n",
      "Epoch [6524/10000], Loss: 0.36645370721817017\n",
      "Epoch [6525/10000], Loss: 0.366433322429657\n",
      "Epoch [6526/10000], Loss: 0.3664129376411438\n",
      "Epoch [6527/10000], Loss: 0.3663925528526306\n",
      "Epoch [6528/10000], Loss: 0.36637216806411743\n",
      "Epoch [6529/10000], Loss: 0.3663519024848938\n",
      "Epoch [6530/10000], Loss: 0.3663315176963806\n",
      "Epoch [6531/10000], Loss: 0.3663111925125122\n",
      "Epoch [6532/10000], Loss: 0.3662908673286438\n",
      "Epoch [6533/10000], Loss: 0.3662705421447754\n",
      "Epoch [6534/10000], Loss: 0.366250216960907\n",
      "Epoch [6535/10000], Loss: 0.3662298917770386\n",
      "Epoch [6536/10000], Loss: 0.36620962619781494\n",
      "Epoch [6537/10000], Loss: 0.36618930101394653\n",
      "Epoch [6538/10000], Loss: 0.3661690354347229\n",
      "Epoch [6539/10000], Loss: 0.36614882946014404\n",
      "Epoch [6540/10000], Loss: 0.36612844467163086\n",
      "Epoch [6541/10000], Loss: 0.366108238697052\n",
      "Epoch [6542/10000], Loss: 0.36608803272247314\n",
      "Epoch [6543/10000], Loss: 0.3660678267478943\n",
      "Epoch [6544/10000], Loss: 0.3660476803779602\n",
      "Epoch [6545/10000], Loss: 0.3660273551940918\n",
      "Epoch [6546/10000], Loss: 0.3660072088241577\n",
      "Epoch [6547/10000], Loss: 0.3659869432449341\n",
      "Epoch [6548/10000], Loss: 0.3659667372703552\n",
      "Epoch [6549/10000], Loss: 0.3659466505050659\n",
      "Epoch [6550/10000], Loss: 0.36592644453048706\n",
      "Epoch [6551/10000], Loss: 0.365906298160553\n",
      "Epoch [6552/10000], Loss: 0.3658861517906189\n",
      "Epoch [6553/10000], Loss: 0.3658660054206848\n",
      "Epoch [6554/10000], Loss: 0.3658457398414612\n",
      "Epoch [6555/10000], Loss: 0.3658256530761719\n",
      "Epoch [6556/10000], Loss: 0.36580556631088257\n",
      "Epoch [6557/10000], Loss: 0.3657854199409485\n",
      "Epoch [6558/10000], Loss: 0.36576539278030396\n",
      "Epoch [6559/10000], Loss: 0.3657451868057251\n",
      "Epoch [6560/10000], Loss: 0.3657251000404358\n",
      "Epoch [6561/10000], Loss: 0.3657050132751465\n",
      "Epoch [6562/10000], Loss: 0.3656849265098572\n",
      "Epoch [6563/10000], Loss: 0.36566489934921265\n",
      "Epoch [6564/10000], Loss: 0.36564481258392334\n",
      "Epoch [6565/10000], Loss: 0.3656248450279236\n",
      "Epoch [6566/10000], Loss: 0.3656046986579895\n",
      "Epoch [6567/10000], Loss: 0.36558473110198975\n",
      "Epoch [6568/10000], Loss: 0.3655647039413452\n",
      "Epoch [6569/10000], Loss: 0.3655446767807007\n",
      "Epoch [6570/10000], Loss: 0.3655245900154114\n",
      "Epoch [6571/10000], Loss: 0.3655046820640564\n",
      "Epoch [6572/10000], Loss: 0.36548465490341187\n",
      "Epoch [6573/10000], Loss: 0.36546462774276733\n",
      "Epoch [6574/10000], Loss: 0.3654446601867676\n",
      "Epoch [6575/10000], Loss: 0.3654246926307678\n",
      "Epoch [6576/10000], Loss: 0.36540472507476807\n",
      "Epoch [6577/10000], Loss: 0.36538469791412354\n",
      "Epoch [6578/10000], Loss: 0.36536484956741333\n",
      "Epoch [6579/10000], Loss: 0.3653448820114136\n",
      "Epoch [6580/10000], Loss: 0.3653249144554138\n",
      "Epoch [6581/10000], Loss: 0.36530494689941406\n",
      "Epoch [6582/10000], Loss: 0.3652850389480591\n",
      "Epoch [6583/10000], Loss: 0.3652651906013489\n",
      "Epoch [6584/10000], Loss: 0.3652452826499939\n",
      "Epoch [6585/10000], Loss: 0.3652254343032837\n",
      "Epoch [6586/10000], Loss: 0.36520546674728394\n",
      "Epoch [6587/10000], Loss: 0.36518561840057373\n",
      "Epoch [6588/10000], Loss: 0.36516571044921875\n",
      "Epoch [6589/10000], Loss: 0.3651459813117981\n",
      "Epoch [6590/10000], Loss: 0.3651260733604431\n",
      "Epoch [6591/10000], Loss: 0.36510616540908813\n",
      "Epoch [6592/10000], Loss: 0.3650863766670227\n",
      "Epoch [6593/10000], Loss: 0.3650665283203125\n",
      "Epoch [6594/10000], Loss: 0.36504679918289185\n",
      "Epoch [6595/10000], Loss: 0.36502689123153687\n",
      "Epoch [6596/10000], Loss: 0.36500710248947144\n",
      "Epoch [6597/10000], Loss: 0.3649873733520508\n",
      "Epoch [6598/10000], Loss: 0.3649675250053406\n",
      "Epoch [6599/10000], Loss: 0.36494773626327515\n",
      "Epoch [6600/10000], Loss: 0.3649279475212097\n",
      "Epoch [6601/10000], Loss: 0.36490827798843384\n",
      "Epoch [6602/10000], Loss: 0.3648884892463684\n",
      "Epoch [6603/10000], Loss: 0.364868700504303\n",
      "Epoch [6604/10000], Loss: 0.3648489713668823\n",
      "Epoch [6605/10000], Loss: 0.36482924222946167\n",
      "Epoch [6606/10000], Loss: 0.3648095726966858\n",
      "Epoch [6607/10000], Loss: 0.36478984355926514\n",
      "Epoch [6608/10000], Loss: 0.3647701144218445\n",
      "Epoch [6609/10000], Loss: 0.36475038528442383\n",
      "Epoch [6610/10000], Loss: 0.36473071575164795\n",
      "Epoch [6611/10000], Loss: 0.36471110582351685\n",
      "Epoch [6612/10000], Loss: 0.3646913170814514\n",
      "Epoch [6613/10000], Loss: 0.36467164754867554\n",
      "Epoch [6614/10000], Loss: 0.36465203762054443\n",
      "Epoch [6615/10000], Loss: 0.3646323084831238\n",
      "Epoch [6616/10000], Loss: 0.3646126985549927\n",
      "Epoch [6617/10000], Loss: 0.3645930886268616\n",
      "Epoch [6618/10000], Loss: 0.36457347869873047\n",
      "Epoch [6619/10000], Loss: 0.3645538091659546\n",
      "Epoch [6620/10000], Loss: 0.3645341992378235\n",
      "Epoch [6621/10000], Loss: 0.3645145297050476\n",
      "Epoch [6622/10000], Loss: 0.3644949793815613\n",
      "Epoch [6623/10000], Loss: 0.3644753694534302\n",
      "Epoch [6624/10000], Loss: 0.3644557595252991\n",
      "Epoch [6625/10000], Loss: 0.36443620920181274\n",
      "Epoch [6626/10000], Loss: 0.3644167184829712\n",
      "Epoch [6627/10000], Loss: 0.3643971085548401\n",
      "Epoch [6628/10000], Loss: 0.364377498626709\n",
      "Epoch [6629/10000], Loss: 0.3643580675125122\n",
      "Epoch [6630/10000], Loss: 0.3643384575843811\n",
      "Epoch [6631/10000], Loss: 0.3643189072608948\n",
      "Epoch [6632/10000], Loss: 0.3642994165420532\n",
      "Epoch [6633/10000], Loss: 0.3642798662185669\n",
      "Epoch [6634/10000], Loss: 0.3642604947090149\n",
      "Epoch [6635/10000], Loss: 0.364240825176239\n",
      "Epoch [6636/10000], Loss: 0.36422139406204224\n",
      "Epoch [6637/10000], Loss: 0.3642019033432007\n",
      "Epoch [6638/10000], Loss: 0.36418241262435913\n",
      "Epoch [6639/10000], Loss: 0.36416298151016235\n",
      "Epoch [6640/10000], Loss: 0.3641434907913208\n",
      "Epoch [6641/10000], Loss: 0.36412400007247925\n",
      "Epoch [6642/10000], Loss: 0.364104688167572\n",
      "Epoch [6643/10000], Loss: 0.36408519744873047\n",
      "Epoch [6644/10000], Loss: 0.3640657067298889\n",
      "Epoch [6645/10000], Loss: 0.36404627561569214\n",
      "Epoch [6646/10000], Loss: 0.3640269637107849\n",
      "Epoch [6647/10000], Loss: 0.36400753259658813\n",
      "Epoch [6648/10000], Loss: 0.3639880418777466\n",
      "Epoch [6649/10000], Loss: 0.36396878957748413\n",
      "Epoch [6650/10000], Loss: 0.3639492988586426\n",
      "Epoch [6651/10000], Loss: 0.3639298677444458\n",
      "Epoch [6652/10000], Loss: 0.36391061544418335\n",
      "Epoch [6653/10000], Loss: 0.36389124393463135\n",
      "Epoch [6654/10000], Loss: 0.36387187242507935\n",
      "Epoch [6655/10000], Loss: 0.36385250091552734\n",
      "Epoch [6656/10000], Loss: 0.3638332486152649\n",
      "Epoch [6657/10000], Loss: 0.3638138175010681\n",
      "Epoch [6658/10000], Loss: 0.3637945055961609\n",
      "Epoch [6659/10000], Loss: 0.36377525329589844\n",
      "Epoch [6660/10000], Loss: 0.36375588178634644\n",
      "Epoch [6661/10000], Loss: 0.3637365698814392\n",
      "Epoch [6662/10000], Loss: 0.363717257976532\n",
      "Epoch [6663/10000], Loss: 0.36369800567626953\n",
      "Epoch [6664/10000], Loss: 0.36367863416671753\n",
      "Epoch [6665/10000], Loss: 0.36365950107574463\n",
      "Epoch [6666/10000], Loss: 0.3636402487754822\n",
      "Epoch [6667/10000], Loss: 0.3636208772659302\n",
      "Epoch [6668/10000], Loss: 0.3636016845703125\n",
      "Epoch [6669/10000], Loss: 0.3635823130607605\n",
      "Epoch [6670/10000], Loss: 0.3635631799697876\n",
      "Epoch [6671/10000], Loss: 0.36354386806488037\n",
      "Epoch [6672/10000], Loss: 0.36352473497390747\n",
      "Epoch [6673/10000], Loss: 0.3635055422782898\n",
      "Epoch [6674/10000], Loss: 0.3634863495826721\n",
      "Epoch [6675/10000], Loss: 0.36346709728240967\n",
      "Epoch [6676/10000], Loss: 0.36344796419143677\n",
      "Epoch [6677/10000], Loss: 0.3634287118911743\n",
      "Epoch [6678/10000], Loss: 0.3634096384048462\n",
      "Epoch [6679/10000], Loss: 0.36339038610458374\n",
      "Epoch [6680/10000], Loss: 0.3633713126182556\n",
      "Epoch [6681/10000], Loss: 0.3633521795272827\n",
      "Epoch [6682/10000], Loss: 0.36333292722702026\n",
      "Epoch [6683/10000], Loss: 0.36331379413604736\n",
      "Epoch [6684/10000], Loss: 0.36329472064971924\n",
      "Epoch [6685/10000], Loss: 0.3632756471633911\n",
      "Epoch [6686/10000], Loss: 0.36325645446777344\n",
      "Epoch [6687/10000], Loss: 0.3632374405860901\n",
      "Epoch [6688/10000], Loss: 0.3632182478904724\n",
      "Epoch [6689/10000], Loss: 0.36319923400878906\n",
      "Epoch [6690/10000], Loss: 0.36318010091781616\n",
      "Epoch [6691/10000], Loss: 0.3631610870361328\n",
      "Epoch [6692/10000], Loss: 0.3631419539451599\n",
      "Epoch [6693/10000], Loss: 0.36312294006347656\n",
      "Epoch [6694/10000], Loss: 0.36310386657714844\n",
      "Epoch [6695/10000], Loss: 0.3630847930908203\n",
      "Epoch [6696/10000], Loss: 0.3630657196044922\n",
      "Epoch [6697/10000], Loss: 0.36304670572280884\n",
      "Epoch [6698/10000], Loss: 0.3630276322364807\n",
      "Epoch [6699/10000], Loss: 0.36300861835479736\n",
      "Epoch [6700/10000], Loss: 0.36298972368240356\n",
      "Epoch [6701/10000], Loss: 0.3629707098007202\n",
      "Epoch [6702/10000], Loss: 0.36295169591903687\n",
      "Epoch [6703/10000], Loss: 0.3629326820373535\n",
      "Epoch [6704/10000], Loss: 0.36291366815567017\n",
      "Epoch [6705/10000], Loss: 0.3628946542739868\n",
      "Epoch [6706/10000], Loss: 0.362875759601593\n",
      "Epoch [6707/10000], Loss: 0.36285674571990967\n",
      "Epoch [6708/10000], Loss: 0.3628377914428711\n",
      "Epoch [6709/10000], Loss: 0.3628188371658325\n",
      "Epoch [6710/10000], Loss: 0.3628000020980835\n",
      "Epoch [6711/10000], Loss: 0.3627810478210449\n",
      "Epoch [6712/10000], Loss: 0.3627620339393616\n",
      "Epoch [6713/10000], Loss: 0.362743079662323\n",
      "Epoch [6714/10000], Loss: 0.36272430419921875\n",
      "Epoch [6715/10000], Loss: 0.3627052903175354\n",
      "Epoch [6716/10000], Loss: 0.36268651485443115\n",
      "Epoch [6717/10000], Loss: 0.3626675009727478\n",
      "Epoch [6718/10000], Loss: 0.3626486659049988\n",
      "Epoch [6719/10000], Loss: 0.36262983083724976\n",
      "Epoch [6720/10000], Loss: 0.3626108765602112\n",
      "Epoch [6721/10000], Loss: 0.36259204149246216\n",
      "Epoch [6722/10000], Loss: 0.3625732660293579\n",
      "Epoch [6723/10000], Loss: 0.3625544309616089\n",
      "Epoch [6724/10000], Loss: 0.3625355362892151\n",
      "Epoch [6725/10000], Loss: 0.36251670122146606\n",
      "Epoch [6726/10000], Loss: 0.36249780654907227\n",
      "Epoch [6727/10000], Loss: 0.362479031085968\n",
      "Epoch [6728/10000], Loss: 0.362460196018219\n",
      "Epoch [6729/10000], Loss: 0.36244142055511475\n",
      "Epoch [6730/10000], Loss: 0.3624225854873657\n",
      "Epoch [6731/10000], Loss: 0.36240386962890625\n",
      "Epoch [6732/10000], Loss: 0.362385094165802\n",
      "Epoch [6733/10000], Loss: 0.362366259098053\n",
      "Epoch [6734/10000], Loss: 0.36234748363494873\n",
      "Epoch [6735/10000], Loss: 0.3623287081718445\n",
      "Epoch [6736/10000], Loss: 0.36230993270874023\n",
      "Epoch [6737/10000], Loss: 0.36229121685028076\n",
      "Epoch [6738/10000], Loss: 0.3622724413871765\n",
      "Epoch [6739/10000], Loss: 0.36225372552871704\n",
      "Epoch [6740/10000], Loss: 0.36223500967025757\n",
      "Epoch [6741/10000], Loss: 0.3622162938117981\n",
      "Epoch [6742/10000], Loss: 0.36219751834869385\n",
      "Epoch [6743/10000], Loss: 0.36217886209487915\n",
      "Epoch [6744/10000], Loss: 0.3621601462364197\n",
      "Epoch [6745/10000], Loss: 0.362141489982605\n",
      "Epoch [6746/10000], Loss: 0.3621228337287903\n",
      "Epoch [6747/10000], Loss: 0.3621041178703308\n",
      "Epoch [6748/10000], Loss: 0.36208534240722656\n",
      "Epoch [6749/10000], Loss: 0.36206674575805664\n",
      "Epoch [6750/10000], Loss: 0.36204808950424194\n",
      "Epoch [6751/10000], Loss: 0.362029492855072\n",
      "Epoch [6752/10000], Loss: 0.362010657787323\n",
      "Epoch [6753/10000], Loss: 0.3619921803474426\n",
      "Epoch [6754/10000], Loss: 0.36197346448898315\n",
      "Epoch [6755/10000], Loss: 0.36195486783981323\n",
      "Epoch [6756/10000], Loss: 0.3619362711906433\n",
      "Epoch [6757/10000], Loss: 0.3619176745414734\n",
      "Epoch [6758/10000], Loss: 0.3618990182876587\n",
      "Epoch [6759/10000], Loss: 0.36188042163848877\n",
      "Epoch [6760/10000], Loss: 0.36186182498931885\n",
      "Epoch [6761/10000], Loss: 0.3618432283401489\n",
      "Epoch [6762/10000], Loss: 0.361824631690979\n",
      "Epoch [6763/10000], Loss: 0.36180609464645386\n",
      "Epoch [6764/10000], Loss: 0.36178749799728394\n",
      "Epoch [6765/10000], Loss: 0.36176902055740356\n",
      "Epoch [6766/10000], Loss: 0.3617505431175232\n",
      "Epoch [6767/10000], Loss: 0.3617318868637085\n",
      "Epoch [6768/10000], Loss: 0.3617134094238281\n",
      "Epoch [6769/10000], Loss: 0.3616947531700134\n",
      "Epoch [6770/10000], Loss: 0.3616762161254883\n",
      "Epoch [6771/10000], Loss: 0.3616577386856079\n",
      "Epoch [6772/10000], Loss: 0.3616393208503723\n",
      "Epoch [6773/10000], Loss: 0.36162078380584717\n",
      "Epoch [6774/10000], Loss: 0.361602246761322\n",
      "Epoch [6775/10000], Loss: 0.3615837097167969\n",
      "Epoch [6776/10000], Loss: 0.3615652322769165\n",
      "Epoch [6777/10000], Loss: 0.3615468740463257\n",
      "Epoch [6778/10000], Loss: 0.36152833700180054\n",
      "Epoch [6779/10000], Loss: 0.3615097999572754\n",
      "Epoch [6780/10000], Loss: 0.3614913821220398\n",
      "Epoch [6781/10000], Loss: 0.361473023891449\n",
      "Epoch [6782/10000], Loss: 0.36145448684692383\n",
      "Epoch [6783/10000], Loss: 0.36143606901168823\n",
      "Epoch [6784/10000], Loss: 0.36141765117645264\n",
      "Epoch [6785/10000], Loss: 0.36139923334121704\n",
      "Epoch [6786/10000], Loss: 0.36138075590133667\n",
      "Epoch [6787/10000], Loss: 0.36136239767074585\n",
      "Epoch [6788/10000], Loss: 0.36134397983551025\n",
      "Epoch [6789/10000], Loss: 0.36132562160491943\n",
      "Epoch [6790/10000], Loss: 0.36130720376968384\n",
      "Epoch [6791/10000], Loss: 0.361288845539093\n",
      "Epoch [6792/10000], Loss: 0.3612704277038574\n",
      "Epoch [6793/10000], Loss: 0.3612521290779114\n",
      "Epoch [6794/10000], Loss: 0.361233651638031\n",
      "Epoch [6795/10000], Loss: 0.3612152934074402\n",
      "Epoch [6796/10000], Loss: 0.36119693517684937\n",
      "Epoch [6797/10000], Loss: 0.3611786961555481\n",
      "Epoch [6798/10000], Loss: 0.3611602187156677\n",
      "Epoch [6799/10000], Loss: 0.36114197969436646\n",
      "Epoch [6800/10000], Loss: 0.3611236810684204\n",
      "Epoch [6801/10000], Loss: 0.3611053228378296\n",
      "Epoch [6802/10000], Loss: 0.36108696460723877\n",
      "Epoch [6803/10000], Loss: 0.3610687255859375\n",
      "Epoch [6804/10000], Loss: 0.36105042695999146\n",
      "Epoch [6805/10000], Loss: 0.36103206872940063\n",
      "Epoch [6806/10000], Loss: 0.36101382970809937\n",
      "Epoch [6807/10000], Loss: 0.3609955906867981\n",
      "Epoch [6808/10000], Loss: 0.3609772324562073\n",
      "Epoch [6809/10000], Loss: 0.360958993434906\n",
      "Epoch [6810/10000], Loss: 0.36094075441360474\n",
      "Epoch [6811/10000], Loss: 0.3609224557876587\n",
      "Epoch [6812/10000], Loss: 0.36090415716171265\n",
      "Epoch [6813/10000], Loss: 0.3608860373497009\n",
      "Epoch [6814/10000], Loss: 0.3608676791191101\n",
      "Epoch [6815/10000], Loss: 0.3608494997024536\n",
      "Epoch [6816/10000], Loss: 0.36083120107650757\n",
      "Epoch [6817/10000], Loss: 0.36081308126449585\n",
      "Epoch [6818/10000], Loss: 0.3607948422431946\n",
      "Epoch [6819/10000], Loss: 0.36077654361724854\n",
      "Epoch [6820/10000], Loss: 0.3607584238052368\n",
      "Epoch [6821/10000], Loss: 0.3607402443885803\n",
      "Epoch [6822/10000], Loss: 0.3607221245765686\n",
      "Epoch [6823/10000], Loss: 0.36070388555526733\n",
      "Epoch [6824/10000], Loss: 0.36068570613861084\n",
      "Epoch [6825/10000], Loss: 0.3606675863265991\n",
      "Epoch [6826/10000], Loss: 0.3606494069099426\n",
      "Epoch [6827/10000], Loss: 0.36063116788864136\n",
      "Epoch [6828/10000], Loss: 0.3606131672859192\n",
      "Epoch [6829/10000], Loss: 0.36059486865997314\n",
      "Epoch [6830/10000], Loss: 0.3605768084526062\n",
      "Epoch [6831/10000], Loss: 0.3605586886405945\n",
      "Epoch [6832/10000], Loss: 0.360540509223938\n",
      "Epoch [6833/10000], Loss: 0.36052244901657104\n",
      "Epoch [6834/10000], Loss: 0.3605043292045593\n",
      "Epoch [6835/10000], Loss: 0.3604862093925476\n",
      "Epoch [6836/10000], Loss: 0.3604680299758911\n",
      "Epoch [6837/10000], Loss: 0.3604500889778137\n",
      "Epoch [6838/10000], Loss: 0.360431969165802\n",
      "Epoch [6839/10000], Loss: 0.36041390895843506\n",
      "Epoch [6840/10000], Loss: 0.3603958487510681\n",
      "Epoch [6841/10000], Loss: 0.3603777289390564\n",
      "Epoch [6842/10000], Loss: 0.36035966873168945\n",
      "Epoch [6843/10000], Loss: 0.3603416681289673\n",
      "Epoch [6844/10000], Loss: 0.36032360792160034\n",
      "Epoch [6845/10000], Loss: 0.3603056073188782\n",
      "Epoch [6846/10000], Loss: 0.36028754711151123\n",
      "Epoch [6847/10000], Loss: 0.36026954650878906\n",
      "Epoch [6848/10000], Loss: 0.36025142669677734\n",
      "Epoch [6849/10000], Loss: 0.3602334260940552\n",
      "Epoch [6850/10000], Loss: 0.3602154850959778\n",
      "Epoch [6851/10000], Loss: 0.3601974844932556\n",
      "Epoch [6852/10000], Loss: 0.36017948389053345\n",
      "Epoch [6853/10000], Loss: 0.3601614236831665\n",
      "Epoch [6854/10000], Loss: 0.3601434826850891\n",
      "Epoch [6855/10000], Loss: 0.36012548208236694\n",
      "Epoch [6856/10000], Loss: 0.3601074814796448\n",
      "Epoch [6857/10000], Loss: 0.3600894808769226\n",
      "Epoch [6858/10000], Loss: 0.36007159948349\n",
      "Epoch [6859/10000], Loss: 0.3600536584854126\n",
      "Epoch [6860/10000], Loss: 0.3600357174873352\n",
      "Epoch [6861/10000], Loss: 0.3600177764892578\n",
      "Epoch [6862/10000], Loss: 0.3599998354911804\n",
      "Epoch [6863/10000], Loss: 0.3599819540977478\n",
      "Epoch [6864/10000], Loss: 0.3599640130996704\n",
      "Epoch [6865/10000], Loss: 0.359946072101593\n",
      "Epoch [6866/10000], Loss: 0.3599281311035156\n",
      "Epoch [6867/10000], Loss: 0.3599103093147278\n",
      "Epoch [6868/10000], Loss: 0.3598923087120056\n",
      "Epoch [6869/10000], Loss: 0.359874427318573\n",
      "Epoch [6870/10000], Loss: 0.35985660552978516\n",
      "Epoch [6871/10000], Loss: 0.35983866453170776\n",
      "Epoch [6872/10000], Loss: 0.35982078313827515\n",
      "Epoch [6873/10000], Loss: 0.3598030209541321\n",
      "Epoch [6874/10000], Loss: 0.3597850799560547\n",
      "Epoch [6875/10000], Loss: 0.35976725816726685\n",
      "Epoch [6876/10000], Loss: 0.35974937677383423\n",
      "Epoch [6877/10000], Loss: 0.3597314953804016\n",
      "Epoch [6878/10000], Loss: 0.359713613986969\n",
      "Epoch [6879/10000], Loss: 0.3596959114074707\n",
      "Epoch [6880/10000], Loss: 0.35967808961868286\n",
      "Epoch [6881/10000], Loss: 0.35966020822525024\n",
      "Epoch [6882/10000], Loss: 0.3596423864364624\n",
      "Epoch [6883/10000], Loss: 0.35962456464767456\n",
      "Epoch [6884/10000], Loss: 0.3596068024635315\n",
      "Epoch [6885/10000], Loss: 0.35958898067474365\n",
      "Epoch [6886/10000], Loss: 0.3595711588859558\n",
      "Epoch [6887/10000], Loss: 0.35955333709716797\n",
      "Epoch [6888/10000], Loss: 0.3595356345176697\n",
      "Epoch [6889/10000], Loss: 0.3595178723335266\n",
      "Epoch [6890/10000], Loss: 0.35950011014938354\n",
      "Epoch [6891/10000], Loss: 0.3594823479652405\n",
      "Epoch [6892/10000], Loss: 0.3594645857810974\n",
      "Epoch [6893/10000], Loss: 0.35944682359695435\n",
      "Epoch [6894/10000], Loss: 0.3594290614128113\n",
      "Epoch [6895/10000], Loss: 0.359411358833313\n",
      "Epoch [6896/10000], Loss: 0.3593935966491699\n",
      "Epoch [6897/10000], Loss: 0.35937589406967163\n",
      "Epoch [6898/10000], Loss: 0.35935819149017334\n",
      "Epoch [6899/10000], Loss: 0.35934048891067505\n",
      "Epoch [6900/10000], Loss: 0.359322726726532\n",
      "Epoch [6901/10000], Loss: 0.3593050241470337\n",
      "Epoch [6902/10000], Loss: 0.3592873215675354\n",
      "Epoch [6903/10000], Loss: 0.3592696189880371\n",
      "Epoch [6904/10000], Loss: 0.3592519760131836\n",
      "Epoch [6905/10000], Loss: 0.3592343330383301\n",
      "Epoch [6906/10000], Loss: 0.35921669006347656\n",
      "Epoch [6907/10000], Loss: 0.35919898748397827\n",
      "Epoch [6908/10000], Loss: 0.35918134450912476\n",
      "Epoch [6909/10000], Loss: 0.35916364192962646\n",
      "Epoch [6910/10000], Loss: 0.35914599895477295\n",
      "Epoch [6911/10000], Loss: 0.35912835597991943\n",
      "Epoch [6912/10000], Loss: 0.3591107130050659\n",
      "Epoch [6913/10000], Loss: 0.3590930700302124\n",
      "Epoch [6914/10000], Loss: 0.35907548666000366\n",
      "Epoch [6915/10000], Loss: 0.35905784368515015\n",
      "Epoch [6916/10000], Loss: 0.35904020071029663\n",
      "Epoch [6917/10000], Loss: 0.3590226173400879\n",
      "Epoch [6918/10000], Loss: 0.35900503396987915\n",
      "Epoch [6919/10000], Loss: 0.35898739099502563\n",
      "Epoch [6920/10000], Loss: 0.3589698076248169\n",
      "Epoch [6921/10000], Loss: 0.3589521646499634\n",
      "Epoch [6922/10000], Loss: 0.35893458127975464\n",
      "Epoch [6923/10000], Loss: 0.3589170575141907\n",
      "Epoch [6924/10000], Loss: 0.3588995337486267\n",
      "Epoch [6925/10000], Loss: 0.35888195037841797\n",
      "Epoch [6926/10000], Loss: 0.35886436700820923\n",
      "Epoch [6927/10000], Loss: 0.3588467836380005\n",
      "Epoch [6928/10000], Loss: 0.3588293194770813\n",
      "Epoch [6929/10000], Loss: 0.35881173610687256\n",
      "Epoch [6930/10000], Loss: 0.3587942123413086\n",
      "Epoch [6931/10000], Loss: 0.35877662897109985\n",
      "Epoch [6932/10000], Loss: 0.3587590456008911\n",
      "Epoch [6933/10000], Loss: 0.3587416410446167\n",
      "Epoch [6934/10000], Loss: 0.35872411727905273\n",
      "Epoch [6935/10000], Loss: 0.35870659351348877\n",
      "Epoch [6936/10000], Loss: 0.35868901014328003\n",
      "Epoch [6937/10000], Loss: 0.3586716055870056\n",
      "Epoch [6938/10000], Loss: 0.35865408182144165\n",
      "Epoch [6939/10000], Loss: 0.35863667726516724\n",
      "Epoch [6940/10000], Loss: 0.3586190938949585\n",
      "Epoch [6941/10000], Loss: 0.3586016297340393\n",
      "Epoch [6942/10000], Loss: 0.3585842251777649\n",
      "Epoch [6943/10000], Loss: 0.3585667014122009\n",
      "Epoch [6944/10000], Loss: 0.35854923725128174\n",
      "Epoch [6945/10000], Loss: 0.3585318326950073\n",
      "Epoch [6946/10000], Loss: 0.35851430892944336\n",
      "Epoch [6947/10000], Loss: 0.3584969639778137\n",
      "Epoch [6948/10000], Loss: 0.35847944021224976\n",
      "Epoch [6949/10000], Loss: 0.35846203565597534\n",
      "Epoch [6950/10000], Loss: 0.3584446310997009\n",
      "Epoch [6951/10000], Loss: 0.3584272265434265\n",
      "Epoch [6952/10000], Loss: 0.3584098219871521\n",
      "Epoch [6953/10000], Loss: 0.3583923578262329\n",
      "Epoch [6954/10000], Loss: 0.3583749532699585\n",
      "Epoch [6955/10000], Loss: 0.35835742950439453\n",
      "Epoch [6956/10000], Loss: 0.35834014415740967\n",
      "Epoch [6957/10000], Loss: 0.35832273960113525\n",
      "Epoch [6958/10000], Loss: 0.3583053946495056\n",
      "Epoch [6959/10000], Loss: 0.3582879304885864\n",
      "Epoch [6960/10000], Loss: 0.3582705855369568\n",
      "Epoch [6961/10000], Loss: 0.3582533001899719\n",
      "Epoch [6962/10000], Loss: 0.3582358956336975\n",
      "Epoch [6963/10000], Loss: 0.35821855068206787\n",
      "Epoch [6964/10000], Loss: 0.35820120573043823\n",
      "Epoch [6965/10000], Loss: 0.3581838607788086\n",
      "Epoch [6966/10000], Loss: 0.3581663966178894\n",
      "Epoch [6967/10000], Loss: 0.35814905166625977\n",
      "Epoch [6968/10000], Loss: 0.3581318259239197\n",
      "Epoch [6969/10000], Loss: 0.35811442136764526\n",
      "Epoch [6970/10000], Loss: 0.3580971360206604\n",
      "Epoch [6971/10000], Loss: 0.35807985067367554\n",
      "Epoch [6972/10000], Loss: 0.35806262493133545\n",
      "Epoch [6973/10000], Loss: 0.35804516077041626\n",
      "Epoch [6974/10000], Loss: 0.35802793502807617\n",
      "Epoch [6975/10000], Loss: 0.3580106496810913\n",
      "Epoch [6976/10000], Loss: 0.3579932451248169\n",
      "Epoch [6977/10000], Loss: 0.3579760193824768\n",
      "Epoch [6978/10000], Loss: 0.3579587936401367\n",
      "Epoch [6979/10000], Loss: 0.35794150829315186\n",
      "Epoch [6980/10000], Loss: 0.35792428255081177\n",
      "Epoch [6981/10000], Loss: 0.35790687799453735\n",
      "Epoch [6982/10000], Loss: 0.35788965225219727\n",
      "Epoch [6983/10000], Loss: 0.3578724265098572\n",
      "Epoch [6984/10000], Loss: 0.3578552007675171\n",
      "Epoch [6985/10000], Loss: 0.357837975025177\n",
      "Epoch [6986/10000], Loss: 0.35782063007354736\n",
      "Epoch [6987/10000], Loss: 0.3578034043312073\n",
      "Epoch [6988/10000], Loss: 0.35778623819351196\n",
      "Epoch [6989/10000], Loss: 0.3577690124511719\n",
      "Epoch [6990/10000], Loss: 0.357751727104187\n",
      "Epoch [6991/10000], Loss: 0.3577345013618469\n",
      "Epoch [6992/10000], Loss: 0.3577173352241516\n",
      "Epoch [6993/10000], Loss: 0.3577001690864563\n",
      "Epoch [6994/10000], Loss: 0.35768288373947144\n",
      "Epoch [6995/10000], Loss: 0.3576657176017761\n",
      "Epoch [6996/10000], Loss: 0.3576485514640808\n",
      "Epoch [6997/10000], Loss: 0.3576313853263855\n",
      "Epoch [6998/10000], Loss: 0.35761409997940063\n",
      "Epoch [6999/10000], Loss: 0.3575969338417053\n",
      "Epoch [7000/10000], Loss: 0.35757976770401\n",
      "Epoch [7001/10000], Loss: 0.3575626015663147\n",
      "Epoch [7002/10000], Loss: 0.3575453758239746\n",
      "Epoch [7003/10000], Loss: 0.3575282692909241\n",
      "Epoch [7004/10000], Loss: 0.357511043548584\n",
      "Epoch [7005/10000], Loss: 0.35749393701553345\n",
      "Epoch [7006/10000], Loss: 0.35747677087783813\n",
      "Epoch [7007/10000], Loss: 0.3574596643447876\n",
      "Epoch [7008/10000], Loss: 0.3574424982070923\n",
      "Epoch [7009/10000], Loss: 0.35742539167404175\n",
      "Epoch [7010/10000], Loss: 0.357408344745636\n",
      "Epoch [7011/10000], Loss: 0.3573911786079407\n",
      "Epoch [7012/10000], Loss: 0.35737401247024536\n",
      "Epoch [7013/10000], Loss: 0.3573569655418396\n",
      "Epoch [7014/10000], Loss: 0.3573397994041443\n",
      "Epoch [7015/10000], Loss: 0.35732269287109375\n",
      "Epoch [7016/10000], Loss: 0.357305645942688\n",
      "Epoch [7017/10000], Loss: 0.3572884798049927\n",
      "Epoch [7018/10000], Loss: 0.3572714924812317\n",
      "Epoch [7019/10000], Loss: 0.35725438594818115\n",
      "Epoch [7020/10000], Loss: 0.3572372794151306\n",
      "Epoch [7021/10000], Loss: 0.3572201728820801\n",
      "Epoch [7022/10000], Loss: 0.3572031259536743\n",
      "Epoch [7023/10000], Loss: 0.3571860194206238\n",
      "Epoch [7024/10000], Loss: 0.3571690320968628\n",
      "Epoch [7025/10000], Loss: 0.35715198516845703\n",
      "Epoch [7026/10000], Loss: 0.3571348786354065\n",
      "Epoch [7027/10000], Loss: 0.35711777210235596\n",
      "Epoch [7028/10000], Loss: 0.3571006655693054\n",
      "Epoch [7029/10000], Loss: 0.3570837378501892\n",
      "Epoch [7030/10000], Loss: 0.35706669092178345\n",
      "Epoch [7031/10000], Loss: 0.3570496439933777\n",
      "Epoch [7032/10000], Loss: 0.3570327162742615\n",
      "Epoch [7033/10000], Loss: 0.3570156693458557\n",
      "Epoch [7034/10000], Loss: 0.3569985628128052\n",
      "Epoch [7035/10000], Loss: 0.3569815754890442\n",
      "Epoch [7036/10000], Loss: 0.3569645881652832\n",
      "Epoch [7037/10000], Loss: 0.35694754123687744\n",
      "Epoch [7038/10000], Loss: 0.35693061351776123\n",
      "Epoch [7039/10000], Loss: 0.35691356658935547\n",
      "Epoch [7040/10000], Loss: 0.3568965196609497\n",
      "Epoch [7041/10000], Loss: 0.3568795919418335\n",
      "Epoch [7042/10000], Loss: 0.3568626046180725\n",
      "Epoch [7043/10000], Loss: 0.35684555768966675\n",
      "Epoch [7044/10000], Loss: 0.35682862997055054\n",
      "Epoch [7045/10000], Loss: 0.35681164264678955\n",
      "Epoch [7046/10000], Loss: 0.35679471492767334\n",
      "Epoch [7047/10000], Loss: 0.3567776679992676\n",
      "Epoch [7048/10000], Loss: 0.3567606806755066\n",
      "Epoch [7049/10000], Loss: 0.3567436933517456\n",
      "Epoch [7050/10000], Loss: 0.3567267656326294\n",
      "Epoch [7051/10000], Loss: 0.35670989751815796\n",
      "Epoch [7052/10000], Loss: 0.356692910194397\n",
      "Epoch [7053/10000], Loss: 0.35667598247528076\n",
      "Epoch [7054/10000], Loss: 0.35665905475616455\n",
      "Epoch [7055/10000], Loss: 0.35664206743240356\n",
      "Epoch [7056/10000], Loss: 0.35662513971328735\n",
      "Epoch [7057/10000], Loss: 0.3566082715988159\n",
      "Epoch [7058/10000], Loss: 0.3565913438796997\n",
      "Epoch [7059/10000], Loss: 0.3565744161605835\n",
      "Epoch [7060/10000], Loss: 0.3565574288368225\n",
      "Epoch [7061/10000], Loss: 0.3565405011177063\n",
      "Epoch [7062/10000], Loss: 0.3565235733985901\n",
      "Epoch [7063/10000], Loss: 0.3565067648887634\n",
      "Epoch [7064/10000], Loss: 0.3564898371696472\n",
      "Epoch [7065/10000], Loss: 0.356472909450531\n",
      "Epoch [7066/10000], Loss: 0.3564559817314148\n",
      "Epoch [7067/10000], Loss: 0.35643917322158813\n",
      "Epoch [7068/10000], Loss: 0.3564223051071167\n",
      "Epoch [7069/10000], Loss: 0.35640525817871094\n",
      "Epoch [7070/10000], Loss: 0.3563884496688843\n",
      "Epoch [7071/10000], Loss: 0.35637158155441284\n",
      "Epoch [7072/10000], Loss: 0.3563547134399414\n",
      "Epoch [7073/10000], Loss: 0.35633784532546997\n",
      "Epoch [7074/10000], Loss: 0.3563210368156433\n",
      "Epoch [7075/10000], Loss: 0.3563041090965271\n",
      "Epoch [7076/10000], Loss: 0.35628730058670044\n",
      "Epoch [7077/10000], Loss: 0.356270432472229\n",
      "Epoch [7078/10000], Loss: 0.35625356435775757\n",
      "Epoch [7079/10000], Loss: 0.35623663663864136\n",
      "Epoch [7080/10000], Loss: 0.3562197685241699\n",
      "Epoch [7081/10000], Loss: 0.35620301961898804\n",
      "Epoch [7082/10000], Loss: 0.3561862111091614\n",
      "Epoch [7083/10000], Loss: 0.35616934299468994\n",
      "Epoch [7084/10000], Loss: 0.3561524748802185\n",
      "Epoch [7085/10000], Loss: 0.3561357259750366\n",
      "Epoch [7086/10000], Loss: 0.35611873865127563\n",
      "Epoch [7087/10000], Loss: 0.356101930141449\n",
      "Epoch [7088/10000], Loss: 0.3560851812362671\n",
      "Epoch [7089/10000], Loss: 0.3560682535171509\n",
      "Epoch [7090/10000], Loss: 0.356051504611969\n",
      "Epoch [7091/10000], Loss: 0.35603469610214233\n",
      "Epoch [7092/10000], Loss: 0.3560178875923157\n",
      "Epoch [7093/10000], Loss: 0.356001079082489\n",
      "Epoch [7094/10000], Loss: 0.3559842109680176\n",
      "Epoch [7095/10000], Loss: 0.35596752166748047\n",
      "Epoch [7096/10000], Loss: 0.35595065355300903\n",
      "Epoch [7097/10000], Loss: 0.3559337854385376\n",
      "Epoch [7098/10000], Loss: 0.3559170365333557\n",
      "Epoch [7099/10000], Loss: 0.35590028762817383\n",
      "Epoch [7100/10000], Loss: 0.35588347911834717\n",
      "Epoch [7101/10000], Loss: 0.35586661100387573\n",
      "Epoch [7102/10000], Loss: 0.3558499217033386\n",
      "Epoch [7103/10000], Loss: 0.35583311319351196\n",
      "Epoch [7104/10000], Loss: 0.3558163046836853\n",
      "Epoch [7105/10000], Loss: 0.3557995557785034\n",
      "Epoch [7106/10000], Loss: 0.35578280687332153\n",
      "Epoch [7107/10000], Loss: 0.3557659983634949\n",
      "Epoch [7108/10000], Loss: 0.3557491898536682\n",
      "Epoch [7109/10000], Loss: 0.35573244094848633\n",
      "Epoch [7110/10000], Loss: 0.35571563243865967\n",
      "Epoch [7111/10000], Loss: 0.3556988835334778\n",
      "Epoch [7112/10000], Loss: 0.3556820750236511\n",
      "Epoch [7113/10000], Loss: 0.355665385723114\n",
      "Epoch [7114/10000], Loss: 0.35564857721328735\n",
      "Epoch [7115/10000], Loss: 0.35563182830810547\n",
      "Epoch [7116/10000], Loss: 0.3556150794029236\n",
      "Epoch [7117/10000], Loss: 0.3555983304977417\n",
      "Epoch [7118/10000], Loss: 0.3555815815925598\n",
      "Epoch [7119/10000], Loss: 0.3555648922920227\n",
      "Epoch [7120/10000], Loss: 0.35554808378219604\n",
      "Epoch [7121/10000], Loss: 0.35553139448165894\n",
      "Epoch [7122/10000], Loss: 0.35551464557647705\n",
      "Epoch [7123/10000], Loss: 0.35549789667129517\n",
      "Epoch [7124/10000], Loss: 0.3554811477661133\n",
      "Epoch [7125/10000], Loss: 0.3554643988609314\n",
      "Epoch [7126/10000], Loss: 0.3554476499557495\n",
      "Epoch [7127/10000], Loss: 0.3554309010505676\n",
      "Epoch [7128/10000], Loss: 0.35541409254074097\n",
      "Epoch [7129/10000], Loss: 0.35539740324020386\n",
      "Epoch [7130/10000], Loss: 0.355380654335022\n",
      "Epoch [7131/10000], Loss: 0.3553639054298401\n",
      "Epoch [7132/10000], Loss: 0.3553471565246582\n",
      "Epoch [7133/10000], Loss: 0.3553304076194763\n",
      "Epoch [7134/10000], Loss: 0.35531383752822876\n",
      "Epoch [7135/10000], Loss: 0.3552970886230469\n",
      "Epoch [7136/10000], Loss: 0.355280339717865\n",
      "Epoch [7137/10000], Loss: 0.3552635908126831\n",
      "Epoch [7138/10000], Loss: 0.355246901512146\n",
      "Epoch [7139/10000], Loss: 0.3552302122116089\n",
      "Epoch [7140/10000], Loss: 0.355213463306427\n",
      "Epoch [7141/10000], Loss: 0.3551967740058899\n",
      "Epoch [7142/10000], Loss: 0.35517996549606323\n",
      "Epoch [7143/10000], Loss: 0.35516321659088135\n",
      "Epoch [7144/10000], Loss: 0.35514652729034424\n",
      "Epoch [7145/10000], Loss: 0.35512983798980713\n",
      "Epoch [7146/10000], Loss: 0.35511308908462524\n",
      "Epoch [7147/10000], Loss: 0.35509639978408813\n",
      "Epoch [7148/10000], Loss: 0.35507965087890625\n",
      "Epoch [7149/10000], Loss: 0.35506290197372437\n",
      "Epoch [7150/10000], Loss: 0.35504627227783203\n",
      "Epoch [7151/10000], Loss: 0.35502946376800537\n",
      "Epoch [7152/10000], Loss: 0.35501277446746826\n",
      "Epoch [7153/10000], Loss: 0.35499608516693115\n",
      "Epoch [7154/10000], Loss: 0.35497939586639404\n",
      "Epoch [7155/10000], Loss: 0.3549625277519226\n",
      "Epoch [7156/10000], Loss: 0.3549458384513855\n",
      "Epoch [7157/10000], Loss: 0.3549291491508484\n",
      "Epoch [7158/10000], Loss: 0.3549124598503113\n",
      "Epoch [7159/10000], Loss: 0.3548957109451294\n",
      "Epoch [7160/10000], Loss: 0.3548790216445923\n",
      "Epoch [7161/10000], Loss: 0.3548622727394104\n",
      "Epoch [7162/10000], Loss: 0.3548455834388733\n",
      "Epoch [7163/10000], Loss: 0.3548288345336914\n",
      "Epoch [7164/10000], Loss: 0.3548121452331543\n",
      "Epoch [7165/10000], Loss: 0.3547953963279724\n",
      "Epoch [7166/10000], Loss: 0.3547787070274353\n",
      "Epoch [7167/10000], Loss: 0.3547619581222534\n",
      "Epoch [7168/10000], Loss: 0.35474520921707153\n",
      "Epoch [7169/10000], Loss: 0.3547285795211792\n",
      "Epoch [7170/10000], Loss: 0.35471171140670776\n",
      "Epoch [7171/10000], Loss: 0.35469502210617065\n",
      "Epoch [7172/10000], Loss: 0.35467827320098877\n",
      "Epoch [7173/10000], Loss: 0.3546615242958069\n",
      "Epoch [7174/10000], Loss: 0.3546448349952698\n",
      "Epoch [7175/10000], Loss: 0.35462820529937744\n",
      "Epoch [7176/10000], Loss: 0.3546113967895508\n",
      "Epoch [7177/10000], Loss: 0.35459470748901367\n",
      "Epoch [7178/10000], Loss: 0.3545779585838318\n",
      "Epoch [7179/10000], Loss: 0.3545612692832947\n",
      "Epoch [7180/10000], Loss: 0.354544460773468\n",
      "Epoch [7181/10000], Loss: 0.3545277714729309\n",
      "Epoch [7182/10000], Loss: 0.3545109033584595\n",
      "Epoch [7183/10000], Loss: 0.35449421405792236\n",
      "Epoch [7184/10000], Loss: 0.3544774055480957\n",
      "Epoch [7185/10000], Loss: 0.3544607162475586\n",
      "Epoch [7186/10000], Loss: 0.35444390773773193\n",
      "Epoch [7187/10000], Loss: 0.3544272184371948\n",
      "Epoch [7188/10000], Loss: 0.35441046953201294\n",
      "Epoch [7189/10000], Loss: 0.3543936014175415\n",
      "Epoch [7190/10000], Loss: 0.3543768525123596\n",
      "Epoch [7191/10000], Loss: 0.35436010360717773\n",
      "Epoch [7192/10000], Loss: 0.3543434143066406\n",
      "Epoch [7193/10000], Loss: 0.35432660579681396\n",
      "Epoch [7194/10000], Loss: 0.35430967807769775\n",
      "Epoch [7195/10000], Loss: 0.35429298877716064\n",
      "Epoch [7196/10000], Loss: 0.35427623987197876\n",
      "Epoch [7197/10000], Loss: 0.3542594313621521\n",
      "Epoch [7198/10000], Loss: 0.354242742061615\n",
      "Epoch [7199/10000], Loss: 0.3542258143424988\n",
      "Epoch [7200/10000], Loss: 0.3542090654373169\n",
      "Epoch [7201/10000], Loss: 0.35419219732284546\n",
      "Epoch [7202/10000], Loss: 0.3541754484176636\n",
      "Epoch [7203/10000], Loss: 0.3541586995124817\n",
      "Epoch [7204/10000], Loss: 0.35414183139801025\n",
      "Epoch [7205/10000], Loss: 0.3541250228881836\n",
      "Epoch [7206/10000], Loss: 0.35410821437835693\n",
      "Epoch [7207/10000], Loss: 0.3540913462638855\n",
      "Epoch [7208/10000], Loss: 0.35407453775405884\n",
      "Epoch [7209/10000], Loss: 0.3540577292442322\n",
      "Epoch [7210/10000], Loss: 0.3540409207344055\n",
      "Epoch [7211/10000], Loss: 0.3540239930152893\n",
      "Epoch [7212/10000], Loss: 0.3540072441101074\n",
      "Epoch [7213/10000], Loss: 0.35399025678634644\n",
      "Epoch [7214/10000], Loss: 0.3539734482765198\n",
      "Epoch [7215/10000], Loss: 0.35395658016204834\n",
      "Epoch [7216/10000], Loss: 0.35393965244293213\n",
      "Epoch [7217/10000], Loss: 0.35392284393310547\n",
      "Epoch [7218/10000], Loss: 0.35390597581863403\n",
      "Epoch [7219/10000], Loss: 0.3538891077041626\n",
      "Epoch [7220/10000], Loss: 0.3538721203804016\n",
      "Epoch [7221/10000], Loss: 0.3538552522659302\n",
      "Epoch [7222/10000], Loss: 0.35383838415145874\n",
      "Epoch [7223/10000], Loss: 0.35382139682769775\n",
      "Epoch [7224/10000], Loss: 0.3538045883178711\n",
      "Epoch [7225/10000], Loss: 0.35378754138946533\n",
      "Epoch [7226/10000], Loss: 0.3537706732749939\n",
      "Epoch [7227/10000], Loss: 0.3537537455558777\n",
      "Epoch [7228/10000], Loss: 0.3537367582321167\n",
      "Epoch [7229/10000], Loss: 0.3537197709083557\n",
      "Epoch [7230/10000], Loss: 0.3537028431892395\n",
      "Epoch [7231/10000], Loss: 0.3536858558654785\n",
      "Epoch [7232/10000], Loss: 0.35366880893707275\n",
      "Epoch [7233/10000], Loss: 0.35365188121795654\n",
      "Epoch [7234/10000], Loss: 0.35363489389419556\n",
      "Epoch [7235/10000], Loss: 0.3536178469657898\n",
      "Epoch [7236/10000], Loss: 0.3536008596420288\n",
      "Epoch [7237/10000], Loss: 0.35358375310897827\n",
      "Epoch [7238/10000], Loss: 0.3535667657852173\n",
      "Epoch [7239/10000], Loss: 0.3535497784614563\n",
      "Epoch [7240/10000], Loss: 0.35353273153305054\n",
      "Epoch [7241/10000], Loss: 0.353515625\n",
      "Epoch [7242/10000], Loss: 0.35349851846694946\n",
      "Epoch [7243/10000], Loss: 0.3534814715385437\n",
      "Epoch [7244/10000], Loss: 0.35346442461013794\n",
      "Epoch [7245/10000], Loss: 0.3534472584724426\n",
      "Epoch [7246/10000], Loss: 0.35343021154403687\n",
      "Epoch [7247/10000], Loss: 0.35341304540634155\n",
      "Epoch [7248/10000], Loss: 0.35339581966400146\n",
      "Epoch [7249/10000], Loss: 0.3533787131309509\n",
      "Epoch [7250/10000], Loss: 0.3533616065979004\n",
      "Epoch [7251/10000], Loss: 0.3533443808555603\n",
      "Epoch [7252/10000], Loss: 0.353327214717865\n",
      "Epoch [7253/10000], Loss: 0.3533099889755249\n",
      "Epoch [7254/10000], Loss: 0.35329294204711914\n",
      "Epoch [7255/10000], Loss: 0.3532756567001343\n",
      "Epoch [7256/10000], Loss: 0.3532584309577942\n",
      "Epoch [7257/10000], Loss: 0.3532411456108093\n",
      "Epoch [7258/10000], Loss: 0.35322386026382446\n",
      "Epoch [7259/10000], Loss: 0.35320669412612915\n",
      "Epoch [7260/10000], Loss: 0.3531893491744995\n",
      "Epoch [7261/10000], Loss: 0.3531720042228699\n",
      "Epoch [7262/10000], Loss: 0.353154718875885\n",
      "Epoch [7263/10000], Loss: 0.35313743352890015\n",
      "Epoch [7264/10000], Loss: 0.35312002897262573\n",
      "Epoch [7265/10000], Loss: 0.35310274362564087\n",
      "Epoch [7266/10000], Loss: 0.35308539867401123\n",
      "Epoch [7267/10000], Loss: 0.35306793451309204\n",
      "Epoch [7268/10000], Loss: 0.3530506491661072\n",
      "Epoch [7269/10000], Loss: 0.353033185005188\n",
      "Epoch [7270/10000], Loss: 0.3530157804489136\n",
      "Epoch [7271/10000], Loss: 0.3529982566833496\n",
      "Epoch [7272/10000], Loss: 0.35298091173171997\n",
      "Epoch [7273/10000], Loss: 0.3529634475708008\n",
      "Epoch [7274/10000], Loss: 0.3529459834098816\n",
      "Epoch [7275/10000], Loss: 0.3529285192489624\n",
      "Epoch [7276/10000], Loss: 0.3529108762741089\n",
      "Epoch [7277/10000], Loss: 0.3528933525085449\n",
      "Epoch [7278/10000], Loss: 0.3528757691383362\n",
      "Epoch [7279/10000], Loss: 0.3528582453727722\n",
      "Epoch [7280/10000], Loss: 0.3528406023979187\n",
      "Epoch [7281/10000], Loss: 0.3528231382369995\n",
      "Epoch [7282/10000], Loss: 0.352805495262146\n",
      "Epoch [7283/10000], Loss: 0.3527878522872925\n",
      "Epoch [7284/10000], Loss: 0.35277020931243896\n",
      "Epoch [7285/10000], Loss: 0.3527525067329407\n",
      "Epoch [7286/10000], Loss: 0.3527347445487976\n",
      "Epoch [7287/10000], Loss: 0.3527170419692993\n",
      "Epoch [7288/10000], Loss: 0.3526993989944458\n",
      "Epoch [7289/10000], Loss: 0.35268157720565796\n",
      "Epoch [7290/10000], Loss: 0.35266387462615967\n",
      "Epoch [7291/10000], Loss: 0.3526460528373718\n",
      "Epoch [7292/10000], Loss: 0.35262829065322876\n",
      "Epoch [7293/10000], Loss: 0.35261040925979614\n",
      "Epoch [7294/10000], Loss: 0.3525925278663635\n",
      "Epoch [7295/10000], Loss: 0.3525747060775757\n",
      "Epoch [7296/10000], Loss: 0.35255682468414307\n",
      "Epoch [7297/10000], Loss: 0.3525390028953552\n",
      "Epoch [7298/10000], Loss: 0.3525209426879883\n",
      "Epoch [7299/10000], Loss: 0.3525030016899109\n",
      "Epoch [7300/10000], Loss: 0.3524850606918335\n",
      "Epoch [7301/10000], Loss: 0.35246700048446655\n",
      "Epoch [7302/10000], Loss: 0.35244905948638916\n",
      "Epoch [7303/10000], Loss: 0.35243093967437744\n",
      "Epoch [7304/10000], Loss: 0.35241299867630005\n",
      "Epoch [7305/10000], Loss: 0.3523949384689331\n",
      "Epoch [7306/10000], Loss: 0.3523767590522766\n",
      "Epoch [7307/10000], Loss: 0.3523586392402649\n",
      "Epoch [7308/10000], Loss: 0.3523405194282532\n",
      "Epoch [7309/10000], Loss: 0.35232239961624146\n",
      "Epoch [7310/10000], Loss: 0.3523041605949402\n",
      "Epoch [7311/10000], Loss: 0.3522859215736389\n",
      "Epoch [7312/10000], Loss: 0.3522677421569824\n",
      "Epoch [7313/10000], Loss: 0.35224950313568115\n",
      "Epoch [7314/10000], Loss: 0.3522312045097351\n",
      "Epoch [7315/10000], Loss: 0.35221290588378906\n",
      "Epoch [7316/10000], Loss: 0.35219448804855347\n",
      "Epoch [7317/10000], Loss: 0.3521762490272522\n",
      "Epoch [7318/10000], Loss: 0.3521578907966614\n",
      "Epoch [7319/10000], Loss: 0.3521394729614258\n",
      "Epoch [7320/10000], Loss: 0.3521210551261902\n",
      "Epoch [7321/10000], Loss: 0.3521026372909546\n",
      "Epoch [7322/10000], Loss: 0.35208410024642944\n",
      "Epoch [7323/10000], Loss: 0.3520656228065491\n",
      "Epoch [7324/10000], Loss: 0.3520471453666687\n",
      "Epoch [7325/10000], Loss: 0.3520285487174988\n",
      "Epoch [7326/10000], Loss: 0.3520101308822632\n",
      "Epoch [7327/10000], Loss: 0.3519914746284485\n",
      "Epoch [7328/10000], Loss: 0.35197287797927856\n",
      "Epoch [7329/10000], Loss: 0.35195428133010864\n",
      "Epoch [7330/10000], Loss: 0.3519356846809387\n",
      "Epoch [7331/10000], Loss: 0.3519169092178345\n",
      "Epoch [7332/10000], Loss: 0.35189831256866455\n",
      "Epoch [7333/10000], Loss: 0.3518795967102051\n",
      "Epoch [7334/10000], Loss: 0.35186082124710083\n",
      "Epoch [7335/10000], Loss: 0.35184210538864136\n",
      "Epoch [7336/10000], Loss: 0.3518233895301819\n",
      "Epoch [7337/10000], Loss: 0.35180461406707764\n",
      "Epoch [7338/10000], Loss: 0.35178571939468384\n",
      "Epoch [7339/10000], Loss: 0.3517668843269348\n",
      "Epoch [7340/10000], Loss: 0.351747989654541\n",
      "Epoch [7341/10000], Loss: 0.3517290949821472\n",
      "Epoch [7342/10000], Loss: 0.3517102599143982\n",
      "Epoch [7343/10000], Loss: 0.3516913056373596\n",
      "Epoch [7344/10000], Loss: 0.35167235136032104\n",
      "Epoch [7345/10000], Loss: 0.35165339708328247\n",
      "Epoch [7346/10000], Loss: 0.35163432359695435\n",
      "Epoch [7347/10000], Loss: 0.35161536931991577\n",
      "Epoch [7348/10000], Loss: 0.35159629583358765\n",
      "Epoch [7349/10000], Loss: 0.3515772223472595\n",
      "Epoch [7350/10000], Loss: 0.35155820846557617\n",
      "Epoch [7351/10000], Loss: 0.35153913497924805\n",
      "Epoch [7352/10000], Loss: 0.35152000188827515\n",
      "Epoch [7353/10000], Loss: 0.35150086879730225\n",
      "Epoch [7354/10000], Loss: 0.3514817953109741\n",
      "Epoch [7355/10000], Loss: 0.35146254301071167\n",
      "Epoch [7356/10000], Loss: 0.351443350315094\n",
      "Epoch [7357/10000], Loss: 0.3514242172241211\n",
      "Epoch [7358/10000], Loss: 0.3514050245285034\n",
      "Epoch [7359/10000], Loss: 0.3513857126235962\n",
      "Epoch [7360/10000], Loss: 0.3513665199279785\n",
      "Epoch [7361/10000], Loss: 0.35134732723236084\n",
      "Epoch [7362/10000], Loss: 0.3513280153274536\n",
      "Epoch [7363/10000], Loss: 0.35130876302719116\n",
      "Epoch [7364/10000], Loss: 0.35128939151763916\n",
      "Epoch [7365/10000], Loss: 0.3512701392173767\n",
      "Epoch [7366/10000], Loss: 0.3512507677078247\n",
      "Epoch [7367/10000], Loss: 0.3512314558029175\n",
      "Epoch [7368/10000], Loss: 0.3512120842933655\n",
      "Epoch [7369/10000], Loss: 0.351192831993103\n",
      "Epoch [7370/10000], Loss: 0.35117340087890625\n",
      "Epoch [7371/10000], Loss: 0.3511539697647095\n",
      "Epoch [7372/10000], Loss: 0.35113465785980225\n",
      "Epoch [7373/10000], Loss: 0.35111522674560547\n",
      "Epoch [7374/10000], Loss: 0.3510957956314087\n",
      "Epoch [7375/10000], Loss: 0.3510764241218567\n",
      "Epoch [7376/10000], Loss: 0.3510569930076599\n",
      "Epoch [7377/10000], Loss: 0.35103750228881836\n",
      "Epoch [7378/10000], Loss: 0.3510180711746216\n",
      "Epoch [7379/10000], Loss: 0.35099858045578003\n",
      "Epoch [7380/10000], Loss: 0.350979208946228\n",
      "Epoch [7381/10000], Loss: 0.3509597182273865\n",
      "Epoch [7382/10000], Loss: 0.3509402275085449\n",
      "Epoch [7383/10000], Loss: 0.35092073678970337\n",
      "Epoch [7384/10000], Loss: 0.3509013056755066\n",
      "Epoch [7385/10000], Loss: 0.3508818745613098\n",
      "Epoch [7386/10000], Loss: 0.35086238384246826\n",
      "Epoch [7387/10000], Loss: 0.3508428931236267\n",
      "Epoch [7388/10000], Loss: 0.35082340240478516\n",
      "Epoch [7389/10000], Loss: 0.35080385208129883\n",
      "Epoch [7390/10000], Loss: 0.3507843613624573\n",
      "Epoch [7391/10000], Loss: 0.3507648706436157\n",
      "Epoch [7392/10000], Loss: 0.35074537992477417\n",
      "Epoch [7393/10000], Loss: 0.35072582960128784\n",
      "Epoch [7394/10000], Loss: 0.3507065176963806\n",
      "Epoch [7395/10000], Loss: 0.3506869077682495\n",
      "Epoch [7396/10000], Loss: 0.35066741704940796\n",
      "Epoch [7397/10000], Loss: 0.3506479859352112\n",
      "Epoch [7398/10000], Loss: 0.3506283760070801\n",
      "Epoch [7399/10000], Loss: 0.3506090044975281\n",
      "Epoch [7400/10000], Loss: 0.350589394569397\n",
      "Epoch [7401/10000], Loss: 0.35057002305984497\n",
      "Epoch [7402/10000], Loss: 0.3505505323410034\n",
      "Epoch [7403/10000], Loss: 0.35053110122680664\n",
      "Epoch [7404/10000], Loss: 0.3505115509033203\n",
      "Epoch [7405/10000], Loss: 0.35049211978912354\n",
      "Epoch [7406/10000], Loss: 0.35047274827957153\n",
      "Epoch [7407/10000], Loss: 0.35045325756073\n",
      "Epoch [7408/10000], Loss: 0.3504338264465332\n",
      "Epoch [7409/10000], Loss: 0.3504143953323364\n",
      "Epoch [7410/10000], Loss: 0.3503950834274292\n",
      "Epoch [7411/10000], Loss: 0.35037559270858765\n",
      "Epoch [7412/10000], Loss: 0.35035616159439087\n",
      "Epoch [7413/10000], Loss: 0.3503367304801941\n",
      "Epoch [7414/10000], Loss: 0.35031741857528687\n",
      "Epoch [7415/10000], Loss: 0.35029804706573486\n",
      "Epoch [7416/10000], Loss: 0.35027867555618286\n",
      "Epoch [7417/10000], Loss: 0.35025930404663086\n",
      "Epoch [7418/10000], Loss: 0.3502400517463684\n",
      "Epoch [7419/10000], Loss: 0.3502206802368164\n",
      "Epoch [7420/10000], Loss: 0.35020142793655396\n",
      "Epoch [7421/10000], Loss: 0.35018211603164673\n",
      "Epoch [7422/10000], Loss: 0.3501628041267395\n",
      "Epoch [7423/10000], Loss: 0.3501434922218323\n",
      "Epoch [7424/10000], Loss: 0.3501242995262146\n",
      "Epoch [7425/10000], Loss: 0.3501049876213074\n",
      "Epoch [7426/10000], Loss: 0.3500857949256897\n",
      "Epoch [7427/10000], Loss: 0.35006654262542725\n",
      "Epoch [7428/10000], Loss: 0.3500472903251648\n",
      "Epoch [7429/10000], Loss: 0.3500280976295471\n",
      "Epoch [7430/10000], Loss: 0.3500089645385742\n",
      "Epoch [7431/10000], Loss: 0.34998977184295654\n",
      "Epoch [7432/10000], Loss: 0.34997063875198364\n",
      "Epoch [7433/10000], Loss: 0.34995144605636597\n",
      "Epoch [7434/10000], Loss: 0.34993237257003784\n",
      "Epoch [7435/10000], Loss: 0.34991323947906494\n",
      "Epoch [7436/10000], Loss: 0.3498941659927368\n",
      "Epoch [7437/10000], Loss: 0.3498750925064087\n",
      "Epoch [7438/10000], Loss: 0.34985601902008057\n",
      "Epoch [7439/10000], Loss: 0.34983694553375244\n",
      "Epoch [7440/10000], Loss: 0.34981805086135864\n",
      "Epoch [7441/10000], Loss: 0.3497989773750305\n",
      "Epoch [7442/10000], Loss: 0.3497800827026367\n",
      "Epoch [7443/10000], Loss: 0.34976106882095337\n",
      "Epoch [7444/10000], Loss: 0.34974205493927\n",
      "Epoch [7445/10000], Loss: 0.3497231602668762\n",
      "Epoch [7446/10000], Loss: 0.3497042655944824\n",
      "Epoch [7447/10000], Loss: 0.3496853709220886\n",
      "Epoch [7448/10000], Loss: 0.3496664762496948\n",
      "Epoch [7449/10000], Loss: 0.349647581577301\n",
      "Epoch [7450/10000], Loss: 0.349628746509552\n",
      "Epoch [7451/10000], Loss: 0.349609911441803\n",
      "Epoch [7452/10000], Loss: 0.34959113597869873\n",
      "Epoch [7453/10000], Loss: 0.3495723009109497\n",
      "Epoch [7454/10000], Loss: 0.34955358505249023\n",
      "Epoch [7455/10000], Loss: 0.349534809589386\n",
      "Epoch [7456/10000], Loss: 0.34951603412628174\n",
      "Epoch [7457/10000], Loss: 0.3494972586631775\n",
      "Epoch [7458/10000], Loss: 0.3494786024093628\n",
      "Epoch [7459/10000], Loss: 0.34946000576019287\n",
      "Epoch [7460/10000], Loss: 0.3494412302970886\n",
      "Epoch [7461/10000], Loss: 0.34942275285720825\n",
      "Epoch [7462/10000], Loss: 0.3494040369987488\n",
      "Epoch [7463/10000], Loss: 0.3493853211402893\n",
      "Epoch [7464/10000], Loss: 0.34936684370040894\n",
      "Epoch [7465/10000], Loss: 0.349348247051239\n",
      "Epoch [7466/10000], Loss: 0.34932976961135864\n",
      "Epoch [7467/10000], Loss: 0.3493112325668335\n",
      "Epoch [7468/10000], Loss: 0.34929269552230835\n",
      "Epoch [7469/10000], Loss: 0.349274218082428\n",
      "Epoch [7470/10000], Loss: 0.34925568103790283\n",
      "Epoch [7471/10000], Loss: 0.349237322807312\n",
      "Epoch [7472/10000], Loss: 0.3492189049720764\n",
      "Epoch [7473/10000], Loss: 0.34920036792755127\n",
      "Epoch [7474/10000], Loss: 0.3491820693016052\n",
      "Epoch [7475/10000], Loss: 0.34916365146636963\n",
      "Epoch [7476/10000], Loss: 0.3491453528404236\n",
      "Epoch [7477/10000], Loss: 0.34912699460983276\n",
      "Epoch [7478/10000], Loss: 0.34910863637924194\n",
      "Epoch [7479/10000], Loss: 0.34909045696258545\n",
      "Epoch [7480/10000], Loss: 0.34907209873199463\n",
      "Epoch [7481/10000], Loss: 0.34905385971069336\n",
      "Epoch [7482/10000], Loss: 0.34903568029403687\n",
      "Epoch [7483/10000], Loss: 0.34901732206344604\n",
      "Epoch [7484/10000], Loss: 0.34899914264678955\n",
      "Epoch [7485/10000], Loss: 0.34898096323013306\n",
      "Epoch [7486/10000], Loss: 0.34896284341812134\n",
      "Epoch [7487/10000], Loss: 0.3489447236061096\n",
      "Epoch [7488/10000], Loss: 0.3489265441894531\n",
      "Epoch [7489/10000], Loss: 0.34890836477279663\n",
      "Epoch [7490/10000], Loss: 0.34889036417007446\n",
      "Epoch [7491/10000], Loss: 0.3488723039627075\n",
      "Epoch [7492/10000], Loss: 0.3488541841506958\n",
      "Epoch [7493/10000], Loss: 0.3488362431526184\n",
      "Epoch [7494/10000], Loss: 0.34881818294525146\n",
      "Epoch [7495/10000], Loss: 0.3488001823425293\n",
      "Epoch [7496/10000], Loss: 0.34878212213516235\n",
      "Epoch [7497/10000], Loss: 0.34876424074172974\n",
      "Epoch [7498/10000], Loss: 0.34874629974365234\n",
      "Epoch [7499/10000], Loss: 0.3487282991409302\n",
      "Epoch [7500/10000], Loss: 0.34871047735214233\n",
      "Epoch [7501/10000], Loss: 0.34869253635406494\n",
      "Epoch [7502/10000], Loss: 0.3486746549606323\n",
      "Epoch [7503/10000], Loss: 0.34865671396255493\n",
      "Epoch [7504/10000], Loss: 0.34863895177841187\n",
      "Epoch [7505/10000], Loss: 0.34862107038497925\n",
      "Epoch [7506/10000], Loss: 0.3486033082008362\n",
      "Epoch [7507/10000], Loss: 0.3485855460166931\n",
      "Epoch [7508/10000], Loss: 0.34856778383255005\n",
      "Epoch [7509/10000], Loss: 0.34854990243911743\n",
      "Epoch [7510/10000], Loss: 0.34853219985961914\n",
      "Epoch [7511/10000], Loss: 0.34851449728012085\n",
      "Epoch [7512/10000], Loss: 0.34849679470062256\n",
      "Epoch [7513/10000], Loss: 0.34847909212112427\n",
      "Epoch [7514/10000], Loss: 0.34846144914627075\n",
      "Epoch [7515/10000], Loss: 0.34844380617141724\n",
      "Epoch [7516/10000], Loss: 0.3484261631965637\n",
      "Epoch [7517/10000], Loss: 0.3484085202217102\n",
      "Epoch [7518/10000], Loss: 0.3483908772468567\n",
      "Epoch [7519/10000], Loss: 0.3483732342720032\n",
      "Epoch [7520/10000], Loss: 0.3483557105064392\n",
      "Epoch [7521/10000], Loss: 0.34833812713623047\n",
      "Epoch [7522/10000], Loss: 0.34832054376602173\n",
      "Epoch [7523/10000], Loss: 0.348302960395813\n",
      "Epoch [7524/10000], Loss: 0.3482854962348938\n",
      "Epoch [7525/10000], Loss: 0.34826797246932983\n",
      "Epoch [7526/10000], Loss: 0.34825050830841064\n",
      "Epoch [7527/10000], Loss: 0.34823304414749146\n",
      "Epoch [7528/10000], Loss: 0.34821563959121704\n",
      "Epoch [7529/10000], Loss: 0.3481981158256531\n",
      "Epoch [7530/10000], Loss: 0.34818071126937866\n",
      "Epoch [7531/10000], Loss: 0.3481632471084595\n",
      "Epoch [7532/10000], Loss: 0.34814590215682983\n",
      "Epoch [7533/10000], Loss: 0.34812861680984497\n",
      "Epoch [7534/10000], Loss: 0.34811121225357056\n",
      "Epoch [7535/10000], Loss: 0.3480938673019409\n",
      "Epoch [7536/10000], Loss: 0.34807640314102173\n",
      "Epoch [7537/10000], Loss: 0.34805917739868164\n",
      "Epoch [7538/10000], Loss: 0.348041832447052\n",
      "Epoch [7539/10000], Loss: 0.34802448749542236\n",
      "Epoch [7540/10000], Loss: 0.3480072617530823\n",
      "Epoch [7541/10000], Loss: 0.3479899764060974\n",
      "Epoch [7542/10000], Loss: 0.3479727506637573\n",
      "Epoch [7543/10000], Loss: 0.34795552492141724\n",
      "Epoch [7544/10000], Loss: 0.3479382395744324\n",
      "Epoch [7545/10000], Loss: 0.3479210138320923\n",
      "Epoch [7546/10000], Loss: 0.3479037880897522\n",
      "Epoch [7547/10000], Loss: 0.3478866219520569\n",
      "Epoch [7548/10000], Loss: 0.3478693962097168\n",
      "Epoch [7549/10000], Loss: 0.3478522300720215\n",
      "Epoch [7550/10000], Loss: 0.34783512353897095\n",
      "Epoch [7551/10000], Loss: 0.34781795740127563\n",
      "Epoch [7552/10000], Loss: 0.3478008508682251\n",
      "Epoch [7553/10000], Loss: 0.3477836847305298\n",
      "Epoch [7554/10000], Loss: 0.34776657819747925\n",
      "Epoch [7555/10000], Loss: 0.34774959087371826\n",
      "Epoch [7556/10000], Loss: 0.34773242473602295\n",
      "Epoch [7557/10000], Loss: 0.3477153182029724\n",
      "Epoch [7558/10000], Loss: 0.3476983308792114\n",
      "Epoch [7559/10000], Loss: 0.34768134355545044\n",
      "Epoch [7560/10000], Loss: 0.3476642370223999\n",
      "Epoch [7561/10000], Loss: 0.34764719009399414\n",
      "Epoch [7562/10000], Loss: 0.34763020277023315\n",
      "Epoch [7563/10000], Loss: 0.34761321544647217\n",
      "Epoch [7564/10000], Loss: 0.3475962281227112\n",
      "Epoch [7565/10000], Loss: 0.3475791811943054\n",
      "Epoch [7566/10000], Loss: 0.3475622534751892\n",
      "Epoch [7567/10000], Loss: 0.347545325756073\n",
      "Epoch [7568/10000], Loss: 0.347528338432312\n",
      "Epoch [7569/10000], Loss: 0.3475114107131958\n",
      "Epoch [7570/10000], Loss: 0.3474944829940796\n",
      "Epoch [7571/10000], Loss: 0.3474775552749634\n",
      "Epoch [7572/10000], Loss: 0.34746062755584717\n",
      "Epoch [7573/10000], Loss: 0.34744375944137573\n",
      "Epoch [7574/10000], Loss: 0.3474268317222595\n",
      "Epoch [7575/10000], Loss: 0.34741002321243286\n",
      "Epoch [7576/10000], Loss: 0.34739309549331665\n",
      "Epoch [7577/10000], Loss: 0.34737628698349\n",
      "Epoch [7578/10000], Loss: 0.3473593592643738\n",
      "Epoch [7579/10000], Loss: 0.3473426103591919\n",
      "Epoch [7580/10000], Loss: 0.34732580184936523\n",
      "Epoch [7581/10000], Loss: 0.3473089933395386\n",
      "Epoch [7582/10000], Loss: 0.34729206562042236\n",
      "Epoch [7583/10000], Loss: 0.3472753167152405\n",
      "Epoch [7584/10000], Loss: 0.3472585678100586\n",
      "Epoch [7585/10000], Loss: 0.34724175930023193\n",
      "Epoch [7586/10000], Loss: 0.34722501039505005\n",
      "Epoch [7587/10000], Loss: 0.3472082018852234\n",
      "Epoch [7588/10000], Loss: 0.3471914529800415\n",
      "Epoch [7589/10000], Loss: 0.3471747636795044\n",
      "Epoch [7590/10000], Loss: 0.3471580147743225\n",
      "Epoch [7591/10000], Loss: 0.34714120626449585\n",
      "Epoch [7592/10000], Loss: 0.3471245765686035\n",
      "Epoch [7593/10000], Loss: 0.34710776805877686\n",
      "Epoch [7594/10000], Loss: 0.34709107875823975\n",
      "Epoch [7595/10000], Loss: 0.34707438945770264\n",
      "Epoch [7596/10000], Loss: 0.3470577597618103\n",
      "Epoch [7597/10000], Loss: 0.3470410108566284\n",
      "Epoch [7598/10000], Loss: 0.3470243215560913\n",
      "Epoch [7599/10000], Loss: 0.347007691860199\n",
      "Epoch [7600/10000], Loss: 0.34699100255966187\n",
      "Epoch [7601/10000], Loss: 0.34697431325912476\n",
      "Epoch [7602/10000], Loss: 0.3469576835632324\n",
      "Epoch [7603/10000], Loss: 0.3469410538673401\n",
      "Epoch [7604/10000], Loss: 0.346924364566803\n",
      "Epoch [7605/10000], Loss: 0.34690773487091064\n",
      "Epoch [7606/10000], Loss: 0.34689122438430786\n",
      "Epoch [7607/10000], Loss: 0.3468745946884155\n",
      "Epoch [7608/10000], Loss: 0.34685802459716797\n",
      "Epoch [7609/10000], Loss: 0.34684133529663086\n",
      "Epoch [7610/10000], Loss: 0.3468247056007385\n",
      "Epoch [7611/10000], Loss: 0.34680813550949097\n",
      "Epoch [7612/10000], Loss: 0.3467915654182434\n",
      "Epoch [7613/10000], Loss: 0.34677499532699585\n",
      "Epoch [7614/10000], Loss: 0.3467584252357483\n",
      "Epoch [7615/10000], Loss: 0.34674185514450073\n",
      "Epoch [7616/10000], Loss: 0.34672534465789795\n",
      "Epoch [7617/10000], Loss: 0.3467087745666504\n",
      "Epoch [7618/10000], Loss: 0.34669214487075806\n",
      "Epoch [7619/10000], Loss: 0.34667569398880005\n",
      "Epoch [7620/10000], Loss: 0.3466591238975525\n",
      "Epoch [7621/10000], Loss: 0.3466426134109497\n",
      "Epoch [7622/10000], Loss: 0.3466261625289917\n",
      "Epoch [7623/10000], Loss: 0.34660959243774414\n",
      "Epoch [7624/10000], Loss: 0.3465930223464966\n",
      "Epoch [7625/10000], Loss: 0.3465765714645386\n",
      "Epoch [7626/10000], Loss: 0.3465600609779358\n",
      "Epoch [7627/10000], Loss: 0.3465436100959778\n",
      "Epoch [7628/10000], Loss: 0.3465270400047302\n",
      "Epoch [7629/10000], Loss: 0.34651052951812744\n",
      "Epoch [7630/10000], Loss: 0.34649407863616943\n",
      "Epoch [7631/10000], Loss: 0.3464776277542114\n",
      "Epoch [7632/10000], Loss: 0.34646111726760864\n",
      "Epoch [7633/10000], Loss: 0.34644466638565063\n",
      "Epoch [7634/10000], Loss: 0.3464282155036926\n",
      "Epoch [7635/10000], Loss: 0.3464117646217346\n",
      "Epoch [7636/10000], Loss: 0.3463953137397766\n",
      "Epoch [7637/10000], Loss: 0.3463788628578186\n",
      "Epoch [7638/10000], Loss: 0.34636247158050537\n",
      "Epoch [7639/10000], Loss: 0.3463459610939026\n",
      "Epoch [7640/10000], Loss: 0.3463295102119446\n",
      "Epoch [7641/10000], Loss: 0.3463130593299866\n",
      "Epoch [7642/10000], Loss: 0.3462967276573181\n",
      "Epoch [7643/10000], Loss: 0.34628021717071533\n",
      "Epoch [7644/10000], Loss: 0.3462638258934021\n",
      "Epoch [7645/10000], Loss: 0.34624749422073364\n",
      "Epoch [7646/10000], Loss: 0.34623098373413086\n",
      "Epoch [7647/10000], Loss: 0.3462146520614624\n",
      "Epoch [7648/10000], Loss: 0.3461981415748596\n",
      "Epoch [7649/10000], Loss: 0.3461817502975464\n",
      "Epoch [7650/10000], Loss: 0.34616535902023315\n",
      "Epoch [7651/10000], Loss: 0.3461489677429199\n",
      "Epoch [7652/10000], Loss: 0.3461325764656067\n",
      "Epoch [7653/10000], Loss: 0.34611618518829346\n",
      "Epoch [7654/10000], Loss: 0.34609973430633545\n",
      "Epoch [7655/10000], Loss: 0.3460833430290222\n",
      "Epoch [7656/10000], Loss: 0.34606701135635376\n",
      "Epoch [7657/10000], Loss: 0.3460506796836853\n",
      "Epoch [7658/10000], Loss: 0.34603434801101685\n",
      "Epoch [7659/10000], Loss: 0.34601789712905884\n",
      "Epoch [7660/10000], Loss: 0.3460015654563904\n",
      "Epoch [7661/10000], Loss: 0.3459851145744324\n",
      "Epoch [7662/10000], Loss: 0.3459687829017639\n",
      "Epoch [7663/10000], Loss: 0.34595245122909546\n",
      "Epoch [7664/10000], Loss: 0.345936119556427\n",
      "Epoch [7665/10000], Loss: 0.34591978788375854\n",
      "Epoch [7666/10000], Loss: 0.34590333700180054\n",
      "Epoch [7667/10000], Loss: 0.34588706493377686\n",
      "Epoch [7668/10000], Loss: 0.34587061405181885\n",
      "Epoch [7669/10000], Loss: 0.34585434198379517\n",
      "Epoch [7670/10000], Loss: 0.3458380103111267\n",
      "Epoch [7671/10000], Loss: 0.3458214998245239\n",
      "Epoch [7672/10000], Loss: 0.345805287361145\n",
      "Epoch [7673/10000], Loss: 0.34578895568847656\n",
      "Epoch [7674/10000], Loss: 0.3457726240158081\n",
      "Epoch [7675/10000], Loss: 0.3457562327384949\n",
      "Epoch [7676/10000], Loss: 0.3457399010658264\n",
      "Epoch [7677/10000], Loss: 0.34572356939315796\n",
      "Epoch [7678/10000], Loss: 0.3457072973251343\n",
      "Epoch [7679/10000], Loss: 0.34569084644317627\n",
      "Epoch [7680/10000], Loss: 0.3456745743751526\n",
      "Epoch [7681/10000], Loss: 0.34565824270248413\n",
      "Epoch [7682/10000], Loss: 0.3456419110298157\n",
      "Epoch [7683/10000], Loss: 0.3456255793571472\n",
      "Epoch [7684/10000], Loss: 0.34560924768447876\n",
      "Epoch [7685/10000], Loss: 0.3455929756164551\n",
      "Epoch [7686/10000], Loss: 0.3455766439437866\n",
      "Epoch [7687/10000], Loss: 0.3455602526664734\n",
      "Epoch [7688/10000], Loss: 0.3455439805984497\n",
      "Epoch [7689/10000], Loss: 0.34552764892578125\n",
      "Epoch [7690/10000], Loss: 0.345511257648468\n",
      "Epoch [7691/10000], Loss: 0.3454950451850891\n",
      "Epoch [7692/10000], Loss: 0.3454786539077759\n",
      "Epoch [7693/10000], Loss: 0.3454623222351074\n",
      "Epoch [7694/10000], Loss: 0.34544605016708374\n",
      "Epoch [7695/10000], Loss: 0.3454297184944153\n",
      "Epoch [7696/10000], Loss: 0.3454135060310364\n",
      "Epoch [7697/10000], Loss: 0.34539711475372314\n",
      "Epoch [7698/10000], Loss: 0.3453807830810547\n",
      "Epoch [7699/10000], Loss: 0.34536445140838623\n",
      "Epoch [7700/10000], Loss: 0.345348060131073\n",
      "Epoch [7701/10000], Loss: 0.3453318476676941\n",
      "Epoch [7702/10000], Loss: 0.34531545639038086\n",
      "Epoch [7703/10000], Loss: 0.3452991843223572\n",
      "Epoch [7704/10000], Loss: 0.3452829122543335\n",
      "Epoch [7705/10000], Loss: 0.3452666401863098\n",
      "Epoch [7706/10000], Loss: 0.3452501893043518\n",
      "Epoch [7707/10000], Loss: 0.3452339172363281\n",
      "Epoch [7708/10000], Loss: 0.34521758556365967\n",
      "Epoch [7709/10000], Loss: 0.345201313495636\n",
      "Epoch [7710/10000], Loss: 0.34518498182296753\n",
      "Epoch [7711/10000], Loss: 0.3451686501502991\n",
      "Epoch [7712/10000], Loss: 0.3451523184776306\n",
      "Epoch [7713/10000], Loss: 0.34513598680496216\n",
      "Epoch [7714/10000], Loss: 0.3451196551322937\n",
      "Epoch [7715/10000], Loss: 0.34510332345962524\n",
      "Epoch [7716/10000], Loss: 0.3450869917869568\n",
      "Epoch [7717/10000], Loss: 0.3450707197189331\n",
      "Epoch [7718/10000], Loss: 0.3450543284416199\n",
      "Epoch [7719/10000], Loss: 0.3450379967689514\n",
      "Epoch [7720/10000], Loss: 0.34502172470092773\n",
      "Epoch [7721/10000], Loss: 0.34500545263290405\n",
      "Epoch [7722/10000], Loss: 0.34498900175094604\n",
      "Epoch [7723/10000], Loss: 0.34497272968292236\n",
      "Epoch [7724/10000], Loss: 0.3449563980102539\n",
      "Epoch [7725/10000], Loss: 0.34494006633758545\n",
      "Epoch [7726/10000], Loss: 0.34492379426956177\n",
      "Epoch [7727/10000], Loss: 0.34490740299224854\n",
      "Epoch [7728/10000], Loss: 0.3448910713195801\n",
      "Epoch [7729/10000], Loss: 0.3448747992515564\n",
      "Epoch [7730/10000], Loss: 0.3448583483695984\n",
      "Epoch [7731/10000], Loss: 0.3448420763015747\n",
      "Epoch [7732/10000], Loss: 0.34482574462890625\n",
      "Epoch [7733/10000], Loss: 0.3448094129562378\n",
      "Epoch [7734/10000], Loss: 0.34479302167892456\n",
      "Epoch [7735/10000], Loss: 0.3447767496109009\n",
      "Epoch [7736/10000], Loss: 0.34476029872894287\n",
      "Epoch [7737/10000], Loss: 0.3447440266609192\n",
      "Epoch [7738/10000], Loss: 0.34472769498825073\n",
      "Epoch [7739/10000], Loss: 0.3447113037109375\n",
      "Epoch [7740/10000], Loss: 0.34469497203826904\n",
      "Epoch [7741/10000], Loss: 0.34467852115631104\n",
      "Epoch [7742/10000], Loss: 0.34466224908828735\n",
      "Epoch [7743/10000], Loss: 0.3446459174156189\n",
      "Epoch [7744/10000], Loss: 0.3446294665336609\n",
      "Epoch [7745/10000], Loss: 0.3446131944656372\n",
      "Epoch [7746/10000], Loss: 0.34459686279296875\n",
      "Epoch [7747/10000], Loss: 0.34458035230636597\n",
      "Epoch [7748/10000], Loss: 0.3445640802383423\n",
      "Epoch [7749/10000], Loss: 0.34454774856567383\n",
      "Epoch [7750/10000], Loss: 0.3445313572883606\n",
      "Epoch [7751/10000], Loss: 0.34451502561569214\n",
      "Epoch [7752/10000], Loss: 0.3444986939430237\n",
      "Epoch [7753/10000], Loss: 0.3444823622703552\n",
      "Epoch [7754/10000], Loss: 0.3444659113883972\n",
      "Epoch [7755/10000], Loss: 0.34444957971572876\n",
      "Epoch [7756/10000], Loss: 0.3444332480430603\n",
      "Epoch [7757/10000], Loss: 0.34441691637039185\n",
      "Epoch [7758/10000], Loss: 0.34440046548843384\n",
      "Epoch [7759/10000], Loss: 0.3443841338157654\n",
      "Epoch [7760/10000], Loss: 0.3443678021430969\n",
      "Epoch [7761/10000], Loss: 0.3443514108657837\n",
      "Epoch [7762/10000], Loss: 0.34433501958847046\n",
      "Epoch [7763/10000], Loss: 0.344318687915802\n",
      "Epoch [7764/10000], Loss: 0.344302237033844\n",
      "Epoch [7765/10000], Loss: 0.3442859649658203\n",
      "Epoch [7766/10000], Loss: 0.34426945447921753\n",
      "Epoch [7767/10000], Loss: 0.3442530632019043\n",
      "Epoch [7768/10000], Loss: 0.3442367911338806\n",
      "Epoch [7769/10000], Loss: 0.34422028064727783\n",
      "Epoch [7770/10000], Loss: 0.3442040681838989\n",
      "Epoch [7771/10000], Loss: 0.3441876769065857\n",
      "Epoch [7772/10000], Loss: 0.3441712260246277\n",
      "Epoch [7773/10000], Loss: 0.34415483474731445\n",
      "Epoch [7774/10000], Loss: 0.344138503074646\n",
      "Epoch [7775/10000], Loss: 0.34412211179733276\n",
      "Epoch [7776/10000], Loss: 0.3441057801246643\n",
      "Epoch [7777/10000], Loss: 0.3440893292427063\n",
      "Epoch [7778/10000], Loss: 0.34407299757003784\n",
      "Epoch [7779/10000], Loss: 0.3440566658973694\n",
      "Epoch [7780/10000], Loss: 0.34404027462005615\n",
      "Epoch [7781/10000], Loss: 0.34402382373809814\n",
      "Epoch [7782/10000], Loss: 0.3440074920654297\n",
      "Epoch [7783/10000], Loss: 0.34399110078811646\n",
      "Epoch [7784/10000], Loss: 0.34397464990615845\n",
      "Epoch [7785/10000], Loss: 0.34395831823349\n",
      "Epoch [7786/10000], Loss: 0.343941867351532\n",
      "Epoch [7787/10000], Loss: 0.34392547607421875\n",
      "Epoch [7788/10000], Loss: 0.3439091444015503\n",
      "Epoch [7789/10000], Loss: 0.34389275312423706\n",
      "Epoch [7790/10000], Loss: 0.34387630224227905\n",
      "Epoch [7791/10000], Loss: 0.3438599705696106\n",
      "Epoch [7792/10000], Loss: 0.3438435196876526\n",
      "Epoch [7793/10000], Loss: 0.34382718801498413\n",
      "Epoch [7794/10000], Loss: 0.3438107371330261\n",
      "Epoch [7795/10000], Loss: 0.3437943458557129\n",
      "Epoch [7796/10000], Loss: 0.34377801418304443\n",
      "Epoch [7797/10000], Loss: 0.3437616229057312\n",
      "Epoch [7798/10000], Loss: 0.3437451720237732\n",
      "Epoch [7799/10000], Loss: 0.34372884035110474\n",
      "Epoch [7800/10000], Loss: 0.3437124490737915\n",
      "Epoch [7801/10000], Loss: 0.3436959981918335\n",
      "Epoch [7802/10000], Loss: 0.34367960691452026\n",
      "Epoch [7803/10000], Loss: 0.3436632752418518\n",
      "Epoch [7804/10000], Loss: 0.3436468243598938\n",
      "Epoch [7805/10000], Loss: 0.34363049268722534\n",
      "Epoch [7806/10000], Loss: 0.3436141014099121\n",
      "Epoch [7807/10000], Loss: 0.3435977101325989\n",
      "Epoch [7808/10000], Loss: 0.3435811996459961\n",
      "Epoch [7809/10000], Loss: 0.3435649275779724\n",
      "Epoch [7810/10000], Loss: 0.34354859590530396\n",
      "Epoch [7811/10000], Loss: 0.34353214502334595\n",
      "Epoch [7812/10000], Loss: 0.3435157537460327\n",
      "Epoch [7813/10000], Loss: 0.34349948167800903\n",
      "Epoch [7814/10000], Loss: 0.343483030796051\n",
      "Epoch [7815/10000], Loss: 0.3434666395187378\n",
      "Epoch [7816/10000], Loss: 0.34345024824142456\n",
      "Epoch [7817/10000], Loss: 0.3434339165687561\n",
      "Epoch [7818/10000], Loss: 0.3434174656867981\n",
      "Epoch [7819/10000], Loss: 0.34340113401412964\n",
      "Epoch [7820/10000], Loss: 0.3433847427368164\n",
      "Epoch [7821/10000], Loss: 0.34336841106414795\n",
      "Epoch [7822/10000], Loss: 0.3433520197868347\n",
      "Epoch [7823/10000], Loss: 0.3433356285095215\n",
      "Epoch [7824/10000], Loss: 0.3433193564414978\n",
      "Epoch [7825/10000], Loss: 0.34330302476882935\n",
      "Epoch [7826/10000], Loss: 0.34328651428222656\n",
      "Epoch [7827/10000], Loss: 0.3432702422142029\n",
      "Epoch [7828/10000], Loss: 0.3432537913322449\n",
      "Epoch [7829/10000], Loss: 0.3432375192642212\n",
      "Epoch [7830/10000], Loss: 0.34322112798690796\n",
      "Epoch [7831/10000], Loss: 0.3432047963142395\n",
      "Epoch [7832/10000], Loss: 0.34318846464157104\n",
      "Epoch [7833/10000], Loss: 0.3431721329689026\n",
      "Epoch [7834/10000], Loss: 0.34315580129623413\n",
      "Epoch [7835/10000], Loss: 0.3431394696235657\n",
      "Epoch [7836/10000], Loss: 0.34312307834625244\n",
      "Epoch [7837/10000], Loss: 0.34310680627822876\n",
      "Epoch [7838/10000], Loss: 0.3430904746055603\n",
      "Epoch [7839/10000], Loss: 0.3430742025375366\n",
      "Epoch [7840/10000], Loss: 0.3430578112602234\n",
      "Epoch [7841/10000], Loss: 0.3430415391921997\n",
      "Epoch [7842/10000], Loss: 0.3430251479148865\n",
      "Epoch [7843/10000], Loss: 0.3430088758468628\n",
      "Epoch [7844/10000], Loss: 0.3429926037788391\n",
      "Epoch [7845/10000], Loss: 0.3429762125015259\n",
      "Epoch [7846/10000], Loss: 0.342960000038147\n",
      "Epoch [7847/10000], Loss: 0.3429436683654785\n",
      "Epoch [7848/10000], Loss: 0.34292733669281006\n",
      "Epoch [7849/10000], Loss: 0.3429110646247864\n",
      "Epoch [7850/10000], Loss: 0.3428947329521179\n",
      "Epoch [7851/10000], Loss: 0.34287846088409424\n",
      "Epoch [7852/10000], Loss: 0.3428621292114258\n",
      "Epoch [7853/10000], Loss: 0.3428459167480469\n",
      "Epoch [7854/10000], Loss: 0.3428296446800232\n",
      "Epoch [7855/10000], Loss: 0.3428134322166443\n",
      "Epoch [7856/10000], Loss: 0.34279710054397583\n",
      "Epoch [7857/10000], Loss: 0.3427807688713074\n",
      "Epoch [7858/10000], Loss: 0.342764675617218\n",
      "Epoch [7859/10000], Loss: 0.3427482843399048\n",
      "Epoch [7860/10000], Loss: 0.3427320718765259\n",
      "Epoch [7861/10000], Loss: 0.3427157998085022\n",
      "Epoch [7862/10000], Loss: 0.3426995873451233\n",
      "Epoch [7863/10000], Loss: 0.3426833748817444\n",
      "Epoch [7864/10000], Loss: 0.3426671624183655\n",
      "Epoch [7865/10000], Loss: 0.3426508903503418\n",
      "Epoch [7866/10000], Loss: 0.3426346778869629\n",
      "Epoch [7867/10000], Loss: 0.3426184058189392\n",
      "Epoch [7868/10000], Loss: 0.3426021933555603\n",
      "Epoch [7869/10000], Loss: 0.34258604049682617\n",
      "Epoch [7870/10000], Loss: 0.3425697684288025\n",
      "Epoch [7871/10000], Loss: 0.34255361557006836\n",
      "Epoch [7872/10000], Loss: 0.3425373435020447\n",
      "Epoch [7873/10000], Loss: 0.34252113103866577\n",
      "Epoch [7874/10000], Loss: 0.34250497817993164\n",
      "Epoch [7875/10000], Loss: 0.34248876571655273\n",
      "Epoch [7876/10000], Loss: 0.3424726128578186\n",
      "Epoch [7877/10000], Loss: 0.3424564003944397\n",
      "Epoch [7878/10000], Loss: 0.3424401879310608\n",
      "Epoch [7879/10000], Loss: 0.3424239754676819\n",
      "Epoch [7880/10000], Loss: 0.34240782260894775\n",
      "Epoch [7881/10000], Loss: 0.3423917889595032\n",
      "Epoch [7882/10000], Loss: 0.34237557649612427\n",
      "Epoch [7883/10000], Loss: 0.34235942363739014\n",
      "Epoch [7884/10000], Loss: 0.342343270778656\n",
      "Epoch [7885/10000], Loss: 0.3423271179199219\n",
      "Epoch [7886/10000], Loss: 0.34231096506118774\n",
      "Epoch [7887/10000], Loss: 0.3422948122024536\n",
      "Epoch [7888/10000], Loss: 0.3422786593437195\n",
      "Epoch [7889/10000], Loss: 0.3422625660896301\n",
      "Epoch [7890/10000], Loss: 0.34224653244018555\n",
      "Epoch [7891/10000], Loss: 0.3422303795814514\n",
      "Epoch [7892/10000], Loss: 0.34221428632736206\n",
      "Epoch [7893/10000], Loss: 0.3421981930732727\n",
      "Epoch [7894/10000], Loss: 0.3421820402145386\n",
      "Epoch [7895/10000], Loss: 0.34216606616973877\n",
      "Epoch [7896/10000], Loss: 0.34214991331100464\n",
      "Epoch [7897/10000], Loss: 0.34213393926620483\n",
      "Epoch [7898/10000], Loss: 0.3421177268028259\n",
      "Epoch [7899/10000], Loss: 0.3421017527580261\n",
      "Epoch [7900/10000], Loss: 0.34208571910858154\n",
      "Epoch [7901/10000], Loss: 0.3420696258544922\n",
      "Epoch [7902/10000], Loss: 0.3420535922050476\n",
      "Epoch [7903/10000], Loss: 0.3420376181602478\n",
      "Epoch [7904/10000], Loss: 0.34202152490615845\n",
      "Epoch [7905/10000], Loss: 0.34200555086135864\n",
      "Epoch [7906/10000], Loss: 0.34198951721191406\n",
      "Epoch [7907/10000], Loss: 0.3419734239578247\n",
      "Epoch [7908/10000], Loss: 0.3419575095176697\n",
      "Epoch [7909/10000], Loss: 0.3419414758682251\n",
      "Epoch [7910/10000], Loss: 0.34192556142807007\n",
      "Epoch [7911/10000], Loss: 0.3419094681739807\n",
      "Epoch [7912/10000], Loss: 0.34189361333847046\n",
      "Epoch [7913/10000], Loss: 0.3418775796890259\n",
      "Epoch [7914/10000], Loss: 0.34186166524887085\n",
      "Epoch [7915/10000], Loss: 0.34184569120407104\n",
      "Epoch [7916/10000], Loss: 0.34182971715927124\n",
      "Epoch [7917/10000], Loss: 0.34181374311447144\n",
      "Epoch [7918/10000], Loss: 0.34179776906967163\n",
      "Epoch [7919/10000], Loss: 0.3417817950248718\n",
      "Epoch [7920/10000], Loss: 0.3417658805847168\n",
      "Epoch [7921/10000], Loss: 0.34175002574920654\n",
      "Epoch [7922/10000], Loss: 0.3417341113090515\n",
      "Epoch [7923/10000], Loss: 0.34171825647354126\n",
      "Epoch [7924/10000], Loss: 0.34170234203338623\n",
      "Epoch [7925/10000], Loss: 0.3416864275932312\n",
      "Epoch [7926/10000], Loss: 0.34167051315307617\n",
      "Epoch [7927/10000], Loss: 0.3416547179222107\n",
      "Epoch [7928/10000], Loss: 0.34163886308670044\n",
      "Epoch [7929/10000], Loss: 0.34162288904190063\n",
      "Epoch [7930/10000], Loss: 0.3416070342063904\n",
      "Epoch [7931/10000], Loss: 0.3415911793708801\n",
      "Epoch [7932/10000], Loss: 0.3415753245353699\n",
      "Epoch [7933/10000], Loss: 0.3415594696998596\n",
      "Epoch [7934/10000], Loss: 0.34154367446899414\n",
      "Epoch [7935/10000], Loss: 0.3415278196334839\n",
      "Epoch [7936/10000], Loss: 0.3415120244026184\n",
      "Epoch [7937/10000], Loss: 0.34149616956710815\n",
      "Epoch [7938/10000], Loss: 0.3414803743362427\n",
      "Epoch [7939/10000], Loss: 0.3414645195007324\n",
      "Epoch [7940/10000], Loss: 0.34144872426986694\n",
      "Epoch [7941/10000], Loss: 0.34143298864364624\n",
      "Epoch [7942/10000], Loss: 0.34141719341278076\n",
      "Epoch [7943/10000], Loss: 0.3414013981819153\n",
      "Epoch [7944/10000], Loss: 0.3413856625556946\n",
      "Epoch [7945/10000], Loss: 0.3413699269294739\n",
      "Epoch [7946/10000], Loss: 0.34135401248931885\n",
      "Epoch [7947/10000], Loss: 0.34133827686309814\n",
      "Epoch [7948/10000], Loss: 0.34132254123687744\n",
      "Epoch [7949/10000], Loss: 0.34130680561065674\n",
      "Epoch [7950/10000], Loss: 0.34129101037979126\n",
      "Epoch [7951/10000], Loss: 0.34127533435821533\n",
      "Epoch [7952/10000], Loss: 0.34125953912734985\n",
      "Epoch [7953/10000], Loss: 0.3412438631057739\n",
      "Epoch [7954/10000], Loss: 0.3412281274795532\n",
      "Epoch [7955/10000], Loss: 0.3412124514579773\n",
      "Epoch [7956/10000], Loss: 0.34119683504104614\n",
      "Epoch [7957/10000], Loss: 0.34118109941482544\n",
      "Epoch [7958/10000], Loss: 0.3411654233932495\n",
      "Epoch [7959/10000], Loss: 0.3411496877670288\n",
      "Epoch [7960/10000], Loss: 0.3411339521408081\n",
      "Epoch [7961/10000], Loss: 0.34111833572387695\n",
      "Epoch [7962/10000], Loss: 0.341102659702301\n",
      "Epoch [7963/10000], Loss: 0.3410869240760803\n",
      "Epoch [7964/10000], Loss: 0.34107130765914917\n",
      "Epoch [7965/10000], Loss: 0.34105557203292847\n",
      "Epoch [7966/10000], Loss: 0.34104007482528687\n",
      "Epoch [7967/10000], Loss: 0.34102433919906616\n",
      "Epoch [7968/10000], Loss: 0.3410087823867798\n",
      "Epoch [7969/10000], Loss: 0.3409930467605591\n",
      "Epoch [7970/10000], Loss: 0.34097737073898315\n",
      "Epoch [7971/10000], Loss: 0.34096187353134155\n",
      "Epoch [7972/10000], Loss: 0.3409461975097656\n",
      "Epoch [7973/10000], Loss: 0.3409305810928345\n",
      "Epoch [7974/10000], Loss: 0.3409150242805481\n",
      "Epoch [7975/10000], Loss: 0.34089934825897217\n",
      "Epoch [7976/10000], Loss: 0.3408837914466858\n",
      "Epoch [7977/10000], Loss: 0.34086811542510986\n",
      "Epoch [7978/10000], Loss: 0.3408525586128235\n",
      "Epoch [7979/10000], Loss: 0.3408370018005371\n",
      "Epoch [7980/10000], Loss: 0.3408213257789612\n",
      "Epoch [7981/10000], Loss: 0.34080588817596436\n",
      "Epoch [7982/10000], Loss: 0.3407902121543884\n",
      "Epoch [7983/10000], Loss: 0.34077465534210205\n",
      "Epoch [7984/10000], Loss: 0.34075915813446045\n",
      "Epoch [7985/10000], Loss: 0.3407435417175293\n",
      "Epoch [7986/10000], Loss: 0.34072792530059814\n",
      "Epoch [7987/10000], Loss: 0.3407124876976013\n",
      "Epoch [7988/10000], Loss: 0.34069687128067017\n",
      "Epoch [7989/10000], Loss: 0.340681254863739\n",
      "Epoch [7990/10000], Loss: 0.3406657576560974\n",
      "Epoch [7991/10000], Loss: 0.3406502604484558\n",
      "Epoch [7992/10000], Loss: 0.3406347632408142\n",
      "Epoch [7993/10000], Loss: 0.34061914682388306\n",
      "Epoch [7994/10000], Loss: 0.34060364961624146\n",
      "Epoch [7995/10000], Loss: 0.3405880928039551\n",
      "Epoch [7996/10000], Loss: 0.34057265520095825\n",
      "Epoch [7997/10000], Loss: 0.34055715799331665\n",
      "Epoch [7998/10000], Loss: 0.3405415415763855\n",
      "Epoch [7999/10000], Loss: 0.3405260443687439\n",
      "Epoch [8000/10000], Loss: 0.3405104875564575\n",
      "Epoch [8001/10000], Loss: 0.3404950499534607\n",
      "Epoch [8002/10000], Loss: 0.3404794931411743\n",
      "Epoch [8003/10000], Loss: 0.34046411514282227\n",
      "Epoch [8004/10000], Loss: 0.3404485583305359\n",
      "Epoch [8005/10000], Loss: 0.34043312072753906\n",
      "Epoch [8006/10000], Loss: 0.3404175639152527\n",
      "Epoch [8007/10000], Loss: 0.34040212631225586\n",
      "Epoch [8008/10000], Loss: 0.3403865694999695\n",
      "Epoch [8009/10000], Loss: 0.3403710722923279\n",
      "Epoch [8010/10000], Loss: 0.3403555750846863\n",
      "Epoch [8011/10000], Loss: 0.34034013748168945\n",
      "Epoch [8012/10000], Loss: 0.3403246998786926\n",
      "Epoch [8013/10000], Loss: 0.3403092622756958\n",
      "Epoch [8014/10000], Loss: 0.3402937650680542\n",
      "Epoch [8015/10000], Loss: 0.3402782082557678\n",
      "Epoch [8016/10000], Loss: 0.340262770652771\n",
      "Epoch [8017/10000], Loss: 0.3402472734451294\n",
      "Epoch [8018/10000], Loss: 0.3402317762374878\n",
      "Epoch [8019/10000], Loss: 0.34021633863449097\n",
      "Epoch [8020/10000], Loss: 0.34020090103149414\n",
      "Epoch [8021/10000], Loss: 0.3401854634284973\n",
      "Epoch [8022/10000], Loss: 0.3401699662208557\n",
      "Epoch [8023/10000], Loss: 0.3401545286178589\n",
      "Epoch [8024/10000], Loss: 0.3401390314102173\n",
      "Epoch [8025/10000], Loss: 0.34012359380722046\n",
      "Epoch [8026/10000], Loss: 0.34010809659957886\n",
      "Epoch [8027/10000], Loss: 0.34009265899658203\n",
      "Epoch [8028/10000], Loss: 0.34007728099823\n",
      "Epoch [8029/10000], Loss: 0.3400617241859436\n",
      "Epoch [8030/10000], Loss: 0.3400462865829468\n",
      "Epoch [8031/10000], Loss: 0.34003084897994995\n",
      "Epoch [8032/10000], Loss: 0.3400154709815979\n",
      "Epoch [8033/10000], Loss: 0.3399999737739563\n",
      "Epoch [8034/10000], Loss: 0.33998459577560425\n",
      "Epoch [8035/10000], Loss: 0.33996909856796265\n",
      "Epoch [8036/10000], Loss: 0.33995360136032104\n",
      "Epoch [8037/10000], Loss: 0.3399381637573242\n",
      "Epoch [8038/10000], Loss: 0.33992278575897217\n",
      "Epoch [8039/10000], Loss: 0.33990728855133057\n",
      "Epoch [8040/10000], Loss: 0.33989185094833374\n",
      "Epoch [8041/10000], Loss: 0.33987653255462646\n",
      "Epoch [8042/10000], Loss: 0.3398609161376953\n",
      "Epoch [8043/10000], Loss: 0.33984553813934326\n",
      "Epoch [8044/10000], Loss: 0.33983004093170166\n",
      "Epoch [8045/10000], Loss: 0.33981454372406006\n",
      "Epoch [8046/10000], Loss: 0.33979910612106323\n",
      "Epoch [8047/10000], Loss: 0.3397836685180664\n",
      "Epoch [8048/10000], Loss: 0.33976829051971436\n",
      "Epoch [8049/10000], Loss: 0.339752733707428\n",
      "Epoch [8050/10000], Loss: 0.3397373557090759\n",
      "Epoch [8051/10000], Loss: 0.3397218585014343\n",
      "Epoch [8052/10000], Loss: 0.3397063612937927\n",
      "Epoch [8053/10000], Loss: 0.3396908640861511\n",
      "Epoch [8054/10000], Loss: 0.3396754860877991\n",
      "Epoch [8055/10000], Loss: 0.33965998888015747\n",
      "Epoch [8056/10000], Loss: 0.33964449167251587\n",
      "Epoch [8057/10000], Loss: 0.33962905406951904\n",
      "Epoch [8058/10000], Loss: 0.33961355686187744\n",
      "Epoch [8059/10000], Loss: 0.3395981192588806\n",
      "Epoch [8060/10000], Loss: 0.339582622051239\n",
      "Epoch [8061/10000], Loss: 0.3395671844482422\n",
      "Epoch [8062/10000], Loss: 0.3395516276359558\n",
      "Epoch [8063/10000], Loss: 0.339536190032959\n",
      "Epoch [8064/10000], Loss: 0.33952075242996216\n",
      "Epoch [8065/10000], Loss: 0.33950525522232056\n",
      "Epoch [8066/10000], Loss: 0.33948975801467896\n",
      "Epoch [8067/10000], Loss: 0.33947426080703735\n",
      "Epoch [8068/10000], Loss: 0.33945876359939575\n",
      "Epoch [8069/10000], Loss: 0.33944326639175415\n",
      "Epoch [8070/10000], Loss: 0.3394278287887573\n",
      "Epoch [8071/10000], Loss: 0.33941227197647095\n",
      "Epoch [8072/10000], Loss: 0.33939677476882935\n",
      "Epoch [8073/10000], Loss: 0.33938121795654297\n",
      "Epoch [8074/10000], Loss: 0.3393658399581909\n",
      "Epoch [8075/10000], Loss: 0.33935022354125977\n",
      "Epoch [8076/10000], Loss: 0.33933478593826294\n",
      "Epoch [8077/10000], Loss: 0.33931928873062134\n",
      "Epoch [8078/10000], Loss: 0.3393036127090454\n",
      "Epoch [8079/10000], Loss: 0.3392881155014038\n",
      "Epoch [8080/10000], Loss: 0.3392726182937622\n",
      "Epoch [8081/10000], Loss: 0.3392571210861206\n",
      "Epoch [8082/10000], Loss: 0.33924150466918945\n",
      "Epoch [8083/10000], Loss: 0.3392259478569031\n",
      "Epoch [8084/10000], Loss: 0.3392104506492615\n",
      "Epoch [8085/10000], Loss: 0.3391948342323303\n",
      "Epoch [8086/10000], Loss: 0.3391793370246887\n",
      "Epoch [8087/10000], Loss: 0.33916372060775757\n",
      "Epoch [8088/10000], Loss: 0.33914822340011597\n",
      "Epoch [8089/10000], Loss: 0.33913272619247437\n",
      "Epoch [8090/10000], Loss: 0.33911705017089844\n",
      "Epoch [8091/10000], Loss: 0.3391014337539673\n",
      "Epoch [8092/10000], Loss: 0.33908599615097046\n",
      "Epoch [8093/10000], Loss: 0.3390703797340393\n",
      "Epoch [8094/10000], Loss: 0.33905476331710815\n",
      "Epoch [8095/10000], Loss: 0.339039146900177\n",
      "Epoch [8096/10000], Loss: 0.3390235900878906\n",
      "Epoch [8097/10000], Loss: 0.3390079140663147\n",
      "Epoch [8098/10000], Loss: 0.33899229764938354\n",
      "Epoch [8099/10000], Loss: 0.33897674083709717\n",
      "Epoch [8100/10000], Loss: 0.33896106481552124\n",
      "Epoch [8101/10000], Loss: 0.3389454483985901\n",
      "Epoch [8102/10000], Loss: 0.33892977237701416\n",
      "Epoch [8103/10000], Loss: 0.3389142155647278\n",
      "Epoch [8104/10000], Loss: 0.33889859914779663\n",
      "Epoch [8105/10000], Loss: 0.3388828635215759\n",
      "Epoch [8106/10000], Loss: 0.3388671875\n",
      "Epoch [8107/10000], Loss: 0.3388515114784241\n",
      "Epoch [8108/10000], Loss: 0.3388358950614929\n",
      "Epoch [8109/10000], Loss: 0.33882027864456177\n",
      "Epoch [8110/10000], Loss: 0.33880460262298584\n",
      "Epoch [8111/10000], Loss: 0.33878880739212036\n",
      "Epoch [8112/10000], Loss: 0.3387731909751892\n",
      "Epoch [8113/10000], Loss: 0.3387574553489685\n",
      "Epoch [8114/10000], Loss: 0.3387417793273926\n",
      "Epoch [8115/10000], Loss: 0.33872610330581665\n",
      "Epoch [8116/10000], Loss: 0.33871030807495117\n",
      "Epoch [8117/10000], Loss: 0.33869457244873047\n",
      "Epoch [8118/10000], Loss: 0.3386789560317993\n",
      "Epoch [8119/10000], Loss: 0.33866316080093384\n",
      "Epoch [8120/10000], Loss: 0.33864742517471313\n",
      "Epoch [8121/10000], Loss: 0.33863162994384766\n",
      "Epoch [8122/10000], Loss: 0.33861595392227173\n",
      "Epoch [8123/10000], Loss: 0.33860015869140625\n",
      "Epoch [8124/10000], Loss: 0.33858442306518555\n",
      "Epoch [8125/10000], Loss: 0.33856868743896484\n",
      "Epoch [8126/10000], Loss: 0.33855295181274414\n",
      "Epoch [8127/10000], Loss: 0.33853715658187866\n",
      "Epoch [8128/10000], Loss: 0.3385213613510132\n",
      "Epoch [8129/10000], Loss: 0.3385055661201477\n",
      "Epoch [8130/10000], Loss: 0.33848971128463745\n",
      "Epoch [8131/10000], Loss: 0.338473916053772\n",
      "Epoch [8132/10000], Loss: 0.3384581208229065\n",
      "Epoch [8133/10000], Loss: 0.338442325592041\n",
      "Epoch [8134/10000], Loss: 0.33842647075653076\n",
      "Epoch [8135/10000], Loss: 0.3384106755256653\n",
      "Epoch [8136/10000], Loss: 0.33839482069015503\n",
      "Epoch [8137/10000], Loss: 0.33837902545928955\n",
      "Epoch [8138/10000], Loss: 0.3383631706237793\n",
      "Epoch [8139/10000], Loss: 0.33834731578826904\n",
      "Epoch [8140/10000], Loss: 0.33833152055740356\n",
      "Epoch [8141/10000], Loss: 0.33831560611724854\n",
      "Epoch [8142/10000], Loss: 0.3382997512817383\n",
      "Epoch [8143/10000], Loss: 0.338283896446228\n",
      "Epoch [8144/10000], Loss: 0.3382681608200073\n",
      "Epoch [8145/10000], Loss: 0.3382522463798523\n",
      "Epoch [8146/10000], Loss: 0.33823639154434204\n",
      "Epoch [8147/10000], Loss: 0.338220477104187\n",
      "Epoch [8148/10000], Loss: 0.338204562664032\n",
      "Epoch [8149/10000], Loss: 0.33818870782852173\n",
      "Epoch [8150/10000], Loss: 0.3381727933883667\n",
      "Epoch [8151/10000], Loss: 0.3381569981575012\n",
      "Epoch [8152/10000], Loss: 0.33814096450805664\n",
      "Epoch [8153/10000], Loss: 0.3381251096725464\n",
      "Epoch [8154/10000], Loss: 0.3381093144416809\n",
      "Epoch [8155/10000], Loss: 0.33809328079223633\n",
      "Epoch [8156/10000], Loss: 0.3380774259567261\n",
      "Epoch [8157/10000], Loss: 0.33806151151657104\n",
      "Epoch [8158/10000], Loss: 0.338045597076416\n",
      "Epoch [8159/10000], Loss: 0.338029682636261\n",
      "Epoch [8160/10000], Loss: 0.3380137085914612\n",
      "Epoch [8161/10000], Loss: 0.3379978537559509\n",
      "Epoch [8162/10000], Loss: 0.3379819989204407\n",
      "Epoch [8163/10000], Loss: 0.3379659652709961\n",
      "Epoch [8164/10000], Loss: 0.33795011043548584\n",
      "Epoch [8165/10000], Loss: 0.33793413639068604\n",
      "Epoch [8166/10000], Loss: 0.3379182815551758\n",
      "Epoch [8167/10000], Loss: 0.3379022479057312\n",
      "Epoch [8168/10000], Loss: 0.33788633346557617\n",
      "Epoch [8169/10000], Loss: 0.3378702998161316\n",
      "Epoch [8170/10000], Loss: 0.3378545045852661\n",
      "Epoch [8171/10000], Loss: 0.3378385305404663\n",
      "Epoch [8172/10000], Loss: 0.33782249689102173\n",
      "Epoch [8173/10000], Loss: 0.33780670166015625\n",
      "Epoch [8174/10000], Loss: 0.3377906084060669\n",
      "Epoch [8175/10000], Loss: 0.33777475357055664\n",
      "Epoch [8176/10000], Loss: 0.3377588391304016\n",
      "Epoch [8177/10000], Loss: 0.3377429246902466\n",
      "Epoch [8178/10000], Loss: 0.337726891040802\n",
      "Epoch [8179/10000], Loss: 0.33771103620529175\n",
      "Epoch [8180/10000], Loss: 0.33769500255584717\n",
      "Epoch [8181/10000], Loss: 0.33767902851104736\n",
      "Epoch [8182/10000], Loss: 0.33766311407089233\n",
      "Epoch [8183/10000], Loss: 0.3376472592353821\n",
      "Epoch [8184/10000], Loss: 0.3376312255859375\n",
      "Epoch [8185/10000], Loss: 0.33761531114578247\n",
      "Epoch [8186/10000], Loss: 0.33759933710098267\n",
      "Epoch [8187/10000], Loss: 0.33758342266082764\n",
      "Epoch [8188/10000], Loss: 0.3375675082206726\n",
      "Epoch [8189/10000], Loss: 0.3375515341758728\n",
      "Epoch [8190/10000], Loss: 0.33753567934036255\n",
      "Epoch [8191/10000], Loss: 0.33751964569091797\n",
      "Epoch [8192/10000], Loss: 0.3375038504600525\n",
      "Epoch [8193/10000], Loss: 0.3374878168106079\n",
      "Epoch [8194/10000], Loss: 0.33747196197509766\n",
      "Epoch [8195/10000], Loss: 0.3374561071395874\n",
      "Epoch [8196/10000], Loss: 0.3374400734901428\n",
      "Epoch [8197/10000], Loss: 0.33742427825927734\n",
      "Epoch [8198/10000], Loss: 0.3374084234237671\n",
      "Epoch [8199/10000], Loss: 0.33739250898361206\n",
      "Epoch [8200/10000], Loss: 0.33737659454345703\n",
      "Epoch [8201/10000], Loss: 0.3373607397079468\n",
      "Epoch [8202/10000], Loss: 0.33734482526779175\n",
      "Epoch [8203/10000], Loss: 0.33732903003692627\n",
      "Epoch [8204/10000], Loss: 0.33731311559677124\n",
      "Epoch [8205/10000], Loss: 0.33729732036590576\n",
      "Epoch [8206/10000], Loss: 0.3372815251350403\n",
      "Epoch [8207/10000], Loss: 0.33726561069488525\n",
      "Epoch [8208/10000], Loss: 0.33724987506866455\n",
      "Epoch [8209/10000], Loss: 0.3372340202331543\n",
      "Epoch [8210/10000], Loss: 0.3372182250022888\n",
      "Epoch [8211/10000], Loss: 0.33720242977142334\n",
      "Epoch [8212/10000], Loss: 0.33718669414520264\n",
      "Epoch [8213/10000], Loss: 0.33717089891433716\n",
      "Epoch [8214/10000], Loss: 0.3371550440788269\n",
      "Epoch [8215/10000], Loss: 0.3371393084526062\n",
      "Epoch [8216/10000], Loss: 0.33712345361709595\n",
      "Epoch [8217/10000], Loss: 0.33710771799087524\n",
      "Epoch [8218/10000], Loss: 0.33709198236465454\n",
      "Epoch [8219/10000], Loss: 0.3370763063430786\n",
      "Epoch [8220/10000], Loss: 0.3370605707168579\n",
      "Epoch [8221/10000], Loss: 0.337044894695282\n",
      "Epoch [8222/10000], Loss: 0.3370291590690613\n",
      "Epoch [8223/10000], Loss: 0.3370135426521301\n",
      "Epoch [8224/10000], Loss: 0.3369978666305542\n",
      "Epoch [8225/10000], Loss: 0.33698219060897827\n",
      "Epoch [8226/10000], Loss: 0.33696651458740234\n",
      "Epoch [8227/10000], Loss: 0.3369508385658264\n",
      "Epoch [8228/10000], Loss: 0.3369351625442505\n",
      "Epoch [8229/10000], Loss: 0.3369196653366089\n",
      "Epoch [8230/10000], Loss: 0.33690398931503296\n",
      "Epoch [8231/10000], Loss: 0.3368883728981018\n",
      "Epoch [8232/10000], Loss: 0.3368726968765259\n",
      "Epoch [8233/10000], Loss: 0.3368571996688843\n",
      "Epoch [8234/10000], Loss: 0.3368416428565979\n",
      "Epoch [8235/10000], Loss: 0.3368261456489563\n",
      "Epoch [8236/10000], Loss: 0.33681052923202515\n",
      "Epoch [8237/10000], Loss: 0.33679503202438354\n",
      "Epoch [8238/10000], Loss: 0.33677953481674194\n",
      "Epoch [8239/10000], Loss: 0.3367639183998108\n",
      "Epoch [8240/10000], Loss: 0.3367486000061035\n",
      "Epoch [8241/10000], Loss: 0.33673298358917236\n",
      "Epoch [8242/10000], Loss: 0.3367176055908203\n",
      "Epoch [8243/10000], Loss: 0.3367021679878235\n",
      "Epoch [8244/10000], Loss: 0.3366866707801819\n",
      "Epoch [8245/10000], Loss: 0.33667129278182983\n",
      "Epoch [8246/10000], Loss: 0.33665579557418823\n",
      "Epoch [8247/10000], Loss: 0.33664047718048096\n",
      "Epoch [8248/10000], Loss: 0.3366250991821289\n",
      "Epoch [8249/10000], Loss: 0.33660978078842163\n",
      "Epoch [8250/10000], Loss: 0.3365944027900696\n",
      "Epoch [8251/10000], Loss: 0.33657896518707275\n",
      "Epoch [8252/10000], Loss: 0.33656370639801025\n",
      "Epoch [8253/10000], Loss: 0.33654844760894775\n",
      "Epoch [8254/10000], Loss: 0.33653318881988525\n",
      "Epoch [8255/10000], Loss: 0.336517870426178\n",
      "Epoch [8256/10000], Loss: 0.3365025520324707\n",
      "Epoch [8257/10000], Loss: 0.336487352848053\n",
      "Epoch [8258/10000], Loss: 0.33647215366363525\n",
      "Epoch [8259/10000], Loss: 0.33645689487457275\n",
      "Epoch [8260/10000], Loss: 0.33644169569015503\n",
      "Epoch [8261/10000], Loss: 0.33642643690109253\n",
      "Epoch [8262/10000], Loss: 0.33641135692596436\n",
      "Epoch [8263/10000], Loss: 0.3363962173461914\n",
      "Epoch [8264/10000], Loss: 0.33638107776641846\n",
      "Epoch [8265/10000], Loss: 0.33636587858200073\n",
      "Epoch [8266/10000], Loss: 0.33635079860687256\n",
      "Epoch [8267/10000], Loss: 0.3363357186317444\n",
      "Epoch [8268/10000], Loss: 0.33632075786590576\n",
      "Epoch [8269/10000], Loss: 0.33630555868148804\n",
      "Epoch [8270/10000], Loss: 0.3362906575202942\n",
      "Epoch [8271/10000], Loss: 0.336275577545166\n",
      "Epoch [8272/10000], Loss: 0.33626049757003784\n",
      "Epoch [8273/10000], Loss: 0.336245596408844\n",
      "Epoch [8274/10000], Loss: 0.3362305760383606\n",
      "Epoch [8275/10000], Loss: 0.3362155556678772\n",
      "Epoch [8276/10000], Loss: 0.33620065450668335\n",
      "Epoch [8277/10000], Loss: 0.3361858129501343\n",
      "Epoch [8278/10000], Loss: 0.3361707925796509\n",
      "Epoch [8279/10000], Loss: 0.33615589141845703\n",
      "Epoch [8280/10000], Loss: 0.33614104986190796\n",
      "Epoch [8281/10000], Loss: 0.3361262083053589\n",
      "Epoch [8282/10000], Loss: 0.3361113667488098\n",
      "Epoch [8283/10000], Loss: 0.33609646558761597\n",
      "Epoch [8284/10000], Loss: 0.3360816240310669\n",
      "Epoch [8285/10000], Loss: 0.3360669016838074\n",
      "Epoch [8286/10000], Loss: 0.3360520005226135\n",
      "Epoch [8287/10000], Loss: 0.336037278175354\n",
      "Epoch [8288/10000], Loss: 0.3360225558280945\n",
      "Epoch [8289/10000], Loss: 0.3360077738761902\n",
      "Epoch [8290/10000], Loss: 0.33599311113357544\n",
      "Epoch [8291/10000], Loss: 0.3359783887863159\n",
      "Epoch [8292/10000], Loss: 0.3359636664390564\n",
      "Epoch [8293/10000], Loss: 0.3359489440917969\n",
      "Epoch [8294/10000], Loss: 0.33593428134918213\n",
      "Epoch [8295/10000], Loss: 0.33591967821121216\n",
      "Epoch [8296/10000], Loss: 0.3359050154685974\n",
      "Epoch [8297/10000], Loss: 0.33589041233062744\n",
      "Epoch [8298/10000], Loss: 0.33587580919265747\n",
      "Epoch [8299/10000], Loss: 0.3358612060546875\n",
      "Epoch [8300/10000], Loss: 0.33584654331207275\n",
      "Epoch [8301/10000], Loss: 0.33583205938339233\n",
      "Epoch [8302/10000], Loss: 0.33581751585006714\n",
      "Epoch [8303/10000], Loss: 0.33580297231674194\n",
      "Epoch [8304/10000], Loss: 0.3357885479927063\n",
      "Epoch [8305/10000], Loss: 0.33577394485473633\n",
      "Epoch [8306/10000], Loss: 0.33575958013534546\n",
      "Epoch [8307/10000], Loss: 0.33574503660202026\n",
      "Epoch [8308/10000], Loss: 0.3357306122779846\n",
      "Epoch [8309/10000], Loss: 0.3357161283493042\n",
      "Epoch [8310/10000], Loss: 0.33570176362991333\n",
      "Epoch [8311/10000], Loss: 0.3356872797012329\n",
      "Epoch [8312/10000], Loss: 0.33567291498184204\n",
      "Epoch [8313/10000], Loss: 0.33565855026245117\n",
      "Epoch [8314/10000], Loss: 0.3356442451477051\n",
      "Epoch [8315/10000], Loss: 0.3356298804283142\n",
      "Epoch [8316/10000], Loss: 0.3356156349182129\n",
      "Epoch [8317/10000], Loss: 0.335601270198822\n",
      "Epoch [8318/10000], Loss: 0.3355869650840759\n",
      "Epoch [8319/10000], Loss: 0.33557265996932983\n",
      "Epoch [8320/10000], Loss: 0.3355584740638733\n",
      "Epoch [8321/10000], Loss: 0.3355441689491272\n",
      "Epoch [8322/10000], Loss: 0.3355298638343811\n",
      "Epoch [8323/10000], Loss: 0.33551567792892456\n",
      "Epoch [8324/10000], Loss: 0.335501492023468\n",
      "Epoch [8325/10000], Loss: 0.3354873061180115\n",
      "Epoch [8326/10000], Loss: 0.33547312021255493\n",
      "Epoch [8327/10000], Loss: 0.3354589343070984\n",
      "Epoch [8328/10000], Loss: 0.33544474840164185\n",
      "Epoch [8329/10000], Loss: 0.3354306221008301\n",
      "Epoch [8330/10000], Loss: 0.3354164958000183\n",
      "Epoch [8331/10000], Loss: 0.3354024291038513\n",
      "Epoch [8332/10000], Loss: 0.33538830280303955\n",
      "Epoch [8333/10000], Loss: 0.33537423610687256\n",
      "Epoch [8334/10000], Loss: 0.3353601098060608\n",
      "Epoch [8335/10000], Loss: 0.3353460431098938\n",
      "Epoch [8336/10000], Loss: 0.33533191680908203\n",
      "Epoch [8337/10000], Loss: 0.3353179097175598\n",
      "Epoch [8338/10000], Loss: 0.3353039622306824\n",
      "Epoch [8339/10000], Loss: 0.33528995513916016\n",
      "Epoch [8340/10000], Loss: 0.33527594804763794\n",
      "Epoch [8341/10000], Loss: 0.3352619409561157\n",
      "Epoch [8342/10000], Loss: 0.3352479934692383\n",
      "Epoch [8343/10000], Loss: 0.33523404598236084\n",
      "Epoch [8344/10000], Loss: 0.3352200388908386\n",
      "Epoch [8345/10000], Loss: 0.3352060914039612\n",
      "Epoch [8346/10000], Loss: 0.3351922035217285\n",
      "Epoch [8347/10000], Loss: 0.33517831563949585\n",
      "Epoch [8348/10000], Loss: 0.3351644277572632\n",
      "Epoch [8349/10000], Loss: 0.3351505994796753\n",
      "Epoch [8350/10000], Loss: 0.3351365923881531\n",
      "Epoch [8351/10000], Loss: 0.3351227641105652\n",
      "Epoch [8352/10000], Loss: 0.3351089358329773\n",
      "Epoch [8353/10000], Loss: 0.33509522676467896\n",
      "Epoch [8354/10000], Loss: 0.33508139848709106\n",
      "Epoch [8355/10000], Loss: 0.33506762981414795\n",
      "Epoch [8356/10000], Loss: 0.3350537419319153\n",
      "Epoch [8357/10000], Loss: 0.33504003286361694\n",
      "Epoch [8358/10000], Loss: 0.33502626419067383\n",
      "Epoch [8359/10000], Loss: 0.3350124955177307\n",
      "Epoch [8360/10000], Loss: 0.3349987268447876\n",
      "Epoch [8361/10000], Loss: 0.3349849581718445\n",
      "Epoch [8362/10000], Loss: 0.3349713683128357\n",
      "Epoch [8363/10000], Loss: 0.3349575996398926\n",
      "Epoch [8364/10000], Loss: 0.33494389057159424\n",
      "Epoch [8365/10000], Loss: 0.3349301218986511\n",
      "Epoch [8366/10000], Loss: 0.33491653203964233\n",
      "Epoch [8367/10000], Loss: 0.33490288257598877\n",
      "Epoch [8368/10000], Loss: 0.33488929271698\n",
      "Epoch [8369/10000], Loss: 0.33487552404403687\n",
      "Epoch [8370/10000], Loss: 0.3348619341850281\n",
      "Epoch [8371/10000], Loss: 0.33484840393066406\n",
      "Epoch [8372/10000], Loss: 0.3348346948623657\n",
      "Epoch [8373/10000], Loss: 0.3348211646080017\n",
      "Epoch [8374/10000], Loss: 0.33480751514434814\n",
      "Epoch [8375/10000], Loss: 0.3347940444946289\n",
      "Epoch [8376/10000], Loss: 0.33478039503097534\n",
      "Epoch [8377/10000], Loss: 0.3347669243812561\n",
      "Epoch [8378/10000], Loss: 0.3347533941268921\n",
      "Epoch [8379/10000], Loss: 0.3347398042678833\n",
      "Epoch [8380/10000], Loss: 0.33472633361816406\n",
      "Epoch [8381/10000], Loss: 0.3347129225730896\n",
      "Epoch [8382/10000], Loss: 0.3346993327140808\n",
      "Epoch [8383/10000], Loss: 0.3346858620643616\n",
      "Epoch [8384/10000], Loss: 0.3346724510192871\n",
      "Epoch [8385/10000], Loss: 0.33465898036956787\n",
      "Epoch [8386/10000], Loss: 0.33464545011520386\n",
      "Epoch [8387/10000], Loss: 0.33463209867477417\n",
      "Epoch [8388/10000], Loss: 0.3346186876296997\n",
      "Epoch [8389/10000], Loss: 0.33460527658462524\n",
      "Epoch [8390/10000], Loss: 0.3345918655395508\n",
      "Epoch [8391/10000], Loss: 0.3345784544944763\n",
      "Epoch [8392/10000], Loss: 0.3345651626586914\n",
      "Epoch [8393/10000], Loss: 0.33455175161361694\n",
      "Epoch [8394/10000], Loss: 0.33453840017318726\n",
      "Epoch [8395/10000], Loss: 0.33452504873275757\n",
      "Epoch [8396/10000], Loss: 0.33451175689697266\n",
      "Epoch [8397/10000], Loss: 0.33449840545654297\n",
      "Epoch [8398/10000], Loss: 0.33448511362075806\n",
      "Epoch [8399/10000], Loss: 0.33447182178497314\n",
      "Epoch [8400/10000], Loss: 0.3344584107398987\n",
      "Epoch [8401/10000], Loss: 0.33444517850875854\n",
      "Epoch [8402/10000], Loss: 0.3344320058822632\n",
      "Epoch [8403/10000], Loss: 0.3344186544418335\n",
      "Epoch [8404/10000], Loss: 0.3344053626060486\n",
      "Epoch [8405/10000], Loss: 0.33439213037490845\n",
      "Epoch [8406/10000], Loss: 0.33437901735305786\n",
      "Epoch [8407/10000], Loss: 0.33436572551727295\n",
      "Epoch [8408/10000], Loss: 0.3343524932861328\n",
      "Epoch [8409/10000], Loss: 0.33433932065963745\n",
      "Epoch [8410/10000], Loss: 0.3343260884284973\n",
      "Epoch [8411/10000], Loss: 0.33431291580200195\n",
      "Epoch [8412/10000], Loss: 0.3342996835708618\n",
      "Epoch [8413/10000], Loss: 0.33428651094436646\n",
      "Epoch [8414/10000], Loss: 0.33427339792251587\n",
      "Epoch [8415/10000], Loss: 0.3342602849006653\n",
      "Epoch [8416/10000], Loss: 0.3342471122741699\n",
      "Epoch [8417/10000], Loss: 0.33423393964767456\n",
      "Epoch [8418/10000], Loss: 0.33422088623046875\n",
      "Epoch [8419/10000], Loss: 0.3342077136039734\n",
      "Epoch [8420/10000], Loss: 0.3341946005821228\n",
      "Epoch [8421/10000], Loss: 0.3341814875602722\n",
      "Epoch [8422/10000], Loss: 0.3341684341430664\n",
      "Epoch [8423/10000], Loss: 0.3341553807258606\n",
      "Epoch [8424/10000], Loss: 0.33414226770401\n",
      "Epoch [8425/10000], Loss: 0.334129273891449\n",
      "Epoch [8426/10000], Loss: 0.3341161608695984\n",
      "Epoch [8427/10000], Loss: 0.3341031074523926\n",
      "Epoch [8428/10000], Loss: 0.33409011363983154\n",
      "Epoch [8429/10000], Loss: 0.33407706022262573\n",
      "Epoch [8430/10000], Loss: 0.3340641260147095\n",
      "Epoch [8431/10000], Loss: 0.33405107259750366\n",
      "Epoch [8432/10000], Loss: 0.3340381383895874\n",
      "Epoch [8433/10000], Loss: 0.3340250253677368\n",
      "Epoch [8434/10000], Loss: 0.3340120315551758\n",
      "Epoch [8435/10000], Loss: 0.33399903774261475\n",
      "Epoch [8436/10000], Loss: 0.3339861035346985\n",
      "Epoch [8437/10000], Loss: 0.33397310972213745\n",
      "Epoch [8438/10000], Loss: 0.3339601755142212\n",
      "Epoch [8439/10000], Loss: 0.3339473009109497\n",
      "Epoch [8440/10000], Loss: 0.33393436670303345\n",
      "Epoch [8441/10000], Loss: 0.3339214324951172\n",
      "Epoch [8442/10000], Loss: 0.3339084982872009\n",
      "Epoch [8443/10000], Loss: 0.33389562368392944\n",
      "Epoch [8444/10000], Loss: 0.33388274908065796\n",
      "Epoch [8445/10000], Loss: 0.3338698148727417\n",
      "Epoch [8446/10000], Loss: 0.333856999874115\n",
      "Epoch [8447/10000], Loss: 0.33384406566619873\n",
      "Epoch [8448/10000], Loss: 0.33383119106292725\n",
      "Epoch [8449/10000], Loss: 0.33381831645965576\n",
      "Epoch [8450/10000], Loss: 0.33380550146102905\n",
      "Epoch [8451/10000], Loss: 0.33379268646240234\n",
      "Epoch [8452/10000], Loss: 0.33377981185913086\n",
      "Epoch [8453/10000], Loss: 0.3337669372558594\n",
      "Epoch [8454/10000], Loss: 0.33375412225723267\n",
      "Epoch [8455/10000], Loss: 0.33374130725860596\n",
      "Epoch [8456/10000], Loss: 0.33372849225997925\n",
      "Epoch [8457/10000], Loss: 0.3337157368659973\n",
      "Epoch [8458/10000], Loss: 0.3337029814720154\n",
      "Epoch [8459/10000], Loss: 0.33369022607803345\n",
      "Epoch [8460/10000], Loss: 0.33367741107940674\n",
      "Epoch [8461/10000], Loss: 0.33366459608078003\n",
      "Epoch [8462/10000], Loss: 0.33365190029144287\n",
      "Epoch [8463/10000], Loss: 0.33363914489746094\n",
      "Epoch [8464/10000], Loss: 0.333626389503479\n",
      "Epoch [8465/10000], Loss: 0.3336135745048523\n",
      "Epoch [8466/10000], Loss: 0.3336009383201599\n",
      "Epoch [8467/10000], Loss: 0.33358824253082275\n",
      "Epoch [8468/10000], Loss: 0.33357542753219604\n",
      "Epoch [8469/10000], Loss: 0.33356279134750366\n",
      "Epoch [8470/10000], Loss: 0.3335500955581665\n",
      "Epoch [8471/10000], Loss: 0.33353739976882935\n",
      "Epoch [8472/10000], Loss: 0.33352476358413696\n",
      "Epoch [8473/10000], Loss: 0.33351194858551025\n",
      "Epoch [8474/10000], Loss: 0.33349937200546265\n",
      "Epoch [8475/10000], Loss: 0.3334866762161255\n",
      "Epoch [8476/10000], Loss: 0.3334740400314331\n",
      "Epoch [8477/10000], Loss: 0.33346134424209595\n",
      "Epoch [8478/10000], Loss: 0.33344876766204834\n",
      "Epoch [8479/10000], Loss: 0.33343613147735596\n",
      "Epoch [8480/10000], Loss: 0.33342355489730835\n",
      "Epoch [8481/10000], Loss: 0.3334108591079712\n",
      "Epoch [8482/10000], Loss: 0.3333982825279236\n",
      "Epoch [8483/10000], Loss: 0.33338576555252075\n",
      "Epoch [8484/10000], Loss: 0.3333730101585388\n",
      "Epoch [8485/10000], Loss: 0.333360493183136\n",
      "Epoch [8486/10000], Loss: 0.3333478569984436\n",
      "Epoch [8487/10000], Loss: 0.333335280418396\n",
      "Epoch [8488/10000], Loss: 0.3333227038383484\n",
      "Epoch [8489/10000], Loss: 0.33331018686294556\n",
      "Epoch [8490/10000], Loss: 0.33329761028289795\n",
      "Epoch [8491/10000], Loss: 0.3332851529121399\n",
      "Epoch [8492/10000], Loss: 0.3332725167274475\n",
      "Epoch [8493/10000], Loss: 0.3332599997520447\n",
      "Epoch [8494/10000], Loss: 0.33324748277664185\n",
      "Epoch [8495/10000], Loss: 0.33323490619659424\n",
      "Epoch [8496/10000], Loss: 0.3332224488258362\n",
      "Epoch [8497/10000], Loss: 0.33320993185043335\n",
      "Epoch [8498/10000], Loss: 0.3331974148750305\n",
      "Epoch [8499/10000], Loss: 0.3331848978996277\n",
      "Epoch [8500/10000], Loss: 0.33317244052886963\n",
      "Epoch [8501/10000], Loss: 0.3331599831581116\n",
      "Epoch [8502/10000], Loss: 0.33314746618270874\n",
      "Epoch [8503/10000], Loss: 0.3331350088119507\n",
      "Epoch [8504/10000], Loss: 0.3331226110458374\n",
      "Epoch [8505/10000], Loss: 0.33311015367507935\n",
      "Epoch [8506/10000], Loss: 0.33309757709503174\n",
      "Epoch [8507/10000], Loss: 0.33308517932891846\n",
      "Epoch [8508/10000], Loss: 0.3330726623535156\n",
      "Epoch [8509/10000], Loss: 0.33306026458740234\n",
      "Epoch [8510/10000], Loss: 0.3330478072166443\n",
      "Epoch [8511/10000], Loss: 0.3330354690551758\n",
      "Epoch [8512/10000], Loss: 0.3330230116844177\n",
      "Epoch [8513/10000], Loss: 0.33301061391830444\n",
      "Epoch [8514/10000], Loss: 0.33299821615219116\n",
      "Epoch [8515/10000], Loss: 0.3329858183860779\n",
      "Epoch [8516/10000], Loss: 0.3329734802246094\n",
      "Epoch [8517/10000], Loss: 0.3329610824584961\n",
      "Epoch [8518/10000], Loss: 0.3329486846923828\n",
      "Epoch [8519/10000], Loss: 0.3329363465309143\n",
      "Epoch [8520/10000], Loss: 0.332923948764801\n",
      "Epoch [8521/10000], Loss: 0.3329116106033325\n",
      "Epoch [8522/10000], Loss: 0.33289921283721924\n",
      "Epoch [8523/10000], Loss: 0.3328869938850403\n",
      "Epoch [8524/10000], Loss: 0.332874596118927\n",
      "Epoch [8525/10000], Loss: 0.33286231756210327\n",
      "Epoch [8526/10000], Loss: 0.33284997940063477\n",
      "Epoch [8527/10000], Loss: 0.3328375220298767\n",
      "Epoch [8528/10000], Loss: 0.33282536268234253\n",
      "Epoch [8529/10000], Loss: 0.33281296491622925\n",
      "Epoch [8530/10000], Loss: 0.3328006863594055\n",
      "Epoch [8531/10000], Loss: 0.3327884078025818\n",
      "Epoch [8532/10000], Loss: 0.3327760696411133\n",
      "Epoch [8533/10000], Loss: 0.3327638506889343\n",
      "Epoch [8534/10000], Loss: 0.3327515721321106\n",
      "Epoch [8535/10000], Loss: 0.33273929357528687\n",
      "Epoch [8536/10000], Loss: 0.33272701501846313\n",
      "Epoch [8537/10000], Loss: 0.3327147364616394\n",
      "Epoch [8538/10000], Loss: 0.3327024579048157\n",
      "Epoch [8539/10000], Loss: 0.3326902389526367\n",
      "Epoch [8540/10000], Loss: 0.332677960395813\n",
      "Epoch [8541/10000], Loss: 0.3326658010482788\n",
      "Epoch [8542/10000], Loss: 0.33265358209609985\n",
      "Epoch [8543/10000], Loss: 0.3326413035392761\n",
      "Epoch [8544/10000], Loss: 0.3326290249824524\n",
      "Epoch [8545/10000], Loss: 0.33261680603027344\n",
      "Epoch [8546/10000], Loss: 0.33260470628738403\n",
      "Epoch [8547/10000], Loss: 0.3325924277305603\n",
      "Epoch [8548/10000], Loss: 0.33258020877838135\n",
      "Epoch [8549/10000], Loss: 0.3325679898262024\n",
      "Epoch [8550/10000], Loss: 0.332555890083313\n",
      "Epoch [8551/10000], Loss: 0.33254367113113403\n",
      "Epoch [8552/10000], Loss: 0.33253151178359985\n",
      "Epoch [8553/10000], Loss: 0.33251941204071045\n",
      "Epoch [8554/10000], Loss: 0.3325071334838867\n",
      "Epoch [8555/10000], Loss: 0.3324950933456421\n",
      "Epoch [8556/10000], Loss: 0.33248287439346313\n",
      "Epoch [8557/10000], Loss: 0.33247071504592896\n",
      "Epoch [8558/10000], Loss: 0.3324585556983948\n",
      "Epoch [8559/10000], Loss: 0.33244645595550537\n",
      "Epoch [8560/10000], Loss: 0.33243435621261597\n",
      "Epoch [8561/10000], Loss: 0.3324221968650818\n",
      "Epoch [8562/10000], Loss: 0.3324100375175476\n",
      "Epoch [8563/10000], Loss: 0.3323979377746582\n",
      "Epoch [8564/10000], Loss: 0.3323858976364136\n",
      "Epoch [8565/10000], Loss: 0.3323737382888794\n",
      "Epoch [8566/10000], Loss: 0.33236163854599\n",
      "Epoch [8567/10000], Loss: 0.33234959840774536\n",
      "Epoch [8568/10000], Loss: 0.3323374390602112\n",
      "Epoch [8569/10000], Loss: 0.3323253393173218\n",
      "Epoch [8570/10000], Loss: 0.3323133587837219\n",
      "Epoch [8571/10000], Loss: 0.33230119943618774\n",
      "Epoch [8572/10000], Loss: 0.33228909969329834\n",
      "Epoch [8573/10000], Loss: 0.33227699995040894\n",
      "Epoch [8574/10000], Loss: 0.3322649598121643\n",
      "Epoch [8575/10000], Loss: 0.3322529196739197\n",
      "Epoch [8576/10000], Loss: 0.3322409391403198\n",
      "Epoch [8577/10000], Loss: 0.3322288393974304\n",
      "Epoch [8578/10000], Loss: 0.3322167992591858\n",
      "Epoch [8579/10000], Loss: 0.33220475912094116\n",
      "Epoch [8580/10000], Loss: 0.3321927785873413\n",
      "Epoch [8581/10000], Loss: 0.3321806788444519\n",
      "Epoch [8582/10000], Loss: 0.3321686387062073\n",
      "Epoch [8583/10000], Loss: 0.3321567177772522\n",
      "Epoch [8584/10000], Loss: 0.33214467763900757\n",
      "Epoch [8585/10000], Loss: 0.33213263750076294\n",
      "Epoch [8586/10000], Loss: 0.3321205973625183\n",
      "Epoch [8587/10000], Loss: 0.33210861682891846\n",
      "Epoch [8588/10000], Loss: 0.3320966958999634\n",
      "Epoch [8589/10000], Loss: 0.33208465576171875\n",
      "Epoch [8590/10000], Loss: 0.3320726752281189\n",
      "Epoch [8591/10000], Loss: 0.3320607542991638\n",
      "Epoch [8592/10000], Loss: 0.33204877376556396\n",
      "Epoch [8593/10000], Loss: 0.3320368528366089\n",
      "Epoch [8594/10000], Loss: 0.33202487230300903\n",
      "Epoch [8595/10000], Loss: 0.3320128321647644\n",
      "Epoch [8596/10000], Loss: 0.3320010304450989\n",
      "Epoch [8597/10000], Loss: 0.33198899030685425\n",
      "Epoch [8598/10000], Loss: 0.3319770097732544\n",
      "Epoch [8599/10000], Loss: 0.3319651484489441\n",
      "Epoch [8600/10000], Loss: 0.331953227519989\n",
      "Epoch [8601/10000], Loss: 0.33194130659103394\n",
      "Epoch [8602/10000], Loss: 0.33192938566207886\n",
      "Epoch [8603/10000], Loss: 0.331917405128479\n",
      "Epoch [8604/10000], Loss: 0.3319055438041687\n",
      "Epoch [8605/10000], Loss: 0.3318937420845032\n",
      "Epoch [8606/10000], Loss: 0.33188170194625854\n",
      "Epoch [8607/10000], Loss: 0.331869900226593\n",
      "Epoch [8608/10000], Loss: 0.33185797929763794\n",
      "Epoch [8609/10000], Loss: 0.33184611797332764\n",
      "Epoch [8610/10000], Loss: 0.33183419704437256\n",
      "Epoch [8611/10000], Loss: 0.33182233572006226\n",
      "Epoch [8612/10000], Loss: 0.3318104147911072\n",
      "Epoch [8613/10000], Loss: 0.3317985534667969\n",
      "Epoch [8614/10000], Loss: 0.33178675174713135\n",
      "Epoch [8615/10000], Loss: 0.33177483081817627\n",
      "Epoch [8616/10000], Loss: 0.33176296949386597\n",
      "Epoch [8617/10000], Loss: 0.33175116777420044\n",
      "Epoch [8618/10000], Loss: 0.33173924684524536\n",
      "Epoch [8619/10000], Loss: 0.33172744512557983\n",
      "Epoch [8620/10000], Loss: 0.33171558380126953\n",
      "Epoch [8621/10000], Loss: 0.33170372247695923\n",
      "Epoch [8622/10000], Loss: 0.3316918611526489\n",
      "Epoch [8623/10000], Loss: 0.3316801190376282\n",
      "Epoch [8624/10000], Loss: 0.33166825771331787\n",
      "Epoch [8625/10000], Loss: 0.33165645599365234\n",
      "Epoch [8626/10000], Loss: 0.3316446542739868\n",
      "Epoch [8627/10000], Loss: 0.3316327929496765\n",
      "Epoch [8628/10000], Loss: 0.33162105083465576\n",
      "Epoch [8629/10000], Loss: 0.33160924911499023\n",
      "Epoch [8630/10000], Loss: 0.33159732818603516\n",
      "Epoch [8631/10000], Loss: 0.3315855860710144\n",
      "Epoch [8632/10000], Loss: 0.3315739035606384\n",
      "Epoch [8633/10000], Loss: 0.3315620422363281\n",
      "Epoch [8634/10000], Loss: 0.3315503001213074\n",
      "Epoch [8635/10000], Loss: 0.33153849840164185\n",
      "Epoch [8636/10000], Loss: 0.33152681589126587\n",
      "Epoch [8637/10000], Loss: 0.33151501417160034\n",
      "Epoch [8638/10000], Loss: 0.3315032720565796\n",
      "Epoch [8639/10000], Loss: 0.33149147033691406\n",
      "Epoch [8640/10000], Loss: 0.3314797282218933\n",
      "Epoch [8641/10000], Loss: 0.33146804571151733\n",
      "Epoch [8642/10000], Loss: 0.33145618438720703\n",
      "Epoch [8643/10000], Loss: 0.3314444422721863\n",
      "Epoch [8644/10000], Loss: 0.3314327597618103\n",
      "Epoch [8645/10000], Loss: 0.33142101764678955\n",
      "Epoch [8646/10000], Loss: 0.33140939474105835\n",
      "Epoch [8647/10000], Loss: 0.3313975930213928\n",
      "Epoch [8648/10000], Loss: 0.33138591051101685\n",
      "Epoch [8649/10000], Loss: 0.3313741683959961\n",
      "Epoch [8650/10000], Loss: 0.33136236667633057\n",
      "Epoch [8651/10000], Loss: 0.33135074377059937\n",
      "Epoch [8652/10000], Loss: 0.3313390612602234\n",
      "Epoch [8653/10000], Loss: 0.3313273787498474\n",
      "Epoch [8654/10000], Loss: 0.33131563663482666\n",
      "Epoch [8655/10000], Loss: 0.3313039541244507\n",
      "Epoch [8656/10000], Loss: 0.3312922716140747\n",
      "Epoch [8657/10000], Loss: 0.33128058910369873\n",
      "Epoch [8658/10000], Loss: 0.33126890659332275\n",
      "Epoch [8659/10000], Loss: 0.33125728368759155\n",
      "Epoch [8660/10000], Loss: 0.3312455415725708\n",
      "Epoch [8661/10000], Loss: 0.3312339186668396\n",
      "Epoch [8662/10000], Loss: 0.3312223553657532\n",
      "Epoch [8663/10000], Loss: 0.3312106728553772\n",
      "Epoch [8664/10000], Loss: 0.3311989903450012\n",
      "Epoch [8665/10000], Loss: 0.33118736743927\n",
      "Epoch [8666/10000], Loss: 0.3311757445335388\n",
      "Epoch [8667/10000], Loss: 0.33116406202316284\n",
      "Epoch [8668/10000], Loss: 0.3311524987220764\n",
      "Epoch [8669/10000], Loss: 0.33114081621170044\n",
      "Epoch [8670/10000], Loss: 0.331129252910614\n",
      "Epoch [8671/10000], Loss: 0.3311176300048828\n",
      "Epoch [8672/10000], Loss: 0.3311060667037964\n",
      "Epoch [8673/10000], Loss: 0.3310943841934204\n",
      "Epoch [8674/10000], Loss: 0.331082820892334\n",
      "Epoch [8675/10000], Loss: 0.33107107877731323\n",
      "Epoch [8676/10000], Loss: 0.3310595154762268\n",
      "Epoch [8677/10000], Loss: 0.33104801177978516\n",
      "Epoch [8678/10000], Loss: 0.33103644847869873\n",
      "Epoch [8679/10000], Loss: 0.33102482557296753\n",
      "Epoch [8680/10000], Loss: 0.3310132622718811\n",
      "Epoch [8681/10000], Loss: 0.3310016989707947\n",
      "Epoch [8682/10000], Loss: 0.33099013566970825\n",
      "Epoch [8683/10000], Loss: 0.33097851276397705\n",
      "Epoch [8684/10000], Loss: 0.3309669494628906\n",
      "Epoch [8685/10000], Loss: 0.330955445766449\n",
      "Epoch [8686/10000], Loss: 0.33094388246536255\n",
      "Epoch [8687/10000], Loss: 0.3309323191642761\n",
      "Epoch [8688/10000], Loss: 0.3309207558631897\n",
      "Epoch [8689/10000], Loss: 0.33090925216674805\n",
      "Epoch [8690/10000], Loss: 0.33089762926101685\n",
      "Epoch [8691/10000], Loss: 0.3308860659599304\n",
      "Epoch [8692/10000], Loss: 0.33087462186813354\n",
      "Epoch [8693/10000], Loss: 0.3308631181716919\n",
      "Epoch [8694/10000], Loss: 0.33085155487060547\n",
      "Epoch [8695/10000], Loss: 0.33083999156951904\n",
      "Epoch [8696/10000], Loss: 0.33082854747772217\n",
      "Epoch [8697/10000], Loss: 0.3308170437812805\n",
      "Epoch [8698/10000], Loss: 0.33080559968948364\n",
      "Epoch [8699/10000], Loss: 0.33079397678375244\n",
      "Epoch [8700/10000], Loss: 0.33078253269195557\n",
      "Epoch [8701/10000], Loss: 0.33077096939086914\n",
      "Epoch [8702/10000], Loss: 0.33075952529907227\n",
      "Epoch [8703/10000], Loss: 0.3307480216026306\n",
      "Epoch [8704/10000], Loss: 0.33073657751083374\n",
      "Epoch [8705/10000], Loss: 0.3307250738143921\n",
      "Epoch [8706/10000], Loss: 0.3307136297225952\n",
      "Epoch [8707/10000], Loss: 0.33070212602615356\n",
      "Epoch [8708/10000], Loss: 0.3306906819343567\n",
      "Epoch [8709/10000], Loss: 0.33067911863327026\n",
      "Epoch [8710/10000], Loss: 0.33066779375076294\n",
      "Epoch [8711/10000], Loss: 0.33065634965896606\n",
      "Epoch [8712/10000], Loss: 0.33064472675323486\n",
      "Epoch [8713/10000], Loss: 0.330633282661438\n",
      "Epoch [8714/10000], Loss: 0.3306218385696411\n",
      "Epoch [8715/10000], Loss: 0.33061039447784424\n",
      "Epoch [8716/10000], Loss: 0.33059900999069214\n",
      "Epoch [8717/10000], Loss: 0.3305875062942505\n",
      "Epoch [8718/10000], Loss: 0.3305760622024536\n",
      "Epoch [8719/10000], Loss: 0.3305646777153015\n",
      "Epoch [8720/10000], Loss: 0.3305533528327942\n",
      "Epoch [8721/10000], Loss: 0.3305419087409973\n",
      "Epoch [8722/10000], Loss: 0.33053046464920044\n",
      "Epoch [8723/10000], Loss: 0.33051908016204834\n",
      "Epoch [8724/10000], Loss: 0.33050763607025146\n",
      "Epoch [8725/10000], Loss: 0.3304961919784546\n",
      "Epoch [8726/10000], Loss: 0.3304848074913025\n",
      "Epoch [8727/10000], Loss: 0.3304734230041504\n",
      "Epoch [8728/10000], Loss: 0.3304619789123535\n",
      "Epoch [8729/10000], Loss: 0.3304506540298462\n",
      "Epoch [8730/10000], Loss: 0.3304392695426941\n",
      "Epoch [8731/10000], Loss: 0.33042794466018677\n",
      "Epoch [8732/10000], Loss: 0.3304165005683899\n",
      "Epoch [8733/10000], Loss: 0.33040517568588257\n",
      "Epoch [8734/10000], Loss: 0.3303937315940857\n",
      "Epoch [8735/10000], Loss: 0.33038240671157837\n",
      "Epoch [8736/10000], Loss: 0.33037108182907104\n",
      "Epoch [8737/10000], Loss: 0.3303595781326294\n",
      "Epoch [8738/10000], Loss: 0.3303483724594116\n",
      "Epoch [8739/10000], Loss: 0.3303369879722595\n",
      "Epoch [8740/10000], Loss: 0.3303256034851074\n",
      "Epoch [8741/10000], Loss: 0.3303142189979553\n",
      "Epoch [8742/10000], Loss: 0.3303028345108032\n",
      "Epoch [8743/10000], Loss: 0.3302915692329407\n",
      "Epoch [8744/10000], Loss: 0.33028024435043335\n",
      "Epoch [8745/10000], Loss: 0.330268919467926\n",
      "Epoch [8746/10000], Loss: 0.3302575349807739\n",
      "Epoch [8747/10000], Loss: 0.3302462697029114\n",
      "Epoch [8748/10000], Loss: 0.33023494482040405\n",
      "Epoch [8749/10000], Loss: 0.3302236795425415\n",
      "Epoch [8750/10000], Loss: 0.3302122950553894\n",
      "Epoch [8751/10000], Loss: 0.3302009701728821\n",
      "Epoch [8752/10000], Loss: 0.33018964529037476\n",
      "Epoch [8753/10000], Loss: 0.3301783800125122\n",
      "Epoch [8754/10000], Loss: 0.33016711473464966\n",
      "Epoch [8755/10000], Loss: 0.33015578985214233\n",
      "Epoch [8756/10000], Loss: 0.33014440536499023\n",
      "Epoch [8757/10000], Loss: 0.3301331400871277\n",
      "Epoch [8758/10000], Loss: 0.3301219344139099\n",
      "Epoch [8759/10000], Loss: 0.3301106095314026\n",
      "Epoch [8760/10000], Loss: 0.33009928464889526\n",
      "Epoch [8761/10000], Loss: 0.3300880789756775\n",
      "Epoch [8762/10000], Loss: 0.33007681369781494\n",
      "Epoch [8763/10000], Loss: 0.33006542921066284\n",
      "Epoch [8764/10000], Loss: 0.33005422353744507\n",
      "Epoch [8765/10000], Loss: 0.3300429582595825\n",
      "Epoch [8766/10000], Loss: 0.33003175258636475\n",
      "Epoch [8767/10000], Loss: 0.3300204873085022\n",
      "Epoch [8768/10000], Loss: 0.3300091624259949\n",
      "Epoch [8769/10000], Loss: 0.3299980163574219\n",
      "Epoch [8770/10000], Loss: 0.32998669147491455\n",
      "Epoch [8771/10000], Loss: 0.3299754858016968\n",
      "Epoch [8772/10000], Loss: 0.32996416091918945\n",
      "Epoch [8773/10000], Loss: 0.3299528956413269\n",
      "Epoch [8774/10000], Loss: 0.3299417495727539\n",
      "Epoch [8775/10000], Loss: 0.32993048429489136\n",
      "Epoch [8776/10000], Loss: 0.3299192786216736\n",
      "Epoch [8777/10000], Loss: 0.32990801334381104\n",
      "Epoch [8778/10000], Loss: 0.32989686727523804\n",
      "Epoch [8779/10000], Loss: 0.32988566160202026\n",
      "Epoch [8780/10000], Loss: 0.3298744559288025\n",
      "Epoch [8781/10000], Loss: 0.32986319065093994\n",
      "Epoch [8782/10000], Loss: 0.32985198497772217\n",
      "Epoch [8783/10000], Loss: 0.3298407793045044\n",
      "Epoch [8784/10000], Loss: 0.3298295736312866\n",
      "Epoch [8785/10000], Loss: 0.32981836795806885\n",
      "Epoch [8786/10000], Loss: 0.32980722188949585\n",
      "Epoch [8787/10000], Loss: 0.3297960162162781\n",
      "Epoch [8788/10000], Loss: 0.3297848105430603\n",
      "Epoch [8789/10000], Loss: 0.3297737240791321\n",
      "Epoch [8790/10000], Loss: 0.32976245880126953\n",
      "Epoch [8791/10000], Loss: 0.32975131273269653\n",
      "Epoch [8792/10000], Loss: 0.32974010705947876\n",
      "Epoch [8793/10000], Loss: 0.32972896099090576\n",
      "Epoch [8794/10000], Loss: 0.32971781492233276\n",
      "Epoch [8795/10000], Loss: 0.32970666885375977\n",
      "Epoch [8796/10000], Loss: 0.3296954035758972\n",
      "Epoch [8797/10000], Loss: 0.329684317111969\n",
      "Epoch [8798/10000], Loss: 0.32967323064804077\n",
      "Epoch [8799/10000], Loss: 0.329662024974823\n",
      "Epoch [8800/10000], Loss: 0.32965087890625\n",
      "Epoch [8801/10000], Loss: 0.3296397924423218\n",
      "Epoch [8802/10000], Loss: 0.329628586769104\n",
      "Epoch [8803/10000], Loss: 0.329617440700531\n",
      "Epoch [8804/10000], Loss: 0.3296063542366028\n",
      "Epoch [8805/10000], Loss: 0.3295952081680298\n",
      "Epoch [8806/10000], Loss: 0.3295840620994568\n",
      "Epoch [8807/10000], Loss: 0.32957297563552856\n",
      "Epoch [8808/10000], Loss: 0.3295617699623108\n",
      "Epoch [8809/10000], Loss: 0.32955068349838257\n",
      "Epoch [8810/10000], Loss: 0.32953959703445435\n",
      "Epoch [8811/10000], Loss: 0.3295283913612366\n",
      "Epoch [8812/10000], Loss: 0.3295173645019531\n",
      "Epoch [8813/10000], Loss: 0.3295062780380249\n",
      "Epoch [8814/10000], Loss: 0.32949525117874146\n",
      "Epoch [8815/10000], Loss: 0.32948410511016846\n",
      "Epoch [8816/10000], Loss: 0.32947301864624023\n",
      "Epoch [8817/10000], Loss: 0.32946187257766724\n",
      "Epoch [8818/10000], Loss: 0.329450786113739\n",
      "Epoch [8819/10000], Loss: 0.32943975925445557\n",
      "Epoch [8820/10000], Loss: 0.32942861318588257\n",
      "Epoch [8821/10000], Loss: 0.3294175863265991\n",
      "Epoch [8822/10000], Loss: 0.3294065594673157\n",
      "Epoch [8823/10000], Loss: 0.3293954133987427\n",
      "Epoch [8824/10000], Loss: 0.32938438653945923\n",
      "Epoch [8825/10000], Loss: 0.329373300075531\n",
      "Epoch [8826/10000], Loss: 0.32936227321624756\n",
      "Epoch [8827/10000], Loss: 0.3293512463569641\n",
      "Epoch [8828/10000], Loss: 0.32934004068374634\n",
      "Epoch [8829/10000], Loss: 0.32932907342910767\n",
      "Epoch [8830/10000], Loss: 0.32931792736053467\n",
      "Epoch [8831/10000], Loss: 0.329306960105896\n",
      "Epoch [8832/10000], Loss: 0.32929593324661255\n",
      "Epoch [8833/10000], Loss: 0.3292848467826843\n",
      "Epoch [8834/10000], Loss: 0.3292737603187561\n",
      "Epoch [8835/10000], Loss: 0.32926273345947266\n",
      "Epoch [8836/10000], Loss: 0.3292517066001892\n",
      "Epoch [8837/10000], Loss: 0.32924073934555054\n",
      "Epoch [8838/10000], Loss: 0.3292296528816223\n",
      "Epoch [8839/10000], Loss: 0.32921868562698364\n",
      "Epoch [8840/10000], Loss: 0.32920771837234497\n",
      "Epoch [8841/10000], Loss: 0.329196572303772\n",
      "Epoch [8842/10000], Loss: 0.3291855454444885\n",
      "Epoch [8843/10000], Loss: 0.32917457818984985\n",
      "Epoch [8844/10000], Loss: 0.3291636109352112\n",
      "Epoch [8845/10000], Loss: 0.32915258407592773\n",
      "Epoch [8846/10000], Loss: 0.3291415572166443\n",
      "Epoch [8847/10000], Loss: 0.3291305899620056\n",
      "Epoch [8848/10000], Loss: 0.32911962270736694\n",
      "Epoch [8849/10000], Loss: 0.32910865545272827\n",
      "Epoch [8850/10000], Loss: 0.32909756898880005\n",
      "Epoch [8851/10000], Loss: 0.32908666133880615\n",
      "Epoch [8852/10000], Loss: 0.32907551527023315\n",
      "Epoch [8853/10000], Loss: 0.32906466722488403\n",
      "Epoch [8854/10000], Loss: 0.3290535807609558\n",
      "Epoch [8855/10000], Loss: 0.32904261350631714\n",
      "Epoch [8856/10000], Loss: 0.32903164625167847\n",
      "Epoch [8857/10000], Loss: 0.3290206789970398\n",
      "Epoch [8858/10000], Loss: 0.3290097117424011\n",
      "Epoch [8859/10000], Loss: 0.328998863697052\n",
      "Epoch [8860/10000], Loss: 0.3289877772331238\n",
      "Epoch [8861/10000], Loss: 0.3289768099784851\n",
      "Epoch [8862/10000], Loss: 0.328965961933136\n",
      "Epoch [8863/10000], Loss: 0.3289549946784973\n",
      "Epoch [8864/10000], Loss: 0.32894402742385864\n",
      "Epoch [8865/10000], Loss: 0.3289329409599304\n",
      "Epoch [8866/10000], Loss: 0.3289220929145813\n",
      "Epoch [8867/10000], Loss: 0.3289111852645874\n",
      "Epoch [8868/10000], Loss: 0.32890021800994873\n",
      "Epoch [8869/10000], Loss: 0.3288893699645996\n",
      "Epoch [8870/10000], Loss: 0.32887834310531616\n",
      "Epoch [8871/10000], Loss: 0.32886743545532227\n",
      "Epoch [8872/10000], Loss: 0.3288564682006836\n",
      "Epoch [8873/10000], Loss: 0.3288455605506897\n",
      "Epoch [8874/10000], Loss: 0.328834593296051\n",
      "Epoch [8875/10000], Loss: 0.3288237452507019\n",
      "Epoch [8876/10000], Loss: 0.32881277799606323\n",
      "Epoch [8877/10000], Loss: 0.32880181074142456\n",
      "Epoch [8878/10000], Loss: 0.32879096269607544\n",
      "Epoch [8879/10000], Loss: 0.32878005504608154\n",
      "Epoch [8880/10000], Loss: 0.32876914739608765\n",
      "Epoch [8881/10000], Loss: 0.32875823974609375\n",
      "Epoch [8882/10000], Loss: 0.32874733209609985\n",
      "Epoch [8883/10000], Loss: 0.32873642444610596\n",
      "Epoch [8884/10000], Loss: 0.32872551679611206\n",
      "Epoch [8885/10000], Loss: 0.32871466875076294\n",
      "Epoch [8886/10000], Loss: 0.32870376110076904\n",
      "Epoch [8887/10000], Loss: 0.32869285345077515\n",
      "Epoch [8888/10000], Loss: 0.32868194580078125\n",
      "Epoch [8889/10000], Loss: 0.32867109775543213\n",
      "Epoch [8890/10000], Loss: 0.32866019010543823\n",
      "Epoch [8891/10000], Loss: 0.3286494016647339\n",
      "Epoch [8892/10000], Loss: 0.32863855361938477\n",
      "Epoch [8893/10000], Loss: 0.3286275863647461\n",
      "Epoch [8894/10000], Loss: 0.3286166787147522\n",
      "Epoch [8895/10000], Loss: 0.3286057710647583\n",
      "Epoch [8896/10000], Loss: 0.32859504222869873\n",
      "Epoch [8897/10000], Loss: 0.3285841941833496\n",
      "Epoch [8898/10000], Loss: 0.3285732865333557\n",
      "Epoch [8899/10000], Loss: 0.3285623788833618\n",
      "Epoch [8900/10000], Loss: 0.32855159044265747\n",
      "Epoch [8901/10000], Loss: 0.3285408020019531\n",
      "Epoch [8902/10000], Loss: 0.32852983474731445\n",
      "Epoch [8903/10000], Loss: 0.32851898670196533\n",
      "Epoch [8904/10000], Loss: 0.32850825786590576\n",
      "Epoch [8905/10000], Loss: 0.32849735021591187\n",
      "Epoch [8906/10000], Loss: 0.32848650217056274\n",
      "Epoch [8907/10000], Loss: 0.3284756541252136\n",
      "Epoch [8908/10000], Loss: 0.3284648656845093\n",
      "Epoch [8909/10000], Loss: 0.32845401763916016\n",
      "Epoch [8910/10000], Loss: 0.32844310998916626\n",
      "Epoch [8911/10000], Loss: 0.3284323811531067\n",
      "Epoch [8912/10000], Loss: 0.32842159271240234\n",
      "Epoch [8913/10000], Loss: 0.32841068506240845\n",
      "Epoch [8914/10000], Loss: 0.3283999562263489\n",
      "Epoch [8915/10000], Loss: 0.32838910818099976\n",
      "Epoch [8916/10000], Loss: 0.3283783197402954\n",
      "Epoch [8917/10000], Loss: 0.3283674120903015\n",
      "Epoch [8918/10000], Loss: 0.32835662364959717\n",
      "Epoch [8919/10000], Loss: 0.3283458352088928\n",
      "Epoch [8920/10000], Loss: 0.3283349871635437\n",
      "Epoch [8921/10000], Loss: 0.32832419872283936\n",
      "Epoch [8922/10000], Loss: 0.3283134698867798\n",
      "Epoch [8923/10000], Loss: 0.32830268144607544\n",
      "Epoch [8924/10000], Loss: 0.3282918334007263\n",
      "Epoch [8925/10000], Loss: 0.328281044960022\n",
      "Epoch [8926/10000], Loss: 0.3282703161239624\n",
      "Epoch [8927/10000], Loss: 0.32825952768325806\n",
      "Epoch [8928/10000], Loss: 0.32824867963790894\n",
      "Epoch [8929/10000], Loss: 0.32823801040649414\n",
      "Epoch [8930/10000], Loss: 0.328227162361145\n",
      "Epoch [8931/10000], Loss: 0.32821643352508545\n",
      "Epoch [8932/10000], Loss: 0.32820558547973633\n",
      "Epoch [8933/10000], Loss: 0.328194797039032\n",
      "Epoch [8934/10000], Loss: 0.32818400859832764\n",
      "Epoch [8935/10000], Loss: 0.32817333936691284\n",
      "Epoch [8936/10000], Loss: 0.3281625509262085\n",
      "Epoch [8937/10000], Loss: 0.32815176248550415\n",
      "Epoch [8938/10000], Loss: 0.32814109325408936\n",
      "Epoch [8939/10000], Loss: 0.32813024520874023\n",
      "Epoch [8940/10000], Loss: 0.32811957597732544\n",
      "Epoch [8941/10000], Loss: 0.3281087875366211\n",
      "Epoch [8942/10000], Loss: 0.328097939491272\n",
      "Epoch [8943/10000], Loss: 0.32808732986450195\n",
      "Epoch [8944/10000], Loss: 0.3280765414237976\n",
      "Epoch [8945/10000], Loss: 0.32806581258773804\n",
      "Epoch [8946/10000], Loss: 0.3280550241470337\n",
      "Epoch [8947/10000], Loss: 0.3280443549156189\n",
      "Epoch [8948/10000], Loss: 0.32803356647491455\n",
      "Epoch [8949/10000], Loss: 0.32802289724349976\n",
      "Epoch [8950/10000], Loss: 0.3280121088027954\n",
      "Epoch [8951/10000], Loss: 0.3280014395713806\n",
      "Epoch [8952/10000], Loss: 0.32799065113067627\n",
      "Epoch [8953/10000], Loss: 0.3279799818992615\n",
      "Epoch [8954/10000], Loss: 0.3279692530632019\n",
      "Epoch [8955/10000], Loss: 0.32795852422714233\n",
      "Epoch [8956/10000], Loss: 0.32794779539108276\n",
      "Epoch [8957/10000], Loss: 0.3279370665550232\n",
      "Epoch [8958/10000], Loss: 0.32792627811431885\n",
      "Epoch [8959/10000], Loss: 0.32791566848754883\n",
      "Epoch [8960/10000], Loss: 0.3279048800468445\n",
      "Epoch [8961/10000], Loss: 0.32789427042007446\n",
      "Epoch [8962/10000], Loss: 0.32788360118865967\n",
      "Epoch [8963/10000], Loss: 0.3278728127479553\n",
      "Epoch [8964/10000], Loss: 0.3278622031211853\n",
      "Epoch [8965/10000], Loss: 0.32785147428512573\n",
      "Epoch [8966/10000], Loss: 0.32784080505371094\n",
      "Epoch [8967/10000], Loss: 0.32783013582229614\n",
      "Epoch [8968/10000], Loss: 0.3278193473815918\n",
      "Epoch [8969/10000], Loss: 0.3278087377548218\n",
      "Epoch [8970/10000], Loss: 0.3277980089187622\n",
      "Epoch [8971/10000], Loss: 0.3277873992919922\n",
      "Epoch [8972/10000], Loss: 0.3277767300605774\n",
      "Epoch [8973/10000], Loss: 0.3277660608291626\n",
      "Epoch [8974/10000], Loss: 0.327755331993103\n",
      "Epoch [8975/10000], Loss: 0.32774460315704346\n",
      "Epoch [8976/10000], Loss: 0.3277340531349182\n",
      "Epoch [8977/10000], Loss: 0.32772332429885864\n",
      "Epoch [8978/10000], Loss: 0.3277127146720886\n",
      "Epoch [8979/10000], Loss: 0.3277021050453186\n",
      "Epoch [8980/10000], Loss: 0.32769137620925903\n",
      "Epoch [8981/10000], Loss: 0.32768070697784424\n",
      "Epoch [8982/10000], Loss: 0.3276700973510742\n",
      "Epoch [8983/10000], Loss: 0.3276594877243042\n",
      "Epoch [8984/10000], Loss: 0.32764875888824463\n",
      "Epoch [8985/10000], Loss: 0.3276382088661194\n",
      "Epoch [8986/10000], Loss: 0.3276275396347046\n",
      "Epoch [8987/10000], Loss: 0.327616810798645\n",
      "Epoch [8988/10000], Loss: 0.3276062607765198\n",
      "Epoch [8989/10000], Loss: 0.327595591545105\n",
      "Epoch [8990/10000], Loss: 0.3275849223136902\n",
      "Epoch [8991/10000], Loss: 0.32757431268692017\n",
      "Epoch [8992/10000], Loss: 0.32756370306015015\n",
      "Epoch [8993/10000], Loss: 0.3275530934333801\n",
      "Epoch [8994/10000], Loss: 0.32754242420196533\n",
      "Epoch [8995/10000], Loss: 0.3275318741798401\n",
      "Epoch [8996/10000], Loss: 0.3275212049484253\n",
      "Epoch [8997/10000], Loss: 0.32751065492630005\n",
      "Epoch [8998/10000], Loss: 0.32749998569488525\n",
      "Epoch [8999/10000], Loss: 0.32748937606811523\n",
      "Epoch [9000/10000], Loss: 0.32747882604599\n",
      "Epoch [9001/10000], Loss: 0.3274680972099304\n",
      "Epoch [9002/10000], Loss: 0.3274577260017395\n",
      "Epoch [9003/10000], Loss: 0.32744699716567993\n",
      "Epoch [9004/10000], Loss: 0.3274363875389099\n",
      "Epoch [9005/10000], Loss: 0.32742583751678467\n",
      "Epoch [9006/10000], Loss: 0.3274151682853699\n",
      "Epoch [9007/10000], Loss: 0.32740455865859985\n",
      "Epoch [9008/10000], Loss: 0.3273940086364746\n",
      "Epoch [9009/10000], Loss: 0.32738345861434937\n",
      "Epoch [9010/10000], Loss: 0.3273729085922241\n",
      "Epoch [9011/10000], Loss: 0.3273622393608093\n",
      "Epoch [9012/10000], Loss: 0.3273516297340393\n",
      "Epoch [9013/10000], Loss: 0.32734107971191406\n",
      "Epoch [9014/10000], Loss: 0.3273305892944336\n",
      "Epoch [9015/10000], Loss: 0.3273199796676636\n",
      "Epoch [9016/10000], Loss: 0.32730942964553833\n",
      "Epoch [9017/10000], Loss: 0.3272988200187683\n",
      "Epoch [9018/10000], Loss: 0.32728832960128784\n",
      "Epoch [9019/10000], Loss: 0.3272777199745178\n",
      "Epoch [9020/10000], Loss: 0.32726722955703735\n",
      "Epoch [9021/10000], Loss: 0.32725656032562256\n",
      "Epoch [9022/10000], Loss: 0.3272460699081421\n",
      "Epoch [9023/10000], Loss: 0.32723551988601685\n",
      "Epoch [9024/10000], Loss: 0.3272249102592468\n",
      "Epoch [9025/10000], Loss: 0.3272143006324768\n",
      "Epoch [9026/10000], Loss: 0.3272038698196411\n",
      "Epoch [9027/10000], Loss: 0.3271932601928711\n",
      "Epoch [9028/10000], Loss: 0.3271827697753906\n",
      "Epoch [9029/10000], Loss: 0.3271721601486206\n",
      "Epoch [9030/10000], Loss: 0.32716166973114014\n",
      "Epoch [9031/10000], Loss: 0.32715117931365967\n",
      "Epoch [9032/10000], Loss: 0.3271405100822449\n",
      "Epoch [9033/10000], Loss: 0.3271300196647644\n",
      "Epoch [9034/10000], Loss: 0.32711946964263916\n",
      "Epoch [9035/10000], Loss: 0.3271089792251587\n",
      "Epoch [9036/10000], Loss: 0.3270984888076782\n",
      "Epoch [9037/10000], Loss: 0.327087938785553\n",
      "Epoch [9038/10000], Loss: 0.3270774483680725\n",
      "Epoch [9039/10000], Loss: 0.32706695795059204\n",
      "Epoch [9040/10000], Loss: 0.327056348323822\n",
      "Epoch [9041/10000], Loss: 0.32704591751098633\n",
      "Epoch [9042/10000], Loss: 0.32703542709350586\n",
      "Epoch [9043/10000], Loss: 0.3270248770713806\n",
      "Epoch [9044/10000], Loss: 0.32701438665390015\n",
      "Epoch [9045/10000], Loss: 0.3270038962364197\n",
      "Epoch [9046/10000], Loss: 0.3269934058189392\n",
      "Epoch [9047/10000], Loss: 0.32698285579681396\n",
      "Epoch [9048/10000], Loss: 0.3269723653793335\n",
      "Epoch [9049/10000], Loss: 0.326961874961853\n",
      "Epoch [9050/10000], Loss: 0.3269513249397278\n",
      "Epoch [9051/10000], Loss: 0.3269408941268921\n",
      "Epoch [9052/10000], Loss: 0.32693034410476685\n",
      "Epoch [9053/10000], Loss: 0.32691991329193115\n",
      "Epoch [9054/10000], Loss: 0.3269093632698059\n",
      "Epoch [9055/10000], Loss: 0.3268989324569702\n",
      "Epoch [9056/10000], Loss: 0.32688838243484497\n",
      "Epoch [9057/10000], Loss: 0.3268779516220093\n",
      "Epoch [9058/10000], Loss: 0.3268674612045288\n",
      "Epoch [9059/10000], Loss: 0.3268570303916931\n",
      "Epoch [9060/10000], Loss: 0.32684653997421265\n",
      "Epoch [9061/10000], Loss: 0.3268360495567322\n",
      "Epoch [9062/10000], Loss: 0.3268255591392517\n",
      "Epoch [9063/10000], Loss: 0.3268151879310608\n",
      "Epoch [9064/10000], Loss: 0.32680463790893555\n",
      "Epoch [9065/10000], Loss: 0.32679420709609985\n",
      "Epoch [9066/10000], Loss: 0.3267836570739746\n",
      "Epoch [9067/10000], Loss: 0.3267732262611389\n",
      "Epoch [9068/10000], Loss: 0.3267627954483032\n",
      "Epoch [9069/10000], Loss: 0.32675236463546753\n",
      "Epoch [9070/10000], Loss: 0.32674187421798706\n",
      "Epoch [9071/10000], Loss: 0.32673150300979614\n",
      "Epoch [9072/10000], Loss: 0.32672107219696045\n",
      "Epoch [9073/10000], Loss: 0.32671058177948\n",
      "Epoch [9074/10000], Loss: 0.3267000913619995\n",
      "Epoch [9075/10000], Loss: 0.3266896605491638\n",
      "Epoch [9076/10000], Loss: 0.3266792297363281\n",
      "Epoch [9077/10000], Loss: 0.32666873931884766\n",
      "Epoch [9078/10000], Loss: 0.32665830850601196\n",
      "Epoch [9079/10000], Loss: 0.32664793729782104\n",
      "Epoch [9080/10000], Loss: 0.3266375660896301\n",
      "Epoch [9081/10000], Loss: 0.3266269564628601\n",
      "Epoch [9082/10000], Loss: 0.32661670446395874\n",
      "Epoch [9083/10000], Loss: 0.32660621404647827\n",
      "Epoch [9084/10000], Loss: 0.3265957832336426\n",
      "Epoch [9085/10000], Loss: 0.3265853524208069\n",
      "Epoch [9086/10000], Loss: 0.32657498121261597\n",
      "Epoch [9087/10000], Loss: 0.3265644907951355\n",
      "Epoch [9088/10000], Loss: 0.3265541195869446\n",
      "Epoch [9089/10000], Loss: 0.3265436887741089\n",
      "Epoch [9090/10000], Loss: 0.3265332579612732\n",
      "Epoch [9091/10000], Loss: 0.3265228271484375\n",
      "Epoch [9092/10000], Loss: 0.3265124559402466\n",
      "Epoch [9093/10000], Loss: 0.3265020251274109\n",
      "Epoch [9094/10000], Loss: 0.32649165391921997\n",
      "Epoch [9095/10000], Loss: 0.32648128271102905\n",
      "Epoch [9096/10000], Loss: 0.32647085189819336\n",
      "Epoch [9097/10000], Loss: 0.32646042108535767\n",
      "Epoch [9098/10000], Loss: 0.32645004987716675\n",
      "Epoch [9099/10000], Loss: 0.3264395594596863\n",
      "Epoch [9100/10000], Loss: 0.32642924785614014\n",
      "Epoch [9101/10000], Loss: 0.32641875743865967\n",
      "Epoch [9102/10000], Loss: 0.32640838623046875\n",
      "Epoch [9103/10000], Loss: 0.32639801502227783\n",
      "Epoch [9104/10000], Loss: 0.3263877034187317\n",
      "Epoch [9105/10000], Loss: 0.326377272605896\n",
      "Epoch [9106/10000], Loss: 0.3263669013977051\n",
      "Epoch [9107/10000], Loss: 0.3263564705848694\n",
      "Epoch [9108/10000], Loss: 0.32634609937667847\n",
      "Epoch [9109/10000], Loss: 0.32633572816848755\n",
      "Epoch [9110/10000], Loss: 0.3263254761695862\n",
      "Epoch [9111/10000], Loss: 0.3263149857521057\n",
      "Epoch [9112/10000], Loss: 0.3263046145439148\n",
      "Epoch [9113/10000], Loss: 0.3262942433357239\n",
      "Epoch [9114/10000], Loss: 0.32628393173217773\n",
      "Epoch [9115/10000], Loss: 0.32627350091934204\n",
      "Epoch [9116/10000], Loss: 0.3262632489204407\n",
      "Epoch [9117/10000], Loss: 0.326252818107605\n",
      "Epoch [9118/10000], Loss: 0.32624250650405884\n",
      "Epoch [9119/10000], Loss: 0.32623207569122314\n",
      "Epoch [9120/10000], Loss: 0.326221764087677\n",
      "Epoch [9121/10000], Loss: 0.32621145248413086\n",
      "Epoch [9122/10000], Loss: 0.32620102167129517\n",
      "Epoch [9123/10000], Loss: 0.32619065046310425\n",
      "Epoch [9124/10000], Loss: 0.3261803984642029\n",
      "Epoch [9125/10000], Loss: 0.3261699676513672\n",
      "Epoch [9126/10000], Loss: 0.32615965604782104\n",
      "Epoch [9127/10000], Loss: 0.3261492848396301\n",
      "Epoch [9128/10000], Loss: 0.326138973236084\n",
      "Epoch [9129/10000], Loss: 0.32612866163253784\n",
      "Epoch [9130/10000], Loss: 0.3261182904243469\n",
      "Epoch [9131/10000], Loss: 0.326107919216156\n",
      "Epoch [9132/10000], Loss: 0.32609760761260986\n",
      "Epoch [9133/10000], Loss: 0.3260872960090637\n",
      "Epoch [9134/10000], Loss: 0.3260769844055176\n",
      "Epoch [9135/10000], Loss: 0.32606661319732666\n",
      "Epoch [9136/10000], Loss: 0.3260563611984253\n",
      "Epoch [9137/10000], Loss: 0.3260459899902344\n",
      "Epoch [9138/10000], Loss: 0.32603561878204346\n",
      "Epoch [9139/10000], Loss: 0.3260253667831421\n",
      "Epoch [9140/10000], Loss: 0.32601505517959595\n",
      "Epoch [9141/10000], Loss: 0.32600468397140503\n",
      "Epoch [9142/10000], Loss: 0.32599443197250366\n",
      "Epoch [9143/10000], Loss: 0.3259841203689575\n",
      "Epoch [9144/10000], Loss: 0.32597386837005615\n",
      "Epoch [9145/10000], Loss: 0.32596349716186523\n",
      "Epoch [9146/10000], Loss: 0.3259531855583191\n",
      "Epoch [9147/10000], Loss: 0.32594287395477295\n",
      "Epoch [9148/10000], Loss: 0.3259325623512268\n",
      "Epoch [9149/10000], Loss: 0.32592231035232544\n",
      "Epoch [9150/10000], Loss: 0.3259119987487793\n",
      "Epoch [9151/10000], Loss: 0.3259016275405884\n",
      "Epoch [9152/10000], Loss: 0.3258914351463318\n",
      "Epoch [9153/10000], Loss: 0.32588112354278564\n",
      "Epoch [9154/10000], Loss: 0.3258708119392395\n",
      "Epoch [9155/10000], Loss: 0.32586055994033813\n",
      "Epoch [9156/10000], Loss: 0.3258501887321472\n",
      "Epoch [9157/10000], Loss: 0.3258399963378906\n",
      "Epoch [9158/10000], Loss: 0.3258296847343445\n",
      "Epoch [9159/10000], Loss: 0.3258194327354431\n",
      "Epoch [9160/10000], Loss: 0.325809121131897\n",
      "Epoch [9161/10000], Loss: 0.3257989287376404\n",
      "Epoch [9162/10000], Loss: 0.32578855752944946\n",
      "Epoch [9163/10000], Loss: 0.32577836513519287\n",
      "Epoch [9164/10000], Loss: 0.32576805353164673\n",
      "Epoch [9165/10000], Loss: 0.32575786113739014\n",
      "Epoch [9166/10000], Loss: 0.325747549533844\n",
      "Epoch [9167/10000], Loss: 0.3257372975349426\n",
      "Epoch [9168/10000], Loss: 0.3257269263267517\n",
      "Epoch [9169/10000], Loss: 0.32571667432785034\n",
      "Epoch [9170/10000], Loss: 0.32570648193359375\n",
      "Epoch [9171/10000], Loss: 0.3256961703300476\n",
      "Epoch [9172/10000], Loss: 0.32568591833114624\n",
      "Epoch [9173/10000], Loss: 0.3256757855415344\n",
      "Epoch [9174/10000], Loss: 0.3256654739379883\n",
      "Epoch [9175/10000], Loss: 0.32565516233444214\n",
      "Epoch [9176/10000], Loss: 0.32564491033554077\n",
      "Epoch [9177/10000], Loss: 0.32563477754592896\n",
      "Epoch [9178/10000], Loss: 0.3256245255470276\n",
      "Epoch [9179/10000], Loss: 0.325614333152771\n",
      "Epoch [9180/10000], Loss: 0.32560402154922485\n",
      "Epoch [9181/10000], Loss: 0.3255937695503235\n",
      "Epoch [9182/10000], Loss: 0.3255835175514221\n",
      "Epoch [9183/10000], Loss: 0.3255733847618103\n",
      "Epoch [9184/10000], Loss: 0.3255630135536194\n",
      "Epoch [9185/10000], Loss: 0.32555288076400757\n",
      "Epoch [9186/10000], Loss: 0.3255425691604614\n",
      "Epoch [9187/10000], Loss: 0.3255324959754944\n",
      "Epoch [9188/10000], Loss: 0.32552212476730347\n",
      "Epoch [9189/10000], Loss: 0.32551199197769165\n",
      "Epoch [9190/10000], Loss: 0.32550179958343506\n",
      "Epoch [9191/10000], Loss: 0.3254915475845337\n",
      "Epoch [9192/10000], Loss: 0.3254812955856323\n",
      "Epoch [9193/10000], Loss: 0.32547104358673096\n",
      "Epoch [9194/10000], Loss: 0.32546085119247437\n",
      "Epoch [9195/10000], Loss: 0.325450599193573\n",
      "Epoch [9196/10000], Loss: 0.3254404664039612\n",
      "Epoch [9197/10000], Loss: 0.3254302740097046\n",
      "Epoch [9198/10000], Loss: 0.325420081615448\n",
      "Epoch [9199/10000], Loss: 0.32540982961654663\n",
      "Epoch [9200/10000], Loss: 0.32539957761764526\n",
      "Epoch [9201/10000], Loss: 0.32538944482803345\n",
      "Epoch [9202/10000], Loss: 0.32537931203842163\n",
      "Epoch [9203/10000], Loss: 0.3253690004348755\n",
      "Epoch [9204/10000], Loss: 0.32535886764526367\n",
      "Epoch [9205/10000], Loss: 0.3253486752510071\n",
      "Epoch [9206/10000], Loss: 0.3253384828567505\n",
      "Epoch [9207/10000], Loss: 0.3253282904624939\n",
      "Epoch [9208/10000], Loss: 0.32531803846359253\n",
      "Epoch [9209/10000], Loss: 0.3253079056739807\n",
      "Epoch [9210/10000], Loss: 0.3252977728843689\n",
      "Epoch [9211/10000], Loss: 0.32528746128082275\n",
      "Epoch [9212/10000], Loss: 0.3252773880958557\n",
      "Epoch [9213/10000], Loss: 0.32526713609695435\n",
      "Epoch [9214/10000], Loss: 0.32525700330734253\n",
      "Epoch [9215/10000], Loss: 0.3252468705177307\n",
      "Epoch [9216/10000], Loss: 0.32523661851882935\n",
      "Epoch [9217/10000], Loss: 0.32522648572921753\n",
      "Epoch [9218/10000], Loss: 0.32521629333496094\n",
      "Epoch [9219/10000], Loss: 0.3252061605453491\n",
      "Epoch [9220/10000], Loss: 0.32519596815109253\n",
      "Epoch [9221/10000], Loss: 0.3251858353614807\n",
      "Epoch [9222/10000], Loss: 0.3251757025718689\n",
      "Epoch [9223/10000], Loss: 0.32516545057296753\n",
      "Epoch [9224/10000], Loss: 0.3251553177833557\n",
      "Epoch [9225/10000], Loss: 0.32514524459838867\n",
      "Epoch [9226/10000], Loss: 0.3251350522041321\n",
      "Epoch [9227/10000], Loss: 0.3251248598098755\n",
      "Epoch [9228/10000], Loss: 0.32511478662490845\n",
      "Epoch [9229/10000], Loss: 0.32510465383529663\n",
      "Epoch [9230/10000], Loss: 0.3250943422317505\n",
      "Epoch [9231/10000], Loss: 0.32508420944213867\n",
      "Epoch [9232/10000], Loss: 0.3250741958618164\n",
      "Epoch [9233/10000], Loss: 0.3250640630722046\n",
      "Epoch [9234/10000], Loss: 0.325053870677948\n",
      "Epoch [9235/10000], Loss: 0.32504379749298096\n",
      "Epoch [9236/10000], Loss: 0.32503360509872437\n",
      "Epoch [9237/10000], Loss: 0.32502347230911255\n",
      "Epoch [9238/10000], Loss: 0.32501327991485596\n",
      "Epoch [9239/10000], Loss: 0.32500314712524414\n",
      "Epoch [9240/10000], Loss: 0.3249931335449219\n",
      "Epoch [9241/10000], Loss: 0.32498300075531006\n",
      "Epoch [9242/10000], Loss: 0.3249727487564087\n",
      "Epoch [9243/10000], Loss: 0.3249627351760864\n",
      "Epoch [9244/10000], Loss: 0.32495254278182983\n",
      "Epoch [9245/10000], Loss: 0.3249424695968628\n",
      "Epoch [9246/10000], Loss: 0.3249322772026062\n",
      "Epoch [9247/10000], Loss: 0.32492226362228394\n",
      "Epoch [9248/10000], Loss: 0.32491207122802734\n",
      "Epoch [9249/10000], Loss: 0.3249020576477051\n",
      "Epoch [9250/10000], Loss: 0.32489192485809326\n",
      "Epoch [9251/10000], Loss: 0.3248818516731262\n",
      "Epoch [9252/10000], Loss: 0.32487165927886963\n",
      "Epoch [9253/10000], Loss: 0.3248615860939026\n",
      "Epoch [9254/10000], Loss: 0.32485151290893555\n",
      "Epoch [9255/10000], Loss: 0.32484138011932373\n",
      "Epoch [9256/10000], Loss: 0.32483118772506714\n",
      "Epoch [9257/10000], Loss: 0.3248211741447449\n",
      "Epoch [9258/10000], Loss: 0.3248111605644226\n",
      "Epoch [9259/10000], Loss: 0.3248010277748108\n",
      "Epoch [9260/10000], Loss: 0.3247910141944885\n",
      "Epoch [9261/10000], Loss: 0.32478082180023193\n",
      "Epoch [9262/10000], Loss: 0.32477080821990967\n",
      "Epoch [9263/10000], Loss: 0.3247607350349426\n",
      "Epoch [9264/10000], Loss: 0.3247506022453308\n",
      "Epoch [9265/10000], Loss: 0.324740469455719\n",
      "Epoch [9266/10000], Loss: 0.32473039627075195\n",
      "Epoch [9267/10000], Loss: 0.3247203826904297\n",
      "Epoch [9268/10000], Loss: 0.32471030950546265\n",
      "Epoch [9269/10000], Loss: 0.32470017671585083\n",
      "Epoch [9270/10000], Loss: 0.32469016313552856\n",
      "Epoch [9271/10000], Loss: 0.32468003034591675\n",
      "Epoch [9272/10000], Loss: 0.3246699571609497\n",
      "Epoch [9273/10000], Loss: 0.32465988397598267\n",
      "Epoch [9274/10000], Loss: 0.3246498107910156\n",
      "Epoch [9275/10000], Loss: 0.32463979721069336\n",
      "Epoch [9276/10000], Loss: 0.3246297240257263\n",
      "Epoch [9277/10000], Loss: 0.3246196508407593\n",
      "Epoch [9278/10000], Loss: 0.324609637260437\n",
      "Epoch [9279/10000], Loss: 0.32459956407546997\n",
      "Epoch [9280/10000], Loss: 0.32458943128585815\n",
      "Epoch [9281/10000], Loss: 0.3245794177055359\n",
      "Epoch [9282/10000], Loss: 0.32456934452056885\n",
      "Epoch [9283/10000], Loss: 0.3245592713356018\n",
      "Epoch [9284/10000], Loss: 0.32454925775527954\n",
      "Epoch [9285/10000], Loss: 0.3245392441749573\n",
      "Epoch [9286/10000], Loss: 0.324529230594635\n",
      "Epoch [9287/10000], Loss: 0.32451915740966797\n",
      "Epoch [9288/10000], Loss: 0.3245091438293457\n",
      "Epoch [9289/10000], Loss: 0.32449913024902344\n",
      "Epoch [9290/10000], Loss: 0.3244890570640564\n",
      "Epoch [9291/10000], Loss: 0.3244791030883789\n",
      "Epoch [9292/10000], Loss: 0.32446902990341187\n",
      "Epoch [9293/10000], Loss: 0.3244590759277344\n",
      "Epoch [9294/10000], Loss: 0.32444900274276733\n",
      "Epoch [9295/10000], Loss: 0.32443898916244507\n",
      "Epoch [9296/10000], Loss: 0.324428915977478\n",
      "Epoch [9297/10000], Loss: 0.32441890239715576\n",
      "Epoch [9298/10000], Loss: 0.3244088292121887\n",
      "Epoch [9299/10000], Loss: 0.32439887523651123\n",
      "Epoch [9300/10000], Loss: 0.3243888020515442\n",
      "Epoch [9301/10000], Loss: 0.3243788480758667\n",
      "Epoch [9302/10000], Loss: 0.32436883449554443\n",
      "Epoch [9303/10000], Loss: 0.32435882091522217\n",
      "Epoch [9304/10000], Loss: 0.3243488669395447\n",
      "Epoch [9305/10000], Loss: 0.32433879375457764\n",
      "Epoch [9306/10000], Loss: 0.32432878017425537\n",
      "Epoch [9307/10000], Loss: 0.3243187665939331\n",
      "Epoch [9308/10000], Loss: 0.3243088126182556\n",
      "Epoch [9309/10000], Loss: 0.32429879903793335\n",
      "Epoch [9310/10000], Loss: 0.3242887854576111\n",
      "Epoch [9311/10000], Loss: 0.3242788314819336\n",
      "Epoch [9312/10000], Loss: 0.32426881790161133\n",
      "Epoch [9313/10000], Loss: 0.3242587447166443\n",
      "Epoch [9314/10000], Loss: 0.3242488503456116\n",
      "Epoch [9315/10000], Loss: 0.32423877716064453\n",
      "Epoch [9316/10000], Loss: 0.32422882318496704\n",
      "Epoch [9317/10000], Loss: 0.3242188096046448\n",
      "Epoch [9318/10000], Loss: 0.3242088556289673\n",
      "Epoch [9319/10000], Loss: 0.324198842048645\n",
      "Epoch [9320/10000], Loss: 0.32418882846832275\n",
      "Epoch [9321/10000], Loss: 0.3241789937019348\n",
      "Epoch [9322/10000], Loss: 0.32416898012161255\n",
      "Epoch [9323/10000], Loss: 0.3241589665412903\n",
      "Epoch [9324/10000], Loss: 0.324148952960968\n",
      "Epoch [9325/10000], Loss: 0.3241389989852905\n",
      "Epoch [9326/10000], Loss: 0.32412898540496826\n",
      "Epoch [9327/10000], Loss: 0.32411909103393555\n",
      "Epoch [9328/10000], Loss: 0.32410913705825806\n",
      "Epoch [9329/10000], Loss: 0.324099063873291\n",
      "Epoch [9330/10000], Loss: 0.3240891695022583\n",
      "Epoch [9331/10000], Loss: 0.32407915592193604\n",
      "Epoch [9332/10000], Loss: 0.3240693211555481\n",
      "Epoch [9333/10000], Loss: 0.32405930757522583\n",
      "Epoch [9334/10000], Loss: 0.32404935359954834\n",
      "Epoch [9335/10000], Loss: 0.3240394592285156\n",
      "Epoch [9336/10000], Loss: 0.32402950525283813\n",
      "Epoch [9337/10000], Loss: 0.32401949167251587\n",
      "Epoch [9338/10000], Loss: 0.32400959730148315\n",
      "Epoch [9339/10000], Loss: 0.3239995837211609\n",
      "Epoch [9340/10000], Loss: 0.32398974895477295\n",
      "Epoch [9341/10000], Loss: 0.3239797353744507\n",
      "Epoch [9342/10000], Loss: 0.32396984100341797\n",
      "Epoch [9343/10000], Loss: 0.3239598274230957\n",
      "Epoch [9344/10000], Loss: 0.3239498734474182\n",
      "Epoch [9345/10000], Loss: 0.3239399790763855\n",
      "Epoch [9346/10000], Loss: 0.3239300847053528\n",
      "Epoch [9347/10000], Loss: 0.3239201307296753\n",
      "Epoch [9348/10000], Loss: 0.3239102363586426\n",
      "Epoch [9349/10000], Loss: 0.3239002823829651\n",
      "Epoch [9350/10000], Loss: 0.3238903284072876\n",
      "Epoch [9351/10000], Loss: 0.3238803744316101\n",
      "Epoch [9352/10000], Loss: 0.32387053966522217\n",
      "Epoch [9353/10000], Loss: 0.3238605856895447\n",
      "Epoch [9354/10000], Loss: 0.32385069131851196\n",
      "Epoch [9355/10000], Loss: 0.3238407373428345\n",
      "Epoch [9356/10000], Loss: 0.323830783367157\n",
      "Epoch [9357/10000], Loss: 0.32382094860076904\n",
      "Epoch [9358/10000], Loss: 0.32381099462509155\n",
      "Epoch [9359/10000], Loss: 0.32380110025405884\n",
      "Epoch [9360/10000], Loss: 0.32379114627838135\n",
      "Epoch [9361/10000], Loss: 0.3237813115119934\n",
      "Epoch [9362/10000], Loss: 0.3237714171409607\n",
      "Epoch [9363/10000], Loss: 0.3237614035606384\n",
      "Epoch [9364/10000], Loss: 0.3237515687942505\n",
      "Epoch [9365/10000], Loss: 0.32374173402786255\n",
      "Epoch [9366/10000], Loss: 0.32373183965682983\n",
      "Epoch [9367/10000], Loss: 0.32372182607650757\n",
      "Epoch [9368/10000], Loss: 0.32371199131011963\n",
      "Epoch [9369/10000], Loss: 0.3237021565437317\n",
      "Epoch [9370/10000], Loss: 0.3236922025680542\n",
      "Epoch [9371/10000], Loss: 0.32368236780166626\n",
      "Epoch [9372/10000], Loss: 0.32367241382598877\n",
      "Epoch [9373/10000], Loss: 0.32366257905960083\n",
      "Epoch [9374/10000], Loss: 0.3236527442932129\n",
      "Epoch [9375/10000], Loss: 0.3236428499221802\n",
      "Epoch [9376/10000], Loss: 0.3236328959465027\n",
      "Epoch [9377/10000], Loss: 0.3236231207847595\n",
      "Epoch [9378/10000], Loss: 0.32361316680908203\n",
      "Epoch [9379/10000], Loss: 0.3236032724380493\n",
      "Epoch [9380/10000], Loss: 0.3235934376716614\n",
      "Epoch [9381/10000], Loss: 0.32358360290527344\n",
      "Epoch [9382/10000], Loss: 0.32357364892959595\n",
      "Epoch [9383/10000], Loss: 0.3235638737678528\n",
      "Epoch [9384/10000], Loss: 0.32355397939682007\n",
      "Epoch [9385/10000], Loss: 0.32354408502578735\n",
      "Epoch [9386/10000], Loss: 0.3235343098640442\n",
      "Epoch [9387/10000], Loss: 0.32352447509765625\n",
      "Epoch [9388/10000], Loss: 0.32351452112197876\n",
      "Epoch [9389/10000], Loss: 0.3235047459602356\n",
      "Epoch [9390/10000], Loss: 0.3234948515892029\n",
      "Epoch [9391/10000], Loss: 0.32348501682281494\n",
      "Epoch [9392/10000], Loss: 0.323475182056427\n",
      "Epoch [9393/10000], Loss: 0.32346534729003906\n",
      "Epoch [9394/10000], Loss: 0.32345545291900635\n",
      "Epoch [9395/10000], Loss: 0.3234456777572632\n",
      "Epoch [9396/10000], Loss: 0.32343584299087524\n",
      "Epoch [9397/10000], Loss: 0.32342594861984253\n",
      "Epoch [9398/10000], Loss: 0.3234161138534546\n",
      "Epoch [9399/10000], Loss: 0.32340627908706665\n",
      "Epoch [9400/10000], Loss: 0.3233965039253235\n",
      "Epoch [9401/10000], Loss: 0.32338666915893555\n",
      "Epoch [9402/10000], Loss: 0.3233768343925476\n",
      "Epoch [9403/10000], Loss: 0.32336699962615967\n",
      "Epoch [9404/10000], Loss: 0.32335710525512695\n",
      "Epoch [9405/10000], Loss: 0.32334738969802856\n",
      "Epoch [9406/10000], Loss: 0.32333749532699585\n",
      "Epoch [9407/10000], Loss: 0.3233276605606079\n",
      "Epoch [9408/10000], Loss: 0.32331782579421997\n",
      "Epoch [9409/10000], Loss: 0.32330799102783203\n",
      "Epoch [9410/10000], Loss: 0.3232980966567993\n",
      "Epoch [9411/10000], Loss: 0.3232883810997009\n",
      "Epoch [9412/10000], Loss: 0.32327860593795776\n",
      "Epoch [9413/10000], Loss: 0.3232687711715698\n",
      "Epoch [9414/10000], Loss: 0.3232589364051819\n",
      "Epoch [9415/10000], Loss: 0.3232491612434387\n",
      "Epoch [9416/10000], Loss: 0.3232393264770508\n",
      "Epoch [9417/10000], Loss: 0.3232296109199524\n",
      "Epoch [9418/10000], Loss: 0.32321977615356445\n",
      "Epoch [9419/10000], Loss: 0.32321006059646606\n",
      "Epoch [9420/10000], Loss: 0.3232001066207886\n",
      "Epoch [9421/10000], Loss: 0.3231903314590454\n",
      "Epoch [9422/10000], Loss: 0.323180615901947\n",
      "Epoch [9423/10000], Loss: 0.3231707215309143\n",
      "Epoch [9424/10000], Loss: 0.3231610059738159\n",
      "Epoch [9425/10000], Loss: 0.323151171207428\n",
      "Epoch [9426/10000], Loss: 0.3231413960456848\n",
      "Epoch [9427/10000], Loss: 0.3231315612792969\n",
      "Epoch [9428/10000], Loss: 0.3231218457221985\n",
      "Epoch [9429/10000], Loss: 0.32311201095581055\n",
      "Epoch [9430/10000], Loss: 0.32310229539871216\n",
      "Epoch [9431/10000], Loss: 0.3230924606323242\n",
      "Epoch [9432/10000], Loss: 0.3230826258659363\n",
      "Epoch [9433/10000], Loss: 0.3230729103088379\n",
      "Epoch [9434/10000], Loss: 0.32306307554244995\n",
      "Epoch [9435/10000], Loss: 0.32305335998535156\n",
      "Epoch [9436/10000], Loss: 0.3230435252189636\n",
      "Epoch [9437/10000], Loss: 0.32303380966186523\n",
      "Epoch [9438/10000], Loss: 0.32302409410476685\n",
      "Epoch [9439/10000], Loss: 0.3230142593383789\n",
      "Epoch [9440/10000], Loss: 0.3230045437812805\n",
      "Epoch [9441/10000], Loss: 0.32299476861953735\n",
      "Epoch [9442/10000], Loss: 0.3229849934577942\n",
      "Epoch [9443/10000], Loss: 0.322975218296051\n",
      "Epoch [9444/10000], Loss: 0.32296550273895264\n",
      "Epoch [9445/10000], Loss: 0.3229557275772095\n",
      "Epoch [9446/10000], Loss: 0.3229459524154663\n",
      "Epoch [9447/10000], Loss: 0.32293617725372314\n",
      "Epoch [9448/10000], Loss: 0.32292646169662476\n",
      "Epoch [9449/10000], Loss: 0.3229166865348816\n",
      "Epoch [9450/10000], Loss: 0.3229069113731384\n",
      "Epoch [9451/10000], Loss: 0.32289713621139526\n",
      "Epoch [9452/10000], Loss: 0.3228874206542969\n",
      "Epoch [9453/10000], Loss: 0.3228777050971985\n",
      "Epoch [9454/10000], Loss: 0.3228679895401001\n",
      "Epoch [9455/10000], Loss: 0.32285821437835693\n",
      "Epoch [9456/10000], Loss: 0.32284849882125854\n",
      "Epoch [9457/10000], Loss: 0.3228387236595154\n",
      "Epoch [9458/10000], Loss: 0.3228289484977722\n",
      "Epoch [9459/10000], Loss: 0.32281923294067383\n",
      "Epoch [9460/10000], Loss: 0.32280951738357544\n",
      "Epoch [9461/10000], Loss: 0.32279980182647705\n",
      "Epoch [9462/10000], Loss: 0.32279008626937866\n",
      "Epoch [9463/10000], Loss: 0.3227802515029907\n",
      "Epoch [9464/10000], Loss: 0.3227706551551819\n",
      "Epoch [9465/10000], Loss: 0.3227609395980835\n",
      "Epoch [9466/10000], Loss: 0.32275116443634033\n",
      "Epoch [9467/10000], Loss: 0.32274144887924194\n",
      "Epoch [9468/10000], Loss: 0.3227316737174988\n",
      "Epoch [9469/10000], Loss: 0.32272201776504517\n",
      "Epoch [9470/10000], Loss: 0.32271236181259155\n",
      "Epoch [9471/10000], Loss: 0.3227025866508484\n",
      "Epoch [9472/10000], Loss: 0.3226929306983948\n",
      "Epoch [9473/10000], Loss: 0.3226832151412964\n",
      "Epoch [9474/10000], Loss: 0.322673499584198\n",
      "Epoch [9475/10000], Loss: 0.3226637840270996\n",
      "Epoch [9476/10000], Loss: 0.3226540684700012\n",
      "Epoch [9477/10000], Loss: 0.3226444125175476\n",
      "Epoch [9478/10000], Loss: 0.3226346969604492\n",
      "Epoch [9479/10000], Loss: 0.3226248621940613\n",
      "Epoch [9480/10000], Loss: 0.32261520624160767\n",
      "Epoch [9481/10000], Loss: 0.32260555028915405\n",
      "Epoch [9482/10000], Loss: 0.3225959539413452\n",
      "Epoch [9483/10000], Loss: 0.3225862383842468\n",
      "Epoch [9484/10000], Loss: 0.32257646322250366\n",
      "Epoch [9485/10000], Loss: 0.3225668668746948\n",
      "Epoch [9486/10000], Loss: 0.32255715131759644\n",
      "Epoch [9487/10000], Loss: 0.32254737615585327\n",
      "Epoch [9488/10000], Loss: 0.32253777980804443\n",
      "Epoch [9489/10000], Loss: 0.32252800464630127\n",
      "Epoch [9490/10000], Loss: 0.32251834869384766\n",
      "Epoch [9491/10000], Loss: 0.32250869274139404\n",
      "Epoch [9492/10000], Loss: 0.32249915599823\n",
      "Epoch [9493/10000], Loss: 0.3224893808364868\n",
      "Epoch [9494/10000], Loss: 0.322479784488678\n",
      "Epoch [9495/10000], Loss: 0.3224700689315796\n",
      "Epoch [9496/10000], Loss: 0.32246047258377075\n",
      "Epoch [9497/10000], Loss: 0.3224506974220276\n",
      "Epoch [9498/10000], Loss: 0.322441041469574\n",
      "Epoch [9499/10000], Loss: 0.32243138551712036\n",
      "Epoch [9500/10000], Loss: 0.322421669960022\n",
      "Epoch [9501/10000], Loss: 0.32241207361221313\n",
      "Epoch [9502/10000], Loss: 0.32240235805511475\n",
      "Epoch [9503/10000], Loss: 0.3223927617073059\n",
      "Epoch [9504/10000], Loss: 0.3223830461502075\n",
      "Epoch [9505/10000], Loss: 0.3223733901977539\n",
      "Epoch [9506/10000], Loss: 0.3223637342453003\n",
      "Epoch [9507/10000], Loss: 0.32235413789749146\n",
      "Epoch [9508/10000], Loss: 0.32234448194503784\n",
      "Epoch [9509/10000], Loss: 0.32233482599258423\n",
      "Epoch [9510/10000], Loss: 0.3223251700401306\n",
      "Epoch [9511/10000], Loss: 0.322315514087677\n",
      "Epoch [9512/10000], Loss: 0.3223058581352234\n",
      "Epoch [9513/10000], Loss: 0.322296142578125\n",
      "Epoch [9514/10000], Loss: 0.32228654623031616\n",
      "Epoch [9515/10000], Loss: 0.3222769498825073\n",
      "Epoch [9516/10000], Loss: 0.3222673535346985\n",
      "Epoch [9517/10000], Loss: 0.3222576379776001\n",
      "Epoch [9518/10000], Loss: 0.32224804162979126\n",
      "Epoch [9519/10000], Loss: 0.3222384452819824\n",
      "Epoch [9520/10000], Loss: 0.3222287893295288\n",
      "Epoch [9521/10000], Loss: 0.32221919298171997\n",
      "Epoch [9522/10000], Loss: 0.32220953702926636\n",
      "Epoch [9523/10000], Loss: 0.32219988107681274\n",
      "Epoch [9524/10000], Loss: 0.3221902847290039\n",
      "Epoch [9525/10000], Loss: 0.3221806287765503\n",
      "Epoch [9526/10000], Loss: 0.3221709132194519\n",
      "Epoch [9527/10000], Loss: 0.32216137647628784\n",
      "Epoch [9528/10000], Loss: 0.32215166091918945\n",
      "Epoch [9529/10000], Loss: 0.3221421241760254\n",
      "Epoch [9530/10000], Loss: 0.32213252782821655\n",
      "Epoch [9531/10000], Loss: 0.3221229314804077\n",
      "Epoch [9532/10000], Loss: 0.3221133351325989\n",
      "Epoch [9533/10000], Loss: 0.32210367918014526\n",
      "Epoch [9534/10000], Loss: 0.3220940828323364\n",
      "Epoch [9535/10000], Loss: 0.3220844864845276\n",
      "Epoch [9536/10000], Loss: 0.32207489013671875\n",
      "Epoch [9537/10000], Loss: 0.3220652937889099\n",
      "Epoch [9538/10000], Loss: 0.3220556378364563\n",
      "Epoch [9539/10000], Loss: 0.3220459818840027\n",
      "Epoch [9540/10000], Loss: 0.3220364451408386\n",
      "Epoch [9541/10000], Loss: 0.3220268487930298\n",
      "Epoch [9542/10000], Loss: 0.32201725244522095\n",
      "Epoch [9543/10000], Loss: 0.3220076560974121\n",
      "Epoch [9544/10000], Loss: 0.32199805974960327\n",
      "Epoch [9545/10000], Loss: 0.32198846340179443\n",
      "Epoch [9546/10000], Loss: 0.32197892665863037\n",
      "Epoch [9547/10000], Loss: 0.32196933031082153\n",
      "Epoch [9548/10000], Loss: 0.3219596743583679\n",
      "Epoch [9549/10000], Loss: 0.32195013761520386\n",
      "Epoch [9550/10000], Loss: 0.321940541267395\n",
      "Epoch [9551/10000], Loss: 0.3219309449195862\n",
      "Epoch [9552/10000], Loss: 0.32192134857177734\n",
      "Epoch [9553/10000], Loss: 0.3219117522239685\n",
      "Epoch [9554/10000], Loss: 0.32190221548080444\n",
      "Epoch [9555/10000], Loss: 0.3218926191329956\n",
      "Epoch [9556/10000], Loss: 0.32188308238983154\n",
      "Epoch [9557/10000], Loss: 0.3218735456466675\n",
      "Epoch [9558/10000], Loss: 0.32186394929885864\n",
      "Epoch [9559/10000], Loss: 0.3218544125556946\n",
      "Epoch [9560/10000], Loss: 0.32184475660324097\n",
      "Epoch [9561/10000], Loss: 0.3218352198600769\n",
      "Epoch [9562/10000], Loss: 0.32182562351226807\n",
      "Epoch [9563/10000], Loss: 0.32181602716445923\n",
      "Epoch [9564/10000], Loss: 0.32180649042129517\n",
      "Epoch [9565/10000], Loss: 0.3217969536781311\n",
      "Epoch [9566/10000], Loss: 0.32178741693496704\n",
      "Epoch [9567/10000], Loss: 0.3217778205871582\n",
      "Epoch [9568/10000], Loss: 0.32176828384399414\n",
      "Epoch [9569/10000], Loss: 0.3217587471008301\n",
      "Epoch [9570/10000], Loss: 0.321749210357666\n",
      "Epoch [9571/10000], Loss: 0.3217396140098572\n",
      "Epoch [9572/10000], Loss: 0.3217300772666931\n",
      "Epoch [9573/10000], Loss: 0.3217204809188843\n",
      "Epoch [9574/10000], Loss: 0.321711003780365\n",
      "Epoch [9575/10000], Loss: 0.32170140743255615\n",
      "Epoch [9576/10000], Loss: 0.3216918706893921\n",
      "Epoch [9577/10000], Loss: 0.321682333946228\n",
      "Epoch [9578/10000], Loss: 0.32167279720306396\n",
      "Epoch [9579/10000], Loss: 0.3216633200645447\n",
      "Epoch [9580/10000], Loss: 0.32165372371673584\n",
      "Epoch [9581/10000], Loss: 0.321644127368927\n",
      "Epoch [9582/10000], Loss: 0.3216346502304077\n",
      "Epoch [9583/10000], Loss: 0.32162511348724365\n",
      "Epoch [9584/10000], Loss: 0.3216155767440796\n",
      "Epoch [9585/10000], Loss: 0.32160598039627075\n",
      "Epoch [9586/10000], Loss: 0.32159656286239624\n",
      "Epoch [9587/10000], Loss: 0.3215870261192322\n",
      "Epoch [9588/10000], Loss: 0.3215774893760681\n",
      "Epoch [9589/10000], Loss: 0.32156795263290405\n",
      "Epoch [9590/10000], Loss: 0.32155841588974\n",
      "Epoch [9591/10000], Loss: 0.3215488791465759\n",
      "Epoch [9592/10000], Loss: 0.32153934240341187\n",
      "Epoch [9593/10000], Loss: 0.3215298056602478\n",
      "Epoch [9594/10000], Loss: 0.3215203285217285\n",
      "Epoch [9595/10000], Loss: 0.32151079177856445\n",
      "Epoch [9596/10000], Loss: 0.32150131464004517\n",
      "Epoch [9597/10000], Loss: 0.3214917778968811\n",
      "Epoch [9598/10000], Loss: 0.3214823007583618\n",
      "Epoch [9599/10000], Loss: 0.321472704410553\n",
      "Epoch [9600/10000], Loss: 0.3214632272720337\n",
      "Epoch [9601/10000], Loss: 0.3214537501335144\n",
      "Epoch [9602/10000], Loss: 0.3214443325996399\n",
      "Epoch [9603/10000], Loss: 0.32143479585647583\n",
      "Epoch [9604/10000], Loss: 0.32142531871795654\n",
      "Epoch [9605/10000], Loss: 0.3214157819747925\n",
      "Epoch [9606/10000], Loss: 0.3214062452316284\n",
      "Epoch [9607/10000], Loss: 0.32139676809310913\n",
      "Epoch [9608/10000], Loss: 0.32138729095458984\n",
      "Epoch [9609/10000], Loss: 0.3213777542114258\n",
      "Epoch [9610/10000], Loss: 0.3213682770729065\n",
      "Epoch [9611/10000], Loss: 0.3213587999343872\n",
      "Epoch [9612/10000], Loss: 0.32134926319122314\n",
      "Epoch [9613/10000], Loss: 0.3213397264480591\n",
      "Epoch [9614/10000], Loss: 0.32133036851882935\n",
      "Epoch [9615/10000], Loss: 0.3213208317756653\n",
      "Epoch [9616/10000], Loss: 0.3213112950325012\n",
      "Epoch [9617/10000], Loss: 0.3213018774986267\n",
      "Epoch [9618/10000], Loss: 0.3212924003601074\n",
      "Epoch [9619/10000], Loss: 0.32128286361694336\n",
      "Epoch [9620/10000], Loss: 0.3212733864784241\n",
      "Epoch [9621/10000], Loss: 0.3212639093399048\n",
      "Epoch [9622/10000], Loss: 0.3212543725967407\n",
      "Epoch [9623/10000], Loss: 0.321245014667511\n",
      "Epoch [9624/10000], Loss: 0.3212355375289917\n",
      "Epoch [9625/10000], Loss: 0.3212260603904724\n",
      "Epoch [9626/10000], Loss: 0.3212165832519531\n",
      "Epoch [9627/10000], Loss: 0.32120710611343384\n",
      "Epoch [9628/10000], Loss: 0.3211975693702698\n",
      "Epoch [9629/10000], Loss: 0.32118815183639526\n",
      "Epoch [9630/10000], Loss: 0.3211786150932312\n",
      "Epoch [9631/10000], Loss: 0.3211691975593567\n",
      "Epoch [9632/10000], Loss: 0.3211597204208374\n",
      "Epoch [9633/10000], Loss: 0.3211502432823181\n",
      "Epoch [9634/10000], Loss: 0.3211408257484436\n",
      "Epoch [9635/10000], Loss: 0.3211313486099243\n",
      "Epoch [9636/10000], Loss: 0.32112187147140503\n",
      "Epoch [9637/10000], Loss: 0.3211124539375305\n",
      "Epoch [9638/10000], Loss: 0.32110297679901123\n",
      "Epoch [9639/10000], Loss: 0.32109349966049194\n",
      "Epoch [9640/10000], Loss: 0.32108402252197266\n",
      "Epoch [9641/10000], Loss: 0.32107454538345337\n",
      "Epoch [9642/10000], Loss: 0.3210652470588684\n",
      "Epoch [9643/10000], Loss: 0.32105571031570435\n",
      "Epoch [9644/10000], Loss: 0.32104629278182983\n",
      "Epoch [9645/10000], Loss: 0.3210368752479553\n",
      "Epoch [9646/10000], Loss: 0.32102739810943604\n",
      "Epoch [9647/10000], Loss: 0.32101792097091675\n",
      "Epoch [9648/10000], Loss: 0.32100850343704224\n",
      "Epoch [9649/10000], Loss: 0.32099902629852295\n",
      "Epoch [9650/10000], Loss: 0.32098960876464844\n",
      "Epoch [9651/10000], Loss: 0.32098013162612915\n",
      "Epoch [9652/10000], Loss: 0.32097071409225464\n",
      "Epoch [9653/10000], Loss: 0.3209612965583801\n",
      "Epoch [9654/10000], Loss: 0.32095181941986084\n",
      "Epoch [9655/10000], Loss: 0.32094240188598633\n",
      "Epoch [9656/10000], Loss: 0.3209329843521118\n",
      "Epoch [9657/10000], Loss: 0.3209236264228821\n",
      "Epoch [9658/10000], Loss: 0.3209141492843628\n",
      "Epoch [9659/10000], Loss: 0.3209046721458435\n",
      "Epoch [9660/10000], Loss: 0.320895254611969\n",
      "Epoch [9661/10000], Loss: 0.3208858370780945\n",
      "Epoch [9662/10000], Loss: 0.32087641954421997\n",
      "Epoch [9663/10000], Loss: 0.32086700201034546\n",
      "Epoch [9664/10000], Loss: 0.32085758447647095\n",
      "Epoch [9665/10000], Loss: 0.32084816694259644\n",
      "Epoch [9666/10000], Loss: 0.3208387494087219\n",
      "Epoch [9667/10000], Loss: 0.3208293318748474\n",
      "Epoch [9668/10000], Loss: 0.3208198547363281\n",
      "Epoch [9669/10000], Loss: 0.3208104372024536\n",
      "Epoch [9670/10000], Loss: 0.32080113887786865\n",
      "Epoch [9671/10000], Loss: 0.32079166173934937\n",
      "Epoch [9672/10000], Loss: 0.32078230381011963\n",
      "Epoch [9673/10000], Loss: 0.3207729458808899\n",
      "Epoch [9674/10000], Loss: 0.3207634687423706\n",
      "Epoch [9675/10000], Loss: 0.3207540512084961\n",
      "Epoch [9676/10000], Loss: 0.3207445740699768\n",
      "Epoch [9677/10000], Loss: 0.32073527574539185\n",
      "Epoch [9678/10000], Loss: 0.32072585821151733\n",
      "Epoch [9679/10000], Loss: 0.3207164406776428\n",
      "Epoch [9680/10000], Loss: 0.3207070231437683\n",
      "Epoch [9681/10000], Loss: 0.3206976652145386\n",
      "Epoch [9682/10000], Loss: 0.32068824768066406\n",
      "Epoch [9683/10000], Loss: 0.32067883014678955\n",
      "Epoch [9684/10000], Loss: 0.32066935300827026\n",
      "Epoch [9685/10000], Loss: 0.3206601142883301\n",
      "Epoch [9686/10000], Loss: 0.3206506371498108\n",
      "Epoch [9687/10000], Loss: 0.3206412196159363\n",
      "Epoch [9688/10000], Loss: 0.32063186168670654\n",
      "Epoch [9689/10000], Loss: 0.3206225037574768\n",
      "Epoch [9690/10000], Loss: 0.3206130266189575\n",
      "Epoch [9691/10000], Loss: 0.32060372829437256\n",
      "Epoch [9692/10000], Loss: 0.32059431076049805\n",
      "Epoch [9693/10000], Loss: 0.3205849528312683\n",
      "Epoch [9694/10000], Loss: 0.3205755352973938\n",
      "Epoch [9695/10000], Loss: 0.3205661177635193\n",
      "Epoch [9696/10000], Loss: 0.32055675983428955\n",
      "Epoch [9697/10000], Loss: 0.3205474615097046\n",
      "Epoch [9698/10000], Loss: 0.3205379843711853\n",
      "Epoch [9699/10000], Loss: 0.32052862644195557\n",
      "Epoch [9700/10000], Loss: 0.3205193281173706\n",
      "Epoch [9701/10000], Loss: 0.3205098509788513\n",
      "Epoch [9702/10000], Loss: 0.3205004930496216\n",
      "Epoch [9703/10000], Loss: 0.32049113512039185\n",
      "Epoch [9704/10000], Loss: 0.32048171758651733\n",
      "Epoch [9705/10000], Loss: 0.3204723596572876\n",
      "Epoch [9706/10000], Loss: 0.32046300172805786\n",
      "Epoch [9707/10000], Loss: 0.3204536437988281\n",
      "Epoch [9708/10000], Loss: 0.3204442262649536\n",
      "Epoch [9709/10000], Loss: 0.32043492794036865\n",
      "Epoch [9710/10000], Loss: 0.32042551040649414\n",
      "Epoch [9711/10000], Loss: 0.32041609287261963\n",
      "Epoch [9712/10000], Loss: 0.32040679454803467\n",
      "Epoch [9713/10000], Loss: 0.32039743661880493\n",
      "Epoch [9714/10000], Loss: 0.3203880190849304\n",
      "Epoch [9715/10000], Loss: 0.32037872076034546\n",
      "Epoch [9716/10000], Loss: 0.32036930322647095\n",
      "Epoch [9717/10000], Loss: 0.320360004901886\n",
      "Epoch [9718/10000], Loss: 0.32035064697265625\n",
      "Epoch [9719/10000], Loss: 0.32034122943878174\n",
      "Epoch [9720/10000], Loss: 0.3203319311141968\n",
      "Epoch [9721/10000], Loss: 0.32032257318496704\n",
      "Epoch [9722/10000], Loss: 0.32031315565109253\n",
      "Epoch [9723/10000], Loss: 0.32030391693115234\n",
      "Epoch [9724/10000], Loss: 0.32029443979263306\n",
      "Epoch [9725/10000], Loss: 0.3202851414680481\n",
      "Epoch [9726/10000], Loss: 0.32027578353881836\n",
      "Epoch [9727/10000], Loss: 0.3202664256095886\n",
      "Epoch [9728/10000], Loss: 0.32025712728500366\n",
      "Epoch [9729/10000], Loss: 0.3202477693557739\n",
      "Epoch [9730/10000], Loss: 0.3202384114265442\n",
      "Epoch [9731/10000], Loss: 0.32022911310195923\n",
      "Epoch [9732/10000], Loss: 0.32021963596343994\n",
      "Epoch [9733/10000], Loss: 0.320210337638855\n",
      "Epoch [9734/10000], Loss: 0.32020097970962524\n",
      "Epoch [9735/10000], Loss: 0.3201916813850403\n",
      "Epoch [9736/10000], Loss: 0.32018232345581055\n",
      "Epoch [9737/10000], Loss: 0.3201729655265808\n",
      "Epoch [9738/10000], Loss: 0.32016366720199585\n",
      "Epoch [9739/10000], Loss: 0.32015424966812134\n",
      "Epoch [9740/10000], Loss: 0.3201450705528259\n",
      "Epoch [9741/10000], Loss: 0.32013559341430664\n",
      "Epoch [9742/10000], Loss: 0.3201262354850769\n",
      "Epoch [9743/10000], Loss: 0.3201169967651367\n",
      "Epoch [9744/10000], Loss: 0.320107638835907\n",
      "Epoch [9745/10000], Loss: 0.320098340511322\n",
      "Epoch [9746/10000], Loss: 0.3200889825820923\n",
      "Epoch [9747/10000], Loss: 0.3200796842575073\n",
      "Epoch [9748/10000], Loss: 0.3200703263282776\n",
      "Epoch [9749/10000], Loss: 0.3200610280036926\n",
      "Epoch [9750/10000], Loss: 0.3200516700744629\n",
      "Epoch [9751/10000], Loss: 0.32004231214523315\n",
      "Epoch [9752/10000], Loss: 0.3200330138206482\n",
      "Epoch [9753/10000], Loss: 0.32002365589141846\n",
      "Epoch [9754/10000], Loss: 0.32001441717147827\n",
      "Epoch [9755/10000], Loss: 0.32000499963760376\n",
      "Epoch [9756/10000], Loss: 0.31999558210372925\n",
      "Epoch [9757/10000], Loss: 0.31998634338378906\n",
      "Epoch [9758/10000], Loss: 0.31997716426849365\n",
      "Epoch [9759/10000], Loss: 0.31996768712997437\n",
      "Epoch [9760/10000], Loss: 0.31995850801467896\n",
      "Epoch [9761/10000], Loss: 0.31994909048080444\n",
      "Epoch [9762/10000], Loss: 0.3199397325515747\n",
      "Epoch [9763/10000], Loss: 0.3199304938316345\n",
      "Epoch [9764/10000], Loss: 0.31992107629776\n",
      "Epoch [9765/10000], Loss: 0.3199118375778198\n",
      "Epoch [9766/10000], Loss: 0.31990253925323486\n",
      "Epoch [9767/10000], Loss: 0.3198932409286499\n",
      "Epoch [9768/10000], Loss: 0.31988388299942017\n",
      "Epoch [9769/10000], Loss: 0.3198745846748352\n",
      "Epoch [9770/10000], Loss: 0.31986522674560547\n",
      "Epoch [9771/10000], Loss: 0.31985604763031006\n",
      "Epoch [9772/10000], Loss: 0.3198466897010803\n",
      "Epoch [9773/10000], Loss: 0.31983739137649536\n",
      "Epoch [9774/10000], Loss: 0.3198280930519104\n",
      "Epoch [9775/10000], Loss: 0.31981879472732544\n",
      "Epoch [9776/10000], Loss: 0.3198094367980957\n",
      "Epoch [9777/10000], Loss: 0.3198001980781555\n",
      "Epoch [9778/10000], Loss: 0.3197908401489258\n",
      "Epoch [9779/10000], Loss: 0.31978148221969604\n",
      "Epoch [9780/10000], Loss: 0.31977224349975586\n",
      "Epoch [9781/10000], Loss: 0.3197628855705261\n",
      "Epoch [9782/10000], Loss: 0.31975364685058594\n",
      "Epoch [9783/10000], Loss: 0.3197442889213562\n",
      "Epoch [9784/10000], Loss: 0.31973499059677124\n",
      "Epoch [9785/10000], Loss: 0.3197256922721863\n",
      "Epoch [9786/10000], Loss: 0.3197163939476013\n",
      "Epoch [9787/10000], Loss: 0.31970709562301636\n",
      "Epoch [9788/10000], Loss: 0.31969785690307617\n",
      "Epoch [9789/10000], Loss: 0.31968849897384644\n",
      "Epoch [9790/10000], Loss: 0.31967926025390625\n",
      "Epoch [9791/10000], Loss: 0.3196699023246765\n",
      "Epoch [9792/10000], Loss: 0.31966060400009155\n",
      "Epoch [9793/10000], Loss: 0.31965142488479614\n",
      "Epoch [9794/10000], Loss: 0.3196420669555664\n",
      "Epoch [9795/10000], Loss: 0.31963270902633667\n",
      "Epoch [9796/10000], Loss: 0.31962352991104126\n",
      "Epoch [9797/10000], Loss: 0.319614052772522\n",
      "Epoch [9798/10000], Loss: 0.31960487365722656\n",
      "Epoch [9799/10000], Loss: 0.3195956349372864\n",
      "Epoch [9800/10000], Loss: 0.31958627700805664\n",
      "Epoch [9801/10000], Loss: 0.31957709789276123\n",
      "Epoch [9802/10000], Loss: 0.3195677399635315\n",
      "Epoch [9803/10000], Loss: 0.31955844163894653\n",
      "Epoch [9804/10000], Loss: 0.3195491433143616\n",
      "Epoch [9805/10000], Loss: 0.3195398449897766\n",
      "Epoch [9806/10000], Loss: 0.31953054666519165\n",
      "Epoch [9807/10000], Loss: 0.31952130794525146\n",
      "Epoch [9808/10000], Loss: 0.3195120096206665\n",
      "Epoch [9809/10000], Loss: 0.31950271129608154\n",
      "Epoch [9810/10000], Loss: 0.3194934129714966\n",
      "Epoch [9811/10000], Loss: 0.3194841742515564\n",
      "Epoch [9812/10000], Loss: 0.31947481632232666\n",
      "Epoch [9813/10000], Loss: 0.3194655776023865\n",
      "Epoch [9814/10000], Loss: 0.3194562792778015\n",
      "Epoch [9815/10000], Loss: 0.31944698095321655\n",
      "Epoch [9816/10000], Loss: 0.31943780183792114\n",
      "Epoch [9817/10000], Loss: 0.3194284439086914\n",
      "Epoch [9818/10000], Loss: 0.3194192051887512\n",
      "Epoch [9819/10000], Loss: 0.31940990686416626\n",
      "Epoch [9820/10000], Loss: 0.3194006085395813\n",
      "Epoch [9821/10000], Loss: 0.31939131021499634\n",
      "Epoch [9822/10000], Loss: 0.3193821310997009\n",
      "Epoch [9823/10000], Loss: 0.3193727731704712\n",
      "Epoch [9824/10000], Loss: 0.319363534450531\n",
      "Epoch [9825/10000], Loss: 0.31935423612594604\n",
      "Epoch [9826/10000], Loss: 0.31934499740600586\n",
      "Epoch [9827/10000], Loss: 0.3193356394767761\n",
      "Epoch [9828/10000], Loss: 0.31932634115219116\n",
      "Epoch [9829/10000], Loss: 0.3193172216415405\n",
      "Epoch [9830/10000], Loss: 0.319307804107666\n",
      "Epoch [9831/10000], Loss: 0.3192986249923706\n",
      "Epoch [9832/10000], Loss: 0.3192893862724304\n",
      "Epoch [9833/10000], Loss: 0.3192800283432007\n",
      "Epoch [9834/10000], Loss: 0.3192707300186157\n",
      "Epoch [9835/10000], Loss: 0.3192615509033203\n",
      "Epoch [9836/10000], Loss: 0.3192523121833801\n",
      "Epoch [9837/10000], Loss: 0.3192429542541504\n",
      "Epoch [9838/10000], Loss: 0.3192337155342102\n",
      "Epoch [9839/10000], Loss: 0.31922447681427\n",
      "Epoch [9840/10000], Loss: 0.3192151188850403\n",
      "Epoch [9841/10000], Loss: 0.3192059397697449\n",
      "Epoch [9842/10000], Loss: 0.3191966414451599\n",
      "Epoch [9843/10000], Loss: 0.31918734312057495\n",
      "Epoch [9844/10000], Loss: 0.31917804479599\n",
      "Epoch [9845/10000], Loss: 0.31916892528533936\n",
      "Epoch [9846/10000], Loss: 0.3191595673561096\n",
      "Epoch [9847/10000], Loss: 0.31915026903152466\n",
      "Epoch [9848/10000], Loss: 0.3191410303115845\n",
      "Epoch [9849/10000], Loss: 0.31913185119628906\n",
      "Epoch [9850/10000], Loss: 0.3191224932670593\n",
      "Epoch [9851/10000], Loss: 0.31911325454711914\n",
      "Epoch [9852/10000], Loss: 0.31910401582717896\n",
      "Epoch [9853/10000], Loss: 0.319094717502594\n",
      "Epoch [9854/10000], Loss: 0.3190855383872986\n",
      "Epoch [9855/10000], Loss: 0.3190762400627136\n",
      "Epoch [9856/10000], Loss: 0.31906700134277344\n",
      "Epoch [9857/10000], Loss: 0.3190576434135437\n",
      "Epoch [9858/10000], Loss: 0.3190484046936035\n",
      "Epoch [9859/10000], Loss: 0.31903916597366333\n",
      "Epoch [9860/10000], Loss: 0.31902992725372314\n",
      "Epoch [9861/10000], Loss: 0.3190206289291382\n",
      "Epoch [9862/10000], Loss: 0.319011390209198\n",
      "Epoch [9863/10000], Loss: 0.3190021514892578\n",
      "Epoch [9864/10000], Loss: 0.3189929127693176\n",
      "Epoch [9865/10000], Loss: 0.31898361444473267\n",
      "Epoch [9866/10000], Loss: 0.3189743161201477\n",
      "Epoch [9867/10000], Loss: 0.3189650774002075\n",
      "Epoch [9868/10000], Loss: 0.31895583868026733\n",
      "Epoch [9869/10000], Loss: 0.3189465403556824\n",
      "Epoch [9870/10000], Loss: 0.3189373016357422\n",
      "Epoch [9871/10000], Loss: 0.318928062915802\n",
      "Epoch [9872/10000], Loss: 0.3189188241958618\n",
      "Epoch [9873/10000], Loss: 0.31890958547592163\n",
      "Epoch [9874/10000], Loss: 0.31890028715133667\n",
      "Epoch [9875/10000], Loss: 0.3188909888267517\n",
      "Epoch [9876/10000], Loss: 0.3188818693161011\n",
      "Epoch [9877/10000], Loss: 0.31887251138687134\n",
      "Epoch [9878/10000], Loss: 0.3188633322715759\n",
      "Epoch [9879/10000], Loss: 0.31885403394699097\n",
      "Epoch [9880/10000], Loss: 0.318844735622406\n",
      "Epoch [9881/10000], Loss: 0.3188355565071106\n",
      "Epoch [9882/10000], Loss: 0.31882625818252563\n",
      "Epoch [9883/10000], Loss: 0.31881701946258545\n",
      "Epoch [9884/10000], Loss: 0.3188077211380005\n",
      "Epoch [9885/10000], Loss: 0.3187984228134155\n",
      "Epoch [9886/10000], Loss: 0.3187893033027649\n",
      "Epoch [9887/10000], Loss: 0.31878000497817993\n",
      "Epoch [9888/10000], Loss: 0.31877076625823975\n",
      "Epoch [9889/10000], Loss: 0.31876152753829956\n",
      "Epoch [9890/10000], Loss: 0.3187522888183594\n",
      "Epoch [9891/10000], Loss: 0.3187430500984192\n",
      "Epoch [9892/10000], Loss: 0.31873375177383423\n",
      "Epoch [9893/10000], Loss: 0.31872451305389404\n",
      "Epoch [9894/10000], Loss: 0.3187152147293091\n",
      "Epoch [9895/10000], Loss: 0.3187059760093689\n",
      "Epoch [9896/10000], Loss: 0.3186967968940735\n",
      "Epoch [9897/10000], Loss: 0.3186874985694885\n",
      "Epoch [9898/10000], Loss: 0.3186783194541931\n",
      "Epoch [9899/10000], Loss: 0.3186689615249634\n",
      "Epoch [9900/10000], Loss: 0.31865978240966797\n",
      "Epoch [9901/10000], Loss: 0.3186505436897278\n",
      "Epoch [9902/10000], Loss: 0.3186413049697876\n",
      "Epoch [9903/10000], Loss: 0.3186320662498474\n",
      "Epoch [9904/10000], Loss: 0.31862276792526245\n",
      "Epoch [9905/10000], Loss: 0.31861358880996704\n",
      "Epoch [9906/10000], Loss: 0.3186042904853821\n",
      "Epoch [9907/10000], Loss: 0.3185950517654419\n",
      "Epoch [9908/10000], Loss: 0.3185858130455017\n",
      "Epoch [9909/10000], Loss: 0.31857651472091675\n",
      "Epoch [9910/10000], Loss: 0.31856733560562134\n",
      "Epoch [9911/10000], Loss: 0.3185581564903259\n",
      "Epoch [9912/10000], Loss: 0.31854885816574097\n",
      "Epoch [9913/10000], Loss: 0.3185396194458008\n",
      "Epoch [9914/10000], Loss: 0.31853044033050537\n",
      "Epoch [9915/10000], Loss: 0.3185211420059204\n",
      "Epoch [9916/10000], Loss: 0.318511962890625\n",
      "Epoch [9917/10000], Loss: 0.3185027241706848\n",
      "Epoch [9918/10000], Loss: 0.31849342584609985\n",
      "Epoch [9919/10000], Loss: 0.31848418712615967\n",
      "Epoch [9920/10000], Loss: 0.3184749484062195\n",
      "Epoch [9921/10000], Loss: 0.3184657096862793\n",
      "Epoch [9922/10000], Loss: 0.3184564709663391\n",
      "Epoch [9923/10000], Loss: 0.31844717264175415\n",
      "Epoch [9924/10000], Loss: 0.3184380531311035\n",
      "Epoch [9925/10000], Loss: 0.31842881441116333\n",
      "Epoch [9926/10000], Loss: 0.31841951608657837\n",
      "Epoch [9927/10000], Loss: 0.3184102773666382\n",
      "Epoch [9928/10000], Loss: 0.3184009790420532\n",
      "Epoch [9929/10000], Loss: 0.3183917999267578\n",
      "Epoch [9930/10000], Loss: 0.3183826208114624\n",
      "Epoch [9931/10000], Loss: 0.31837326288223267\n",
      "Epoch [9932/10000], Loss: 0.31836408376693726\n",
      "Epoch [9933/10000], Loss: 0.31835490465164185\n",
      "Epoch [9934/10000], Loss: 0.3183456063270569\n",
      "Epoch [9935/10000], Loss: 0.3183364272117615\n",
      "Epoch [9936/10000], Loss: 0.3183271288871765\n",
      "Epoch [9937/10000], Loss: 0.31831789016723633\n",
      "Epoch [9938/10000], Loss: 0.31830865144729614\n",
      "Epoch [9939/10000], Loss: 0.31829947233200073\n",
      "Epoch [9940/10000], Loss: 0.31829017400741577\n",
      "Epoch [9941/10000], Loss: 0.31828105449676514\n",
      "Epoch [9942/10000], Loss: 0.3182717561721802\n",
      "Epoch [9943/10000], Loss: 0.31826251745224\n",
      "Epoch [9944/10000], Loss: 0.31825321912765503\n",
      "Epoch [9945/10000], Loss: 0.3182440400123596\n",
      "Epoch [9946/10000], Loss: 0.3182348608970642\n",
      "Epoch [9947/10000], Loss: 0.31822556257247925\n",
      "Epoch [9948/10000], Loss: 0.31821632385253906\n",
      "Epoch [9949/10000], Loss: 0.3182070851325989\n",
      "Epoch [9950/10000], Loss: 0.3181978464126587\n",
      "Epoch [9951/10000], Loss: 0.3181886672973633\n",
      "Epoch [9952/10000], Loss: 0.31817948818206787\n",
      "Epoch [9953/10000], Loss: 0.31817013025283813\n",
      "Epoch [9954/10000], Loss: 0.3181609511375427\n",
      "Epoch [9955/10000], Loss: 0.3181517720222473\n",
      "Epoch [9956/10000], Loss: 0.31814253330230713\n",
      "Epoch [9957/10000], Loss: 0.31813329458236694\n",
      "Epoch [9958/10000], Loss: 0.31812411546707153\n",
      "Epoch [9959/10000], Loss: 0.3181147575378418\n",
      "Epoch [9960/10000], Loss: 0.3181055784225464\n",
      "Epoch [9961/10000], Loss: 0.3180963397026062\n",
      "Epoch [9962/10000], Loss: 0.318087100982666\n",
      "Epoch [9963/10000], Loss: 0.3180779218673706\n",
      "Epoch [9964/10000], Loss: 0.3180686831474304\n",
      "Epoch [9965/10000], Loss: 0.31805944442749023\n",
      "Epoch [9966/10000], Loss: 0.3180502653121948\n",
      "Epoch [9967/10000], Loss: 0.31804102659225464\n",
      "Epoch [9968/10000], Loss: 0.31803178787231445\n",
      "Epoch [9969/10000], Loss: 0.31802260875701904\n",
      "Epoch [9970/10000], Loss: 0.3180133104324341\n",
      "Epoch [9971/10000], Loss: 0.3180040717124939\n",
      "Epoch [9972/10000], Loss: 0.31799495220184326\n",
      "Epoch [9973/10000], Loss: 0.3179856538772583\n",
      "Epoch [9974/10000], Loss: 0.3179764747619629\n",
      "Epoch [9975/10000], Loss: 0.3179672360420227\n",
      "Epoch [9976/10000], Loss: 0.3179579973220825\n",
      "Epoch [9977/10000], Loss: 0.31794875860214233\n",
      "Epoch [9978/10000], Loss: 0.3179394602775574\n",
      "Epoch [9979/10000], Loss: 0.31793034076690674\n",
      "Epoch [9980/10000], Loss: 0.31792110204696655\n",
      "Epoch [9981/10000], Loss: 0.31791192293167114\n",
      "Epoch [9982/10000], Loss: 0.3179026246070862\n",
      "Epoch [9983/10000], Loss: 0.31789344549179077\n",
      "Epoch [9984/10000], Loss: 0.31788426637649536\n",
      "Epoch [9985/10000], Loss: 0.3178750276565552\n",
      "Epoch [9986/10000], Loss: 0.317865788936615\n",
      "Epoch [9987/10000], Loss: 0.3178566098213196\n",
      "Epoch [9988/10000], Loss: 0.3178473711013794\n",
      "Epoch [9989/10000], Loss: 0.3178381323814392\n",
      "Epoch [9990/10000], Loss: 0.3178290128707886\n",
      "Epoch [9991/10000], Loss: 0.31781965494155884\n",
      "Epoch [9992/10000], Loss: 0.31781041622161865\n",
      "Epoch [9993/10000], Loss: 0.317801296710968\n",
      "Epoch [9994/10000], Loss: 0.31779205799102783\n",
      "Epoch [9995/10000], Loss: 0.31778275966644287\n",
      "Epoch [9996/10000], Loss: 0.317773699760437\n",
      "Epoch [9997/10000], Loss: 0.31776440143585205\n",
      "Epoch [9998/10000], Loss: 0.3177551031112671\n",
      "Epoch [9999/10000], Loss: 0.31774604320526123\n",
      "Epoch [10000/10000], Loss: 0.31773674488067627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Convert DataFrames or arrays to PyTorch tensors if they are not already\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32) if isinstance(X_train, pd.DataFrame) else torch.from_numpy(X_train).float()\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.int64) if isinstance(Y_train, pd.Series) else torch.from_numpy(Y_train).long()\n",
    "\n",
    "# Convert Y_train_tensor to one-hot encoding for multi-class\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    outputs = model(X_train_tensor)  # Forward pass\n",
    "    \n",
    "    loss = soft_f1_loss(Y_train_one_hot.float(), outputs)  # Calculate loss using the custom F1 loss function\n",
    "    \n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:  # Display loss every epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.62        81\n",
      "           1       0.61      0.55      0.58       127\n",
      "           2       0.82      0.87      0.84       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.84      0.67      0.74        48\n",
      "           5       0.79      0.78      0.78        72\n",
      "           6       0.90      0.73      0.81       178\n",
      "           7       0.80      0.69      0.74        54\n",
      "           8       0.92      0.67      0.77        18\n",
      "           9       0.78      0.79      0.79        91\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.63      0.77      0.69       286\n",
      "          12       0.88      0.69      0.78       110\n",
      "          13       0.76      0.73      0.75       258\n",
      "          14       0.78      0.78      0.78       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.57      0.52      0.54        33\n",
      "          17       0.77      0.38      0.51        26\n",
      "          18       0.83      0.84      0.83       383\n",
      "          19       0.79      0.79      0.79       611\n",
      "          20       0.64      0.70      0.67        98\n",
      "          21       0.83      0.86      0.85      1636\n",
      "          22       0.61      0.64      0.62       264\n",
      "          23       0.82      0.88      0.85        16\n",
      "          24       0.60      0.60      0.60        89\n",
      "          25       0.66      0.56      0.60       183\n",
      "          26       0.49      0.60      0.54       227\n",
      "          27       0.61      0.79      0.69        14\n",
      "\n",
      "    accuracy                           0.76      5550\n",
      "   macro avg       0.66      0.62      0.63      5550\n",
      "weighted avg       0.75      0.76      0.76      5550\n",
      "\n",
      "Exact F1 Score (micro): 0.7609009009009009\n",
      "Exact F1 Score (macro): 0.6349214426998113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. REGRESSION WITH CUSTOM LOSS macro F1**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [2/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [3/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [4/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [5/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [6/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [7/10000], Loss: 0.3177, macro F1 Train: 0.7090, macro F1 Test: 0.6349\n",
      "Epoch [8/10000], Loss: 0.3176, macro F1 Train: 0.7091, macro F1 Test: 0.6351\n",
      "Epoch [9/10000], Loss: 0.3176, macro F1 Train: 0.7091, macro F1 Test: 0.6351\n",
      "Epoch [10/10000], Loss: 0.3176, macro F1 Train: 0.7091, macro F1 Test: 0.6351\n",
      "Epoch [11/10000], Loss: 0.3176, macro F1 Train: 0.7095, macro F1 Test: 0.6351\n",
      "Epoch [12/10000], Loss: 0.3176, macro F1 Train: 0.7095, macro F1 Test: 0.6351\n",
      "Epoch [13/10000], Loss: 0.3176, macro F1 Train: 0.7095, macro F1 Test: 0.6351\n",
      "Epoch [14/10000], Loss: 0.3176, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [15/10000], Loss: 0.3176, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [16/10000], Loss: 0.3176, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [17/10000], Loss: 0.3176, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [18/10000], Loss: 0.3176, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [19/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [20/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [21/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [22/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [23/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [24/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [25/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [26/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [27/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6351\n",
      "Epoch [28/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [29/10000], Loss: 0.3175, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [30/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [31/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [32/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [33/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [34/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [35/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [36/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [37/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [38/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [39/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [40/10000], Loss: 0.3174, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [41/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [42/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [43/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [44/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [45/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [46/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [47/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [48/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [49/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [50/10000], Loss: 0.3173, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [51/10000], Loss: 0.3172, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [52/10000], Loss: 0.3172, macro F1 Train: 0.7094, macro F1 Test: 0.6327\n",
      "Epoch [53/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [54/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [55/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [56/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [57/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [58/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [59/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [60/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [61/10000], Loss: 0.3172, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [62/10000], Loss: 0.3171, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [63/10000], Loss: 0.3171, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [64/10000], Loss: 0.3171, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [65/10000], Loss: 0.3171, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [66/10000], Loss: 0.3171, macro F1 Train: 0.7095, macro F1 Test: 0.6327\n",
      "Epoch [67/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [68/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [69/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [70/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [71/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [72/10000], Loss: 0.3171, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [73/10000], Loss: 0.3170, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [74/10000], Loss: 0.3170, macro F1 Train: 0.7096, macro F1 Test: 0.6327\n",
      "Epoch [75/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6327\n",
      "Epoch [76/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6327\n",
      "Epoch [77/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [78/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [79/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [80/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [81/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [82/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [83/10000], Loss: 0.3170, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [84/10000], Loss: 0.3169, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [85/10000], Loss: 0.3169, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [86/10000], Loss: 0.3169, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [87/10000], Loss: 0.3169, macro F1 Train: 0.7097, macro F1 Test: 0.6329\n",
      "Epoch [88/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6329\n",
      "Epoch [89/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6329\n",
      "Epoch [90/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6329\n",
      "Epoch [91/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6329\n",
      "Epoch [92/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6329\n",
      "Epoch [93/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [94/10000], Loss: 0.3169, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [95/10000], Loss: 0.3168, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [96/10000], Loss: 0.3168, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [97/10000], Loss: 0.3168, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [98/10000], Loss: 0.3168, macro F1 Train: 0.7098, macro F1 Test: 0.6328\n",
      "Epoch [99/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [100/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [101/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [102/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [103/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [104/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [105/10000], Loss: 0.3168, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [106/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [107/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [108/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [109/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [110/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [111/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6328\n",
      "Epoch [112/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [113/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [114/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [115/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [116/10000], Loss: 0.3167, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [117/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [118/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [119/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [120/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [121/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [122/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [123/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [124/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [125/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [126/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [127/10000], Loss: 0.3166, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [128/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [129/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [130/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6331\n",
      "Epoch [131/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [132/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [133/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [134/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [135/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [136/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [137/10000], Loss: 0.3165, macro F1 Train: 0.7099, macro F1 Test: 0.6333\n",
      "Epoch [138/10000], Loss: 0.3165, macro F1 Train: 0.7100, macro F1 Test: 0.6333\n",
      "Epoch [139/10000], Loss: 0.3164, macro F1 Train: 0.7100, macro F1 Test: 0.6331\n",
      "Epoch [140/10000], Loss: 0.3164, macro F1 Train: 0.7100, macro F1 Test: 0.6331\n",
      "Epoch [141/10000], Loss: 0.3164, macro F1 Train: 0.7100, macro F1 Test: 0.6331\n",
      "Epoch [142/10000], Loss: 0.3164, macro F1 Train: 0.7100, macro F1 Test: 0.6331\n",
      "Epoch [143/10000], Loss: 0.3164, macro F1 Train: 0.7100, macro F1 Test: 0.6331\n",
      "Epoch [144/10000], Loss: 0.3164, macro F1 Train: 0.7101, macro F1 Test: 0.6331\n",
      "Epoch [145/10000], Loss: 0.3164, macro F1 Train: 0.7101, macro F1 Test: 0.6331\n",
      "Epoch [146/10000], Loss: 0.3164, macro F1 Train: 0.7102, macro F1 Test: 0.6331\n",
      "Epoch [147/10000], Loss: 0.3164, macro F1 Train: 0.7102, macro F1 Test: 0.6331\n",
      "Epoch [148/10000], Loss: 0.3164, macro F1 Train: 0.7102, macro F1 Test: 0.6331\n",
      "Epoch [149/10000], Loss: 0.3164, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [150/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [151/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [152/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [153/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [154/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [155/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [156/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6333\n",
      "Epoch [157/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [158/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [159/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [160/10000], Loss: 0.3163, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [161/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [162/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [163/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [164/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [165/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [166/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [167/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [168/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [169/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [170/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [171/10000], Loss: 0.3162, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [172/10000], Loss: 0.3161, macro F1 Train: 0.7102, macro F1 Test: 0.6338\n",
      "Epoch [173/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [174/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [175/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [176/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [177/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [178/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [179/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [180/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [181/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [182/10000], Loss: 0.3161, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [183/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [184/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [185/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [186/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [187/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [188/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6338\n",
      "Epoch [189/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [190/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [191/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [192/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [193/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [194/10000], Loss: 0.3160, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [195/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [196/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [197/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [198/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [199/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6337\n",
      "Epoch [200/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6336\n",
      "Epoch [201/10000], Loss: 0.3159, macro F1 Train: 0.7101, macro F1 Test: 0.6336\n",
      "Epoch [202/10000], Loss: 0.3159, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [203/10000], Loss: 0.3159, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [204/10000], Loss: 0.3159, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [205/10000], Loss: 0.3159, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [206/10000], Loss: 0.3158, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [207/10000], Loss: 0.3158, macro F1 Train: 0.7102, macro F1 Test: 0.6336\n",
      "Epoch [208/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [209/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [210/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [211/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [212/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [213/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [214/10000], Loss: 0.3158, macro F1 Train: 0.7103, macro F1 Test: 0.6336\n",
      "Epoch [215/10000], Loss: 0.3158, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [216/10000], Loss: 0.3158, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [217/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [218/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [219/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [220/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [221/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6336\n",
      "Epoch [222/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [223/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [224/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [225/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [226/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [227/10000], Loss: 0.3157, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [228/10000], Loss: 0.3156, macro F1 Train: 0.7104, macro F1 Test: 0.6338\n",
      "Epoch [229/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6338\n",
      "Epoch [230/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6338\n",
      "Epoch [231/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6338\n",
      "Epoch [232/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6338\n",
      "Epoch [233/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [234/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [235/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [236/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [237/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [238/10000], Loss: 0.3156, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [239/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [240/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [241/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [242/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [243/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [244/10000], Loss: 0.3155, macro F1 Train: 0.7105, macro F1 Test: 0.6339\n",
      "Epoch [245/10000], Loss: 0.3155, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [246/10000], Loss: 0.3155, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [247/10000], Loss: 0.3155, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [248/10000], Loss: 0.3155, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [249/10000], Loss: 0.3155, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [250/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [251/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [252/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [253/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [254/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [255/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6339\n",
      "Epoch [256/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6340\n",
      "Epoch [257/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [258/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [259/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [260/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [261/10000], Loss: 0.3154, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [262/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [263/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [264/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [265/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [266/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [267/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [268/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [269/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [270/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [271/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [272/10000], Loss: 0.3153, macro F1 Train: 0.7106, macro F1 Test: 0.6344\n",
      "Epoch [273/10000], Loss: 0.3152, macro F1 Train: 0.7106, macro F1 Test: 0.6343\n",
      "Epoch [274/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [275/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [276/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [277/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [278/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [279/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [280/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6343\n",
      "Epoch [281/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [282/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [283/10000], Loss: 0.3152, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [284/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [285/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [286/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [287/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [288/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [289/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [290/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [291/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [292/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [293/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6344\n",
      "Epoch [294/10000], Loss: 0.3151, macro F1 Train: 0.7107, macro F1 Test: 0.6345\n",
      "Epoch [295/10000], Loss: 0.3150, macro F1 Train: 0.7107, macro F1 Test: 0.6345\n",
      "Epoch [296/10000], Loss: 0.3150, macro F1 Train: 0.7107, macro F1 Test: 0.6345\n",
      "Epoch [297/10000], Loss: 0.3150, macro F1 Train: 0.7107, macro F1 Test: 0.6345\n",
      "Epoch [298/10000], Loss: 0.3150, macro F1 Train: 0.7107, macro F1 Test: 0.6345\n",
      "Epoch [299/10000], Loss: 0.3150, macro F1 Train: 0.7108, macro F1 Test: 0.6345\n",
      "Epoch [300/10000], Loss: 0.3150, macro F1 Train: 0.7108, macro F1 Test: 0.6345\n",
      "Epoch [301/10000], Loss: 0.3150, macro F1 Train: 0.7108, macro F1 Test: 0.6346\n",
      "Epoch [302/10000], Loss: 0.3150, macro F1 Train: 0.7109, macro F1 Test: 0.6346\n",
      "Epoch [303/10000], Loss: 0.3150, macro F1 Train: 0.7109, macro F1 Test: 0.6345\n",
      "Epoch [304/10000], Loss: 0.3150, macro F1 Train: 0.7109, macro F1 Test: 0.6345\n",
      "Epoch [305/10000], Loss: 0.3150, macro F1 Train: 0.7109, macro F1 Test: 0.6345\n",
      "Epoch [306/10000], Loss: 0.3149, macro F1 Train: 0.7109, macro F1 Test: 0.6345\n",
      "Epoch [307/10000], Loss: 0.3149, macro F1 Train: 0.7109, macro F1 Test: 0.6345\n",
      "Epoch [308/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [309/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [310/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [311/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [312/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [313/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [314/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [315/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6347\n",
      "Epoch [316/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6344\n",
      "Epoch [317/10000], Loss: 0.3149, macro F1 Train: 0.7110, macro F1 Test: 0.6344\n",
      "Epoch [318/10000], Loss: 0.3148, macro F1 Train: 0.7110, macro F1 Test: 0.6344\n",
      "Epoch [319/10000], Loss: 0.3148, macro F1 Train: 0.7110, macro F1 Test: 0.6345\n",
      "Epoch [320/10000], Loss: 0.3148, macro F1 Train: 0.7110, macro F1 Test: 0.6345\n",
      "Epoch [321/10000], Loss: 0.3148, macro F1 Train: 0.7110, macro F1 Test: 0.6345\n",
      "Epoch [322/10000], Loss: 0.3148, macro F1 Train: 0.7110, macro F1 Test: 0.6345\n",
      "Epoch [323/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [324/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [325/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [326/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [327/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [328/10000], Loss: 0.3148, macro F1 Train: 0.7115, macro F1 Test: 0.6345\n",
      "Epoch [329/10000], Loss: 0.3147, macro F1 Train: 0.7115, macro F1 Test: 0.6346\n",
      "Epoch [330/10000], Loss: 0.3147, macro F1 Train: 0.7115, macro F1 Test: 0.6346\n",
      "Epoch [331/10000], Loss: 0.3147, macro F1 Train: 0.7115, macro F1 Test: 0.6346\n",
      "Epoch [332/10000], Loss: 0.3147, macro F1 Train: 0.7119, macro F1 Test: 0.6346\n",
      "Epoch [333/10000], Loss: 0.3147, macro F1 Train: 0.7119, macro F1 Test: 0.6346\n",
      "Epoch [334/10000], Loss: 0.3147, macro F1 Train: 0.7119, macro F1 Test: 0.6346\n",
      "Epoch [335/10000], Loss: 0.3147, macro F1 Train: 0.7119, macro F1 Test: 0.6346\n",
      "Epoch [336/10000], Loss: 0.3147, macro F1 Train: 0.7119, macro F1 Test: 0.6346\n",
      "Epoch [337/10000], Loss: 0.3147, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [338/10000], Loss: 0.3147, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [339/10000], Loss: 0.3147, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [340/10000], Loss: 0.3146, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [341/10000], Loss: 0.3146, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [342/10000], Loss: 0.3146, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [343/10000], Loss: 0.3146, macro F1 Train: 0.7120, macro F1 Test: 0.6346\n",
      "Epoch [344/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6346\n",
      "Epoch [345/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [346/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [347/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [348/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [349/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [350/10000], Loss: 0.3146, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [351/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [352/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [353/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [354/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [355/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [356/10000], Loss: 0.3145, macro F1 Train: 0.7121, macro F1 Test: 0.6347\n",
      "Epoch [357/10000], Loss: 0.3145, macro F1 Train: 0.7122, macro F1 Test: 0.6347\n",
      "Epoch [358/10000], Loss: 0.3145, macro F1 Train: 0.7122, macro F1 Test: 0.6347\n",
      "Epoch [359/10000], Loss: 0.3145, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [360/10000], Loss: 0.3145, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [361/10000], Loss: 0.3145, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [362/10000], Loss: 0.3145, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [363/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [364/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [365/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [366/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [367/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [368/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [369/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [370/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [371/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [372/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [373/10000], Loss: 0.3144, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [374/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [375/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [376/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6347\n",
      "Epoch [377/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [378/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [379/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [380/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [381/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [382/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [383/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [384/10000], Loss: 0.3143, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [385/10000], Loss: 0.3142, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [386/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [387/10000], Loss: 0.3142, macro F1 Train: 0.7123, macro F1 Test: 0.6350\n",
      "Epoch [388/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [389/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [390/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [391/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [392/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [393/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [394/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [395/10000], Loss: 0.3142, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [396/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [397/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [398/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [399/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [400/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [401/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [402/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [403/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [404/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [405/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [406/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [407/10000], Loss: 0.3141, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [408/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [409/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [410/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [411/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6350\n",
      "Epoch [412/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6347\n",
      "Epoch [413/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6347\n",
      "Epoch [414/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6347\n",
      "Epoch [415/10000], Loss: 0.3140, macro F1 Train: 0.7124, macro F1 Test: 0.6347\n",
      "Epoch [416/10000], Loss: 0.3140, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [417/10000], Loss: 0.3140, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [418/10000], Loss: 0.3140, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [419/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [420/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [421/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [422/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [423/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [424/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [425/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [426/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [427/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [428/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [429/10000], Loss: 0.3139, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [430/10000], Loss: 0.3138, macro F1 Train: 0.7128, macro F1 Test: 0.6347\n",
      "Epoch [431/10000], Loss: 0.3138, macro F1 Train: 0.7128, macro F1 Test: 0.6360\n",
      "Epoch [432/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [433/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [434/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [435/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [436/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [437/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6360\n",
      "Epoch [438/10000], Loss: 0.3138, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [439/10000], Loss: 0.3138, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [440/10000], Loss: 0.3138, macro F1 Train: 0.7129, macro F1 Test: 0.6358\n",
      "Epoch [441/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [442/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [443/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [444/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [445/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [446/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [447/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [448/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [449/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6358\n",
      "Epoch [450/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [451/10000], Loss: 0.3137, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [452/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [453/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [454/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [455/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [456/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [457/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [458/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6359\n",
      "Epoch [459/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [460/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [461/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [462/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [463/10000], Loss: 0.3136, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [464/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [465/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [466/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [467/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [468/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [469/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [470/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [471/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [472/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [473/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [474/10000], Loss: 0.3135, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [475/10000], Loss: 0.3134, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [476/10000], Loss: 0.3134, macro F1 Train: 0.7130, macro F1 Test: 0.6360\n",
      "Epoch [477/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [478/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [479/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [480/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [481/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [482/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [483/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [484/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [485/10000], Loss: 0.3134, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [486/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [487/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [488/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [489/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [490/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [491/10000], Loss: 0.3133, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [492/10000], Loss: 0.3133, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [493/10000], Loss: 0.3133, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [494/10000], Loss: 0.3133, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [495/10000], Loss: 0.3133, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [496/10000], Loss: 0.3133, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [497/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [498/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [499/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [500/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [501/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [502/10000], Loss: 0.3132, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [503/10000], Loss: 0.3132, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [504/10000], Loss: 0.3132, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [505/10000], Loss: 0.3132, macro F1 Train: 0.7131, macro F1 Test: 0.6360\n",
      "Epoch [506/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [507/10000], Loss: 0.3132, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [508/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [509/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [510/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [511/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [512/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [513/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6360\n",
      "Epoch [514/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [515/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [516/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [517/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [518/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [519/10000], Loss: 0.3131, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [520/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [521/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [522/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [523/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [524/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [525/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [526/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [527/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [528/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [529/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [530/10000], Loss: 0.3130, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [531/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [532/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [533/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [534/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6362\n",
      "Epoch [535/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [536/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [537/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [538/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [539/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [540/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [541/10000], Loss: 0.3129, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [542/10000], Loss: 0.3128, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [543/10000], Loss: 0.3128, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [544/10000], Loss: 0.3128, macro F1 Train: 0.7132, macro F1 Test: 0.6363\n",
      "Epoch [545/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [546/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [547/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [548/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [549/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [550/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [551/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [552/10000], Loss: 0.3128, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [553/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [554/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6363\n",
      "Epoch [555/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [556/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [557/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [558/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [559/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [560/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [561/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [562/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [563/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [564/10000], Loss: 0.3127, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [565/10000], Loss: 0.3126, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [566/10000], Loss: 0.3126, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [567/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [568/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [569/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [570/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [571/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [572/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [573/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [574/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [575/10000], Loss: 0.3126, macro F1 Train: 0.7132, macro F1 Test: 0.6369\n",
      "Epoch [576/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [577/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [578/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [579/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [580/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [581/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [582/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6369\n",
      "Epoch [583/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [584/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [585/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [586/10000], Loss: 0.3125, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [587/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [588/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [589/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [590/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [591/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [592/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [593/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [594/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [595/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [596/10000], Loss: 0.3124, macro F1 Train: 0.7133, macro F1 Test: 0.6368\n",
      "Epoch [597/10000], Loss: 0.3124, macro F1 Train: 0.7134, macro F1 Test: 0.6368\n",
      "Epoch [598/10000], Loss: 0.3124, macro F1 Train: 0.7134, macro F1 Test: 0.6368\n",
      "Epoch [599/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [600/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [601/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [602/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [603/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [604/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [605/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [606/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [607/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [608/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [609/10000], Loss: 0.3123, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [610/10000], Loss: 0.3122, macro F1 Train: 0.7138, macro F1 Test: 0.6368\n",
      "Epoch [611/10000], Loss: 0.3122, macro F1 Train: 0.7141, macro F1 Test: 0.6368\n",
      "Epoch [612/10000], Loss: 0.3122, macro F1 Train: 0.7141, macro F1 Test: 0.6368\n",
      "Epoch [613/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6368\n",
      "Epoch [614/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [615/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [616/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [617/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [618/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [619/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [620/10000], Loss: 0.3122, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [621/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [622/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [623/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [624/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [625/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [626/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [627/10000], Loss: 0.3121, macro F1 Train: 0.7142, macro F1 Test: 0.6367\n",
      "Epoch [628/10000], Loss: 0.3121, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [629/10000], Loss: 0.3121, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [630/10000], Loss: 0.3121, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [631/10000], Loss: 0.3121, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [632/10000], Loss: 0.3121, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [633/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [634/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [635/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [636/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [637/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [638/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [639/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [640/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [641/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [642/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [643/10000], Loss: 0.3120, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [644/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [645/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [646/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [647/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [648/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [649/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [650/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [651/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [652/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [653/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [654/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [655/10000], Loss: 0.3119, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [656/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [657/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [658/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [659/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [660/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [661/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [662/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [663/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [664/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [665/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [666/10000], Loss: 0.3118, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [667/10000], Loss: 0.3117, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [668/10000], Loss: 0.3117, macro F1 Train: 0.7143, macro F1 Test: 0.6368\n",
      "Epoch [669/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [670/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [671/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [672/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [673/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [674/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [675/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [676/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [677/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [678/10000], Loss: 0.3117, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [679/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [680/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [681/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6367\n",
      "Epoch [682/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [683/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [684/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [685/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [686/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [687/10000], Loss: 0.3116, macro F1 Train: 0.7144, macro F1 Test: 0.6359\n",
      "Epoch [688/10000], Loss: 0.3116, macro F1 Train: 0.7145, macro F1 Test: 0.6359\n",
      "Epoch [689/10000], Loss: 0.3116, macro F1 Train: 0.7145, macro F1 Test: 0.6359\n",
      "Epoch [690/10000], Loss: 0.3116, macro F1 Train: 0.7145, macro F1 Test: 0.6359\n",
      "Epoch [691/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [692/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [693/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [694/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [695/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [696/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [697/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [698/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [699/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [700/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [701/10000], Loss: 0.3115, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [702/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [703/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [704/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [705/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [706/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [707/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6359\n",
      "Epoch [708/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [709/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [710/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [711/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [712/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [713/10000], Loss: 0.3114, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [714/10000], Loss: 0.3113, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [715/10000], Loss: 0.3113, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [716/10000], Loss: 0.3113, macro F1 Train: 0.7146, macro F1 Test: 0.6362\n",
      "Epoch [717/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [718/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6361\n",
      "Epoch [719/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6361\n",
      "Epoch [720/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [721/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [722/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [723/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [724/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [725/10000], Loss: 0.3113, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [726/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [727/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [728/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [729/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [730/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [731/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [732/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [733/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [734/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [735/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [736/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [737/10000], Loss: 0.3112, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [738/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [739/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [740/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [741/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [742/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [743/10000], Loss: 0.3111, macro F1 Train: 0.7148, macro F1 Test: 0.6362\n",
      "Epoch [744/10000], Loss: 0.3111, macro F1 Train: 0.7149, macro F1 Test: 0.6362\n",
      "Epoch [745/10000], Loss: 0.3111, macro F1 Train: 0.7151, macro F1 Test: 0.6362\n",
      "Epoch [746/10000], Loss: 0.3111, macro F1 Train: 0.7151, macro F1 Test: 0.6362\n",
      "Epoch [747/10000], Loss: 0.3111, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [748/10000], Loss: 0.3111, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [749/10000], Loss: 0.3111, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [750/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [751/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [752/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [753/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6359\n",
      "Epoch [754/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [755/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [756/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [757/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [758/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [759/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [760/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [761/10000], Loss: 0.3110, macro F1 Train: 0.7151, macro F1 Test: 0.6360\n",
      "Epoch [762/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [763/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [764/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [765/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [766/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [767/10000], Loss: 0.3109, macro F1 Train: 0.7151, macro F1 Test: 0.6358\n",
      "Epoch [768/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [769/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [770/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [771/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [772/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [773/10000], Loss: 0.3109, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [774/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [775/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [776/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [777/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [778/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [779/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [780/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [781/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [782/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [783/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [784/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [785/10000], Loss: 0.3108, macro F1 Train: 0.7152, macro F1 Test: 0.6358\n",
      "Epoch [786/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [787/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [788/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [789/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [790/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [791/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [792/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [793/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [794/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [795/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [796/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [797/10000], Loss: 0.3107, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [798/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [799/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [800/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [801/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [802/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [803/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [804/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [805/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [806/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [807/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [808/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [809/10000], Loss: 0.3106, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [810/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [811/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [812/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [813/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [814/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [815/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [816/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [817/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [818/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [819/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [820/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [821/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6359\n",
      "Epoch [822/10000], Loss: 0.3105, macro F1 Train: 0.7153, macro F1 Test: 0.6359\n",
      "Epoch [823/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6359\n",
      "Epoch [824/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6359\n",
      "Epoch [825/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [826/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [827/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [828/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [829/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [830/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [831/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [832/10000], Loss: 0.3104, macro F1 Train: 0.7153, macro F1 Test: 0.6358\n",
      "Epoch [833/10000], Loss: 0.3104, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [834/10000], Loss: 0.3104, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [835/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [836/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [837/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [838/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6358\n",
      "Epoch [839/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [840/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [841/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [842/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [843/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [844/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6357\n",
      "Epoch [845/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [846/10000], Loss: 0.3103, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [847/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [848/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [849/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [850/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [851/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [852/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [853/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [854/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [855/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [856/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [857/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [858/10000], Loss: 0.3102, macro F1 Train: 0.7154, macro F1 Test: 0.6366\n",
      "Epoch [859/10000], Loss: 0.3102, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [860/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [861/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [862/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [863/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [864/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [865/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [866/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [867/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [868/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [869/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [870/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [871/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [872/10000], Loss: 0.3101, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [873/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6367\n",
      "Epoch [874/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [875/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [876/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [877/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [878/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [879/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [880/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [881/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [882/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [883/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [884/10000], Loss: 0.3100, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [885/10000], Loss: 0.3099, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [886/10000], Loss: 0.3099, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [887/10000], Loss: 0.3099, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [888/10000], Loss: 0.3099, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [889/10000], Loss: 0.3099, macro F1 Train: 0.7155, macro F1 Test: 0.6366\n",
      "Epoch [890/10000], Loss: 0.3099, macro F1 Train: 0.7156, macro F1 Test: 0.6366\n",
      "Epoch [891/10000], Loss: 0.3099, macro F1 Train: 0.7156, macro F1 Test: 0.6366\n",
      "Epoch [892/10000], Loss: 0.3099, macro F1 Train: 0.7156, macro F1 Test: 0.6366\n",
      "Epoch [893/10000], Loss: 0.3099, macro F1 Train: 0.7156, macro F1 Test: 0.6366\n",
      "Epoch [894/10000], Loss: 0.3099, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [895/10000], Loss: 0.3099, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [896/10000], Loss: 0.3099, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [897/10000], Loss: 0.3099, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [898/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [899/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [900/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [901/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [902/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [903/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [904/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [905/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [906/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6366\n",
      "Epoch [907/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6369\n",
      "Epoch [908/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6369\n",
      "Epoch [909/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6369\n",
      "Epoch [910/10000], Loss: 0.3098, macro F1 Train: 0.7157, macro F1 Test: 0.6369\n",
      "Epoch [911/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6369\n",
      "Epoch [912/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6369\n",
      "Epoch [913/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6369\n",
      "Epoch [914/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [915/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [916/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [917/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [918/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [919/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [920/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [921/10000], Loss: 0.3097, macro F1 Train: 0.7158, macro F1 Test: 0.6368\n",
      "Epoch [922/10000], Loss: 0.3097, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [923/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [924/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [925/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [926/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [927/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [928/10000], Loss: 0.3096, macro F1 Train: 0.7157, macro F1 Test: 0.6368\n",
      "Epoch [929/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [930/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [931/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [932/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [933/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [934/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [935/10000], Loss: 0.3096, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [936/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [937/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [938/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [939/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6370\n",
      "Epoch [940/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6371\n",
      "Epoch [941/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6371\n",
      "Epoch [942/10000], Loss: 0.3095, macro F1 Train: 0.7158, macro F1 Test: 0.6371\n",
      "Epoch [943/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [944/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [945/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [946/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [947/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [948/10000], Loss: 0.3095, macro F1 Train: 0.7159, macro F1 Test: 0.6371\n",
      "Epoch [949/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [950/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [951/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [952/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [953/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [954/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [955/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [956/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [957/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [958/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [959/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6369\n",
      "Epoch [960/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6367\n",
      "Epoch [961/10000], Loss: 0.3094, macro F1 Train: 0.7160, macro F1 Test: 0.6367\n",
      "Epoch [962/10000], Loss: 0.3093, macro F1 Train: 0.7160, macro F1 Test: 0.6367\n",
      "Epoch [963/10000], Loss: 0.3093, macro F1 Train: 0.7160, macro F1 Test: 0.6367\n",
      "Epoch [964/10000], Loss: 0.3093, macro F1 Train: 0.7160, macro F1 Test: 0.6365\n",
      "Epoch [965/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [966/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [967/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [968/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [969/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [970/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [971/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [972/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [973/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [974/10000], Loss: 0.3093, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [975/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [976/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [977/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [978/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [979/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [980/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [981/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [982/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [983/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [984/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [985/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [986/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [987/10000], Loss: 0.3092, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [988/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [989/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [990/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [991/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [992/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [993/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [994/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6365\n",
      "Epoch [995/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [996/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [997/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [998/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [999/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1000/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1001/10000], Loss: 0.3091, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1002/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1003/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1004/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1005/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1006/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1007/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6364\n",
      "Epoch [1008/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6362\n",
      "Epoch [1009/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6362\n",
      "Epoch [1010/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6362\n",
      "Epoch [1011/10000], Loss: 0.3090, macro F1 Train: 0.7161, macro F1 Test: 0.6362\n",
      "Epoch [1012/10000], Loss: 0.3090, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1013/10000], Loss: 0.3090, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1014/10000], Loss: 0.3090, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1015/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1016/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1017/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1018/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1019/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1020/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1021/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1022/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1023/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1024/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1025/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1026/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6362\n",
      "Epoch [1027/10000], Loss: 0.3089, macro F1 Train: 0.7162, macro F1 Test: 0.6360\n",
      "Epoch [1028/10000], Loss: 0.3088, macro F1 Train: 0.7162, macro F1 Test: 0.6360\n",
      "Epoch [1029/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1030/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1031/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1032/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1033/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1034/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1035/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1036/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1037/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1038/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1039/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1040/10000], Loss: 0.3088, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1041/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1042/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1043/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1044/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1045/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1046/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1047/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1048/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1049/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1050/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1051/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1052/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1053/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1054/10000], Loss: 0.3087, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1055/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1056/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1057/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1058/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1059/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1060/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1061/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1062/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1063/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1064/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1065/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1066/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1067/10000], Loss: 0.3086, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1068/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1069/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1070/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1071/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1072/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1073/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1074/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1075/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1076/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1077/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1078/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1079/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1080/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1081/10000], Loss: 0.3085, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1082/10000], Loss: 0.3084, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1083/10000], Loss: 0.3084, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1084/10000], Loss: 0.3084, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1085/10000], Loss: 0.3084, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1086/10000], Loss: 0.3084, macro F1 Train: 0.7163, macro F1 Test: 0.6360\n",
      "Epoch [1087/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1088/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1089/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1090/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1091/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1092/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6360\n",
      "Epoch [1093/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1094/10000], Loss: 0.3084, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1095/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1096/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1097/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1098/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1099/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1100/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1101/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1102/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1103/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1104/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1105/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1106/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1107/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1108/10000], Loss: 0.3083, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1109/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1110/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1111/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1112/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1113/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1114/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1115/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1116/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1117/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1118/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1119/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1120/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1121/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1122/10000], Loss: 0.3082, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1123/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1124/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1125/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1126/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1127/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1128/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1129/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1130/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1131/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1132/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1133/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1134/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1135/10000], Loss: 0.3081, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1136/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1137/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1138/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1139/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1140/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1141/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1142/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1143/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1144/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1145/10000], Loss: 0.3080, macro F1 Train: 0.7164, macro F1 Test: 0.6343\n",
      "Epoch [1146/10000], Loss: 0.3080, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1147/10000], Loss: 0.3080, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1148/10000], Loss: 0.3080, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1149/10000], Loss: 0.3080, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1150/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1151/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1152/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1153/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1154/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1155/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1156/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1157/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1158/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1159/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1160/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1161/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1162/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1163/10000], Loss: 0.3079, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1164/10000], Loss: 0.3078, macro F1 Train: 0.7166, macro F1 Test: 0.6345\n",
      "Epoch [1165/10000], Loss: 0.3078, macro F1 Train: 0.7167, macro F1 Test: 0.6345\n",
      "Epoch [1166/10000], Loss: 0.3078, macro F1 Train: 0.7167, macro F1 Test: 0.6345\n",
      "Epoch [1167/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1168/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1169/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1170/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1171/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1172/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1173/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1174/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1175/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1176/10000], Loss: 0.3078, macro F1 Train: 0.7168, macro F1 Test: 0.6345\n",
      "Epoch [1177/10000], Loss: 0.3078, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1178/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1179/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1180/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1181/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1182/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1183/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1184/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1185/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1186/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1187/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1188/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1189/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1190/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1191/10000], Loss: 0.3077, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1192/10000], Loss: 0.3076, macro F1 Train: 0.7169, macro F1 Test: 0.6345\n",
      "Epoch [1193/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1194/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1195/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1196/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1197/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1198/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1199/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1200/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1201/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1202/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1203/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1204/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1205/10000], Loss: 0.3076, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1206/10000], Loss: 0.3075, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1207/10000], Loss: 0.3075, macro F1 Train: 0.7170, macro F1 Test: 0.6345\n",
      "Epoch [1208/10000], Loss: 0.3075, macro F1 Train: 0.7172, macro F1 Test: 0.6345\n",
      "Epoch [1209/10000], Loss: 0.3075, macro F1 Train: 0.7172, macro F1 Test: 0.6345\n",
      "Epoch [1210/10000], Loss: 0.3075, macro F1 Train: 0.7172, macro F1 Test: 0.6345\n",
      "Epoch [1211/10000], Loss: 0.3075, macro F1 Train: 0.7172, macro F1 Test: 0.6345\n",
      "Epoch [1212/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6345\n",
      "Epoch [1213/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6345\n",
      "Epoch [1214/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6345\n",
      "Epoch [1215/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6341\n",
      "Epoch [1216/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6341\n",
      "Epoch [1217/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6341\n",
      "Epoch [1218/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6340\n",
      "Epoch [1219/10000], Loss: 0.3075, macro F1 Train: 0.7173, macro F1 Test: 0.6340\n",
      "Epoch [1220/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6340\n",
      "Epoch [1221/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1222/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1223/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1224/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1225/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1226/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1227/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1228/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1229/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1230/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1231/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1232/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1233/10000], Loss: 0.3074, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1234/10000], Loss: 0.3073, macro F1 Train: 0.7173, macro F1 Test: 0.6339\n",
      "Epoch [1235/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1236/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1237/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1238/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1239/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1240/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1241/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1242/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1243/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1244/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1245/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1246/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1247/10000], Loss: 0.3073, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1248/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1249/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1250/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1251/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1252/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1253/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1254/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1255/10000], Loss: 0.3072, macro F1 Train: 0.7174, macro F1 Test: 0.6339\n",
      "Epoch [1256/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1257/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1258/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1259/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1260/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1261/10000], Loss: 0.3072, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1262/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1263/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1264/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1265/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1266/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1267/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1268/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1269/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1270/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1271/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1272/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1273/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1274/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1275/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6339\n",
      "Epoch [1276/10000], Loss: 0.3071, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1277/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1278/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1279/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1280/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1281/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1282/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1283/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1284/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1285/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1286/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1287/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1288/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1289/10000], Loss: 0.3070, macro F1 Train: 0.7175, macro F1 Test: 0.6336\n",
      "Epoch [1290/10000], Loss: 0.3070, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1291/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1292/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1293/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1294/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1295/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1296/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1297/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1298/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1299/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1300/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1301/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1302/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1303/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1304/10000], Loss: 0.3069, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1305/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1306/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1307/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1308/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1309/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1310/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1311/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1312/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1313/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1314/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1315/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1316/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1317/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1318/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1319/10000], Loss: 0.3068, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1320/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1321/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1322/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1323/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1324/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1325/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1326/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1327/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1328/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1329/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1330/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1331/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1332/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1333/10000], Loss: 0.3067, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1334/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1335/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1336/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1337/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1338/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1339/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1340/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1341/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1342/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1343/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1344/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1345/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6335\n",
      "Epoch [1346/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1347/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1348/10000], Loss: 0.3066, macro F1 Train: 0.7176, macro F1 Test: 0.6336\n",
      "Epoch [1349/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1350/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1351/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1352/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1353/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1354/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1355/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1356/10000], Loss: 0.3065, macro F1 Train: 0.7178, macro F1 Test: 0.6336\n",
      "Epoch [1357/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1358/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1359/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1360/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1361/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1362/10000], Loss: 0.3065, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1363/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1364/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1365/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1366/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1367/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1368/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1369/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6336\n",
      "Epoch [1370/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1371/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1372/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1373/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1374/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1375/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1376/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1377/10000], Loss: 0.3064, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1378/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1379/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1380/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1381/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1382/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1383/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1384/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1385/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1386/10000], Loss: 0.3063, macro F1 Train: 0.7179, macro F1 Test: 0.6337\n",
      "Epoch [1387/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1388/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1389/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1390/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1391/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1392/10000], Loss: 0.3063, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1393/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1394/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1395/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1396/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1397/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1398/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1399/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1400/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1401/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1402/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1403/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1404/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1405/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1406/10000], Loss: 0.3062, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1407/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1408/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1409/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1410/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1411/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1412/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1413/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1414/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1415/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1416/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1417/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1418/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1419/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1420/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1421/10000], Loss: 0.3061, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1422/10000], Loss: 0.3060, macro F1 Train: 0.7180, macro F1 Test: 0.6336\n",
      "Epoch [1423/10000], Loss: 0.3060, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1424/10000], Loss: 0.3060, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1425/10000], Loss: 0.3060, macro F1 Train: 0.7180, macro F1 Test: 0.6337\n",
      "Epoch [1426/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1427/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1428/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1429/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1430/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1431/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1432/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1433/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1434/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1435/10000], Loss: 0.3060, macro F1 Train: 0.7181, macro F1 Test: 0.6337\n",
      "Epoch [1436/10000], Loss: 0.3060, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1437/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1438/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1439/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1440/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1441/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1442/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1443/10000], Loss: 0.3059, macro F1 Train: 0.7182, macro F1 Test: 0.6337\n",
      "Epoch [1444/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1445/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1446/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1447/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1448/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1449/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1450/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1451/10000], Loss: 0.3059, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1452/10000], Loss: 0.3058, macro F1 Train: 0.7183, macro F1 Test: 0.6337\n",
      "Epoch [1453/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1454/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1455/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1456/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1457/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1458/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1459/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1460/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1461/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1462/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1463/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1464/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1465/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1466/10000], Loss: 0.3058, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1467/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1468/10000], Loss: 0.3057, macro F1 Train: 0.7184, macro F1 Test: 0.6337\n",
      "Epoch [1469/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1470/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1471/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1472/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1473/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1474/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1475/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1476/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1477/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1478/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1479/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1480/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1481/10000], Loss: 0.3057, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1482/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1483/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1484/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1485/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1486/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1487/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1488/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1489/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1490/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1491/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1492/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1493/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1494/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1495/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1496/10000], Loss: 0.3056, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1497/10000], Loss: 0.3055, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1498/10000], Loss: 0.3055, macro F1 Train: 0.7185, macro F1 Test: 0.6337\n",
      "Epoch [1499/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1500/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1501/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1502/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1503/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1504/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1505/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1506/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1507/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1508/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1509/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1510/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1511/10000], Loss: 0.3055, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1512/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1513/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1514/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1515/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1516/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1517/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1518/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1519/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1520/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1521/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1522/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1523/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1524/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1525/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1526/10000], Loss: 0.3054, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1527/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1528/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1529/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1530/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1531/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1532/10000], Loss: 0.3053, macro F1 Train: 0.7186, macro F1 Test: 0.6337\n",
      "Epoch [1533/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1534/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1535/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1536/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1537/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1538/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1539/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1540/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1541/10000], Loss: 0.3053, macro F1 Train: 0.7187, macro F1 Test: 0.6337\n",
      "Epoch [1542/10000], Loss: 0.3052, macro F1 Train: 0.7187, macro F1 Test: 0.6334\n",
      "Epoch [1543/10000], Loss: 0.3052, macro F1 Train: 0.7188, macro F1 Test: 0.6334\n",
      "Epoch [1544/10000], Loss: 0.3052, macro F1 Train: 0.7188, macro F1 Test: 0.6334\n",
      "Epoch [1545/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1546/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1547/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1548/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1549/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1550/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1551/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1552/10000], Loss: 0.3052, macro F1 Train: 0.7189, macro F1 Test: 0.6334\n",
      "Epoch [1553/10000], Loss: 0.3052, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1554/10000], Loss: 0.3052, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1555/10000], Loss: 0.3052, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1556/10000], Loss: 0.3052, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1557/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1558/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1559/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1560/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1561/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1562/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1563/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1564/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1565/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1566/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1567/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1568/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1569/10000], Loss: 0.3051, macro F1 Train: 0.7190, macro F1 Test: 0.6334\n",
      "Epoch [1570/10000], Loss: 0.3051, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1571/10000], Loss: 0.3051, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1572/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1573/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1574/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1575/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1576/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1577/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1578/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1579/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1580/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1581/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1582/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1583/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1584/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1585/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1586/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1587/10000], Loss: 0.3050, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1588/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1589/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1590/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1591/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1592/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1593/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1594/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1595/10000], Loss: 0.3049, macro F1 Train: 0.7191, macro F1 Test: 0.6334\n",
      "Epoch [1596/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6334\n",
      "Epoch [1597/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1598/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1599/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1600/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1601/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1602/10000], Loss: 0.3049, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1603/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1604/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1605/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1606/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1607/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1608/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1609/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1610/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1611/10000], Loss: 0.3048, macro F1 Train: 0.7191, macro F1 Test: 0.6336\n",
      "Epoch [1612/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1613/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1614/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1615/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1616/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1617/10000], Loss: 0.3048, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1618/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1619/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1620/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1621/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1622/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1623/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1624/10000], Loss: 0.3047, macro F1 Train: 0.7192, macro F1 Test: 0.6336\n",
      "Epoch [1625/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1626/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1627/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1628/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1629/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1630/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1631/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1632/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1633/10000], Loss: 0.3047, macro F1 Train: 0.7193, macro F1 Test: 0.6333\n",
      "Epoch [1634/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1635/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6333\n",
      "Epoch [1636/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6333\n",
      "Epoch [1637/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1638/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1639/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1640/10000], Loss: 0.3046, macro F1 Train: 0.7193, macro F1 Test: 0.6336\n",
      "Epoch [1641/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6336\n",
      "Epoch [1642/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1643/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1644/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1645/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1646/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1647/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6321\n",
      "Epoch [1648/10000], Loss: 0.3046, macro F1 Train: 0.7194, macro F1 Test: 0.6320\n",
      "Epoch [1649/10000], Loss: 0.3045, macro F1 Train: 0.7194, macro F1 Test: 0.6320\n",
      "Epoch [1650/10000], Loss: 0.3045, macro F1 Train: 0.7194, macro F1 Test: 0.6320\n",
      "Epoch [1651/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1652/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1653/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1654/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1655/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1656/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1657/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1658/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6320\n",
      "Epoch [1659/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1660/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1661/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1662/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1663/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1664/10000], Loss: 0.3045, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1665/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1666/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1667/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6321\n",
      "Epoch [1668/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1669/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1670/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1671/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1672/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1673/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1674/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1675/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1676/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1677/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1678/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1679/10000], Loss: 0.3044, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1680/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1681/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1682/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1683/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1684/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1685/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1686/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1687/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1688/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1689/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6323\n",
      "Epoch [1690/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1691/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1692/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1693/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1694/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1695/10000], Loss: 0.3043, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1696/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1697/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1698/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1699/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1700/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1701/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1702/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1703/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1704/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1705/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1706/10000], Loss: 0.3042, macro F1 Train: 0.7195, macro F1 Test: 0.6324\n",
      "Epoch [1707/10000], Loss: 0.3042, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1708/10000], Loss: 0.3042, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1709/10000], Loss: 0.3042, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1710/10000], Loss: 0.3042, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1711/10000], Loss: 0.3042, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1712/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1713/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1714/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1715/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1716/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1717/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1718/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1719/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1720/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1721/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1722/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1723/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1724/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1725/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1726/10000], Loss: 0.3041, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1727/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1728/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6324\n",
      "Epoch [1729/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1730/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1731/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1732/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1733/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1734/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1735/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1736/10000], Loss: 0.3040, macro F1 Train: 0.7196, macro F1 Test: 0.6327\n",
      "Epoch [1737/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6327\n",
      "Epoch [1738/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6327\n",
      "Epoch [1739/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6327\n",
      "Epoch [1740/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6327\n",
      "Epoch [1741/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6327\n",
      "Epoch [1742/10000], Loss: 0.3040, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1743/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1744/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1745/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1746/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1747/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1748/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1749/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1750/10000], Loss: 0.3039, macro F1 Train: 0.7197, macro F1 Test: 0.6328\n",
      "Epoch [1751/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1752/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1753/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1754/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1755/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1756/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1757/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1758/10000], Loss: 0.3039, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1759/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1760/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1761/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1762/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1763/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1764/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1765/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1766/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1767/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1768/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1769/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1770/10000], Loss: 0.3038, macro F1 Train: 0.7198, macro F1 Test: 0.6328\n",
      "Epoch [1771/10000], Loss: 0.3038, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1772/10000], Loss: 0.3038, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1773/10000], Loss: 0.3038, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1774/10000], Loss: 0.3038, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1775/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1776/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1777/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1778/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1779/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1780/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1781/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1782/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1783/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1784/10000], Loss: 0.3037, macro F1 Train: 0.7199, macro F1 Test: 0.6328\n",
      "Epoch [1785/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1786/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1787/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1788/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1789/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1790/10000], Loss: 0.3037, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1791/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1792/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1793/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1794/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1795/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1796/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1797/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1798/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1799/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1800/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1801/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1802/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1803/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1804/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1805/10000], Loss: 0.3036, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1806/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1807/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1808/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1809/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1810/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1811/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1812/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1813/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1814/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1815/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1816/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6327\n",
      "Epoch [1817/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1818/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1819/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1820/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1821/10000], Loss: 0.3035, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1822/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1823/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1824/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1825/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1826/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1827/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1828/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6328\n",
      "Epoch [1829/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1830/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1831/10000], Loss: 0.3034, macro F1 Train: 0.7201, macro F1 Test: 0.6323\n",
      "Epoch [1832/10000], Loss: 0.3034, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1833/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1834/10000], Loss: 0.3034, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1835/10000], Loss: 0.3034, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1836/10000], Loss: 0.3034, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1837/10000], Loss: 0.3034, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1838/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1839/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1840/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1841/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1842/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1843/10000], Loss: 0.3033, macro F1 Train: 0.7201, macro F1 Test: 0.6322\n",
      "Epoch [1844/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1845/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1846/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1847/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1848/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6322\n",
      "Epoch [1849/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1850/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1851/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1852/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1853/10000], Loss: 0.3033, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1854/10000], Loss: 0.3032, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1855/10000], Loss: 0.3032, macro F1 Train: 0.7200, macro F1 Test: 0.6323\n",
      "Epoch [1856/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6323\n",
      "Epoch [1857/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6323\n",
      "Epoch [1858/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1859/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1860/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1861/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1862/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1863/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1864/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1865/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1866/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1867/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1868/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1869/10000], Loss: 0.3032, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1870/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1871/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1872/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1873/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1874/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1875/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1876/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1877/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1878/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1879/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1880/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1881/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1882/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1883/10000], Loss: 0.3031, macro F1 Train: 0.7201, macro F1 Test: 0.6325\n",
      "Epoch [1884/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1885/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1886/10000], Loss: 0.3031, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1887/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1888/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1889/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1890/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1891/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1892/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1893/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1894/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1895/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1896/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1897/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1898/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1899/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1900/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1901/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1902/10000], Loss: 0.3030, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1903/10000], Loss: 0.3029, macro F1 Train: 0.7202, macro F1 Test: 0.6325\n",
      "Epoch [1904/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6325\n",
      "Epoch [1905/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6325\n",
      "Epoch [1906/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6325\n",
      "Epoch [1907/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6325\n",
      "Epoch [1908/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6325\n",
      "Epoch [1909/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1910/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1911/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1912/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1913/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1914/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1915/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1916/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1917/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1918/10000], Loss: 0.3029, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1919/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1920/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1921/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1922/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1923/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1924/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1925/10000], Loss: 0.3028, macro F1 Train: 0.7203, macro F1 Test: 0.6326\n",
      "Epoch [1926/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1927/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1928/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1929/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1930/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1931/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1932/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1933/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1934/10000], Loss: 0.3028, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1935/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1936/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1937/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1938/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1939/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1940/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1941/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1942/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1943/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1944/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1945/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1946/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1947/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1948/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6326\n",
      "Epoch [1949/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1950/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1951/10000], Loss: 0.3027, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1952/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1953/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1954/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1955/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1956/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1957/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1958/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1959/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1960/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1961/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1962/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1963/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1964/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1965/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1966/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1967/10000], Loss: 0.3026, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1968/10000], Loss: 0.3025, macro F1 Train: 0.7205, macro F1 Test: 0.6325\n",
      "Epoch [1969/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1970/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1971/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1972/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1973/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1974/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1975/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1976/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1977/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1978/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1979/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1980/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1981/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1982/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1983/10000], Loss: 0.3025, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1984/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1985/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1986/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1987/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1988/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1989/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1990/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1991/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1992/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1993/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1994/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1995/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1996/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1997/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1998/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [1999/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2000/10000], Loss: 0.3024, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2001/10000], Loss: 0.3023, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2002/10000], Loss: 0.3023, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2003/10000], Loss: 0.3023, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2004/10000], Loss: 0.3023, macro F1 Train: 0.7206, macro F1 Test: 0.6325\n",
      "Epoch [2005/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2006/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2007/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2008/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2009/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2010/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2011/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2012/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2013/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2014/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2015/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2016/10000], Loss: 0.3023, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2017/10000], Loss: 0.3022, macro F1 Train: 0.7207, macro F1 Test: 0.6325\n",
      "Epoch [2018/10000], Loss: 0.3022, macro F1 Train: 0.7208, macro F1 Test: 0.6325\n",
      "Epoch [2019/10000], Loss: 0.3022, macro F1 Train: 0.7208, macro F1 Test: 0.6325\n",
      "Epoch [2020/10000], Loss: 0.3022, macro F1 Train: 0.7208, macro F1 Test: 0.6325\n",
      "Epoch [2021/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6325\n",
      "Epoch [2022/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6325\n",
      "Epoch [2023/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6325\n",
      "Epoch [2024/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6325\n",
      "Epoch [2025/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6326\n",
      "Epoch [2026/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6326\n",
      "Epoch [2027/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6326\n",
      "Epoch [2028/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6326\n",
      "Epoch [2029/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6326\n",
      "Epoch [2030/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6327\n",
      "Epoch [2031/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6327\n",
      "Epoch [2032/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6327\n",
      "Epoch [2033/10000], Loss: 0.3022, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2034/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2035/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2036/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2037/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2038/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2039/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2040/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2041/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2042/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2043/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2044/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2045/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2046/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2047/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2048/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2049/10000], Loss: 0.3021, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2050/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2051/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2052/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2053/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2054/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2055/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2056/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2057/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2058/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2059/10000], Loss: 0.3020, macro F1 Train: 0.7209, macro F1 Test: 0.6324\n",
      "Epoch [2060/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2061/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2062/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2063/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2064/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2065/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2066/10000], Loss: 0.3020, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2067/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2068/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2069/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2070/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2071/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2072/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2073/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2074/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2075/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2076/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6324\n",
      "Epoch [2077/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6325\n",
      "Epoch [2078/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6325\n",
      "Epoch [2079/10000], Loss: 0.3019, macro F1 Train: 0.7212, macro F1 Test: 0.6325\n",
      "Epoch [2080/10000], Loss: 0.3019, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2081/10000], Loss: 0.3019, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2082/10000], Loss: 0.3019, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2083/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2084/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2085/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2086/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2087/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2088/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2089/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2090/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2091/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2092/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2093/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2094/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2095/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2096/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2097/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2098/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2099/10000], Loss: 0.3018, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2100/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2101/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2102/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2103/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2104/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2105/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2106/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2107/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2108/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2109/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2110/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2111/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2112/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2113/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2114/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2115/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2116/10000], Loss: 0.3017, macro F1 Train: 0.7213, macro F1 Test: 0.6325\n",
      "Epoch [2117/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2118/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2119/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2120/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2121/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2122/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2123/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2124/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2125/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2126/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2127/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2128/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2129/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2130/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2131/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2132/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2133/10000], Loss: 0.3016, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2134/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2135/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2136/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2137/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2138/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2139/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2140/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2141/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2142/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2143/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2144/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2145/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2146/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2147/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2148/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2149/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2150/10000], Loss: 0.3015, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2151/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2152/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2153/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2154/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2155/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2156/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2157/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2158/10000], Loss: 0.3014, macro F1 Train: 0.7214, macro F1 Test: 0.6327\n",
      "Epoch [2159/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6327\n",
      "Epoch [2160/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6327\n",
      "Epoch [2161/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2162/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2163/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2164/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2165/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2166/10000], Loss: 0.3014, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2167/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2168/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2169/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2170/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2171/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2172/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2173/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2174/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2175/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2176/10000], Loss: 0.3013, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2177/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2178/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2179/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2180/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2181/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2182/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2183/10000], Loss: 0.3013, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2184/10000], Loss: 0.3012, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2185/10000], Loss: 0.3012, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2186/10000], Loss: 0.3012, macro F1 Train: 0.7214, macro F1 Test: 0.6325\n",
      "Epoch [2187/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2188/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2189/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2190/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2191/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2192/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2193/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2194/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2195/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2196/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2197/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2198/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2199/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2200/10000], Loss: 0.3012, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2201/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2202/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2203/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2204/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2205/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2206/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2207/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2208/10000], Loss: 0.3011, macro F1 Train: 0.7215, macro F1 Test: 0.6325\n",
      "Epoch [2209/10000], Loss: 0.3011, macro F1 Train: 0.7216, macro F1 Test: 0.6325\n",
      "Epoch [2210/10000], Loss: 0.3011, macro F1 Train: 0.7216, macro F1 Test: 0.6325\n",
      "Epoch [2211/10000], Loss: 0.3011, macro F1 Train: 0.7216, macro F1 Test: 0.6325\n",
      "Epoch [2212/10000], Loss: 0.3011, macro F1 Train: 0.7216, macro F1 Test: 0.6325\n",
      "Epoch [2213/10000], Loss: 0.3011, macro F1 Train: 0.7217, macro F1 Test: 0.6325\n",
      "Epoch [2214/10000], Loss: 0.3011, macro F1 Train: 0.7217, macro F1 Test: 0.6325\n",
      "Epoch [2215/10000], Loss: 0.3011, macro F1 Train: 0.7217, macro F1 Test: 0.6325\n",
      "Epoch [2216/10000], Loss: 0.3011, macro F1 Train: 0.7218, macro F1 Test: 0.6325\n",
      "Epoch [2217/10000], Loss: 0.3011, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2218/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2219/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2220/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2221/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2222/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2223/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2224/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2225/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2226/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2227/10000], Loss: 0.3010, macro F1 Train: 0.7218, macro F1 Test: 0.6311\n",
      "Epoch [2228/10000], Loss: 0.3010, macro F1 Train: 0.7219, macro F1 Test: 0.6311\n",
      "Epoch [2229/10000], Loss: 0.3010, macro F1 Train: 0.7219, macro F1 Test: 0.6311\n",
      "Epoch [2230/10000], Loss: 0.3010, macro F1 Train: 0.7219, macro F1 Test: 0.6311\n",
      "Epoch [2231/10000], Loss: 0.3010, macro F1 Train: 0.7219, macro F1 Test: 0.6311\n",
      "Epoch [2232/10000], Loss: 0.3010, macro F1 Train: 0.7220, macro F1 Test: 0.6311\n",
      "Epoch [2233/10000], Loss: 0.3010, macro F1 Train: 0.7220, macro F1 Test: 0.6311\n",
      "Epoch [2234/10000], Loss: 0.3010, macro F1 Train: 0.7220, macro F1 Test: 0.6311\n",
      "Epoch [2235/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6311\n",
      "Epoch [2236/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6311\n",
      "Epoch [2237/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2238/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2239/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2240/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2241/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2242/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2243/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2244/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2245/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2246/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2247/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2248/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2249/10000], Loss: 0.3009, macro F1 Train: 0.7220, macro F1 Test: 0.6312\n",
      "Epoch [2250/10000], Loss: 0.3009, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2251/10000], Loss: 0.3009, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2252/10000], Loss: 0.3008, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2253/10000], Loss: 0.3008, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2254/10000], Loss: 0.3008, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2255/10000], Loss: 0.3008, macro F1 Train: 0.7221, macro F1 Test: 0.6312\n",
      "Epoch [2256/10000], Loss: 0.3008, macro F1 Train: 0.7221, macro F1 Test: 0.6311\n",
      "Epoch [2257/10000], Loss: 0.3008, macro F1 Train: 0.7222, macro F1 Test: 0.6311\n",
      "Epoch [2258/10000], Loss: 0.3008, macro F1 Train: 0.7222, macro F1 Test: 0.6311\n",
      "Epoch [2259/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6311\n",
      "Epoch [2260/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2261/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2262/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2263/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2264/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2265/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2266/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2267/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2268/10000], Loss: 0.3008, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2269/10000], Loss: 0.3007, macro F1 Train: 0.7223, macro F1 Test: 0.6310\n",
      "Epoch [2270/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2271/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2272/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2273/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2274/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2275/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2276/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2277/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2278/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2279/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2280/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2281/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2282/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2283/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2284/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2285/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2286/10000], Loss: 0.3007, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2287/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2288/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2289/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2290/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2291/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2292/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2293/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2294/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2295/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2296/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2297/10000], Loss: 0.3006, macro F1 Train: 0.7224, macro F1 Test: 0.6310\n",
      "Epoch [2298/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2299/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2300/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2301/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2302/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2303/10000], Loss: 0.3006, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2304/10000], Loss: 0.3005, macro F1 Train: 0.7225, macro F1 Test: 0.6310\n",
      "Epoch [2305/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2306/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2307/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2308/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2309/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2310/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2311/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2312/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2313/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2314/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2315/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2316/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2317/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2318/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6311\n",
      "Epoch [2319/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2320/10000], Loss: 0.3005, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2321/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2322/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2323/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2324/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6309\n",
      "Epoch [2325/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2326/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2327/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2328/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2329/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2330/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2331/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2332/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2333/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2334/10000], Loss: 0.3004, macro F1 Train: 0.7226, macro F1 Test: 0.6310\n",
      "Epoch [2335/10000], Loss: 0.3004, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2336/10000], Loss: 0.3004, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2337/10000], Loss: 0.3004, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2338/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2339/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2340/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2341/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2342/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2343/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2344/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2345/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2346/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2347/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2348/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2349/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2350/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2351/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2352/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2353/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2354/10000], Loss: 0.3003, macro F1 Train: 0.7227, macro F1 Test: 0.6310\n",
      "Epoch [2355/10000], Loss: 0.3003, macro F1 Train: 0.7228, macro F1 Test: 0.6310\n",
      "Epoch [2356/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6310\n",
      "Epoch [2357/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6310\n",
      "Epoch [2358/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2359/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2360/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2361/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2362/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2363/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2364/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2365/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2366/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2367/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2368/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6309\n",
      "Epoch [2369/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2370/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2371/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2372/10000], Loss: 0.3002, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2373/10000], Loss: 0.3001, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2374/10000], Loss: 0.3001, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2375/10000], Loss: 0.3001, macro F1 Train: 0.7228, macro F1 Test: 0.6324\n",
      "Epoch [2376/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2377/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2378/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2379/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2380/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2381/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2382/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2383/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2384/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2385/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2386/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2387/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2388/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2389/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2390/10000], Loss: 0.3001, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2391/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2392/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2393/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2394/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2395/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2396/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2397/10000], Loss: 0.3000, macro F1 Train: 0.7229, macro F1 Test: 0.6324\n",
      "Epoch [2398/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2399/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2400/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2401/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2402/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2403/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2404/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6324\n",
      "Epoch [2405/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2406/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2407/10000], Loss: 0.3000, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2408/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2409/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2410/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2411/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2412/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2413/10000], Loss: 0.2999, macro F1 Train: 0.7230, macro F1 Test: 0.6327\n",
      "Epoch [2414/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6327\n",
      "Epoch [2415/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6327\n",
      "Epoch [2416/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6327\n",
      "Epoch [2417/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6327\n",
      "Epoch [2418/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6328\n",
      "Epoch [2419/10000], Loss: 0.2999, macro F1 Train: 0.7231, macro F1 Test: 0.6328\n",
      "Epoch [2420/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2421/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2422/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2423/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2424/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2425/10000], Loss: 0.2999, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2426/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2427/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2428/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2429/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2430/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6328\n",
      "Epoch [2431/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6329\n",
      "Epoch [2432/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6329\n",
      "Epoch [2433/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6329\n",
      "Epoch [2434/10000], Loss: 0.2998, macro F1 Train: 0.7232, macro F1 Test: 0.6329\n",
      "Epoch [2435/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2436/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2437/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2438/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2439/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2440/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2441/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2442/10000], Loss: 0.2998, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2443/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2444/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2445/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2446/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2447/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2448/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2449/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2450/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2451/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2452/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6329\n",
      "Epoch [2453/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2454/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2455/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2456/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2457/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2458/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2459/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2460/10000], Loss: 0.2997, macro F1 Train: 0.7234, macro F1 Test: 0.6330\n",
      "Epoch [2461/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2462/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2463/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2464/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2465/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2466/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2467/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2468/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2469/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2470/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2471/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2472/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2473/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2474/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2475/10000], Loss: 0.2996, macro F1 Train: 0.7235, macro F1 Test: 0.6330\n",
      "Epoch [2476/10000], Loss: 0.2996, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2477/10000], Loss: 0.2996, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2478/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2479/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2480/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2481/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2482/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2483/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2484/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2485/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2486/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2487/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2488/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2489/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2490/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2491/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2492/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2493/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2494/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2495/10000], Loss: 0.2995, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2496/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2497/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2498/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2499/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6330\n",
      "Epoch [2500/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6331\n",
      "Epoch [2501/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6331\n",
      "Epoch [2502/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6331\n",
      "Epoch [2503/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6331\n",
      "Epoch [2504/10000], Loss: 0.2994, macro F1 Train: 0.7236, macro F1 Test: 0.6331\n",
      "Epoch [2505/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2506/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2507/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2508/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2509/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2510/10000], Loss: 0.2994, macro F1 Train: 0.7237, macro F1 Test: 0.6331\n",
      "Epoch [2511/10000], Loss: 0.2994, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2512/10000], Loss: 0.2994, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2513/10000], Loss: 0.2994, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2514/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2515/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2516/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2517/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2518/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2519/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2520/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2521/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2522/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2523/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2524/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2525/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2526/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2527/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2528/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2529/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2530/10000], Loss: 0.2993, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2531/10000], Loss: 0.2992, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2532/10000], Loss: 0.2992, macro F1 Train: 0.7240, macro F1 Test: 0.6331\n",
      "Epoch [2533/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2534/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2535/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2536/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2537/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2538/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2539/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2540/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2541/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2542/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2543/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2544/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2545/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2546/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2547/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2548/10000], Loss: 0.2992, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2549/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2550/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2551/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2552/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2553/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2554/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2555/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2556/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2557/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2558/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2559/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2560/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2561/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2562/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2563/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2564/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2565/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2566/10000], Loss: 0.2991, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2567/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2568/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2569/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6332\n",
      "Epoch [2570/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2571/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2572/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2573/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2574/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2575/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2576/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2577/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2578/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2579/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2580/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2581/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2582/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2583/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2584/10000], Loss: 0.2990, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2585/10000], Loss: 0.2989, macro F1 Train: 0.7241, macro F1 Test: 0.6331\n",
      "Epoch [2586/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2587/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2588/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2589/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2590/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2591/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2592/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2593/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2594/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2595/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2596/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2597/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2598/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2599/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2600/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2601/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6331\n",
      "Epoch [2602/10000], Loss: 0.2989, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2603/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2604/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2605/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2606/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2607/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2608/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2609/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2610/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2611/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2612/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2613/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6328\n",
      "Epoch [2614/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2615/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2616/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2617/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2618/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2619/10000], Loss: 0.2988, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2620/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2621/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2622/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2623/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2624/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2625/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2626/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2627/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2628/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2629/10000], Loss: 0.2987, macro F1 Train: 0.7243, macro F1 Test: 0.6324\n",
      "Epoch [2630/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2631/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2632/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2633/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2634/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2635/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2636/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2637/10000], Loss: 0.2987, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2638/10000], Loss: 0.2986, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2639/10000], Loss: 0.2986, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2640/10000], Loss: 0.2986, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2641/10000], Loss: 0.2986, macro F1 Train: 0.7245, macro F1 Test: 0.6324\n",
      "Epoch [2642/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2643/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2644/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2645/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2646/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2647/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2648/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2649/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2650/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2651/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2652/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2653/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2654/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2655/10000], Loss: 0.2986, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2656/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2657/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2658/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2659/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2660/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2661/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2662/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2663/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2664/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2665/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2666/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2667/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2668/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2669/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2670/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2671/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2672/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2673/10000], Loss: 0.2985, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2674/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2675/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2676/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2677/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2678/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2679/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2680/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2681/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2682/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2683/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2684/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2685/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2686/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2687/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2688/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2689/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2690/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2691/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2692/10000], Loss: 0.2984, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2693/10000], Loss: 0.2983, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2694/10000], Loss: 0.2983, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2695/10000], Loss: 0.2983, macro F1 Train: 0.7246, macro F1 Test: 0.6324\n",
      "Epoch [2696/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2697/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2698/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2699/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2700/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2701/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2702/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2703/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2704/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2705/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2706/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2707/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2708/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2709/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2710/10000], Loss: 0.2983, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2711/10000], Loss: 0.2982, macro F1 Train: 0.7247, macro F1 Test: 0.6324\n",
      "Epoch [2712/10000], Loss: 0.2982, macro F1 Train: 0.7248, macro F1 Test: 0.6324\n",
      "Epoch [2713/10000], Loss: 0.2982, macro F1 Train: 0.7248, macro F1 Test: 0.6324\n",
      "Epoch [2714/10000], Loss: 0.2982, macro F1 Train: 0.7248, macro F1 Test: 0.6324\n",
      "Epoch [2715/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2716/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2717/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2718/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2719/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2720/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2721/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2722/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2723/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2724/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2725/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2726/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2727/10000], Loss: 0.2982, macro F1 Train: 0.7250, macro F1 Test: 0.6329\n",
      "Epoch [2728/10000], Loss: 0.2982, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2729/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2730/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2731/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2732/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2733/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2734/10000], Loss: 0.2981, macro F1 Train: 0.7251, macro F1 Test: 0.6329\n",
      "Epoch [2735/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2736/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2737/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2738/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2739/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2740/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2741/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2742/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2743/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2744/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2745/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2746/10000], Loss: 0.2981, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2747/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2748/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2749/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2750/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2751/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2752/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2753/10000], Loss: 0.2980, macro F1 Train: 0.7253, macro F1 Test: 0.6329\n",
      "Epoch [2754/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2755/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2756/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2757/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2758/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2759/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2760/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2761/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6329\n",
      "Epoch [2762/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6327\n",
      "Epoch [2763/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6327\n",
      "Epoch [2764/10000], Loss: 0.2980, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2765/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2766/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2767/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2768/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2769/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2770/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2771/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2772/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2773/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2774/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2775/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2776/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2777/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2778/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2779/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2780/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2781/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2782/10000], Loss: 0.2979, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2783/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2784/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2785/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2786/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2787/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2788/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2789/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2790/10000], Loss: 0.2978, macro F1 Train: 0.7254, macro F1 Test: 0.6330\n",
      "Epoch [2791/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2792/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2793/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2794/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2795/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2796/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2797/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2798/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2799/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2800/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2801/10000], Loss: 0.2978, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2802/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2803/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2804/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2805/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2806/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2807/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2808/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2809/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2810/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2811/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2812/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2813/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2814/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2815/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2816/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2817/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2818/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2819/10000], Loss: 0.2977, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2820/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2821/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2822/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2823/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2824/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2825/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2826/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2827/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2828/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2829/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2830/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2831/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2832/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2833/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2834/10000], Loss: 0.2976, macro F1 Train: 0.7256, macro F1 Test: 0.6330\n",
      "Epoch [2835/10000], Loss: 0.2976, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2836/10000], Loss: 0.2976, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2837/10000], Loss: 0.2976, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2838/10000], Loss: 0.2976, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2839/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2840/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2841/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2842/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2843/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2844/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2845/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2846/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2847/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2848/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2849/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2850/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2851/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2852/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2853/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2854/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2855/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2856/10000], Loss: 0.2975, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2857/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2858/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2859/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2860/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2861/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2862/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2863/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2864/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2865/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2866/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2867/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2868/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2869/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2870/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2871/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2872/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2873/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2874/10000], Loss: 0.2974, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2875/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2876/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2877/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2878/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2879/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2880/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2881/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2882/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2883/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2884/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2885/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2886/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2887/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2888/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2889/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2890/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2891/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2892/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2893/10000], Loss: 0.2973, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2894/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2895/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6330\n",
      "Epoch [2896/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2897/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2898/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2899/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2900/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2901/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2902/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2903/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2904/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2905/10000], Loss: 0.2972, macro F1 Train: 0.7257, macro F1 Test: 0.6329\n",
      "Epoch [2906/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2907/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2908/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2909/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2910/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2911/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2912/10000], Loss: 0.2972, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2913/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2914/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2915/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2916/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2917/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2918/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2919/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2920/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2921/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2922/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2923/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2924/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2925/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2926/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2927/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2928/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2929/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2930/10000], Loss: 0.2971, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2931/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2932/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2933/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2934/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2935/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2936/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2937/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2938/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2939/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2940/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2941/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2942/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2943/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2944/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2945/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2946/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2947/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2948/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2949/10000], Loss: 0.2970, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2950/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2951/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6329\n",
      "Epoch [2952/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2953/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2954/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2955/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2956/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2957/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2958/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2959/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2960/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2961/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2962/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2963/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2964/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2965/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2966/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2967/10000], Loss: 0.2969, macro F1 Train: 0.7258, macro F1 Test: 0.6325\n",
      "Epoch [2968/10000], Loss: 0.2969, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2969/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2970/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2971/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2972/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2973/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2974/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2975/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2976/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2977/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2978/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2979/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2980/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2981/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2982/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2983/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2984/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2985/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2986/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2987/10000], Loss: 0.2968, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2988/10000], Loss: 0.2967, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2989/10000], Loss: 0.2967, macro F1 Train: 0.7259, macro F1 Test: 0.6325\n",
      "Epoch [2990/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6325\n",
      "Epoch [2991/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6325\n",
      "Epoch [2992/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6325\n",
      "Epoch [2993/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2994/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2995/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2996/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2997/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2998/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [2999/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3000/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3001/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3002/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3003/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3004/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3005/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3006/10000], Loss: 0.2967, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3007/10000], Loss: 0.2966, macro F1 Train: 0.7260, macro F1 Test: 0.6326\n",
      "Epoch [3008/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3009/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3010/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3011/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3012/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3013/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3014/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3015/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3016/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3017/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3018/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3019/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3020/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3021/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3022/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3023/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3024/10000], Loss: 0.2966, macro F1 Train: 0.7261, macro F1 Test: 0.6326\n",
      "Epoch [3025/10000], Loss: 0.2966, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3026/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3027/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3028/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3029/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3030/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3031/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6326\n",
      "Epoch [3032/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3033/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3034/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3035/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3036/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3037/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3038/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3039/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3040/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3041/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3042/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3043/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3044/10000], Loss: 0.2965, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3045/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3046/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3047/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3048/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3049/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3050/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3051/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3052/10000], Loss: 0.2964, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3053/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3054/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3055/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3056/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3057/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3058/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3059/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3060/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3061/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3062/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3063/10000], Loss: 0.2964, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3064/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3065/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3066/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3067/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3068/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3069/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3070/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3071/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3072/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3073/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3074/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3075/10000], Loss: 0.2963, macro F1 Train: 0.7263, macro F1 Test: 0.6324\n",
      "Epoch [3076/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3077/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3078/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3079/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3080/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3081/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3082/10000], Loss: 0.2963, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3083/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3084/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3085/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3086/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3087/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3088/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3089/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3090/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3091/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3092/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3093/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3094/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3095/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3096/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3097/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3098/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3099/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3100/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3101/10000], Loss: 0.2962, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3102/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3103/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3104/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3105/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3106/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3107/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3108/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3109/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3110/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3111/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3112/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3113/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3114/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3115/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3116/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3117/10000], Loss: 0.2961, macro F1 Train: 0.7262, macro F1 Test: 0.6324\n",
      "Epoch [3118/10000], Loss: 0.2961, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3119/10000], Loss: 0.2961, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3120/10000], Loss: 0.2961, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3121/10000], Loss: 0.2961, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3122/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3123/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3124/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3125/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3126/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3127/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3128/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3129/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3130/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3131/10000], Loss: 0.2960, macro F1 Train: 0.7264, macro F1 Test: 0.6324\n",
      "Epoch [3132/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3133/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3134/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3135/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3136/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6324\n",
      "Epoch [3137/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3138/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3139/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3140/10000], Loss: 0.2960, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3141/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3142/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3143/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3144/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3145/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3146/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3147/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3148/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3149/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6325\n",
      "Epoch [3150/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3151/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3152/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3153/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3154/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3155/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3156/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3157/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3158/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3159/10000], Loss: 0.2959, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3160/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3161/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3162/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3163/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6326\n",
      "Epoch [3164/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3165/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3166/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3167/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3168/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3169/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3170/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3171/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3172/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3173/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3174/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3175/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3176/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3177/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3178/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6322\n",
      "Epoch [3179/10000], Loss: 0.2958, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3180/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3181/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3182/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3183/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3184/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3185/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3186/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3187/10000], Loss: 0.2957, macro F1 Train: 0.7265, macro F1 Test: 0.6319\n",
      "Epoch [3188/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3189/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3190/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3191/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3192/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3193/10000], Loss: 0.2957, macro F1 Train: 0.7266, macro F1 Test: 0.6319\n",
      "Epoch [3194/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3195/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3196/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3197/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3198/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3199/10000], Loss: 0.2957, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3200/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3201/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3202/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3203/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3204/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3205/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3206/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3207/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3208/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3209/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3210/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3211/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3212/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3213/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3214/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3215/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3216/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3217/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3218/10000], Loss: 0.2956, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3219/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3220/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3221/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3222/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3223/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3224/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3225/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3226/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3227/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3228/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3229/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3230/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3231/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3232/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3233/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3234/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3235/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3236/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3237/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3238/10000], Loss: 0.2955, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3239/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3240/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3241/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3242/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3243/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3244/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3245/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3246/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3247/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3248/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3249/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3250/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3251/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3252/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3253/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3254/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3255/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3256/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3257/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3258/10000], Loss: 0.2954, macro F1 Train: 0.7267, macro F1 Test: 0.6319\n",
      "Epoch [3259/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3260/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3261/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3262/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3263/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3264/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3265/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3266/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3267/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3268/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3269/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3270/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3271/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3272/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3273/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3274/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3275/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3276/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3277/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3278/10000], Loss: 0.2953, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3279/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3280/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3281/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3282/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3283/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3284/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3285/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3286/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3287/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3288/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3289/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3290/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3291/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3292/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3293/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6319\n",
      "Epoch [3294/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6320\n",
      "Epoch [3295/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6320\n",
      "Epoch [3296/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6320\n",
      "Epoch [3297/10000], Loss: 0.2952, macro F1 Train: 0.7268, macro F1 Test: 0.6320\n",
      "Epoch [3298/10000], Loss: 0.2952, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3299/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3300/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3301/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3302/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3303/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3304/10000], Loss: 0.2951, macro F1 Train: 0.7269, macro F1 Test: 0.6320\n",
      "Epoch [3305/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3306/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3307/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3308/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3309/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3310/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3311/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3312/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3313/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3314/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3315/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3316/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3317/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3318/10000], Loss: 0.2951, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3319/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3320/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3321/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3322/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3323/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3324/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3325/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3326/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3327/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3328/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3329/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3330/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3331/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3332/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3333/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3334/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3335/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3336/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3337/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3338/10000], Loss: 0.2950, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3339/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3340/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3341/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3342/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3343/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3344/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3345/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3346/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3347/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3348/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3349/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3350/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3351/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3352/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3353/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3354/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3355/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3356/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3357/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3358/10000], Loss: 0.2949, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3359/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3360/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3361/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3362/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3363/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3364/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3365/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3366/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3367/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3368/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3369/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3370/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3371/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3372/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3373/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3374/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3375/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3376/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3377/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3378/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3379/10000], Loss: 0.2948, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3380/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3381/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3382/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3383/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3384/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3385/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3386/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3387/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3388/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3389/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6320\n",
      "Epoch [3390/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3391/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3392/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3393/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3394/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3395/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3396/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3397/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3398/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3399/10000], Loss: 0.2947, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3400/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3401/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6317\n",
      "Epoch [3402/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3403/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3404/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3405/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3406/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3407/10000], Loss: 0.2946, macro F1 Train: 0.7270, macro F1 Test: 0.6318\n",
      "Epoch [3408/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3409/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3410/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3411/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3412/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3413/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3414/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3415/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3416/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3417/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3418/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3419/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3420/10000], Loss: 0.2946, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3421/10000], Loss: 0.2945, macro F1 Train: 0.7271, macro F1 Test: 0.6318\n",
      "Epoch [3422/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3423/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3424/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3425/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3426/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3427/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3428/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3429/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3430/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3431/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3432/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3433/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3434/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3435/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3436/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3437/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3438/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3439/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3440/10000], Loss: 0.2945, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3441/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3442/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6318\n",
      "Epoch [3443/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3444/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3445/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3446/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3447/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3448/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3449/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3450/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3451/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3452/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3453/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3454/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3455/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3456/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3457/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3458/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3459/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3460/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3461/10000], Loss: 0.2944, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3462/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3463/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3464/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3465/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3466/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3467/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3468/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3469/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3470/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3471/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3472/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3473/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3474/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3475/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3476/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3477/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3478/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3479/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3480/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3481/10000], Loss: 0.2943, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3482/10000], Loss: 0.2942, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3483/10000], Loss: 0.2942, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3484/10000], Loss: 0.2942, macro F1 Train: 0.7272, macro F1 Test: 0.6317\n",
      "Epoch [3485/10000], Loss: 0.2942, macro F1 Train: 0.7273, macro F1 Test: 0.6317\n",
      "Epoch [3486/10000], Loss: 0.2942, macro F1 Train: 0.7273, macro F1 Test: 0.6317\n",
      "Epoch [3487/10000], Loss: 0.2942, macro F1 Train: 0.7273, macro F1 Test: 0.6317\n",
      "Epoch [3488/10000], Loss: 0.2942, macro F1 Train: 0.7273, macro F1 Test: 0.6317\n",
      "Epoch [3489/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3490/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3491/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3492/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3493/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3494/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3495/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3496/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3497/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3498/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3499/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3500/10000], Loss: 0.2942, macro F1 Train: 0.7275, macro F1 Test: 0.6317\n",
      "Epoch [3501/10000], Loss: 0.2942, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3502/10000], Loss: 0.2942, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3503/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3504/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3505/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3506/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3507/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6317\n",
      "Epoch [3508/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3509/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3510/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3511/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3512/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3513/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3514/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3515/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3516/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3517/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3518/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3519/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3520/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3521/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3522/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3523/10000], Loss: 0.2941, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3524/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3525/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3526/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3527/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3528/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3529/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3530/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3531/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3532/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3533/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3534/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3535/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3536/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3537/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3538/10000], Loss: 0.2940, macro F1 Train: 0.7277, macro F1 Test: 0.6323\n",
      "Epoch [3539/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3540/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3541/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3542/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3543/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3544/10000], Loss: 0.2940, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3545/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3546/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6323\n",
      "Epoch [3547/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3548/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3549/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3550/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3551/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3552/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3553/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3554/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3555/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3556/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3557/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3558/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3559/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3560/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3561/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3562/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3563/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3564/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3565/10000], Loss: 0.2939, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3566/10000], Loss: 0.2938, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3567/10000], Loss: 0.2938, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3568/10000], Loss: 0.2938, macro F1 Train: 0.7276, macro F1 Test: 0.6324\n",
      "Epoch [3569/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3570/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3571/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3572/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3573/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3574/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3575/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6324\n",
      "Epoch [3576/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3577/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3578/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3579/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3580/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3581/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3582/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3583/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3584/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3585/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3586/10000], Loss: 0.2938, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3587/10000], Loss: 0.2937, macro F1 Train: 0.7279, macro F1 Test: 0.6320\n",
      "Epoch [3588/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3589/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3590/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3591/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3592/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3593/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3594/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3595/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3596/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3597/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3598/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3599/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3600/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3601/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3602/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3603/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3604/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3605/10000], Loss: 0.2937, macro F1 Train: 0.7280, macro F1 Test: 0.6320\n",
      "Epoch [3606/10000], Loss: 0.2937, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3607/10000], Loss: 0.2937, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3608/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3609/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3610/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3611/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3612/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3613/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3614/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3615/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3616/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3617/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3618/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3619/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3620/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3621/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3622/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3623/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3624/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3625/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3626/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3627/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3628/10000], Loss: 0.2936, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3629/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3630/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3631/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3632/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3633/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3634/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3635/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3636/10000], Loss: 0.2935, macro F1 Train: 0.7281, macro F1 Test: 0.6320\n",
      "Epoch [3637/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3638/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3639/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3640/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3641/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3642/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3643/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3644/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3645/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3646/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3647/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3648/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3649/10000], Loss: 0.2935, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3650/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3651/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3652/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3653/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3654/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3655/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3656/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3657/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3658/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3659/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3660/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3661/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3662/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3663/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3664/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3665/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3666/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3667/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3668/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3669/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3670/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3671/10000], Loss: 0.2934, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3672/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3673/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3674/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3675/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3676/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3677/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3678/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3679/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3680/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3681/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3682/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3683/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3684/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3685/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3686/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3687/10000], Loss: 0.2933, macro F1 Train: 0.7282, macro F1 Test: 0.6320\n",
      "Epoch [3688/10000], Loss: 0.2933, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3689/10000], Loss: 0.2933, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3690/10000], Loss: 0.2933, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3691/10000], Loss: 0.2933, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3692/10000], Loss: 0.2933, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3693/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6320\n",
      "Epoch [3694/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3695/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3696/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3697/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3698/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3699/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3700/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3701/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3702/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3703/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3704/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3705/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3706/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3707/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6321\n",
      "Epoch [3708/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3709/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3710/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3711/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3712/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3713/10000], Loss: 0.2932, macro F1 Train: 0.7283, macro F1 Test: 0.6318\n",
      "Epoch [3714/10000], Loss: 0.2932, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3715/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3716/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3717/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3718/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3719/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3720/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3721/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3722/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3723/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3724/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6318\n",
      "Epoch [3725/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3726/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3727/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3728/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3729/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3730/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3731/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3732/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3733/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3734/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3735/10000], Loss: 0.2931, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3736/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3737/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3738/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3739/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3740/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3741/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3742/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3743/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3744/10000], Loss: 0.2930, macro F1 Train: 0.7282, macro F1 Test: 0.6317\n",
      "Epoch [3745/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3746/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3747/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3748/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3749/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3750/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3751/10000], Loss: 0.2930, macro F1 Train: 0.7283, macro F1 Test: 0.6317\n",
      "Epoch [3752/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3753/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3754/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3755/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3756/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3757/10000], Loss: 0.2930, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3758/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3759/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3760/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3761/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3762/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3763/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3764/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3765/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3766/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3767/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3768/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3769/10000], Loss: 0.2929, macro F1 Train: 0.7284, macro F1 Test: 0.6317\n",
      "Epoch [3770/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3771/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3772/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3773/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3774/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3775/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3776/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3777/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3778/10000], Loss: 0.2929, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3779/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3780/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3781/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3782/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3783/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3784/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3785/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3786/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3787/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3788/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3789/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3790/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3791/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3792/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3793/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3794/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3795/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3796/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3797/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3798/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3799/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3800/10000], Loss: 0.2928, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3801/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3802/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3803/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3804/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3805/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3806/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3807/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3808/10000], Loss: 0.2927, macro F1 Train: 0.7285, macro F1 Test: 0.6317\n",
      "Epoch [3809/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3810/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3811/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3812/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3813/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3814/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3815/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3816/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3817/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3818/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3819/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3820/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3821/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3822/10000], Loss: 0.2927, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3823/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3824/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3825/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3826/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3827/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3828/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3829/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6317\n",
      "Epoch [3830/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3831/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3832/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3833/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3834/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3835/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3836/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3837/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3838/10000], Loss: 0.2926, macro F1 Train: 0.7286, macro F1 Test: 0.6318\n",
      "Epoch [3839/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3840/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3841/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3842/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3843/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3844/10000], Loss: 0.2926, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3845/10000], Loss: 0.2925, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3846/10000], Loss: 0.2925, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3847/10000], Loss: 0.2925, macro F1 Train: 0.7287, macro F1 Test: 0.6318\n",
      "Epoch [3848/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6318\n",
      "Epoch [3849/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6318\n",
      "Epoch [3850/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3851/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3852/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3853/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3854/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3855/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3856/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3857/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3858/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3859/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3860/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3861/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3862/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3863/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3864/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3865/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3866/10000], Loss: 0.2925, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3867/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3868/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3869/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3870/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3871/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6317\n",
      "Epoch [3872/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3873/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3874/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3875/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3876/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3877/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3878/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3879/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3880/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3881/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3882/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6313\n",
      "Epoch [3883/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3884/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3885/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3886/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3887/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3888/10000], Loss: 0.2924, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3889/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3890/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3891/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3892/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3893/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3894/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3895/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3896/10000], Loss: 0.2923, macro F1 Train: 0.7288, macro F1 Test: 0.6314\n",
      "Epoch [3897/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3898/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3899/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3900/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3901/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3902/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3903/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3904/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3905/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3906/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3907/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3908/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3909/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3910/10000], Loss: 0.2923, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3911/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3912/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3913/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3914/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3915/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3916/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3917/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3918/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3919/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3920/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3921/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3922/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3923/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3924/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3925/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3926/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3927/10000], Loss: 0.2922, macro F1 Train: 0.7289, macro F1 Test: 0.6314\n",
      "Epoch [3928/10000], Loss: 0.2922, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3929/10000], Loss: 0.2922, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3930/10000], Loss: 0.2922, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3931/10000], Loss: 0.2922, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3932/10000], Loss: 0.2922, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3933/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3934/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3935/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3936/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3937/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3938/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3939/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3940/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3941/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3942/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3943/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3944/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3945/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3946/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3947/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3948/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3949/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3950/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3951/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3952/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3953/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3954/10000], Loss: 0.2921, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3955/10000], Loss: 0.2920, macro F1 Train: 0.7290, macro F1 Test: 0.6314\n",
      "Epoch [3956/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3957/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3958/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3959/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3960/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3961/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3962/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3963/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3964/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3965/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3966/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3967/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3968/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3969/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3970/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3971/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3972/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3973/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3974/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3975/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3976/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3977/10000], Loss: 0.2920, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3978/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3979/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3980/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3981/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3982/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3983/10000], Loss: 0.2919, macro F1 Train: 0.7291, macro F1 Test: 0.6314\n",
      "Epoch [3984/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3985/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3986/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3987/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3988/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3989/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3990/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3991/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3992/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3993/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3994/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3995/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3996/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3997/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3998/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [3999/10000], Loss: 0.2919, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4000/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4001/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4002/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4003/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4004/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4005/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4006/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4007/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4008/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4009/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4010/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4011/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4012/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4013/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4014/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4015/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4016/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4017/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4018/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4019/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4020/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4021/10000], Loss: 0.2918, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4022/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4023/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4024/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4025/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4026/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4027/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4028/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4029/10000], Loss: 0.2917, macro F1 Train: 0.7292, macro F1 Test: 0.6314\n",
      "Epoch [4030/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6314\n",
      "Epoch [4031/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6314\n",
      "Epoch [4032/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6314\n",
      "Epoch [4033/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4034/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4035/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4036/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4037/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4038/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4039/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4040/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4041/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4042/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4043/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4044/10000], Loss: 0.2917, macro F1 Train: 0.7293, macro F1 Test: 0.6315\n",
      "Epoch [4045/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4046/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4047/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4048/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4049/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4050/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4051/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4052/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4053/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4054/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4055/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4056/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4057/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4058/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6315\n",
      "Epoch [4059/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4060/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4061/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4062/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4063/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4064/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4065/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4066/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4067/10000], Loss: 0.2916, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4068/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4069/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4070/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4071/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4072/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4073/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4074/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4075/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4076/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4077/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4078/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4079/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4080/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4081/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4082/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4083/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4084/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4085/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4086/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4087/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4088/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4089/10000], Loss: 0.2915, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4090/10000], Loss: 0.2914, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4091/10000], Loss: 0.2914, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4092/10000], Loss: 0.2914, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4093/10000], Loss: 0.2914, macro F1 Train: 0.7294, macro F1 Test: 0.6316\n",
      "Epoch [4094/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4095/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4096/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4097/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4098/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4099/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4100/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4101/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4102/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4103/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4104/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4105/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4106/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4107/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4108/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4109/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4110/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4111/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4112/10000], Loss: 0.2914, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4113/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4114/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4115/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4116/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4117/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4118/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4119/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4120/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4121/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4122/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4123/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4124/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4125/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4126/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4127/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4128/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4129/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4130/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4131/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4132/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4133/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4134/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4135/10000], Loss: 0.2913, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4136/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4137/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4138/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6316\n",
      "Epoch [4139/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4140/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4141/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4142/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4143/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4144/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4145/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4146/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4147/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4148/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4149/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4150/10000], Loss: 0.2912, macro F1 Train: 0.7295, macro F1 Test: 0.6317\n",
      "Epoch [4151/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4152/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4153/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4154/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4155/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4156/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4157/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4158/10000], Loss: 0.2912, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4159/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4160/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4161/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4162/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4163/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4164/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4165/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4166/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4167/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4168/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4169/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4170/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4171/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4172/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6317\n",
      "Epoch [4173/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4174/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4175/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4176/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4177/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4178/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4179/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4180/10000], Loss: 0.2911, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4181/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4182/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6319\n",
      "Epoch [4183/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4184/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4185/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4186/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4187/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4188/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4189/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6316\n",
      "Epoch [4190/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4191/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4192/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4193/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4194/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4195/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4196/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4197/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4198/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4199/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4200/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4201/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4202/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4203/10000], Loss: 0.2910, macro F1 Train: 0.7298, macro F1 Test: 0.6325\n",
      "Epoch [4204/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4205/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4206/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4207/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4208/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4209/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4210/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4211/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4212/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4213/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4214/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4215/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4216/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4217/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4218/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4219/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4220/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4221/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4222/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4223/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6325\n",
      "Epoch [4224/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4225/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4226/10000], Loss: 0.2909, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4227/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4228/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4229/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4230/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4231/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4232/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4233/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4234/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4235/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4236/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4237/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4238/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4239/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4240/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4241/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4242/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6327\n",
      "Epoch [4243/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4244/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4245/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4246/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4247/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4248/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4249/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4250/10000], Loss: 0.2908, macro F1 Train: 0.7299, macro F1 Test: 0.6328\n",
      "Epoch [4251/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4252/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4253/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4254/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4255/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4256/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4257/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4258/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4259/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4260/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4261/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4262/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4263/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4264/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4265/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4266/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4267/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4268/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4269/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4270/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4271/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4272/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4273/10000], Loss: 0.2907, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4274/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4275/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4276/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4277/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4278/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4279/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4280/10000], Loss: 0.2906, macro F1 Train: 0.7299, macro F1 Test: 0.6329\n",
      "Epoch [4281/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6329\n",
      "Epoch [4282/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4283/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4284/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4285/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4286/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4287/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4288/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4289/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4290/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4291/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4292/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4293/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4294/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4295/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4296/10000], Loss: 0.2906, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4297/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4298/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4299/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4300/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4301/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4302/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4303/10000], Loss: 0.2905, macro F1 Train: 0.7300, macro F1 Test: 0.6322\n",
      "Epoch [4304/10000], Loss: 0.2905, macro F1 Train: 0.7301, macro F1 Test: 0.6322\n",
      "Epoch [4305/10000], Loss: 0.2905, macro F1 Train: 0.7301, macro F1 Test: 0.6322\n",
      "Epoch [4306/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4307/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4308/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4309/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4310/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4311/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4312/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4313/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4314/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4315/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4316/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4317/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4318/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4319/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4320/10000], Loss: 0.2905, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4321/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4322/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4323/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4324/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4325/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4326/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4327/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4328/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4329/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4330/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4331/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4332/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4333/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4334/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4335/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4336/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4337/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4338/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4339/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4340/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4341/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4342/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4343/10000], Loss: 0.2904, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4344/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4345/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4346/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4347/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4348/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4349/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4350/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4351/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4352/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4353/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4354/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4355/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4356/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4357/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4358/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4359/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4360/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4361/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4362/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4363/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4364/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6322\n",
      "Epoch [4365/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4366/10000], Loss: 0.2903, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4367/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4368/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4369/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4370/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4371/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4372/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4373/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4374/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4375/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4376/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4377/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4378/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4379/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4380/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4381/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4382/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4383/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4384/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4385/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4386/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4387/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4388/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4389/10000], Loss: 0.2902, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4390/10000], Loss: 0.2902, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4391/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4392/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4393/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4394/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4395/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4396/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4397/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4398/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4399/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4400/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4401/10000], Loss: 0.2901, macro F1 Train: 0.7302, macro F1 Test: 0.6321\n",
      "Epoch [4402/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4403/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4404/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4405/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4406/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4407/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4408/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4409/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4410/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4411/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4412/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4413/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4414/10000], Loss: 0.2901, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4415/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4416/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4417/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4418/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4419/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4420/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4421/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4422/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4423/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4424/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4425/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4426/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4427/10000], Loss: 0.2900, macro F1 Train: 0.7303, macro F1 Test: 0.6321\n",
      "Epoch [4428/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4429/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4430/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4431/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4432/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4433/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4434/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6321\n",
      "Epoch [4435/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4436/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4437/10000], Loss: 0.2900, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4438/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4439/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4440/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4441/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6323\n",
      "Epoch [4442/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4443/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4444/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4445/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4446/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4447/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4448/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4449/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4450/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4451/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4452/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4453/10000], Loss: 0.2899, macro F1 Train: 0.7304, macro F1 Test: 0.6319\n",
      "Epoch [4454/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4455/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4456/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4457/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4458/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4459/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4460/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4461/10000], Loss: 0.2899, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4462/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4463/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4464/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4465/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4466/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4467/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4468/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4469/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4470/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4471/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4472/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4473/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4474/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4475/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4476/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4477/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4478/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4479/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4480/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4481/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4482/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4483/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4484/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4485/10000], Loss: 0.2898, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4486/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4487/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4488/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4489/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4490/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4491/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4492/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4493/10000], Loss: 0.2897, macro F1 Train: 0.7305, macro F1 Test: 0.6319\n",
      "Epoch [4494/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4495/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4496/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4497/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4498/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4499/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4500/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4501/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4502/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4503/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4504/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4505/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4506/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4507/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4508/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4509/10000], Loss: 0.2897, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4510/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4511/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4512/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4513/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4514/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4515/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4516/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4517/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4518/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4519/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4520/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4521/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4522/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4523/10000], Loss: 0.2896, macro F1 Train: 0.7306, macro F1 Test: 0.6319\n",
      "Epoch [4524/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4525/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4526/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4527/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4528/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4529/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4530/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4531/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4532/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4533/10000], Loss: 0.2896, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4534/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4535/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4536/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4537/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4538/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4539/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4540/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4541/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4542/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4543/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4544/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4545/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4546/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4547/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4548/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4549/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4550/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4551/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4552/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4553/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4554/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4555/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4556/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4557/10000], Loss: 0.2895, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4558/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4559/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4560/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4561/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4562/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4563/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4564/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4565/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4566/10000], Loss: 0.2894, macro F1 Train: 0.7307, macro F1 Test: 0.6319\n",
      "Epoch [4567/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4568/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4569/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4570/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4571/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4572/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4573/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4574/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4575/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4576/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4577/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4578/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4579/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4580/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4581/10000], Loss: 0.2894, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4582/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4583/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4584/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4585/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4586/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4587/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4588/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4589/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4590/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4591/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4592/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4593/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4594/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4595/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4596/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4597/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4598/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4599/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4600/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4601/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4602/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4603/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4604/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4605/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4606/10000], Loss: 0.2893, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4607/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4608/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4609/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4610/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4611/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4612/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4613/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6318\n",
      "Epoch [4614/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6318\n",
      "Epoch [4615/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6318\n",
      "Epoch [4616/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4617/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4618/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4619/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4620/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4621/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4622/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4623/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4624/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4625/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4626/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4627/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4628/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4629/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4630/10000], Loss: 0.2892, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4631/10000], Loss: 0.2891, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4632/10000], Loss: 0.2891, macro F1 Train: 0.7308, macro F1 Test: 0.6319\n",
      "Epoch [4633/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4634/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4635/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4636/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4637/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4638/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4639/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4640/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4641/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4642/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4643/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4644/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4645/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4646/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4647/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4648/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4649/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4650/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4651/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4652/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4653/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4654/10000], Loss: 0.2891, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4655/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4656/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4657/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4658/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4659/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6319\n",
      "Epoch [4660/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4661/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4662/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4663/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4664/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4665/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4666/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4667/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4668/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4669/10000], Loss: 0.2890, macro F1 Train: 0.7309, macro F1 Test: 0.6320\n",
      "Epoch [4670/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4671/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4672/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4673/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4674/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4675/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4676/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4677/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4678/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4679/10000], Loss: 0.2890, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4680/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4681/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4682/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4683/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4684/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4685/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4686/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4687/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4688/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4689/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4690/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4691/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4692/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4693/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4694/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4695/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4696/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4697/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4698/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4699/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4700/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4701/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4702/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4703/10000], Loss: 0.2889, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4704/10000], Loss: 0.2888, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4705/10000], Loss: 0.2888, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4706/10000], Loss: 0.2888, macro F1 Train: 0.7310, macro F1 Test: 0.6320\n",
      "Epoch [4707/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4708/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4709/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4710/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4711/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4712/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4713/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4714/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4715/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4716/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4717/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4718/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4719/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4720/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4721/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4722/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4723/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4724/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4725/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4726/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4727/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4728/10000], Loss: 0.2888, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4729/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4730/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4731/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4732/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4733/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4734/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4735/10000], Loss: 0.2887, macro F1 Train: 0.7311, macro F1 Test: 0.6320\n",
      "Epoch [4736/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6320\n",
      "Epoch [4737/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4738/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4739/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4740/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4741/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4742/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6324\n",
      "Epoch [4743/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4744/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4745/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4746/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4747/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4748/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4749/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4750/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4751/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4752/10000], Loss: 0.2887, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4753/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4754/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4755/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4756/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4757/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4758/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4759/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4760/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4761/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6319\n",
      "Epoch [4762/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4763/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4764/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4765/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4766/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6322\n",
      "Epoch [4767/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4768/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4769/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6318\n",
      "Epoch [4770/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4771/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4772/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4773/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4774/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4775/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4776/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4777/10000], Loss: 0.2886, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4778/10000], Loss: 0.2885, macro F1 Train: 0.7312, macro F1 Test: 0.6318\n",
      "Epoch [4779/10000], Loss: 0.2885, macro F1 Train: 0.7312, macro F1 Test: 0.6321\n",
      "Epoch [4780/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4781/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4782/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4783/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4784/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4785/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4786/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4787/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4788/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4789/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4790/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4791/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4792/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4793/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4794/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4795/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4796/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4797/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4798/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4799/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4800/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4801/10000], Loss: 0.2885, macro F1 Train: 0.7313, macro F1 Test: 0.6321\n",
      "Epoch [4802/10000], Loss: 0.2885, macro F1 Train: 0.7314, macro F1 Test: 0.6321\n",
      "Epoch [4803/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4804/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4805/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4806/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4807/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4808/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4809/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4810/10000], Loss: 0.2884, macro F1 Train: 0.7314, macro F1 Test: 0.6322\n",
      "Epoch [4811/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4812/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4813/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4814/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4815/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4816/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4817/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4818/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4819/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4820/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4821/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4822/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4823/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4824/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4825/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4826/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4827/10000], Loss: 0.2884, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4828/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4829/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4830/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4831/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4832/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4833/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4834/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4835/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4836/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4837/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4838/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4839/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4840/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4841/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4842/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4843/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4844/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4845/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4846/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4847/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4848/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4849/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4850/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4851/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4852/10000], Loss: 0.2883, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4853/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4854/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4855/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4856/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6322\n",
      "Epoch [4857/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6323\n",
      "Epoch [4858/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6323\n",
      "Epoch [4859/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6323\n",
      "Epoch [4860/10000], Loss: 0.2882, macro F1 Train: 0.7315, macro F1 Test: 0.6323\n",
      "Epoch [4861/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4862/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4863/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4864/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4865/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4866/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4867/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4868/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4869/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4870/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4871/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4872/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4873/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4874/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4875/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4876/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4877/10000], Loss: 0.2882, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4878/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4879/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4880/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4881/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4882/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4883/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4884/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4885/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4886/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4887/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4888/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4889/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4890/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4891/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4892/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4893/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4894/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4895/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4896/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4897/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4898/10000], Loss: 0.2881, macro F1 Train: 0.7316, macro F1 Test: 0.6323\n",
      "Epoch [4899/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4900/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4901/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4902/10000], Loss: 0.2881, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4903/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4904/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4905/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4906/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4907/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4908/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4909/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4910/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4911/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4912/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4913/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4914/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4915/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4916/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4917/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4918/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4919/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4920/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4921/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4922/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4923/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4924/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4925/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4926/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4927/10000], Loss: 0.2880, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4928/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4929/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4930/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4931/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4932/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4933/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4934/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6323\n",
      "Epoch [4935/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4936/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4937/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4938/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4939/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4940/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4941/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4942/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4943/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4944/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4945/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4946/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4947/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4948/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4949/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4950/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4951/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4952/10000], Loss: 0.2879, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4953/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4954/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4955/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4956/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4957/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4958/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4959/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4960/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4961/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4962/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4963/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4964/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4965/10000], Loss: 0.2878, macro F1 Train: 0.7317, macro F1 Test: 0.6324\n",
      "Epoch [4966/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4967/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4968/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4969/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4970/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4971/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4972/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4973/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4974/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4975/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4976/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4977/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4978/10000], Loss: 0.2878, macro F1 Train: 0.7318, macro F1 Test: 0.6324\n",
      "Epoch [4979/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4980/10000], Loss: 0.2877, macro F1 Train: 0.7319, macro F1 Test: 0.6326\n",
      "Epoch [4981/10000], Loss: 0.2877, macro F1 Train: 0.7319, macro F1 Test: 0.6326\n",
      "Epoch [4982/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4983/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4984/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4985/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4986/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4987/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4988/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4989/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4990/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4991/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4992/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4993/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6326\n",
      "Epoch [4994/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6311\n",
      "Epoch [4995/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6311\n",
      "Epoch [4996/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6311\n",
      "Epoch [4997/10000], Loss: 0.2877, macro F1 Train: 0.7318, macro F1 Test: 0.6311\n",
      "Epoch [4998/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [4999/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5000/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5001/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5002/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5003/10000], Loss: 0.2877, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5004/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5005/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5006/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5007/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5008/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5009/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5010/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5011/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5012/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5013/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5014/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5015/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5016/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5017/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5018/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5019/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5020/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5021/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5022/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5023/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5024/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5025/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5026/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5027/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5028/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5029/10000], Loss: 0.2876, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5030/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5031/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5032/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5033/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5034/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5035/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5036/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5037/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6311\n",
      "Epoch [5038/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5039/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5040/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5041/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5042/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5043/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5044/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5045/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5046/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5047/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5048/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5049/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5050/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5051/10000], Loss: 0.2875, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5052/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5053/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5054/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5055/10000], Loss: 0.2875, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5056/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5057/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5058/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5059/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5060/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6308\n",
      "Epoch [5061/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6308\n",
      "Epoch [5062/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6306\n",
      "Epoch [5063/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6306\n",
      "Epoch [5064/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6306\n",
      "Epoch [5065/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6306\n",
      "Epoch [5066/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6306\n",
      "Epoch [5067/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5068/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5069/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5070/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5071/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5072/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5073/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5074/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5075/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5076/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5077/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5078/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5079/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5080/10000], Loss: 0.2874, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5081/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5082/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5083/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5084/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6310\n",
      "Epoch [5085/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5086/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5087/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5088/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5089/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6307\n",
      "Epoch [5090/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6310\n",
      "Epoch [5091/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6324\n",
      "Epoch [5092/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6324\n",
      "Epoch [5093/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6324\n",
      "Epoch [5094/10000], Loss: 0.2873, macro F1 Train: 0.7320, macro F1 Test: 0.6324\n",
      "Epoch [5095/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5096/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5097/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5098/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5099/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5100/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5101/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5102/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5103/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5104/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5105/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5106/10000], Loss: 0.2873, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5107/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5108/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5109/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5110/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5111/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5112/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5113/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6321\n",
      "Epoch [5114/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6320\n",
      "Epoch [5115/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6320\n",
      "Epoch [5116/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6320\n",
      "Epoch [5117/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5118/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5119/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5120/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5121/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5122/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5123/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5124/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5125/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5126/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5127/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6323\n",
      "Epoch [5128/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5129/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5130/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5131/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5132/10000], Loss: 0.2872, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5133/10000], Loss: 0.2871, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5134/10000], Loss: 0.2871, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5135/10000], Loss: 0.2871, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5136/10000], Loss: 0.2871, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5137/10000], Loss: 0.2871, macro F1 Train: 0.7321, macro F1 Test: 0.6319\n",
      "Epoch [5138/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5139/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5140/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5141/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5142/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5143/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5144/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5145/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5146/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5147/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5148/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5149/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5150/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5151/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5152/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5153/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5154/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5155/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5156/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6319\n",
      "Epoch [5157/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5158/10000], Loss: 0.2871, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5159/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5160/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5161/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5162/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5163/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5164/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5165/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5166/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5167/10000], Loss: 0.2870, macro F1 Train: 0.7322, macro F1 Test: 0.6321\n",
      "Epoch [5168/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5169/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5170/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5171/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5172/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5173/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5174/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6321\n",
      "Epoch [5175/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5176/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5177/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5178/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5179/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5180/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5181/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5182/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5183/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5184/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5185/10000], Loss: 0.2870, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5186/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5187/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5188/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5189/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5190/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5191/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5192/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5193/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5194/10000], Loss: 0.2869, macro F1 Train: 0.7323, macro F1 Test: 0.6322\n",
      "Epoch [5195/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5196/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5197/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5198/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5199/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5200/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5201/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5202/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5203/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5204/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5205/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5206/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5207/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5208/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5209/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5210/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5211/10000], Loss: 0.2869, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5212/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5213/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5214/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5215/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5216/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5217/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5218/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5219/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5220/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5221/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5222/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5223/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5224/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5225/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5226/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5227/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5228/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5229/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5230/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5231/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5232/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5233/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5234/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5235/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5236/10000], Loss: 0.2868, macro F1 Train: 0.7324, macro F1 Test: 0.6322\n",
      "Epoch [5237/10000], Loss: 0.2868, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5238/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5239/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5240/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5241/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5242/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5243/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5244/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5245/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5246/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5247/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5248/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6322\n",
      "Epoch [5249/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5250/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5251/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5252/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5253/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5254/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5255/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5256/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5257/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5258/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5259/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5260/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5261/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5262/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5263/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5264/10000], Loss: 0.2867, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5265/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5266/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5267/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5268/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5269/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5270/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5271/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5272/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5273/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5274/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5275/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5276/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5277/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5278/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5279/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5280/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5281/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5282/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5283/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5284/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5285/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5286/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5287/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5288/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5289/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5290/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5291/10000], Loss: 0.2866, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5292/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5293/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5294/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5295/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5296/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5297/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5298/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5299/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5300/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5301/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5302/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5303/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5304/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5305/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5306/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5307/10000], Loss: 0.2865, macro F1 Train: 0.7325, macro F1 Test: 0.6321\n",
      "Epoch [5308/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5309/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5310/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5311/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5312/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5313/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6321\n",
      "Epoch [5314/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6320\n",
      "Epoch [5315/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6320\n",
      "Epoch [5316/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6320\n",
      "Epoch [5317/10000], Loss: 0.2865, macro F1 Train: 0.7326, macro F1 Test: 0.6320\n",
      "Epoch [5318/10000], Loss: 0.2864, macro F1 Train: 0.7326, macro F1 Test: 0.6320\n",
      "Epoch [5319/10000], Loss: 0.2864, macro F1 Train: 0.7326, macro F1 Test: 0.6316\n",
      "Epoch [5320/10000], Loss: 0.2864, macro F1 Train: 0.7326, macro F1 Test: 0.6316\n",
      "Epoch [5321/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5322/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5323/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5324/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5325/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5326/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5327/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5328/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5329/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5330/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5331/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5332/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5333/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5334/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5335/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5336/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5337/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5338/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5339/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5340/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5341/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5342/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5343/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5344/10000], Loss: 0.2864, macro F1 Train: 0.7327, macro F1 Test: 0.6316\n",
      "Epoch [5345/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5346/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5347/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5348/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5349/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5350/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5351/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5352/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5353/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5354/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5355/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5356/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5357/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5358/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5359/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5360/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5361/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5362/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5363/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5364/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5365/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5366/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5367/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5368/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5369/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5370/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5371/10000], Loss: 0.2863, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5372/10000], Loss: 0.2862, macro F1 Train: 0.7327, macro F1 Test: 0.6318\n",
      "Epoch [5373/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5374/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5375/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5376/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5377/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5378/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5379/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5380/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5381/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5382/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5383/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5384/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5385/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5386/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5387/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5388/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5389/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5390/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5391/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5392/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5393/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5394/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5395/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5396/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5397/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5398/10000], Loss: 0.2862, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5399/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5400/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5401/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5402/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5403/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5404/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5405/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5406/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5407/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5408/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5409/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5410/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5411/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5412/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5413/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5414/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5415/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5416/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5417/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5418/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5419/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5420/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5421/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5422/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5423/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5424/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5425/10000], Loss: 0.2861, macro F1 Train: 0.7328, macro F1 Test: 0.6318\n",
      "Epoch [5426/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5427/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5428/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5429/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5430/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5431/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5432/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5433/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5434/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5435/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5436/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5437/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5438/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5439/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5440/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5441/10000], Loss: 0.2860, macro F1 Train: 0.7329, macro F1 Test: 0.6318\n",
      "Epoch [5442/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5443/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5444/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5445/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5446/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5447/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5448/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5449/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5450/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5451/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5452/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5453/10000], Loss: 0.2860, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5454/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5455/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5456/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6318\n",
      "Epoch [5457/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5458/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5459/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5460/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5461/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5462/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5463/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5464/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5465/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5466/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5467/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5468/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5469/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5470/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5471/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5472/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5473/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5474/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5475/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5476/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5477/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5478/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5479/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5480/10000], Loss: 0.2859, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5481/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5482/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5483/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5484/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5485/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5486/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5487/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5488/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5489/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5490/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5491/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5492/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5493/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5494/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5495/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5496/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5497/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5498/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5499/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5500/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5501/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5502/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5503/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5504/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5505/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5506/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5507/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5508/10000], Loss: 0.2858, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5509/10000], Loss: 0.2857, macro F1 Train: 0.7330, macro F1 Test: 0.6319\n",
      "Epoch [5510/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6319\n",
      "Epoch [5511/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6319\n",
      "Epoch [5512/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6319\n",
      "Epoch [5513/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6319\n",
      "Epoch [5514/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6319\n",
      "Epoch [5515/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5516/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5517/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5518/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5519/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5520/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5521/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5522/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5523/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5524/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5525/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5526/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5527/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5528/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5529/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5530/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5531/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5532/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5533/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5534/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5535/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5536/10000], Loss: 0.2857, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5537/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5538/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5539/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5540/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5541/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5542/10000], Loss: 0.2856, macro F1 Train: 0.7331, macro F1 Test: 0.6318\n",
      "Epoch [5543/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5544/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5545/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5546/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5547/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5548/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5549/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5550/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5551/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5552/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5553/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5554/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5555/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5556/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5557/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5558/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5559/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5560/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5561/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5562/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5563/10000], Loss: 0.2856, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5564/10000], Loss: 0.2855, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5565/10000], Loss: 0.2855, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5566/10000], Loss: 0.2855, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5567/10000], Loss: 0.2855, macro F1 Train: 0.7333, macro F1 Test: 0.6318\n",
      "Epoch [5568/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5569/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5570/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5571/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5572/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5573/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5574/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5575/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5576/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5577/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5578/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5579/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5580/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5581/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5582/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5583/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5584/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5585/10000], Loss: 0.2855, macro F1 Train: 0.7334, macro F1 Test: 0.6318\n",
      "Epoch [5586/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5587/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5588/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5589/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5590/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5591/10000], Loss: 0.2855, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5592/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5593/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5594/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5595/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5596/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5597/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5598/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5599/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5600/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5601/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5602/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5603/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5604/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5605/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5606/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5607/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5608/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5609/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5610/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5611/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5612/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5613/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6318\n",
      "Epoch [5614/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5615/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5616/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5617/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5618/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5619/10000], Loss: 0.2854, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5620/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5621/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5622/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5623/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5624/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5625/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5626/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5627/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5628/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5629/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5630/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5631/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5632/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5633/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5634/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5635/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5636/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5637/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5638/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5639/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5640/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5641/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5642/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5643/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5644/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5645/10000], Loss: 0.2853, macro F1 Train: 0.7335, macro F1 Test: 0.6319\n",
      "Epoch [5646/10000], Loss: 0.2853, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5647/10000], Loss: 0.2853, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5648/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5649/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5650/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5651/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5652/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5653/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5654/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5655/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5656/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5657/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5658/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5659/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5660/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5661/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5662/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5663/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5664/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5665/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5666/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5667/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5668/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5669/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5670/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5671/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5672/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5673/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5674/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5675/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5676/10000], Loss: 0.2852, macro F1 Train: 0.7336, macro F1 Test: 0.6319\n",
      "Epoch [5677/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5678/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5679/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5680/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5681/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5682/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5683/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5684/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5685/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5686/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5687/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5688/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5689/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5690/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5691/10000], Loss: 0.2851, macro F1 Train: 0.7337, macro F1 Test: 0.6319\n",
      "Epoch [5692/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5693/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5694/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5695/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5696/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5697/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5698/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5699/10000], Loss: 0.2851, macro F1 Train: 0.7338, macro F1 Test: 0.6319\n",
      "Epoch [5700/10000], Loss: 0.2851, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5701/10000], Loss: 0.2851, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5702/10000], Loss: 0.2851, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5703/10000], Loss: 0.2851, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5704/10000], Loss: 0.2851, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5705/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5706/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5707/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5708/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5709/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5710/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5711/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5712/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5713/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5714/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5715/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5716/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5717/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5718/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5719/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5720/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5721/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5722/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5723/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5724/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5725/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5726/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5727/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5728/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5729/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5730/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5731/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5732/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5733/10000], Loss: 0.2850, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5734/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5735/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5736/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5737/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5738/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5739/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6319\n",
      "Epoch [5740/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5741/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5742/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5743/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5744/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5745/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5746/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5747/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5748/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6320\n",
      "Epoch [5749/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5750/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5751/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5752/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5753/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5754/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5755/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5756/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5757/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5758/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5759/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5760/10000], Loss: 0.2849, macro F1 Train: 0.7339, macro F1 Test: 0.6322\n",
      "Epoch [5761/10000], Loss: 0.2849, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5762/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5763/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5764/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5765/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5766/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5767/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5768/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5769/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5770/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5771/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5772/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5773/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5774/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5775/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5776/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5777/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5778/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5779/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5780/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5781/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5782/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5783/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5784/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5785/10000], Loss: 0.2848, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5786/10000], Loss: 0.2848, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5787/10000], Loss: 0.2848, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5788/10000], Loss: 0.2848, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5789/10000], Loss: 0.2848, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5790/10000], Loss: 0.2848, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5791/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5792/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5793/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5794/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5795/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5796/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5797/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5798/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5799/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5800/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5801/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5802/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5803/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5804/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5805/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5806/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5807/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5808/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5809/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5810/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5811/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5812/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5813/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5814/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5815/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5816/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5817/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5818/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5819/10000], Loss: 0.2847, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5820/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5821/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5822/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5823/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5824/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5825/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5826/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5827/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5828/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5829/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5830/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5831/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5832/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5833/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6324\n",
      "Epoch [5834/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5835/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5836/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5837/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5838/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5839/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5840/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5841/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5842/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5843/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5844/10000], Loss: 0.2846, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5845/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5846/10000], Loss: 0.2846, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5847/10000], Loss: 0.2846, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5848/10000], Loss: 0.2846, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5849/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5850/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5851/10000], Loss: 0.2845, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5852/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5853/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5854/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5855/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5856/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5857/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5858/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5859/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5860/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5861/10000], Loss: 0.2845, macro F1 Train: 0.7340, macro F1 Test: 0.6322\n",
      "Epoch [5862/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5863/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5864/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5865/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5866/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5867/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5868/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5869/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5870/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5871/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5872/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5873/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5874/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5875/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5876/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5877/10000], Loss: 0.2845, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5878/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5879/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5880/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5881/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5882/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5883/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5884/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5885/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5886/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5887/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5888/10000], Loss: 0.2844, macro F1 Train: 0.7341, macro F1 Test: 0.6322\n",
      "Epoch [5889/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5890/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5891/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5892/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5893/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5894/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5895/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5896/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5897/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5898/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5899/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5900/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5901/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5902/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5903/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5904/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5905/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5906/10000], Loss: 0.2844, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5907/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5908/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5909/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5910/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5911/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5912/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5913/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5914/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5915/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5916/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5917/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5918/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5919/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5920/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5921/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5922/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6322\n",
      "Epoch [5923/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5924/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5925/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5926/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5927/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5928/10000], Loss: 0.2843, macro F1 Train: 0.7342, macro F1 Test: 0.6320\n",
      "Epoch [5929/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5930/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5931/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5932/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5933/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5934/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5935/10000], Loss: 0.2843, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5936/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5937/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5938/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5939/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5940/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5941/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5942/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5943/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5944/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5945/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5946/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5947/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5948/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5949/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5950/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5951/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5952/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5953/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5954/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5955/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5956/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5957/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5958/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5959/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5960/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5961/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5962/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5963/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5964/10000], Loss: 0.2842, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5965/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5966/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5967/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5968/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5969/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5970/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5971/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5972/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5973/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5974/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5975/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5976/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5977/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5978/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5979/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5980/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5981/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5982/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5983/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5984/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5985/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5986/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5987/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5988/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5989/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5990/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5991/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5992/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5993/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5994/10000], Loss: 0.2841, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5995/10000], Loss: 0.2840, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5996/10000], Loss: 0.2840, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5997/10000], Loss: 0.2840, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5998/10000], Loss: 0.2840, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [5999/10000], Loss: 0.2840, macro F1 Train: 0.7343, macro F1 Test: 0.6320\n",
      "Epoch [6000/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6001/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6002/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6003/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6004/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6005/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6006/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6007/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6320\n",
      "Epoch [6008/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6009/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6010/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6011/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6012/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6013/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6014/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6015/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6016/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6017/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6018/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6019/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6020/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6021/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6022/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6023/10000], Loss: 0.2840, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6024/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6025/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6026/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6027/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6028/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6029/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6030/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6031/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6032/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6033/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6034/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6035/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6036/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6037/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6038/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6039/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6040/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6041/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6042/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6043/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6044/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6045/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6046/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6047/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6048/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6049/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6050/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6051/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6052/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6053/10000], Loss: 0.2839, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6054/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6055/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6056/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6057/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6058/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6059/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6060/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6061/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6062/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6063/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6064/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6065/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6066/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6067/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6068/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6069/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6070/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6071/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6072/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6073/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6074/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6075/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6076/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6077/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6322\n",
      "Epoch [6078/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6323\n",
      "Epoch [6079/10000], Loss: 0.2838, macro F1 Train: 0.7344, macro F1 Test: 0.6323\n",
      "Epoch [6080/10000], Loss: 0.2838, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6081/10000], Loss: 0.2838, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6082/10000], Loss: 0.2838, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6083/10000], Loss: 0.2838, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6084/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6085/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6086/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6087/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6088/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6089/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6090/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6091/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6092/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6093/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6094/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6095/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6096/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6097/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6098/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6099/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6100/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6101/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6102/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6103/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6104/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6105/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6106/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6107/10000], Loss: 0.2837, macro F1 Train: 0.7345, macro F1 Test: 0.6323\n",
      "Epoch [6108/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6109/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6110/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6111/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6112/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6113/10000], Loss: 0.2837, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6114/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6115/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6116/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6117/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6118/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6119/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6120/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6121/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6122/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6123/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6124/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6125/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6126/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6127/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6128/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6129/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6130/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6131/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6132/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6133/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6134/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6135/10000], Loss: 0.2836, macro F1 Train: 0.7346, macro F1 Test: 0.6323\n",
      "Epoch [6136/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6137/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6138/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6139/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6140/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6141/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6142/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6143/10000], Loss: 0.2836, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6144/10000], Loss: 0.2835, macro F1 Train: 0.7347, macro F1 Test: 0.6323\n",
      "Epoch [6145/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6146/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6147/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6148/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6149/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6150/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6151/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6152/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6153/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6154/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6155/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6156/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6157/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6158/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6159/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6324\n",
      "Epoch [6160/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6161/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6162/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6163/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6164/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6165/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6166/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6167/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6168/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6169/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6170/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6171/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6172/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6173/10000], Loss: 0.2835, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6174/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6175/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6176/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6177/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6178/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6179/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6180/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6181/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6182/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6183/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6184/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6185/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6186/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6187/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6188/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6189/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6190/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6191/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6323\n",
      "Epoch [6192/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6193/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6194/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6195/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6196/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6197/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6198/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6199/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6200/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6201/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6202/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6203/10000], Loss: 0.2834, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6204/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6205/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6206/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6207/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6208/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6209/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6210/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6211/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6212/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6213/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6214/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6215/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6216/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6217/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6218/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6219/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6220/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6221/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6222/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6223/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6224/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6225/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6226/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6227/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6228/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6229/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6230/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6231/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6232/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6233/10000], Loss: 0.2833, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6234/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6235/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6236/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6237/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6238/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6239/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6240/10000], Loss: 0.2832, macro F1 Train: 0.7348, macro F1 Test: 0.6325\n",
      "Epoch [6241/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6242/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6243/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6244/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6245/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6246/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6247/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6248/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6249/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6250/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6251/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6252/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6253/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6254/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6255/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6256/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6325\n",
      "Epoch [6257/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6258/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6259/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6260/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6261/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6262/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6263/10000], Loss: 0.2832, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6264/10000], Loss: 0.2831, macro F1 Train: 0.7349, macro F1 Test: 0.6324\n",
      "Epoch [6265/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6324\n",
      "Epoch [6266/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6324\n",
      "Epoch [6267/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6324\n",
      "Epoch [6268/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6269/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6270/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6271/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6272/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6273/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6274/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6275/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6276/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6277/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6278/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6279/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6280/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6281/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6282/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6283/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6284/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6285/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6286/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6320\n",
      "Epoch [6287/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6288/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6289/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6290/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6291/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6292/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6293/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6294/10000], Loss: 0.2831, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6295/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6296/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6297/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6298/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6299/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6300/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6301/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6302/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6303/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6304/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6305/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6306/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6307/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6308/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6309/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6310/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6311/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6312/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6313/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6314/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6315/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6316/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6317/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6318/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6319/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6320/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6321/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6322/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6323/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6324/10000], Loss: 0.2830, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6325/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6326/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6327/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6328/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6329/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6330/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6331/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6332/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6333/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6334/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6335/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6336/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6337/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6338/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6339/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6340/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6341/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6342/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6343/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6344/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6345/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6346/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6347/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6348/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6349/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6350/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6351/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6352/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6353/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6354/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6355/10000], Loss: 0.2829, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6356/10000], Loss: 0.2828, macro F1 Train: 0.7350, macro F1 Test: 0.6319\n",
      "Epoch [6357/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6358/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6359/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6360/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6361/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6362/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6363/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6364/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6319\n",
      "Epoch [6365/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6366/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6367/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6368/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6369/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6370/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6371/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6372/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6373/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6374/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6375/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6376/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6377/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6378/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6379/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6380/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6381/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6382/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6383/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6384/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6385/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6386/10000], Loss: 0.2828, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6387/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6388/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6389/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6390/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6391/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6392/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6393/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6394/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6395/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6396/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6397/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6398/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6399/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6400/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6401/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6402/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6403/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6404/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6405/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6406/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6407/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6408/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6409/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6410/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6411/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6412/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6413/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6414/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6415/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6416/10000], Loss: 0.2827, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6417/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6418/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6419/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6420/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6421/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6422/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6423/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6424/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6425/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6426/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6427/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6428/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6429/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6430/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6431/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6432/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6433/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6434/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6435/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6436/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6437/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6438/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6439/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6440/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6441/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6442/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6443/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6444/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6445/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6446/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6447/10000], Loss: 0.2826, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6448/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6449/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6450/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6451/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6452/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6453/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6454/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6455/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6456/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6457/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6458/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6459/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6460/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6461/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6462/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6463/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6464/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6465/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6466/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6467/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6468/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6469/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6470/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6471/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6472/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6473/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6474/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6475/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6476/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6323\n",
      "Epoch [6477/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6324\n",
      "Epoch [6478/10000], Loss: 0.2825, macro F1 Train: 0.7351, macro F1 Test: 0.6324\n",
      "Epoch [6479/10000], Loss: 0.2824, macro F1 Train: 0.7351, macro F1 Test: 0.6324\n",
      "Epoch [6480/10000], Loss: 0.2824, macro F1 Train: 0.7351, macro F1 Test: 0.6324\n",
      "Epoch [6481/10000], Loss: 0.2824, macro F1 Train: 0.7351, macro F1 Test: 0.6324\n",
      "Epoch [6482/10000], Loss: 0.2824, macro F1 Train: 0.7351, macro F1 Test: 0.6320\n",
      "Epoch [6483/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6484/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6485/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6486/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6487/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6488/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6489/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6490/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6491/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6492/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6493/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6494/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6495/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6496/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6497/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6498/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6499/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6500/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6501/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6502/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6503/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6504/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6505/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6506/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6507/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6508/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6509/10000], Loss: 0.2824, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6510/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6511/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6512/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6513/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6514/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6515/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6516/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6517/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6518/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6519/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6520/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6521/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6522/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6523/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6524/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6525/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6526/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6527/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6528/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6529/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6530/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6531/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6532/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6533/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6534/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6535/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6536/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6537/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6538/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6539/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6540/10000], Loss: 0.2823, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6541/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6542/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6543/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6544/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6545/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6546/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6547/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6548/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6549/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6550/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6551/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6552/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6553/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6554/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6555/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6556/10000], Loss: 0.2822, macro F1 Train: 0.7352, macro F1 Test: 0.6320\n",
      "Epoch [6557/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6558/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6559/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6560/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6561/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6562/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6563/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6564/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6565/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6566/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6567/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6568/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6569/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6570/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6571/10000], Loss: 0.2822, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6572/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6573/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6574/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6575/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6576/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6577/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6578/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6579/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6580/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6581/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6582/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6583/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6584/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6585/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6586/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6587/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6588/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6589/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6590/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6591/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6592/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6593/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6594/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6595/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6596/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6320\n",
      "Epoch [6597/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6598/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6599/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6600/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6601/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6602/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6603/10000], Loss: 0.2821, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6604/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6605/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6606/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6607/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6608/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6609/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6610/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6611/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6612/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6613/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6614/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6615/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6616/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6617/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6618/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6619/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6620/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6621/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6622/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6623/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6624/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6625/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6626/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6627/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6628/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6629/10000], Loss: 0.2820, macro F1 Train: 0.7353, macro F1 Test: 0.6321\n",
      "Epoch [6630/10000], Loss: 0.2820, macro F1 Train: 0.7354, macro F1 Test: 0.6321\n",
      "Epoch [6631/10000], Loss: 0.2820, macro F1 Train: 0.7354, macro F1 Test: 0.6321\n",
      "Epoch [6632/10000], Loss: 0.2820, macro F1 Train: 0.7354, macro F1 Test: 0.6321\n",
      "Epoch [6633/10000], Loss: 0.2820, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6634/10000], Loss: 0.2820, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6635/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6636/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6637/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6638/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6639/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6640/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6641/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6642/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6643/10000], Loss: 0.2819, macro F1 Train: 0.7354, macro F1 Test: 0.6325\n",
      "Epoch [6644/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6645/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6646/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6647/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6648/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6649/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6650/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6651/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6652/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6653/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6654/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6655/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6656/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6657/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6658/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6659/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6660/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6661/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6662/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6663/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6664/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6665/10000], Loss: 0.2819, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6666/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6667/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6668/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6669/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6670/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6671/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6672/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6673/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6674/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6675/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6676/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6677/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6678/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6679/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6680/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6681/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6682/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6683/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6684/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6685/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6686/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6687/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6688/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6689/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6690/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6691/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6692/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6693/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6694/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6695/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6696/10000], Loss: 0.2818, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6697/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6698/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6699/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6700/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6701/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6702/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6703/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6704/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6325\n",
      "Epoch [6705/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6706/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6707/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6708/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6709/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6710/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6711/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6712/10000], Loss: 0.2817, macro F1 Train: 0.7355, macro F1 Test: 0.6327\n",
      "Epoch [6713/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6714/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6715/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6716/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6717/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6718/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6719/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6720/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6721/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6722/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6723/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6724/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6725/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6726/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6727/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6728/10000], Loss: 0.2817, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6729/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6730/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6731/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6732/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6733/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6734/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6735/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6736/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6737/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6738/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6739/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6740/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6741/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6742/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6743/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6744/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6745/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6746/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6747/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6748/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6749/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6750/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6751/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6752/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6753/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6754/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6755/10000], Loss: 0.2816, macro F1 Train: 0.7356, macro F1 Test: 0.6327\n",
      "Epoch [6756/10000], Loss: 0.2816, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6757/10000], Loss: 0.2816, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6758/10000], Loss: 0.2816, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6759/10000], Loss: 0.2816, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6760/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6761/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6762/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6763/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6764/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6765/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6766/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6767/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6327\n",
      "Epoch [6768/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6769/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6770/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6771/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6772/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6773/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6774/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6775/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6776/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6777/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6778/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6779/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6780/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6781/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6782/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6783/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6784/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6785/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6786/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6787/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6788/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6789/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6790/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6791/10000], Loss: 0.2815, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6792/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6793/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6794/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6795/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6796/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6797/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6798/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6799/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6800/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6801/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6802/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6803/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6804/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6805/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6806/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6807/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6808/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6809/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6810/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6811/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6812/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6813/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6814/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6815/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6816/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6817/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6818/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6819/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6820/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6821/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6822/10000], Loss: 0.2814, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6823/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6824/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6825/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6826/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6827/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6828/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6829/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6830/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6831/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6832/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6833/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6834/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6835/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6836/10000], Loss: 0.2813, macro F1 Train: 0.7357, macro F1 Test: 0.6328\n",
      "Epoch [6837/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6838/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6839/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6840/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6841/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6842/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6843/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6844/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6845/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6846/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6847/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6848/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6849/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6850/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6851/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6852/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6853/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6854/10000], Loss: 0.2813, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6855/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6856/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6857/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6858/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6859/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6860/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6861/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6862/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6863/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6864/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6865/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6866/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6867/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6868/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6869/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6870/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6871/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6872/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6873/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6874/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6875/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6876/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6877/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6328\n",
      "Epoch [6878/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6879/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6880/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6881/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6882/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6883/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6884/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6885/10000], Loss: 0.2812, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6886/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6887/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6888/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6889/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6890/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6891/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6892/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6893/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6894/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6895/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6896/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6897/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6898/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6899/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6900/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6901/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6902/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6903/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6904/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6905/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6906/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6907/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6908/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6909/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6910/10000], Loss: 0.2811, macro F1 Train: 0.7358, macro F1 Test: 0.6330\n",
      "Epoch [6911/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6912/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6913/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6914/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6915/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6916/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6917/10000], Loss: 0.2811, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6918/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6919/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6920/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6921/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6922/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6923/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6924/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6925/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6926/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6330\n",
      "Epoch [6927/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6928/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6929/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6930/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6931/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6932/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6933/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6934/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6935/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6936/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6937/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6938/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6939/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6940/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6941/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6942/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6943/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6944/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6945/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6946/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6947/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6948/10000], Loss: 0.2810, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6949/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6950/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6951/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6952/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6953/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6954/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6955/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6956/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6957/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6958/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6959/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6960/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6961/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6962/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6963/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6964/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6965/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6966/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6967/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6968/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6969/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6970/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6971/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6972/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6973/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6974/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6975/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6976/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6977/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6978/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6979/10000], Loss: 0.2809, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6980/10000], Loss: 0.2808, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6981/10000], Loss: 0.2808, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6982/10000], Loss: 0.2808, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6983/10000], Loss: 0.2808, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6984/10000], Loss: 0.2808, macro F1 Train: 0.7359, macro F1 Test: 0.6331\n",
      "Epoch [6985/10000], Loss: 0.2808, macro F1 Train: 0.7360, macro F1 Test: 0.6331\n",
      "Epoch [6986/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6987/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6988/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6989/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6990/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6991/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6992/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6993/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6994/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6995/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6996/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6997/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6998/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [6999/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7000/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7001/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7002/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7003/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7004/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7005/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7006/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7007/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7008/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7009/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7010/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7011/10000], Loss: 0.2808, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7012/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7013/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7014/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7015/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7016/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7017/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7018/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7019/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7020/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7021/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7022/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7023/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7024/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7025/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7026/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7027/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7028/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7029/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7030/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7031/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7032/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7033/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7034/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7035/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7036/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7037/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6331\n",
      "Epoch [7038/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7039/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7040/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7041/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7042/10000], Loss: 0.2807, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7043/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7044/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7045/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7046/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7047/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7048/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7049/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7050/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7051/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7052/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7053/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7054/10000], Loss: 0.2806, macro F1 Train: 0.7361, macro F1 Test: 0.6330\n",
      "Epoch [7055/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6330\n",
      "Epoch [7056/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6330\n",
      "Epoch [7057/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6330\n",
      "Epoch [7058/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6330\n",
      "Epoch [7059/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7060/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7061/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7062/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7063/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7064/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7065/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7066/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7067/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7068/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7069/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7070/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7071/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7072/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7073/10000], Loss: 0.2806, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7074/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7075/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7076/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7077/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7078/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7079/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7080/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7081/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7082/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7083/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7084/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7085/10000], Loss: 0.2805, macro F1 Train: 0.7362, macro F1 Test: 0.6329\n",
      "Epoch [7086/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7087/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7088/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7089/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7090/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7091/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7092/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7093/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7094/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7095/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7096/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7097/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7098/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7099/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7100/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7101/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7102/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7103/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7104/10000], Loss: 0.2805, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7105/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6329\n",
      "Epoch [7106/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7107/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7108/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7109/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7110/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7111/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7112/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7113/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7114/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7115/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7116/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7117/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7118/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7119/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7120/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7121/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7122/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7123/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7124/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7125/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7126/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7127/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7128/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7129/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7130/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7131/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7132/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7133/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7134/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7135/10000], Loss: 0.2804, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7136/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7137/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7138/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7139/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7140/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7141/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7142/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7143/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7144/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7145/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7146/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6328\n",
      "Epoch [7147/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7148/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7149/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7150/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7151/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7152/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7153/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7154/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7155/10000], Loss: 0.2803, macro F1 Train: 0.7363, macro F1 Test: 0.6327\n",
      "Epoch [7156/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7157/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7158/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7159/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7160/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7161/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6327\n",
      "Epoch [7162/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6330\n",
      "Epoch [7163/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6330\n",
      "Epoch [7164/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6330\n",
      "Epoch [7165/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6330\n",
      "Epoch [7166/10000], Loss: 0.2803, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7167/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7168/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7169/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7170/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7171/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7172/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7173/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7174/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7175/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7176/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7177/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7178/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7179/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7180/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7181/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7182/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7183/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7184/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7185/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7186/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7187/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7188/10000], Loss: 0.2802, macro F1 Train: 0.7364, macro F1 Test: 0.6332\n",
      "Epoch [7189/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7190/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7191/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7192/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7193/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7194/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7195/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7196/10000], Loss: 0.2802, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7197/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7198/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7199/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7200/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7201/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7202/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7203/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7204/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7205/10000], Loss: 0.2801, macro F1 Train: 0.7365, macro F1 Test: 0.6332\n",
      "Epoch [7206/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7207/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7208/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7209/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7210/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7211/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7212/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7213/10000], Loss: 0.2801, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7214/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7215/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7216/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7217/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7218/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7219/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7220/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7221/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7222/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7223/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7224/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7225/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7226/10000], Loss: 0.2801, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7227/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7228/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7229/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7230/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7231/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7232/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7233/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7234/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7235/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7236/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7237/10000], Loss: 0.2800, macro F1 Train: 0.7366, macro F1 Test: 0.6332\n",
      "Epoch [7238/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7239/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7240/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7241/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7242/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7243/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7244/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7245/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7246/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7247/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7248/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7249/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7250/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7251/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7252/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7253/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7254/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7255/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7256/10000], Loss: 0.2800, macro F1 Train: 0.7367, macro F1 Test: 0.6332\n",
      "Epoch [7257/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7258/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7259/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7260/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7261/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7262/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6331\n",
      "Epoch [7263/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6329\n",
      "Epoch [7264/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6329\n",
      "Epoch [7265/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6329\n",
      "Epoch [7266/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6329\n",
      "Epoch [7267/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7268/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7269/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7270/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7271/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7272/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7273/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7274/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7275/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7276/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7277/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7278/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7279/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7280/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7281/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7282/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7283/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7284/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7285/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7286/10000], Loss: 0.2799, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7287/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7288/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7289/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7290/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7291/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7292/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7293/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7294/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7295/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7296/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7297/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7298/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7299/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7300/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7301/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7302/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7303/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7304/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7305/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7306/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7307/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7308/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7309/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7310/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7311/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7312/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7313/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7314/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7315/10000], Loss: 0.2798, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7316/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7317/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7318/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7319/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7320/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7321/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7322/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7323/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6327\n",
      "Epoch [7324/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7325/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7326/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7327/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7328/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7329/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7330/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7331/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7332/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7333/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7334/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7335/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7336/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7337/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7338/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7339/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7340/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7341/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7342/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7343/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7344/10000], Loss: 0.2797, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7345/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7346/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7347/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7348/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7349/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7350/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7351/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7352/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7353/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7354/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7355/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7356/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7357/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7358/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7359/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7360/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7361/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7362/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7363/10000], Loss: 0.2796, macro F1 Train: 0.7367, macro F1 Test: 0.6328\n",
      "Epoch [7364/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7365/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7366/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7367/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7368/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7369/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7370/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7371/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7372/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7373/10000], Loss: 0.2796, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7374/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7375/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7376/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7377/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7378/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7379/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7380/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7381/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7382/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7383/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7384/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7385/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7386/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7387/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7388/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7389/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7390/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7391/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7392/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7393/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7394/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7395/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7396/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7397/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7398/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7399/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7400/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7401/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7402/10000], Loss: 0.2795, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7403/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7404/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6327\n",
      "Epoch [7405/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6327\n",
      "Epoch [7406/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6327\n",
      "Epoch [7407/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6327\n",
      "Epoch [7408/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6327\n",
      "Epoch [7409/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7410/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7411/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7412/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7413/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7414/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7415/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7416/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7417/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7418/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7419/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7420/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7421/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7422/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7423/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7424/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7425/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7426/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7427/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7428/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7429/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7430/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7431/10000], Loss: 0.2794, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7432/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7433/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6328\n",
      "Epoch [7434/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7435/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7436/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7437/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7438/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7439/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7440/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7441/10000], Loss: 0.2793, macro F1 Train: 0.7368, macro F1 Test: 0.6329\n",
      "Epoch [7442/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6329\n",
      "Epoch [7443/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6329\n",
      "Epoch [7444/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7445/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7446/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7447/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7448/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7449/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7450/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7451/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7452/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7453/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7454/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7455/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7456/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7457/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7458/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7459/10000], Loss: 0.2793, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7460/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7461/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7462/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7463/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7464/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7465/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7466/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7467/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7468/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7469/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7470/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7471/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7472/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7473/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7474/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7475/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7476/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7477/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7478/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7479/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7480/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7481/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7482/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7483/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7484/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7485/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7486/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7487/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7488/10000], Loss: 0.2792, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7489/10000], Loss: 0.2791, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7490/10000], Loss: 0.2791, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7491/10000], Loss: 0.2791, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7492/10000], Loss: 0.2791, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7493/10000], Loss: 0.2791, macro F1 Train: 0.7371, macro F1 Test: 0.6326\n",
      "Epoch [7494/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7495/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7496/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7497/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7498/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7499/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7500/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7501/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7502/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7503/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7504/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7505/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7506/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7507/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7508/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7509/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7510/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7511/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7512/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7513/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7514/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7515/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7516/10000], Loss: 0.2791, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7517/10000], Loss: 0.2790, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7518/10000], Loss: 0.2790, macro F1 Train: 0.7372, macro F1 Test: 0.6326\n",
      "Epoch [7519/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7520/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7521/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7522/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7523/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7524/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7525/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7526/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7527/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7528/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7529/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7530/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7531/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7532/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7533/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7534/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7535/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7536/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7537/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7538/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7539/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7540/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7541/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7542/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7543/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7544/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7545/10000], Loss: 0.2790, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7546/10000], Loss: 0.2789, macro F1 Train: 0.7376, macro F1 Test: 0.6326\n",
      "Epoch [7547/10000], Loss: 0.2789, macro F1 Train: 0.7376, macro F1 Test: 0.6318\n",
      "Epoch [7548/10000], Loss: 0.2789, macro F1 Train: 0.7376, macro F1 Test: 0.6318\n",
      "Epoch [7549/10000], Loss: 0.2789, macro F1 Train: 0.7376, macro F1 Test: 0.6318\n",
      "Epoch [7550/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7551/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7552/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7553/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7554/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7555/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7556/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7557/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7558/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7559/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7560/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7561/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7562/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7563/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7564/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7565/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7566/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7567/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7568/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7569/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7570/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7571/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7572/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7573/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7574/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7575/10000], Loss: 0.2789, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7576/10000], Loss: 0.2788, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7577/10000], Loss: 0.2788, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7578/10000], Loss: 0.2788, macro F1 Train: 0.7377, macro F1 Test: 0.6318\n",
      "Epoch [7579/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7580/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7581/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7582/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7583/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7584/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7585/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7586/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7587/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7588/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7589/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7590/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7591/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7592/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7593/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7594/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7595/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7596/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7597/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7598/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7599/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7600/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7601/10000], Loss: 0.2788, macro F1 Train: 0.7378, macro F1 Test: 0.6318\n",
      "Epoch [7602/10000], Loss: 0.2788, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7603/10000], Loss: 0.2788, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7604/10000], Loss: 0.2788, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7605/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7606/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7607/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7608/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7609/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7610/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7611/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7612/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7613/10000], Loss: 0.2787, macro F1 Train: 0.7379, macro F1 Test: 0.6318\n",
      "Epoch [7614/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7615/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7616/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7617/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7618/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7619/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7620/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7621/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7622/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7623/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7624/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7625/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7626/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7627/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7628/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7629/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7630/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7631/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7632/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7633/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7634/10000], Loss: 0.2787, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7635/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7636/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7637/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7638/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7639/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7640/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7641/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7642/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7643/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7644/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7645/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7646/10000], Loss: 0.2786, macro F1 Train: 0.7382, macro F1 Test: 0.6318\n",
      "Epoch [7647/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7648/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7649/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7650/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7651/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7652/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7653/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7654/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7655/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7656/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7657/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7658/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7659/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7660/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7661/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7662/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7663/10000], Loss: 0.2786, macro F1 Train: 0.7383, macro F1 Test: 0.6318\n",
      "Epoch [7664/10000], Loss: 0.2786, macro F1 Train: 0.7384, macro F1 Test: 0.6318\n",
      "Epoch [7665/10000], Loss: 0.2785, macro F1 Train: 0.7384, macro F1 Test: 0.6318\n",
      "Epoch [7666/10000], Loss: 0.2785, macro F1 Train: 0.7384, macro F1 Test: 0.6318\n",
      "Epoch [7667/10000], Loss: 0.2785, macro F1 Train: 0.7384, macro F1 Test: 0.6318\n",
      "Epoch [7668/10000], Loss: 0.2785, macro F1 Train: 0.7384, macro F1 Test: 0.6318\n",
      "Epoch [7669/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7670/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7671/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7672/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7673/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7674/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7675/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7676/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7677/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7678/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7679/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7680/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7681/10000], Loss: 0.2785, macro F1 Train: 0.7385, macro F1 Test: 0.6318\n",
      "Epoch [7682/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7683/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7684/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7685/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7686/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7687/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7688/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7689/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7690/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7691/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7692/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7693/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7694/10000], Loss: 0.2785, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7695/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7696/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7697/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7698/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7699/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7700/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7701/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7702/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7703/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7704/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7705/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7706/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7707/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7708/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7709/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7710/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7711/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7712/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7713/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7714/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7715/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7716/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7717/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7718/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7719/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7720/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7721/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7722/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7723/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7724/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7725/10000], Loss: 0.2784, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7726/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7727/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7728/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7729/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7730/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7731/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7732/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7733/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7734/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7735/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7736/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7737/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7738/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7739/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7740/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7741/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7742/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7743/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7744/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7745/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7746/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7747/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7748/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7749/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7750/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7751/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7752/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7753/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7754/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7755/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7756/10000], Loss: 0.2783, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7757/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7758/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7759/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7760/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7761/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7762/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7763/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7764/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7765/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7766/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7767/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7768/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7769/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7770/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7771/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7772/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7773/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7774/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7775/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7776/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7777/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7778/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7779/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7780/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7781/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7782/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7783/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7784/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7785/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7786/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7787/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7788/10000], Loss: 0.2782, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7789/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7790/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7791/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7792/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7793/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7794/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7795/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7796/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7797/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7798/10000], Loss: 0.2781, macro F1 Train: 0.7386, macro F1 Test: 0.6318\n",
      "Epoch [7799/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6318\n",
      "Epoch [7800/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7801/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7802/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7803/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7804/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7805/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7806/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7807/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7808/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7809/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7810/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7811/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7812/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7813/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7814/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7815/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6319\n",
      "Epoch [7816/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6323\n",
      "Epoch [7817/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6323\n",
      "Epoch [7818/10000], Loss: 0.2781, macro F1 Train: 0.7387, macro F1 Test: 0.6323\n",
      "Epoch [7819/10000], Loss: 0.2781, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7820/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7821/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7822/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7823/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7824/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7825/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7826/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7827/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7828/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7829/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7830/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7831/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7832/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7833/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7834/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7835/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7836/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7837/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7838/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7839/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7840/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7841/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7842/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7843/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7844/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7845/10000], Loss: 0.2780, macro F1 Train: 0.7388, macro F1 Test: 0.6323\n",
      "Epoch [7846/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7847/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7848/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7849/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7850/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7851/10000], Loss: 0.2780, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7852/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7853/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7854/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7855/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7856/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7857/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7858/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7859/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7860/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7861/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7862/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7863/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7864/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7865/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6323\n",
      "Epoch [7866/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7867/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7868/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7869/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7870/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7871/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7872/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7873/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7874/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7875/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7876/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7877/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7878/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7879/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7880/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7881/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7882/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7883/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7884/10000], Loss: 0.2779, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7885/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7886/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7887/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7888/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7889/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7890/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7891/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7892/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7893/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7894/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7895/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7896/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7897/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7898/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7899/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7900/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7901/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7902/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7903/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7904/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7905/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7906/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7907/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7908/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7909/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7910/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7911/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7912/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7913/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7914/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7915/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7916/10000], Loss: 0.2778, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7917/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7918/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7919/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7920/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7921/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7922/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7923/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7924/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7925/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7926/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7927/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7928/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7929/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7930/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7931/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7932/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7933/10000], Loss: 0.2777, macro F1 Train: 0.7389, macro F1 Test: 0.6324\n",
      "Epoch [7934/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7935/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7936/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7937/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7938/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7939/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7940/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7941/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7942/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7943/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7944/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7945/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7946/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7947/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7948/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7949/10000], Loss: 0.2777, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7950/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7951/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7952/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7953/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7954/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7955/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7956/10000], Loss: 0.2776, macro F1 Train: 0.7390, macro F1 Test: 0.6324\n",
      "Epoch [7957/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7958/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7959/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7960/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7961/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7962/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7963/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7964/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7965/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7966/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7967/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7968/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7969/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7970/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7971/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7972/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7973/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7974/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7975/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7976/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7977/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7978/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7979/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7980/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7981/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7982/10000], Loss: 0.2776, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7983/10000], Loss: 0.2775, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7984/10000], Loss: 0.2775, macro F1 Train: 0.7392, macro F1 Test: 0.6324\n",
      "Epoch [7985/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7986/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7987/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7988/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7989/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7990/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7991/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7992/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7993/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7994/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7995/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7996/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7997/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7998/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [7999/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8000/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8001/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8002/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8003/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8004/10000], Loss: 0.2775, macro F1 Train: 0.7393, macro F1 Test: 0.6324\n",
      "Epoch [8005/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8006/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8007/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8008/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8009/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8010/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8011/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8012/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8013/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8014/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8015/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8016/10000], Loss: 0.2775, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8017/10000], Loss: 0.2774, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8018/10000], Loss: 0.2774, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8019/10000], Loss: 0.2774, macro F1 Train: 0.7394, macro F1 Test: 0.6324\n",
      "Epoch [8020/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8021/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8022/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8023/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8024/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8025/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8026/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8027/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8028/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8029/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8030/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8031/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8032/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8033/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8034/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8035/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8036/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8037/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8038/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8039/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8040/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8041/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8042/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8043/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8044/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8045/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8046/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8047/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8048/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8049/10000], Loss: 0.2774, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8050/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8051/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8052/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8053/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8054/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8055/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8056/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8057/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8058/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8059/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8060/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8061/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8062/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8063/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8064/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8065/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8066/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8067/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8068/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8069/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8070/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8071/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8072/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8073/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8074/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8075/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8076/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8077/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8078/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8079/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6323\n",
      "Epoch [8080/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8081/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6324\n",
      "Epoch [8082/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8083/10000], Loss: 0.2773, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8084/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8085/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8086/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8087/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8088/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8089/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8090/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8091/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8092/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8093/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8094/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8095/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8096/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8097/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8098/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8099/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8100/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8101/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8102/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8103/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8104/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8105/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6309\n",
      "Epoch [8106/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8107/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8108/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8109/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8110/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8111/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8112/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8113/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8114/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8115/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8116/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8117/10000], Loss: 0.2772, macro F1 Train: 0.7395, macro F1 Test: 0.6308\n",
      "Epoch [8118/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6308\n",
      "Epoch [8119/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6308\n",
      "Epoch [8120/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6308\n",
      "Epoch [8121/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6308\n",
      "Epoch [8122/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6308\n",
      "Epoch [8123/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8124/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8125/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8126/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8127/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8128/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8129/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8130/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8131/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8132/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8133/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8134/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8135/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8136/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8137/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8138/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8139/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8140/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8141/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8142/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8143/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8144/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8145/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8146/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8147/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8148/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8149/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8150/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6306\n",
      "Epoch [8151/10000], Loss: 0.2771, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8152/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8153/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8154/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8155/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8156/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8157/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8158/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8159/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8160/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8161/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6305\n",
      "Epoch [8162/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8163/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8164/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8165/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8166/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8167/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8168/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8169/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8170/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8171/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8172/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8173/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8174/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8175/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8176/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8177/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8178/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8179/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8180/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8181/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8182/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8183/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8184/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8185/10000], Loss: 0.2770, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8186/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8187/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8188/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8189/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8190/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8191/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8192/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8193/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8194/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8195/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8196/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8197/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8198/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8199/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8200/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8201/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8202/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8203/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8204/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8205/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8206/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8207/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8208/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8209/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8210/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8211/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8212/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8213/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8214/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8215/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8216/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8217/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8218/10000], Loss: 0.2769, macro F1 Train: 0.7396, macro F1 Test: 0.6300\n",
      "Epoch [8219/10000], Loss: 0.2769, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8220/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8221/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8222/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8223/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8224/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8225/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8226/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8227/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8228/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8229/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8230/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8231/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8232/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8233/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8234/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8235/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8236/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8237/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8238/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8239/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8240/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8241/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8242/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8243/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8244/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8245/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8246/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8247/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8248/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8249/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8250/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8251/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8252/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8253/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8254/10000], Loss: 0.2768, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8255/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8256/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8257/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8258/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8259/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8260/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8261/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8262/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6300\n",
      "Epoch [8263/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6301\n",
      "Epoch [8264/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6301\n",
      "Epoch [8265/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6301\n",
      "Epoch [8266/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6301\n",
      "Epoch [8267/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8268/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8269/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8270/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8271/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8272/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8273/10000], Loss: 0.2767, macro F1 Train: 0.7397, macro F1 Test: 0.6302\n",
      "Epoch [8274/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8275/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8276/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8277/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8278/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8279/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8280/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8281/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8282/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8283/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8284/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8285/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8286/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8287/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8288/10000], Loss: 0.2767, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8289/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8290/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8291/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8292/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8293/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6302\n",
      "Epoch [8294/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8295/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8296/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8297/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8298/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8299/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8300/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8301/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8302/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8303/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8304/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8305/10000], Loss: 0.2766, macro F1 Train: 0.7398, macro F1 Test: 0.6300\n",
      "Epoch [8306/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8307/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8308/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8309/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8310/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8311/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8312/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8313/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8314/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8315/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8316/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8317/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8318/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8319/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8320/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8321/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8322/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8323/10000], Loss: 0.2766, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8324/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8325/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8326/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8327/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8328/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8329/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8330/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8331/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8332/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8333/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8334/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8335/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8336/10000], Loss: 0.2765, macro F1 Train: 0.7399, macro F1 Test: 0.6300\n",
      "Epoch [8337/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8338/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8339/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8340/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8341/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8342/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8343/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8344/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8345/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8346/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8347/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8348/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6300\n",
      "Epoch [8349/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8350/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8351/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8352/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8353/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8354/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8355/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8356/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8357/10000], Loss: 0.2765, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8358/10000], Loss: 0.2764, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8359/10000], Loss: 0.2764, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8360/10000], Loss: 0.2764, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8361/10000], Loss: 0.2764, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8362/10000], Loss: 0.2764, macro F1 Train: 0.7400, macro F1 Test: 0.6299\n",
      "Epoch [8363/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8364/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8365/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8366/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8367/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8368/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8369/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8370/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8371/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8372/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8373/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8374/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8375/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8376/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8377/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8378/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8379/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8380/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8381/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8382/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8383/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8384/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8385/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8386/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8387/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8388/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8389/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8390/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8391/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8392/10000], Loss: 0.2764, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8393/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8394/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8395/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8396/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8397/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8398/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8399/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8400/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8401/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8402/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8403/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6299\n",
      "Epoch [8404/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8405/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8406/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8407/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8408/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8409/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8410/10000], Loss: 0.2763, macro F1 Train: 0.7401, macro F1 Test: 0.6302\n",
      "Epoch [8411/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8412/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8413/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8414/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8415/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8416/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8417/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8418/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8419/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8420/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8421/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8422/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6302\n",
      "Epoch [8423/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8424/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8425/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8426/10000], Loss: 0.2763, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8427/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8428/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8429/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8430/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8431/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8432/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8433/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8434/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8435/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8436/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8437/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8438/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8439/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8440/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8441/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8442/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8443/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8444/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8445/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8446/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8447/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8448/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8449/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8450/10000], Loss: 0.2762, macro F1 Train: 0.7402, macro F1 Test: 0.6301\n",
      "Epoch [8451/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8452/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8453/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8454/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8455/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8456/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8457/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8458/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8459/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8460/10000], Loss: 0.2762, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8461/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8462/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8463/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8464/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8465/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8466/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8467/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8468/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8469/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8470/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8471/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8472/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8473/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8474/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8475/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8476/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8477/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8478/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8479/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8480/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8481/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8482/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8483/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8484/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6301\n",
      "Epoch [8485/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8486/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8487/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8488/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8489/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8490/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8491/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8492/10000], Loss: 0.2761, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8493/10000], Loss: 0.2761, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8494/10000], Loss: 0.2761, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8495/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8496/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8497/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8498/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8499/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8500/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8501/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8502/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8503/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8504/10000], Loss: 0.2760, macro F1 Train: 0.7403, macro F1 Test: 0.6303\n",
      "Epoch [8505/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8506/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8507/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8508/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8509/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8510/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8511/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8512/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8513/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8514/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8515/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8516/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8517/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8518/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8519/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8520/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8521/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8522/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8523/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8524/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8525/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8526/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8527/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8528/10000], Loss: 0.2760, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8529/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8530/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8531/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8532/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8533/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8534/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8535/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8536/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8537/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8538/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8539/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8540/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8541/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8542/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8543/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8544/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8545/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8546/10000], Loss: 0.2759, macro F1 Train: 0.7404, macro F1 Test: 0.6303\n",
      "Epoch [8547/10000], Loss: 0.2759, macro F1 Train: 0.7406, macro F1 Test: 0.6303\n",
      "Epoch [8548/10000], Loss: 0.2759, macro F1 Train: 0.7406, macro F1 Test: 0.6303\n",
      "Epoch [8549/10000], Loss: 0.2759, macro F1 Train: 0.7406, macro F1 Test: 0.6303\n",
      "Epoch [8550/10000], Loss: 0.2759, macro F1 Train: 0.7406, macro F1 Test: 0.6303\n",
      "Epoch [8551/10000], Loss: 0.2759, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8552/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8553/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8554/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8555/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8556/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8557/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8558/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8559/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8560/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8561/10000], Loss: 0.2759, macro F1 Train: 0.7405, macro F1 Test: 0.6305\n",
      "Epoch [8562/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8563/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8564/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8565/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8566/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8567/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8568/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8569/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8570/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8571/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8572/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8573/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8574/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8575/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8576/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8577/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8578/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8579/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8580/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8581/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8582/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8583/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8584/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8585/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8586/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8587/10000], Loss: 0.2758, macro F1 Train: 0.7406, macro F1 Test: 0.6305\n",
      "Epoch [8588/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8589/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8590/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8591/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8592/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8593/10000], Loss: 0.2758, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8594/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8595/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8596/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8597/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8598/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8599/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8600/10000], Loss: 0.2757, macro F1 Train: 0.7409, macro F1 Test: 0.6305\n",
      "Epoch [8601/10000], Loss: 0.2757, macro F1 Train: 0.7410, macro F1 Test: 0.6305\n",
      "Epoch [8602/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8603/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8604/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8605/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8606/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8607/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8608/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8609/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8610/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8611/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8612/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8613/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8614/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8615/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8616/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8617/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8618/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8619/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8620/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8621/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8622/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8623/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8624/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8625/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8626/10000], Loss: 0.2757, macro F1 Train: 0.7411, macro F1 Test: 0.6305\n",
      "Epoch [8627/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8628/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8629/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8630/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8631/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8632/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8633/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8634/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8635/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8636/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8637/10000], Loss: 0.2756, macro F1 Train: 0.7411, macro F1 Test: 0.6306\n",
      "Epoch [8638/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8639/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8640/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8641/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8642/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8643/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8644/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8645/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8646/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8647/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8648/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8649/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8650/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8651/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8652/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8653/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8654/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8655/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8656/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8657/10000], Loss: 0.2756, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8658/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8659/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8660/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8661/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8662/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8663/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8664/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8665/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8666/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8667/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8668/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8669/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8670/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8671/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8672/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8673/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8674/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8675/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8676/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8677/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8678/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8679/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8680/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8681/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8682/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8683/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8684/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8685/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8686/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6305\n",
      "Epoch [8687/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8688/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8689/10000], Loss: 0.2755, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8690/10000], Loss: 0.2754, macro F1 Train: 0.7412, macro F1 Test: 0.6306\n",
      "Epoch [8691/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8692/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8693/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8694/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8695/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8696/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8697/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8698/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8699/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8700/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8701/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8702/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8703/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8704/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8705/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8706/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8707/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8708/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8709/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8710/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8711/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8712/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8713/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8714/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8715/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8716/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8717/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8718/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8719/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8720/10000], Loss: 0.2754, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8721/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8722/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8723/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8724/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8725/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8726/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8727/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8728/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6305\n",
      "Epoch [8729/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8730/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8731/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8732/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8733/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8734/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8735/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8736/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8737/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8738/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8739/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8740/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8741/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8742/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8743/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8744/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8745/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8746/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6306\n",
      "Epoch [8747/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6308\n",
      "Epoch [8748/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6308\n",
      "Epoch [8749/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6308\n",
      "Epoch [8750/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6308\n",
      "Epoch [8751/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8752/10000], Loss: 0.2753, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8753/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8754/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8755/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8756/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8757/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8758/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8759/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8760/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8761/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8762/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8763/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8764/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8765/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8766/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8767/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8768/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8769/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8770/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8771/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8772/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8773/10000], Loss: 0.2752, macro F1 Train: 0.7413, macro F1 Test: 0.6310\n",
      "Epoch [8774/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8775/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8776/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8777/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8778/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8779/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8780/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8781/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8782/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8783/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8784/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8785/10000], Loss: 0.2752, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8786/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8787/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8788/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8789/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8790/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8791/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8792/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8793/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8794/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8795/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8796/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8797/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8798/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8799/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8800/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8801/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8802/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8803/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8804/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8805/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8806/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8807/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8808/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8809/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8810/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8811/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8812/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8813/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8814/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8815/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8816/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8817/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8818/10000], Loss: 0.2751, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8819/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8820/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8821/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8822/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8823/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8824/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8825/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8826/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8827/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8828/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8829/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8830/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8831/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8832/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8833/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8834/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8835/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8836/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8837/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8838/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8839/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8840/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8841/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8842/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8843/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8844/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8845/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8846/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8847/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8848/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8849/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8850/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8851/10000], Loss: 0.2750, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8852/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8853/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8854/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8855/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8856/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6310\n",
      "Epoch [8857/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6324\n",
      "Epoch [8858/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6324\n",
      "Epoch [8859/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6324\n",
      "Epoch [8860/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6324\n",
      "Epoch [8861/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6324\n",
      "Epoch [8862/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8863/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8864/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8865/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8866/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8867/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8868/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8869/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8870/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8871/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8872/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8873/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8874/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8875/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8876/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8877/10000], Loss: 0.2749, macro F1 Train: 0.7414, macro F1 Test: 0.6323\n",
      "Epoch [8878/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8879/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8880/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8881/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8882/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8883/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8884/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8885/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8886/10000], Loss: 0.2749, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8887/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8888/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8889/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8890/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8891/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8892/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8893/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8894/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8895/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8896/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8897/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8898/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8899/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8900/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8901/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8902/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8903/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8904/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6323\n",
      "Epoch [8905/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8906/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8907/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8908/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8909/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8910/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8911/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8912/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8913/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6328\n",
      "Epoch [8914/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8915/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8916/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8917/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8918/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8919/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8920/10000], Loss: 0.2748, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8921/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8922/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6326\n",
      "Epoch [8923/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8924/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8925/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8926/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8927/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8928/10000], Loss: 0.2747, macro F1 Train: 0.7415, macro F1 Test: 0.6325\n",
      "Epoch [8929/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8930/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8931/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8932/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8933/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8934/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8935/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8936/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8937/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8938/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8939/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8940/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8941/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8942/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8943/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8944/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8945/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8946/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8947/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8948/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8949/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8950/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8951/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8952/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8953/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8954/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8955/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8956/10000], Loss: 0.2747, macro F1 Train: 0.7416, macro F1 Test: 0.6325\n",
      "Epoch [8957/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8958/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8959/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8960/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8961/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8962/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8963/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8964/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6324\n",
      "Epoch [8965/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8966/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6328\n",
      "Epoch [8967/10000], Loss: 0.2746, macro F1 Train: 0.7416, macro F1 Test: 0.6328\n",
      "Epoch [8968/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8969/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8970/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8971/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8972/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8973/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8974/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8975/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8976/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8977/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8978/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8979/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8980/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8981/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8982/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8983/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8984/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8985/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8986/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8987/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8988/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8989/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8990/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8991/10000], Loss: 0.2746, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8992/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6328\n",
      "Epoch [8993/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8994/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8995/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8996/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8997/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8998/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6311\n",
      "Epoch [8999/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6311\n",
      "Epoch [9000/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9001/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9002/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9003/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9004/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9005/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9006/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9007/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9008/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9009/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9010/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9011/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9012/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9013/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9014/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9015/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9016/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9017/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9018/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9019/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9020/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9021/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9022/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9023/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9024/10000], Loss: 0.2745, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9025/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9026/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9027/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9028/10000], Loss: 0.2745, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9029/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9030/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9031/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9032/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9033/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9034/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9035/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9036/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9037/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9038/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9039/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9040/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9041/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9042/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9043/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9044/10000], Loss: 0.2744, macro F1 Train: 0.7416, macro F1 Test: 0.6312\n",
      "Epoch [9045/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9046/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9047/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9048/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9049/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9050/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9051/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9052/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9053/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9054/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9055/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9056/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9057/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9058/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9059/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9060/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9061/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9062/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9063/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9064/10000], Loss: 0.2744, macro F1 Train: 0.7417, macro F1 Test: 0.6312\n",
      "Epoch [9065/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9066/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9067/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9068/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9069/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9070/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9071/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9072/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9073/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9074/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9075/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9076/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9077/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9078/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9079/10000], Loss: 0.2743, macro F1 Train: 0.7417, macro F1 Test: 0.6309\n",
      "Epoch [9080/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9081/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9082/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9083/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9084/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9085/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9086/10000], Loss: 0.2743, macro F1 Train: 0.7418, macro F1 Test: 0.6309\n",
      "Epoch [9087/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9088/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9089/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9090/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9091/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9092/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9093/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9094/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9095/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9096/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9097/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9098/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9099/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9100/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9101/10000], Loss: 0.2743, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9102/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9103/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9104/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9105/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9106/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9107/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9108/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9109/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9110/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9111/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9112/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9113/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9114/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9115/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9116/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9117/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9118/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9119/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9120/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9121/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9122/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9123/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9124/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9125/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9126/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9127/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9128/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9129/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9130/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9131/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9132/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9133/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6310\n",
      "Epoch [9134/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6308\n",
      "Epoch [9135/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6308\n",
      "Epoch [9136/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9137/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9138/10000], Loss: 0.2742, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9139/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9140/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9141/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9142/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9143/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9144/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9145/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9146/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9147/10000], Loss: 0.2741, macro F1 Train: 0.7419, macro F1 Test: 0.6309\n",
      "Epoch [9148/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9149/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9150/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9151/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9152/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9153/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9154/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9155/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9156/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9157/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9158/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9159/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9160/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9161/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9162/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9163/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9164/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9165/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9166/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9167/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9168/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9169/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9170/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9171/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9172/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9173/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9174/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9175/10000], Loss: 0.2741, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9176/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9177/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9178/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9179/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9180/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9181/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9182/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9183/10000], Loss: 0.2740, macro F1 Train: 0.7420, macro F1 Test: 0.6309\n",
      "Epoch [9184/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9185/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9186/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9187/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9188/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9189/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9190/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9191/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9192/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9193/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9194/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9195/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9196/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9197/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9198/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9199/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9200/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9201/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9202/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9203/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9204/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9205/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9206/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9207/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9208/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9209/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9210/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9211/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9212/10000], Loss: 0.2740, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9213/10000], Loss: 0.2739, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9214/10000], Loss: 0.2739, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9215/10000], Loss: 0.2739, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9216/10000], Loss: 0.2739, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9217/10000], Loss: 0.2739, macro F1 Train: 0.7421, macro F1 Test: 0.6309\n",
      "Epoch [9218/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9219/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9220/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9221/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9222/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9223/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9224/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9225/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9226/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9227/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9228/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9229/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9230/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9231/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9232/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9233/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9234/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9235/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9236/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9237/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9238/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9239/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9240/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9241/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9242/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9243/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9244/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9245/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9246/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9247/10000], Loss: 0.2739, macro F1 Train: 0.7422, macro F1 Test: 0.6309\n",
      "Epoch [9248/10000], Loss: 0.2739, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9249/10000], Loss: 0.2739, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9250/10000], Loss: 0.2739, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9251/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9252/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9253/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9254/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9255/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9256/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9257/10000], Loss: 0.2738, macro F1 Train: 0.7423, macro F1 Test: 0.6309\n",
      "Epoch [9258/10000], Loss: 0.2738, macro F1 Train: 0.7425, macro F1 Test: 0.6309\n",
      "Epoch [9259/10000], Loss: 0.2738, macro F1 Train: 0.7425, macro F1 Test: 0.6309\n",
      "Epoch [9260/10000], Loss: 0.2738, macro F1 Train: 0.7425, macro F1 Test: 0.6309\n",
      "Epoch [9261/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9262/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9263/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9264/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9265/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9266/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9267/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9268/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9269/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9270/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9271/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9272/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9273/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9274/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9275/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9276/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9277/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9278/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9279/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9280/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9281/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9282/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9283/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9284/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9285/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9286/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9287/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9288/10000], Loss: 0.2738, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9289/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9290/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9291/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9292/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9293/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9294/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9295/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9296/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9297/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9298/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9299/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9300/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6312\n",
      "Epoch [9301/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6312\n",
      "Epoch [9302/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6309\n",
      "Epoch [9303/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6312\n",
      "Epoch [9304/10000], Loss: 0.2737, macro F1 Train: 0.7426, macro F1 Test: 0.6312\n",
      "Epoch [9305/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9306/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9307/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9308/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9309/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9310/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9311/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9312/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9313/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9314/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9315/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9316/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9317/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9318/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9319/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9320/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9321/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9322/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9323/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9324/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9325/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9326/10000], Loss: 0.2737, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9327/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9328/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9329/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9330/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9331/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9332/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9333/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9334/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9335/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9336/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9337/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9338/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9339/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9340/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9341/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9342/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9343/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9344/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6311\n",
      "Epoch [9345/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6311\n",
      "Epoch [9346/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6311\n",
      "Epoch [9347/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9348/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9349/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9350/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9351/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9352/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9353/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9354/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9355/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9356/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9357/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9358/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9359/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9360/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9361/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9362/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9363/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9364/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9365/10000], Loss: 0.2736, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9366/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9367/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9368/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9369/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9370/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9371/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9372/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6310\n",
      "Epoch [9373/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9374/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9375/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9376/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9377/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9378/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9379/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9380/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9381/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9382/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9383/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9384/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9385/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9386/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9387/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9388/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9389/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9390/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9391/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9392/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9393/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9394/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9395/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9396/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9397/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9398/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9399/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9400/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9401/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9402/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9403/10000], Loss: 0.2735, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9404/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9405/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9406/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9407/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9408/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9409/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9410/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9411/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9412/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9413/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9414/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9415/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9416/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9417/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9418/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9419/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9420/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9421/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9422/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9423/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9424/10000], Loss: 0.2734, macro F1 Train: 0.7427, macro F1 Test: 0.6312\n",
      "Epoch [9425/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9426/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9427/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9428/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9429/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9430/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9431/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9432/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9433/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9434/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9435/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9436/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9437/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9438/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9439/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9440/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9441/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6312\n",
      "Epoch [9442/10000], Loss: 0.2734, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9443/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9444/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9445/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9446/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9447/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9448/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9449/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9450/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9451/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9452/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9453/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9454/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9455/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9456/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9457/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9458/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9459/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9460/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9461/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9462/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9463/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9464/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9465/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9466/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9467/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9468/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9469/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9470/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9471/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9472/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9473/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9474/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9475/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9476/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9477/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9478/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9479/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9480/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9481/10000], Loss: 0.2733, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9482/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9483/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9484/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9485/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9486/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9487/10000], Loss: 0.2732, macro F1 Train: 0.7428, macro F1 Test: 0.6313\n",
      "Epoch [9488/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6313\n",
      "Epoch [9489/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6313\n",
      "Epoch [9490/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9491/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9492/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9493/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9494/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9495/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9496/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9497/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9498/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9499/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9500/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9501/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9502/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9503/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9504/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9505/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9506/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9507/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9508/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9509/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9510/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9511/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6312\n",
      "Epoch [9512/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9513/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9514/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9515/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9516/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9517/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9518/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9519/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9520/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9521/10000], Loss: 0.2732, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9522/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9523/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9524/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9525/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9526/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9527/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9528/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9529/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9530/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9531/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9532/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9533/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9534/10000], Loss: 0.2731, macro F1 Train: 0.7429, macro F1 Test: 0.6311\n",
      "Epoch [9535/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9536/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9537/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9538/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9539/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9540/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9541/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9542/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9543/10000], Loss: 0.2731, macro F1 Train: 0.7430, macro F1 Test: 0.6311\n",
      "Epoch [9544/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9545/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9546/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9547/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9548/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9549/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9550/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9551/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9552/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9553/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9554/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9555/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9556/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9557/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9558/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9559/10000], Loss: 0.2731, macro F1 Train: 0.7431, macro F1 Test: 0.6311\n",
      "Epoch [9560/10000], Loss: 0.2731, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9561/10000], Loss: 0.2731, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9562/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9563/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9564/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9565/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9566/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9567/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9568/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9569/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9570/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9571/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9572/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9573/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9574/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9575/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9576/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9577/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9578/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9579/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9580/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9581/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9582/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9583/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9584/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9585/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9586/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9587/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9588/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9589/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9590/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9591/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9592/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9593/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9594/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9595/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9596/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9597/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9598/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9599/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9600/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9601/10000], Loss: 0.2730, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9602/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9603/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9604/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9605/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9606/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9607/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9608/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9609/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9610/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9611/10000], Loss: 0.2729, macro F1 Train: 0.7432, macro F1 Test: 0.6311\n",
      "Epoch [9612/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9613/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9614/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9615/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9616/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9617/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9618/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9619/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9620/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9621/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9622/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9623/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9624/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9625/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9626/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9627/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9628/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9629/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9630/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9631/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9632/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9633/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9634/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9635/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9636/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9637/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9638/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9639/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9640/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9641/10000], Loss: 0.2729, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9642/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9643/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9644/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9645/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9646/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9647/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9648/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9649/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9650/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9651/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9652/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9653/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9654/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9655/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9656/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9657/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9658/10000], Loss: 0.2728, macro F1 Train: 0.7433, macro F1 Test: 0.6313\n",
      "Epoch [9659/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9660/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9661/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9662/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9663/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9664/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9665/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9666/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9667/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9668/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9669/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9670/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9671/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9672/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9673/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9674/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9675/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9676/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9677/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9678/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9679/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9680/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9681/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9682/10000], Loss: 0.2728, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9683/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9684/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9685/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9686/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9687/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9688/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9689/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9690/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6313\n",
      "Epoch [9691/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9692/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9693/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9694/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9695/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9696/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9697/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9698/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9699/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9700/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6311\n",
      "Epoch [9701/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9702/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9703/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9704/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9705/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9706/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9707/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9708/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9709/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9710/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9711/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9712/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9713/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9714/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9715/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9716/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9717/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9718/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9719/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9720/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9721/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9722/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9723/10000], Loss: 0.2727, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9724/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9725/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9726/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9727/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9728/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9729/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9730/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9731/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9732/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9733/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9734/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9735/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9736/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9737/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9738/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9739/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9740/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9741/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9742/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9743/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9744/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9745/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9746/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9747/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9748/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9749/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9750/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9751/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9752/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9753/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9754/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9755/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9756/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9757/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9758/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9759/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9760/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9761/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9762/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9763/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9764/10000], Loss: 0.2726, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9765/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9766/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9767/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9768/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9769/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9770/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9771/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9772/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9773/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9774/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9775/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9776/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9777/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9778/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9779/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9780/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9781/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9782/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9783/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9784/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9785/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9786/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9787/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9788/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9789/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9790/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9791/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9792/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9793/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9794/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9795/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6310\n",
      "Epoch [9796/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9797/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9798/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9799/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9800/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9801/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9802/10000], Loss: 0.2725, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9803/10000], Loss: 0.2725, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9804/10000], Loss: 0.2725, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9805/10000], Loss: 0.2725, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9806/10000], Loss: 0.2725, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9807/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9808/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9809/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9810/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9811/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9812/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9813/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9814/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9815/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9816/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9817/10000], Loss: 0.2724, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9818/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9819/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9820/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9821/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9822/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9823/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9824/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9825/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9826/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9827/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9828/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9829/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9830/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9831/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9832/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9833/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9834/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9835/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9836/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9837/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9838/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9839/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9840/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9841/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9842/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9843/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9844/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9845/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9846/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9847/10000], Loss: 0.2724, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9848/10000], Loss: 0.2723, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9849/10000], Loss: 0.2723, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9850/10000], Loss: 0.2723, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9851/10000], Loss: 0.2723, macro F1 Train: 0.7434, macro F1 Test: 0.6309\n",
      "Epoch [9852/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9853/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9854/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9855/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9856/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9857/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9858/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9859/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9860/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9861/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9862/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9863/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9864/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9865/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9866/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9867/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9868/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9869/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9870/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9871/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9872/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9873/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9874/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9875/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9876/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9877/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9878/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9879/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6309\n",
      "Epoch [9880/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9881/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9882/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9883/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9884/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9885/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9886/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9887/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9888/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9889/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9890/10000], Loss: 0.2723, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9891/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9892/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9893/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9894/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9895/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9896/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9897/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9898/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9899/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9900/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9901/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9902/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9903/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9904/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9905/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9906/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9907/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9908/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9909/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9910/10000], Loss: 0.2722, macro F1 Train: 0.7435, macro F1 Test: 0.6310\n",
      "Epoch [9911/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9912/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9913/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9914/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9915/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9916/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9917/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9918/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9919/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9920/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9921/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9922/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9923/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9924/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9925/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9926/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9927/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9928/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6310\n",
      "Epoch [9929/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9930/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9931/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9932/10000], Loss: 0.2722, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9933/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9934/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9935/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9936/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9937/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9938/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9939/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9940/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9941/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6309\n",
      "Epoch [9942/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9943/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9944/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9945/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9946/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9947/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9948/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9949/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9950/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9951/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9952/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9953/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9954/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9955/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9956/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9957/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9958/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9959/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9960/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9961/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9962/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9963/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9964/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9965/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9966/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9967/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9968/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9969/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9970/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9971/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9972/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9973/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9974/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9975/10000], Loss: 0.2721, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9976/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9977/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9978/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9979/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9980/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6306\n",
      "Epoch [9981/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9982/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9983/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9984/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9985/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9986/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9987/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9988/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9989/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9990/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9991/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9992/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9993/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9994/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9995/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9996/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9997/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9998/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [9999/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n",
      "Epoch [10000/10000], Loss: 0.2720, macro F1 Train: 0.7436, macro F1 Test: 0.6312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, optimizer, X_train_tensor, Y_train_one_hot, X_test_tensor, Y_test are already defined\n",
    "\n",
    "# Convert Y_test to one-hot encoding if it's not already one-hot encoded\n",
    "# This is necessary for consistency in our loss function calculations\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.int64) if isinstance(Y_test, pd.Series) else torch.from_numpy(Y_test).long()\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    # Forward pass on the training data\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = macro_soft_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No gradient computation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Forward pass on the validation data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        # Calculate the exact macro F1 score for both training and validation data\n",
    "        f1_train = calculate_exact_macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "        f1_test = calculate_exact_macro_f1(Y_test_one_hot.float(), outputs_test)\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    \n",
    "    # Print loss and F1 score\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train.item():.4f}, macro F1 Train: {f1_train:.4f}, macro F1 Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.64        81\n",
      "           1       0.63      0.57      0.60       127\n",
      "           2       0.81      0.86      0.84       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.82      0.69      0.75        48\n",
      "           5       0.77      0.78      0.77        72\n",
      "           6       0.89      0.75      0.81       178\n",
      "           7       0.83      0.70      0.76        54\n",
      "           8       1.00      0.56      0.71        18\n",
      "           9       0.80      0.78      0.79        91\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.64      0.76      0.70       286\n",
      "          12       0.86      0.67      0.76       110\n",
      "          13       0.76      0.76      0.76       258\n",
      "          14       0.82      0.76      0.79       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.48      0.48      0.48        33\n",
      "          17       0.73      0.42      0.54        26\n",
      "          18       0.81      0.86      0.83       383\n",
      "          19       0.77      0.81      0.79       611\n",
      "          20       0.65      0.67      0.66        98\n",
      "          21       0.83      0.87      0.85      1636\n",
      "          22       0.64      0.65      0.65       264\n",
      "          23       0.76      0.81      0.79        16\n",
      "          24       0.63      0.58      0.61        89\n",
      "          25       0.73      0.57      0.64       183\n",
      "          26       0.52      0.60      0.55       227\n",
      "          27       0.53      0.71      0.61        14\n",
      "\n",
      "    accuracy                           0.77      5550\n",
      "   macro avg       0.66      0.62      0.63      5550\n",
      "weighted avg       0.76      0.77      0.76      5550\n",
      "\n",
      "Exact F1 Score (micro): 0.7655855855855855\n",
      "Exact F1 Score (macro): 0.6311973979928818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUSTON LOSS FUNCTION TRP GAP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gap_TPR(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap for each class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Initialize TPR storage\n",
    "    tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        # Calculate TPR for the current class across all groups\n",
    "        tpr_list = []\n",
    "        \n",
    "        # Calculate overall TPR for the current class\n",
    "        overall_mask = y_true_indices == class_idx\n",
    "        overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / torch.sum(overall_mask).float()\n",
    "        \n",
    "        # Calculate TPR for each protected group\n",
    "        for group_val in protected_attribute.unique():\n",
    "            group_mask = (protected_attribute == group_val) & overall_mask\n",
    "            group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / torch.sum(group_mask).float()\n",
    "            tpr_list.append(group_tpr)\n",
    "        \n",
    "        # Calculate TPR gap for the current class and store it\n",
    "        tpr_gaps.append(torch.abs(torch.tensor(tpr_list) - overall_tpr))\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir une fonction de loss personalisée sur F1 score\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Seuils pour décider des classifications\n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + K.epsilon())\n",
    "    return 1 - K.mean(f1)  # Moyenne du F1 sur le batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=28, bias=True)\n",
      "  (1): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 22:49:54.308305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-10 22:49:54.309295: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train)  \u001b[38;5;66;03m# X_train doit être votre tensor de données d'entrée\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# calcul de la perte\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Y_train doit être votre tensor de cibles\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# calcul des gradients\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m, in \u001b[0;36mcustom_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_loss\u001b[39m(y_true, y_pred):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Seuils pour décider des classifications\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mround(y_pred)\n\u001b[0;32m----> 7\u001b[0m     tp \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(K\u001b[38;5;241m.\u001b[39mcast(\u001b[43my_true\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43my_pred\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m     fp \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(K\u001b[38;5;241m.\u001b[39mcast((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_true)\u001b[38;5;241m*\u001b[39my_pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m     fn \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(K\u001b[38;5;241m.\u001b[39mcast(y_true\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_pred), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Définir le modèle en utilisant nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Couche linéaire avec 768 entrées et 28 sorties (classes)\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour obtenir des probabilités log sur la dimension des classes\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Définir un optimiseur\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs =1\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):  \n",
    "    \n",
    "    optimizer.zero_grad()  # Zéro aux gradients\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(X_train)  # X_train doit être votre tensor de données d'entrée\n",
    "    \n",
    "    # calcul de la perte\n",
    "    loss = custom_loss(outputs, Y_train)  # Y_train doit être votre tensor de cibles\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()  # calcul des gradients\n",
    "    \n",
    "    # mise à jour des poids\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:  # Affichage toutes les 100 époques\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

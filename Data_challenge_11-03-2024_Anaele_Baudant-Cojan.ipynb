{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from evaluator_ANAELE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model,name):\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the F1 score as a loss function.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    soft_f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    loss = 1 - soft_f1.mean()  # Mean to aggregate over all classes\n",
    "    return loss\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)  # Average F1 score across all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "X Y Z: torch.Size([27749, 768]) torch.Size([27749]) torch.Size([27749]) <class 'torch.Tensor'>\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_one_hot = torch.nn.functional.one_hot(Y_tensor, num_classes=Y.nunique())\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('X Y Z:',X_tensor.shape,Y_tensor.shape,S_tensor.shape, type(X_tensor))\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN with customized loss function (final score)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "Epoch 100, Loss: 9.229928970336914, Final Score Train: 0.5473254323005676, Final Score Test: 0.5484995245933533, macro F1 Train: 0.10194340859892699, macro F1 Test: 0.10541046804592533, 1-TPR Gap Train: 0.9927074313163757, 1-TPR Gap Test: 0.9915885925292969\n",
      "Epoch 200, Loss: 8.115486145019531, Final Score Train: 0.6071764826774597, Final Score Test: 0.6037586331367493, macro F1 Train: 0.2593545053976912, macro F1 Test: 0.2511065513502886, 1-TPR Gap Train: 0.9549984335899353, 1-TPR Gap Test: 0.9564107060432434\n",
      "Epoch 300, Loss: 6.555143356323242, Final Score Train: 0.6743472218513489, Final Score Test: 0.6614153981208801, macro F1 Train: 0.4129127061666681, macro F1 Test: 0.39077676498786784, 1-TPR Gap Train: 0.935781717300415, 1-TPR Gap Test: 0.9320540428161621\n",
      "Epoch 400, Loss: 5.279150009155273, Final Score Train: 0.7287893295288086, Final Score Test: 0.7133634090423584, macro F1 Train: 0.5488655092629875, macro F1 Test: 0.5112115882009814, 1-TPR Gap Train: 0.9087131023406982, 1-TPR Gap Test: 0.9155152440071106\n",
      "Epoch 500, Loss: 4.445042133331299, Final Score Train: 0.754325270652771, Final Score Test: 0.7281235456466675, macro F1 Train: 0.6013988275126432, macro F1 Test: 0.5619869681022976, 1-TPR Gap Train: 0.90725177526474, 1-TPR Gap Test: 0.8942601680755615\n",
      "Epoch 600, Loss: 3.746938943862915, Final Score Train: 0.7957514524459839, Final Score Test: 0.7654415965080261, macro F1 Train: 0.6682559775594538, macro F1 Test: 0.6171344528449793, 1-TPR Gap Train: 0.9232468605041504, 1-TPR Gap Test: 0.9137487411499023\n",
      "Epoch 700, Loss: 3.5552239418029785, Final Score Train: 0.8060061931610107, Final Score Test: 0.7674438953399658, macro F1 Train: 0.6814613406812237, macro F1 Test: 0.6184053355328469, 1-TPR Gap Train: 0.9305511116981506, 1-TPR Gap Test: 0.9164824485778809\n",
      "Epoch 800, Loss: 3.390350580215454, Final Score Train: 0.8155328035354614, Final Score Test: 0.7652165293693542, macro F1 Train: 0.6940205601553497, macro F1 Test: 0.6324574460810327, 1-TPR Gap Train: 0.9370449781417847, 1-TPR Gap Test: 0.8979756236076355\n",
      "Epoch 900, Loss: 3.243572473526001, Final Score Train: 0.8253369331359863, Final Score Test: 0.7811043858528137, macro F1 Train: 0.7063193996449474, macro F1 Test: 0.6452946454296967, 1-TPR Gap Train: 0.9443544149398804, 1-TPR Gap Test: 0.9169141054153442\n",
      "Epoch 1000, Loss: 3.1158578395843506, Final Score Train: 0.8282827138900757, Final Score Test: 0.7871873378753662, macro F1 Train: 0.7130116338752722, macro F1 Test: 0.6532851423870488, 1-TPR Gap Train: 0.9435538053512573, 1-TPR Gap Test: 0.9210895299911499\n",
      "Final Evaluation Score: 0.7871873378753662 Macro F1: 0.6532851423870488 1-TPR_gap: 0.9210895299911499\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*10 + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_Y_pred_tofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_true_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36msave_Y_pred_tofile\u001b[0;34m(X, model, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_Y_pred_tofile\u001b[39m(X, model,name):\n\u001b[1;32m     26\u001b[0m     Y_pred_probs \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 28\u001b[0m     results\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43my_pred\u001b[49m, columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_Challenge_MDI_341_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(name)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     results\u001b[38;5;241m.\u001b[39mto_csv(file_name, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "save_Y_pred_tofile(X_test_true_tensor, model,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64        81\n",
      "           1       0.70      0.57      0.63       127\n",
      "           2       0.81      0.87      0.84       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.89      0.65      0.75        48\n",
      "           5       0.81      0.78      0.79        72\n",
      "           6       0.84      0.71      0.77       178\n",
      "           7       0.75      0.74      0.75        54\n",
      "           8       0.92      0.67      0.77        18\n",
      "           9       0.79      0.81      0.80        91\n",
      "          10       0.77      0.45      0.57        22\n",
      "          11       0.55      0.83      0.66       286\n",
      "          12       0.86      0.70      0.77       110\n",
      "          13       0.74      0.73      0.73       258\n",
      "          14       0.86      0.70      0.77       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.48      0.48      0.48        33\n",
      "          17       0.74      0.54      0.62        26\n",
      "          18       0.80      0.87      0.84       383\n",
      "          19       0.74      0.81      0.77       611\n",
      "          20       0.59      0.70      0.64        98\n",
      "          21       0.88      0.78      0.83      1636\n",
      "          22       0.51      0.69      0.59       264\n",
      "          23       0.70      0.88      0.78        16\n",
      "          24       0.54      0.66      0.60        89\n",
      "          25       0.64      0.50      0.56       183\n",
      "          26       0.58      0.54      0.56       227\n",
      "          27       0.68      0.93      0.79        14\n",
      "\n",
      "    accuracy                           0.75      5550\n",
      "   macro avg       0.67      0.65      0.65      5550\n",
      "weighted avg       0.75      0.75      0.75      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_probs = model(X_test_true_tensor)\n",
    "Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)\n",
    "\n",
    "results=pd.DataFrame(Y_pred_tensor, columns= ['score'])\n",
    "name = 'NN_with_custom_loss'\n",
    "file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "results.to_csv(file_name, header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRET ANTICIPE DU NN**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        val_loss = soft_macro_f1_loss(Y_val_one_hot.float(), outputs_val)*10 + get_macro_tpr_gap(Y_val_one_hot.float(), outputs_val, S_val_tensor)  # Assurez-vous d'avoir Y_val_one_hot et S_val_tensor\n",
    "        \n",
    "    # Vérifier si la perte de validation s'est améliorée\n",
    "    if best_loss is None or val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Arrêt précoce après {epoch+1} époques')\n",
    "            break  # Arrêter l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.367233008146286, Final Score Train: 0.8263262510299683, Final Score Test: 0.763079047203064, macro F1 Train: 0.7199188930261188, macro F1 Test: 0.6212968514276803, 1-TPR Gap Train: 0.9327335953712463, 1-TPR Gap Test: 0.9048612713813782\n",
      "Epoch 200, Loss: 0.30855512619018555, Final Score Train: 0.8504070043563843, Final Score Test: 0.768197238445282, macro F1 Train: 0.763051131741269, macro F1 Test: 0.6219238814740339, 1-TPR Gap Train: 0.9377628564834595, 1-TPR Gap Test: 0.9144706130027771\n",
      "Epoch 300, Loss: 0.29183658957481384, Final Score Train: 0.8571170568466187, Final Score Test: 0.7679703235626221, macro F1 Train: 0.7761557708995407, macro F1 Test: 0.6199102248892311, 1-TPR Gap Train: 0.9380784034729004, 1-TPR Gap Test: 0.9160304069519043\n",
      "Epoch 400, Loss: 0.29364678263664246, Final Score Train: 0.859931230545044, Final Score Test: 0.7678018808364868, macro F1 Train: 0.7797660653962958, macro F1 Test: 0.6168873762925632, 1-TPR Gap Train: 0.940096378326416, 1-TPR Gap Test: 0.9187164306640625\n",
      "Epoch 500, Loss: 0.276767760515213, Final Score Train: 0.8632205724716187, Final Score Test: 0.7687324285507202, macro F1 Train: 0.7854988549611975, macro F1 Test: 0.6162585537720815, 1-TPR Gap Train: 0.9409423470497131, 1-TPR Gap Test: 0.9212062954902649\n",
      "Epoch 600, Loss: 0.2715463936328888, Final Score Train: 0.8654229640960693, Final Score Test: 0.7683165073394775, macro F1 Train: 0.7886741024870585, macro F1 Test: 0.6170313667633761, 1-TPR Gap Train: 0.9421718716621399, 1-TPR Gap Test: 0.919601559638977\n",
      "Epoch 700, Loss: 0.2676090598106384, Final Score Train: 0.867177426815033, Final Score Test: 0.770453155040741, macro F1 Train: 0.7927085875273624, macro F1 Test: 0.618411772805055, 1-TPR Gap Train: 0.9416462779045105, 1-TPR Gap Test: 0.9224945306777954\n",
      "Epoch 800, Loss: 0.26596173644065857, Final Score Train: 0.8678464889526367, Final Score Test: 0.7709736824035645, macro F1 Train: 0.7939159388237169, macro F1 Test: 0.6195838660186718, 1-TPR Gap Train: 0.9417769908905029, 1-TPR Gap Test: 0.9223635792732239\n",
      "Epoch 900, Loss: 0.2630162239074707, Final Score Train: 0.8691693544387817, Final Score Test: 0.7694396376609802, macro F1 Train: 0.795118903209959, macro F1 Test: 0.6148205902713659, 1-TPR Gap Train: 0.943219780921936, 1-TPR Gap Test: 0.9240586757659912\n",
      "Epoch 1000, Loss: 0.26157256960868835, Final Score Train: 0.8697226643562317, Final Score Test: 0.7696104049682617, macro F1 Train: 0.7956051489111594, macro F1 Test: 0.6157124854347391, 1-TPR Gap Train: 0.9438402056694031, 1-TPR Gap Test: 0.9235082864761353\n",
      "Arrêt précoce après 1100 époques\n",
      "Final Evaluation Score: 0.7697201371192932 Macro F1: 0.6173101376637667 1-TPR_gap: 0.9221301078796387\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# 2. Paramètres pour l'arrêt précoce\n",
    "# -------------------------------\n",
    "patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "best_loss = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 3. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "num_epochs = 10000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*alpha + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "\n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "            # Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "            if best_loss is None or final_score_test < best_loss:\n",
    "                best_loss = final_score_test\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                    break  # Arrêter l'entraînement\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_with_custom_loss(model, optimizer, alpha, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs_train = model(X_train_tensor)\n",
    "    \n",
    "        #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "        loss = alpha * soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # 1. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "\n",
    "        # Evaluate predictions on test (validation) data\n",
    "        final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or final_score_test < best_loss:\n",
    "            best_loss = final_score_test\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "\n",
    "        # 2. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.675230860710144 Macro F1: 0.4531695708422587 1-TPR_gap: 0.8972921371459961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([18, 21, 11,  ..., 22,  2, 19]),\n",
       " tensor([[-8.0641e+00, -8.1178e+00, -1.0308e+01,  ..., -1.1489e+01,\n",
       "          -6.5027e+00, -1.8435e+01],\n",
       "         [-5.6457e+00, -6.9941e+00, -5.8324e+00,  ..., -2.7598e+00,\n",
       "          -6.6884e+00, -1.7697e+01],\n",
       "         [-3.1560e+00, -4.2739e+00, -5.8266e+00,  ..., -6.9044e+00,\n",
       "          -1.8112e+00, -1.1995e+01],\n",
       "         ...,\n",
       "         [-7.2882e+00, -1.1879e+01, -5.2471e+00,  ..., -5.9260e+00,\n",
       "          -6.9725e+00, -1.7098e+01],\n",
       "         [-6.2491e+00, -8.6699e+00, -2.8882e-03,  ..., -9.1193e+00,\n",
       "          -9.4354e+00, -1.8560e+01],\n",
       "         [-4.4839e+00, -8.3085e+00, -3.7016e+00,  ..., -6.5290e+00,\n",
       "          -4.9225e+00, -1.4240e+01]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.1_alpha_0\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.5020763874053955 Macro F1: 0.010791581137944496 1-TPR_gap: 0.9933611750602722\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.05_alpha_1\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.5026302337646484 Macro F1: 0.012990667388902453 1-TPR_gap: 0.99226975440979\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.01_alpha_2\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.5033435821533203 Macro F1: 0.015455152064649338 1-TPR_gap: 0.9912320375442505\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.005_alpha_3\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.5048056244850159 Macro F1: 0.019014682180549167 1-TPR_gap: 0.9905965924263\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.001_alpha_4\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.5052502155303955 Macro F1: 0.021093229775516368 1-TPR_gap: 0.9894072413444519\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.1_alpha_5\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.5059606432914734 Macro F1: 0.02245143252832778 1-TPR_gap: 0.9894698262214661\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.05_alpha_6\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5064230561256409 Macro F1: 0.021503515309445838 1-TPR_gap: 0.9913425445556641\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.01_alpha_7\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5067638158798218 Macro F1: 0.022275753802847402 1-TPR_gap: 0.9912518262863159\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.005_alpha_8\n",
      "Arrêt précoce après 75 époques\n",
      "Final Evaluation Score: 0.5018950700759888 Macro F1: 0.016950146878337437 1-TPR_gap: 0.9868399500846863\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.001_alpha_9\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.5018148422241211 Macro F1: 0.013922044166638463 1-TPR_gap: 0.9897076487541199\n",
      "\n",
      "\n",
      " Starting to train model Adam_lr_0.1_alpha_10\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5949091911315918 Macro F1: 0.25686293558536905 1-TPR_gap: 0.9329555034637451\n",
      "\n",
      "\n",
      " Starting to train model Adam_lr_0.05_alpha_11\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.6524097919464111 Macro F1: 0.40165733377224516 1-TPR_gap: 0.9031621813774109\n",
      "\n",
      "\n",
      " Starting to train model Adam_lr_0.01_alpha_12\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7059663534164429 Macro F1: 0.5065355248761026 1-TPR_gap: 0.9053971767425537\n",
      "\n",
      "\n",
      " Starting to train model Adam_lr_0.005_alpha_13\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7431290149688721 Macro F1: 0.5614610511377466 1-TPR_gap: 0.924796998500824\n",
      "\n",
      "\n",
      " Starting to train model Adam_lr_0.001_alpha_14\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7593281269073486 Macro F1: 0.5981429182995034 1-TPR_gap: 0.9205132722854614\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.1_alpha_15\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.758647084236145 Macro F1: 0.6016877207797163 1-TPR_gap: 0.9156064391136169\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.05_alpha_16\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7605323791503906 Macro F1: 0.6058730671080357 1-TPR_gap: 0.9151917695999146\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.01_alpha_17\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7653982639312744 Macro F1: 0.6209449115884771 1-TPR_gap: 0.9098515510559082\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.005_alpha_18\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7662844657897949 Macro F1: 0.6285188070166104 1-TPR_gap: 0.9040501117706299\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.001_alpha_19\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7671113014221191 Macro F1: 0.6255544748129179 1-TPR_gap: 0.9086681008338928\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.1_alpha_20\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7677419185638428 Macro F1: 0.6286935594385107 1-TPR_gap: 0.9067901968955994\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.05_alpha_21\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7675563097000122 Macro F1: 0.6291773686149675 1-TPR_gap: 0.9059352278709412\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.01_alpha_22\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7675257921218872 Macro F1: 0.6298112076287976 1-TPR_gap: 0.9052403569221497\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.005_alpha_23\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.767676591873169 Macro F1: 0.630070357427558 1-TPR_gap: 0.9052828550338745\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.001_alpha_24\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7676122188568115 Macro F1: 0.6303023821634628 1-TPR_gap: 0.9049221277236938\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        name = opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "        print('\\n\\n Starting to train model', name)\n",
    "        model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "        Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "        save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model allMomentum_lr_0.1_alpha_1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (22199) must match the size of tensor b (27749) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mopt_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(learning_rate)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_alpha_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Starting to train model\u001b[39m\u001b[38;5;124m'\u001b[39m, name)\n\u001b[0;32m---> 13\u001b[0m model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_NN_with_custom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m Res\u001b[38;5;241m.\u001b[39mloc[i]\u001b[38;5;241m=\u001b[39m[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n\u001b[1;32m     15\u001b[0m save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
      "Cell \u001b[0;32mIn[70], line 16\u001b[0m, in \u001b[0;36mtrain_NN_with_custom_loss\u001b[0;34m(model, optimizer, alpha, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs_train \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m \u001b[43msoft_macro_f1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m get_macro_tpr_gap(Y_train_one_hot\u001b[38;5;241m.\u001b[39mfloat(), outputs_train, S_train_tensor )\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[42], line 74\u001b[0m, in \u001b[0;36msoft_macro_f1_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mDifferentiable approximation of the macro F1 score as a loss function.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mCalculates the F1 score for each class independently and then takes the average.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m tp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred_probs\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     75\u001b[0m pp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(y_pred_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     76\u001b[0m ap \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(y_true, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (22199) must match the size of tensor b (27749) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#ENTRAINEMENT SUR TOUT LE MODELE\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for i in range(1,10):\n",
    "            alpha=i\n",
    "            name = 'all'+opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "            print('\\n\\n Starting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_tensor, Y_tensor, S_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Couche d'entrée à la première couche cachée\n",
    "    nn.ReLU(),  # Fonction d'activation ReLU\n",
    "    nn.Dropout(p=0.5),  # Dropout avec une probabilité de désactivation de 50%\n",
    "    nn.Linear(2048, 512),  # De la première couche cachée à la deuxième couche cachée\n",
    "    nn.ReLU(),  # Une autre fonction d'activation ReLU après la deuxième couche cachée\n",
    "    nn.Dropout(p=0.5),  # Un autre dropout après la deuxième couche cachée\n",
    "    nn.Linear(512, 28),  # De la deuxième couche cachée à la couche de sortie\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour la classification multiclasse\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28-dropout_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(2048,512),  # Assuming 768 input features and 28 classes\n",
    "    nn.Linear(512,28),  \n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. REGRESSION WITH CUSTOM LOSS macro F1**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, optimizer, X_train_tensor, Y_train_one_hot, X_test_tensor, Y_test are already defined\n",
    "\n",
    "# Convert Y_test to one-hot encoding if it's not already one-hot encoded\n",
    "# This is necessary for consistency in our loss function calculations\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.int64) if isinstance(Y_test, pd.Series) else torch.from_numpy(Y_test).long()\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    # Forward pass on the training data\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = macro_soft_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No gradient computation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Forward pass on the validation data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        # Calculate the exact macro F1 score for both training and validation data\n",
    "        f1_train = calculate_exact_macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "        f1_test = calculate_exact_macro_f1(Y_test_one_hot.float(), outputs_test)\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    \n",
    "    # Print loss and F1 score\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train.item():.4f}, macro F1 Train: {f1_train:.4f}, macro F1 Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUSTON LOSS FUNCTION TRP GAP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gap_TPR(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap for each class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Initialize TPR storage\n",
    "    tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        # Calculate TPR for the current class across all groups\n",
    "        tpr_list = []\n",
    "        \n",
    "        # Calculate overall TPR for the current class\n",
    "        overall_mask = y_true_indices == class_idx\n",
    "        overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / torch.sum(overall_mask).float()\n",
    "        \n",
    "        # Calculate TPR for each protected group\n",
    "        for group_val in protected_attribute.unique():\n",
    "            group_mask = (protected_attribute == group_val) & overall_mask\n",
    "            group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / torch.sum(group_mask).float()\n",
    "            tpr_list.append(group_tpr)\n",
    "        \n",
    "        # Calculate TPR gap for the current class and store it\n",
    "        tpr_gaps.append(torch.abs(torch.tensor(tpr_list) - overall_tpr))\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> (5550,)\n",
      "<class 'torch.Tensor'> torch.Size([5550, 28])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_test),Y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_pred_probs),Y_pred_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_macro_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mget_macro_tpr_gap\u001b[0;34m(y_true, y_pred, protected_attribute)\u001b[0m\n\u001b[1;32m    110\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m--> 112\u001b[0m     class_tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mget_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotected_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     class_tpr_gaps\u001b[38;5;241m.\u001b[39mappend(class_tpr_gap)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Calculate the average TPR gap across all classes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mget_tpr_gap\u001b[0;34m(y_true, y_pred_probs, protected_attribute, class_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mCalculate the TPR gap for a specific class across protected groups.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m- TPR gap for the specified class.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert one-hot labels to class indices for gathering\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m y_true_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate overall TPR for the current class\u001b[39;00m\n\u001b[1;32m     74\u001b[0m overall_mask \u001b[38;5;241m=\u001b[39m y_true_indices \u001b[38;5;241m==\u001b[39m class_idx\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not Series"
     ]
    }
   ],
   "source": [
    "print(type(Y_test),Y_test.shape)\n",
    "print(type(Y_pred_probs),Y_pred_probs.shape)\n",
    "get_macro_tpr_gap(Y_test,Y_pred_probs,S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from evaluator_ANAELE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model,name):\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the F1 score as a loss function.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    soft_f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    loss = 1 - soft_f1.mean()  # Mean to aggregate over all classes\n",
    "    return loss\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)  # Average F1 score across all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN with customized loss function (final score)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "Epoch 100, Loss: 9.229928970336914, Final Score Train: 0.5473254323005676, Final Score Test: 0.5484995245933533, macro F1 Train: 0.10194340859892699, macro F1 Test: 0.10541046804592533, 1-TPR Gap Train: 0.9927074313163757, 1-TPR Gap Test: 0.9915885925292969\n",
      "Epoch 200, Loss: 8.115486145019531, Final Score Train: 0.6071764826774597, Final Score Test: 0.6037586331367493, macro F1 Train: 0.2593545053976912, macro F1 Test: 0.2511065513502886, 1-TPR Gap Train: 0.9549984335899353, 1-TPR Gap Test: 0.9564107060432434\n",
      "Epoch 300, Loss: 6.555143356323242, Final Score Train: 0.6743472218513489, Final Score Test: 0.6614153981208801, macro F1 Train: 0.4129127061666681, macro F1 Test: 0.39077676498786784, 1-TPR Gap Train: 0.935781717300415, 1-TPR Gap Test: 0.9320540428161621\n",
      "Epoch 400, Loss: 5.279150009155273, Final Score Train: 0.7287893295288086, Final Score Test: 0.7133634090423584, macro F1 Train: 0.5488655092629875, macro F1 Test: 0.5112115882009814, 1-TPR Gap Train: 0.9087131023406982, 1-TPR Gap Test: 0.9155152440071106\n",
      "Epoch 500, Loss: 4.445042133331299, Final Score Train: 0.754325270652771, Final Score Test: 0.7281235456466675, macro F1 Train: 0.6013988275126432, macro F1 Test: 0.5619869681022976, 1-TPR Gap Train: 0.90725177526474, 1-TPR Gap Test: 0.8942601680755615\n",
      "Epoch 600, Loss: 3.746938943862915, Final Score Train: 0.7957514524459839, Final Score Test: 0.7654415965080261, macro F1 Train: 0.6682559775594538, macro F1 Test: 0.6171344528449793, 1-TPR Gap Train: 0.9232468605041504, 1-TPR Gap Test: 0.9137487411499023\n",
      "Epoch 700, Loss: 3.5552239418029785, Final Score Train: 0.8060061931610107, Final Score Test: 0.7674438953399658, macro F1 Train: 0.6814613406812237, macro F1 Test: 0.6184053355328469, 1-TPR Gap Train: 0.9305511116981506, 1-TPR Gap Test: 0.9164824485778809\n",
      "Epoch 800, Loss: 3.390350580215454, Final Score Train: 0.8155328035354614, Final Score Test: 0.7652165293693542, macro F1 Train: 0.6940205601553497, macro F1 Test: 0.6324574460810327, 1-TPR Gap Train: 0.9370449781417847, 1-TPR Gap Test: 0.8979756236076355\n",
      "Epoch 900, Loss: 3.243572473526001, Final Score Train: 0.8253369331359863, Final Score Test: 0.7811043858528137, macro F1 Train: 0.7063193996449474, macro F1 Test: 0.6452946454296967, 1-TPR Gap Train: 0.9443544149398804, 1-TPR Gap Test: 0.9169141054153442\n",
      "Epoch 1000, Loss: 3.1158578395843506, Final Score Train: 0.8282827138900757, Final Score Test: 0.7871873378753662, macro F1 Train: 0.7130116338752722, macro F1 Test: 0.6532851423870488, 1-TPR Gap Train: 0.9435538053512573, 1-TPR Gap Test: 0.9210895299911499\n",
      "Final Evaluation Score: 0.7871873378753662 Macro F1: 0.6532851423870488 1-TPR_gap: 0.9210895299911499\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*10 + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_Y_pred_tofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_true_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36msave_Y_pred_tofile\u001b[0;34m(X, model, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_Y_pred_tofile\u001b[39m(X, model,name):\n\u001b[1;32m     26\u001b[0m     Y_pred_probs \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 28\u001b[0m     results\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43my_pred\u001b[49m, columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData_Challenge_MDI_341_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(name)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     results\u001b[38;5;241m.\u001b[39mto_csv(file_name, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "save_Y_pred_tofile(X_test_true_tensor, model,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64        81\n",
      "           1       0.70      0.57      0.63       127\n",
      "           2       0.81      0.87      0.84       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.89      0.65      0.75        48\n",
      "           5       0.81      0.78      0.79        72\n",
      "           6       0.84      0.71      0.77       178\n",
      "           7       0.75      0.74      0.75        54\n",
      "           8       0.92      0.67      0.77        18\n",
      "           9       0.79      0.81      0.80        91\n",
      "          10       0.77      0.45      0.57        22\n",
      "          11       0.55      0.83      0.66       286\n",
      "          12       0.86      0.70      0.77       110\n",
      "          13       0.74      0.73      0.73       258\n",
      "          14       0.86      0.70      0.77       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.48      0.48      0.48        33\n",
      "          17       0.74      0.54      0.62        26\n",
      "          18       0.80      0.87      0.84       383\n",
      "          19       0.74      0.81      0.77       611\n",
      "          20       0.59      0.70      0.64        98\n",
      "          21       0.88      0.78      0.83      1636\n",
      "          22       0.51      0.69      0.59       264\n",
      "          23       0.70      0.88      0.78        16\n",
      "          24       0.54      0.66      0.60        89\n",
      "          25       0.64      0.50      0.56       183\n",
      "          26       0.58      0.54      0.56       227\n",
      "          27       0.68      0.93      0.79        14\n",
      "\n",
      "    accuracy                           0.75      5550\n",
      "   macro avg       0.67      0.65      0.65      5550\n",
      "weighted avg       0.75      0.75      0.75      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_probs = model(X_test_true_tensor)\n",
    "Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)\n",
    "\n",
    "results=pd.DataFrame(Y_pred_tensor, columns= ['score'])\n",
    "name = 'NN_with_custom_loss'\n",
    "file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "results.to_csv(file_name, header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRET ANTICIPE DU NN**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        val_loss = soft_macro_f1_loss(Y_val_one_hot.float(), outputs_val)*10 + get_macro_tpr_gap(Y_val_one_hot.float(), outputs_val, S_val_tensor)  # Assurez-vous d'avoir Y_val_one_hot et S_val_tensor\n",
    "        \n",
    "    # Vérifier si la perte de validation s'est améliorée\n",
    "    if best_loss is None or val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Arrêt précoce après {epoch+1} époques')\n",
    "            break  # Arrêter l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.980758786201477, Final Score Train: 0.8257572054862976, Final Score Test: 0.7689204216003418, macro F1 Train: 0.7131982118809204, macro F1 Test: 0.6150132138136468, 1-TPR Gap Train: 0.9383162260055542, 1-TPR Gap Test: 0.9228276014328003\n",
      "Epoch 200, Loss: 0.8239663243293762, Final Score Train: 0.8505457639694214, Final Score Test: 0.7687321901321411, macro F1 Train: 0.7565347517471814, macro F1 Test: 0.6086528022331705, 1-TPR Gap Train: 0.9445567727088928, 1-TPR Gap Test: 0.9288116097450256\n",
      "Epoch 300, Loss: 0.7659595012664795, Final Score Train: 0.8582633137702942, Final Score Test: 0.7669168710708618, macro F1 Train: 0.7717500860280669, macro F1 Test: 0.6037370920367443, 1-TPR Gap Train: 0.9447765350341797, 1-TPR Gap Test: 0.9300966262817383\n",
      "Epoch 400, Loss: 0.7373118996620178, Final Score Train: 0.8617070913314819, Final Score Test: 0.7658792734146118, macro F1 Train: 0.7784565046319429, macro F1 Test: 0.5997586318884097, 1-TPR Gap Train: 0.9449577331542969, 1-TPR Gap Test: 0.931999921798706\n",
      "Epoch 500, Loss: 0.7190021276473999, Final Score Train: 0.8641678094863892, Final Score Test: 0.765182614326477, macro F1 Train: 0.7826793938090555, macro F1 Test: 0.6007408541370209, 1-TPR Gap Train: 0.9456562995910645, 1-TPR Gap Test: 0.9296243190765381\n",
      "Epoch 600, Loss: 0.707707941532135, Final Score Train: 0.8654959201812744, Final Score Test: 0.7680709362030029, macro F1 Train: 0.7857487920716018, macro F1 Test: 0.6027838825282482, 1-TPR Gap Train: 0.9452430009841919, 1-TPR Gap Test: 0.9333579540252686\n",
      "Epoch 700, Loss: 0.699079155921936, Final Score Train: 0.8667556047439575, Final Score Test: 0.7668406963348389, macro F1 Train: 0.7882621058941106, macro F1 Test: 0.6026679915789652, 1-TPR Gap Train: 0.9452490210533142, 1-TPR Gap Test: 0.9310134649276733\n",
      "Epoch 800, Loss: 0.6897113919258118, Final Score Train: 0.8679980039596558, Final Score Test: 0.7708643674850464, macro F1 Train: 0.7904197089116849, macro F1 Test: 0.600636266442295, 1-TPR Gap Train: 0.9455763101577759, 1-TPR Gap Test: 0.9410924911499023\n",
      "Epoch 900, Loss: 0.6858434677124023, Final Score Train: 0.8684165477752686, Final Score Test: 0.7675408720970154, macro F1 Train: 0.7913619819203274, macro F1 Test: 0.5985319801762005, 1-TPR Gap Train: 0.9454711079597473, 1-TPR Gap Test: 0.9365497827529907\n",
      "Epoch 1000, Loss: 0.6818181872367859, Final Score Train: 0.8689423203468323, Final Score Test: 0.7691802978515625, macro F1 Train: 0.7924761382823454, macro F1 Test: 0.6022109995264634, 1-TPR Gap Train: 0.9454085230827332, 1-TPR Gap Test: 0.936149537563324\n",
      "Epoch 1100, Loss: 0.6783075332641602, Final Score Train: 0.8694074749946594, Final Score Test: 0.7685235738754272, macro F1 Train: 0.793364773685621, macro F1 Test: 0.6015708437133328, 1-TPR Gap Train: 0.9454501867294312, 1-TPR Gap Test: 0.9354762434959412\n",
      "Epoch 1200, Loss: 0.6760591864585876, Final Score Train: 0.8696061372756958, Final Score Test: 0.7670387625694275, macro F1 Train: 0.7939721458502944, macro F1 Test: 0.5997008761095174, 1-TPR Gap Train: 0.9452401995658875, 1-TPR Gap Test: 0.9343766570091248\n",
      "Epoch 1300, Loss: 0.6740994453430176, Final Score Train: 0.8699019551277161, Final Score Test: 0.7669484615325928, macro F1 Train: 0.7944772486078351, macro F1 Test: 0.602283560152663, 1-TPR Gap Train: 0.9453266859054565, 1-TPR Gap Test: 0.9316134452819824\n",
      "Epoch 1400, Loss: 0.6727051734924316, Final Score Train: 0.8700944185256958, Final Score Test: 0.7668229937553406, macro F1 Train: 0.7947695219403756, macro F1 Test: 0.5994707799333431, 1-TPR Gap Train: 0.9454193711280823, 1-TPR Gap Test: 0.9341751933097839\n",
      "Arrêt précoce après 1500 époques\n",
      "Final Evaluation Score: 0.766398549079895 Macro F1: 0.5974246448657548 1-TPR_gap: 0.9353724718093872\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# 2. Paramètres pour l'arrêt précoce\n",
    "# -------------------------------\n",
    "patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "best_loss = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 3. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "num_epochs = 10000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*alpha + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "\n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "            # Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "            if best_loss is None or final_score_test < best_loss:\n",
    "                best_loss = final_score_test\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                    break  # Arrêter l'entraînement\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_with_custom_loss(model, optimizer, alpha, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs_train = model(X_train_tensor)\n",
    "    \n",
    "        #loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "        loss = alpha * soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # 1. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "\n",
    "        # Evaluate predictions on test (validation) data\n",
    "        final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or final_score_test < best_loss:\n",
    "            best_loss = final_score_test\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "\n",
    "        # 2. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6582198739051819 Macro F1: 0.4365978290566592 1-TPR_gap: 0.8798419237136841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([18, 21, 11,  ..., 21,  2, 19]),\n",
       " tensor([[-9.6625e+00, -7.2440e+00, -9.7094e+00,  ..., -1.0444e+01,\n",
       "          -6.8781e+00, -7.8592e+00],\n",
       "         [-6.5108e+00, -5.8126e+00, -5.0276e+00,  ..., -2.4211e+00,\n",
       "          -7.0888e+00, -8.4636e+00],\n",
       "         [-3.8640e+00, -3.9272e+00, -5.2261e+00,  ..., -6.0567e+00,\n",
       "          -2.4829e+00, -4.7992e+00],\n",
       "         ...,\n",
       "         [-7.6490e+00, -1.0700e+01, -4.2136e+00,  ..., -5.2082e+00,\n",
       "          -8.0097e+00, -5.9439e+00],\n",
       "         [-6.7491e+00, -7.8273e+00, -2.8641e-03,  ..., -8.2286e+00,\n",
       "          -9.9370e+00, -9.9463e+00],\n",
       "         [-5.8481e+00, -8.0534e+00, -3.7037e+00,  ..., -6.3331e+00,\n",
       "          -7.2380e+00, -4.9324e+00]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.1_alpha_0\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5008360147476196 Macro F1: 0.0068063568906823494 1-TPR_gap: 0.9948656558990479\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.05_alpha_1\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5026751160621643 Macro F1: 0.0054541114691490635 1-TPR_gap: 0.9998961687088013\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.01_alpha_2\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.5027225613594055 Macro F1: 0.005445120791325851 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.005_alpha_3\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5027225613594055 Macro F1: 0.005445120791325851 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model Momentum_lr_0.001_alpha_4\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5027225613594055 Macro F1: 0.005445120791325851 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.1_alpha_5\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5027225613594055 Macro F1: 0.005445120791325851 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.05_alpha_6\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5027225613594055 Macro F1: 0.005445120791325851 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.01_alpha_7\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.5077654719352722 Macro F1: 0.019750201288930486 1-TPR_gap: 0.9957807660102844\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.005_alpha_8\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.508148193359375 Macro F1: 0.01641692408443502 1-TPR_gap: 0.9998794198036194\n",
      "\n",
      "\n",
      " Starting to train model NAG_lr_0.001_alpha_9\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.5081309080123901 Macro F1: 0.016261778855711503 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      " Starting to train model Adma_lr_0.1_alpha_10\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.610215425491333 Macro F1: 0.2884313831077848 1-TPR_gap: 0.9319994449615479\n",
      "\n",
      "\n",
      " Starting to train model Adma_lr_0.05_alpha_11\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.6729667782783508 Macro F1: 0.43985645465277406 1-TPR_gap: 0.9060770869255066\n",
      "\n",
      "\n",
      " Starting to train model Adma_lr_0.01_alpha_12\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7240715026855469 Macro F1: 0.5529521551638693 1-TPR_gap: 0.8951907753944397\n",
      "\n",
      "\n",
      " Starting to train model Adma_lr_0.005_alpha_13\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7440690398216248 Macro F1: 0.59091773152892 1-TPR_gap: 0.8972203731536865\n",
      "\n",
      "\n",
      " Starting to train model Adma_lr_0.001_alpha_14\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7567566633224487 Macro F1: 0.6099412401266535 1-TPR_gap: 0.9035720229148865\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.1_alpha_15\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7548733949661255 Macro F1: 0.6116656652360131 1-TPR_gap: 0.8980810642242432\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.05_alpha_16\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7586581707000732 Macro F1: 0.6183110339704988 1-TPR_gap: 0.8990052342414856\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.01_alpha_17\n",
      "Arrêt précoce après 44 époques\n",
      "Final Evaluation Score: 0.7582281827926636 Macro F1: 0.6188853060696073 1-TPR_gap: 0.8975710868835449\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.005_alpha_18\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7602351307868958 Macro F1: 0.6197660848878055 1-TPR_gap: 0.9007042050361633\n",
      "\n",
      "\n",
      " Starting to train model Adagrad_lr_0.001_alpha_19\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7627360820770264 Macro F1: 0.62092891454705 1-TPR_gap: 0.9045431613922119\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.1_alpha_20\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7643085718154907 Macro F1: 0.6258803003918539 1-TPR_gap: 0.9027368426322937\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.05_alpha_21\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7637962102890015 Macro F1: 0.6251463847376646 1-TPR_gap: 0.9024459719657898\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.01_alpha_22\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7637962102890015 Macro F1: 0.6251463847376646 1-TPR_gap: 0.9024459719657898\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.005_alpha_23\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7637323141098022 Macro F1: 0.6250185936229492 1-TPR_gap: 0.9024459719657898\n",
      "\n",
      "\n",
      " Starting to train model SGD_lr_0.001_alpha_24\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7637323141098022 Macro F1: 0.6250185936229492 1-TPR_gap: 0.9024459719657898\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        name = opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "        print('\\n\\n Starting to train model', name)\n",
    "        model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "        Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "        save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINEMENT SUR TOUT LE MODELE\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for i in range(1,10):\n",
    "            alpha=i\n",
    "            name = 'all'+opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "            print('\\n\\n Starting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_tensor, Y_tensor, S_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Couche d'entrée à la première couche cachée\n",
    "    nn.ReLU(),  # Fonction d'activation ReLU\n",
    "    nn.Dropout(p=0.5),  # Dropout avec une probabilité de désactivation de 50%\n",
    "    nn.Linear(2048, 512),  # De la première couche cachée à la deuxième couche cachée\n",
    "    nn.ReLU(),  # Une autre fonction d'activation ReLU après la deuxième couche cachée\n",
    "    nn.Dropout(p=0.5),  # Un autre dropout après la deuxième couche cachée\n",
    "    nn.Linear(512, 28),  # De la deuxième couche cachée à la couche de sortie\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour la classification multiclasse\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28-dropout_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(2048,512),  # Assuming 768 input features and 28 classes\n",
    "    nn.Linear(512,28),  \n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. REGRESSION WITH CUSTOM LOSS macro F1**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, optimizer, X_train_tensor, Y_train_one_hot, X_test_tensor, Y_test are already defined\n",
    "\n",
    "# Convert Y_test to one-hot encoding if it's not already one-hot encoded\n",
    "# This is necessary for consistency in our loss function calculations\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.int64) if isinstance(Y_test, pd.Series) else torch.from_numpy(Y_test).long()\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    # Forward pass on the training data\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = macro_soft_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No gradient computation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Forward pass on the validation data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        # Calculate the exact macro F1 score for both training and validation data\n",
    "        f1_train = calculate_exact_macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "        f1_test = calculate_exact_macro_f1(Y_test_one_hot.float(), outputs_test)\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    \n",
    "    # Print loss and F1 score\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train.item():.4f}, macro F1 Train: {f1_train:.4f}, macro F1 Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUSTON LOSS FUNCTION TRP GAP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gap_TPR(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap for each class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Initialize TPR storage\n",
    "    tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        # Calculate TPR for the current class across all groups\n",
    "        tpr_list = []\n",
    "        \n",
    "        # Calculate overall TPR for the current class\n",
    "        overall_mask = y_true_indices == class_idx\n",
    "        overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / torch.sum(overall_mask).float()\n",
    "        \n",
    "        # Calculate TPR for each protected group\n",
    "        for group_val in protected_attribute.unique():\n",
    "            group_mask = (protected_attribute == group_val) & overall_mask\n",
    "            group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / torch.sum(group_mask).float()\n",
    "            tpr_list.append(group_tpr)\n",
    "        \n",
    "        # Calculate TPR gap for the current class and store it\n",
    "        tpr_gaps.append(torch.abs(torch.tensor(tpr_list) - overall_tpr))\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> (5550,)\n",
      "<class 'torch.Tensor'> torch.Size([5550, 28])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_test),Y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_pred_probs),Y_pred_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_macro_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mget_macro_tpr_gap\u001b[0;34m(y_true, y_pred, protected_attribute)\u001b[0m\n\u001b[1;32m    110\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m--> 112\u001b[0m     class_tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mget_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotected_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     class_tpr_gaps\u001b[38;5;241m.\u001b[39mappend(class_tpr_gap)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Calculate the average TPR gap across all classes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mget_tpr_gap\u001b[0;34m(y_true, y_pred_probs, protected_attribute, class_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mCalculate the TPR gap for a specific class across protected groups.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m- TPR gap for the specified class.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert one-hot labels to class indices for gathering\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m y_true_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate overall TPR for the current class\u001b[39;00m\n\u001b[1;32m     74\u001b[0m overall_mask \u001b[38;5;241m=\u001b[39m y_true_indices \u001b[38;5;241m==\u001b[39m class_idx\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not Series"
     ]
    }
   ],
   "source": [
    "print(type(Y_test),Y_test.shape)\n",
    "print(type(Y_pred_probs),Y_pred_probs.shape)\n",
    "get_macro_tpr_gap(Y_test,Y_pred_probs,S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

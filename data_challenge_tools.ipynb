{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python 2.7, the / operator is integer division if inputs are integers.\n",
    "# If you want float division (which is something I always prefer), just use this special import:\n",
    "# Alwasy use it at the first line of code\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def confusion_matrix_based_scores(cnf):\n",
    "    \"\"\"Calculate confusion matrix based scores.\n",
    "    Implementation from https://stackoverflow.com/a/43331484\n",
    "    See https://en.wikipedia.org/wiki/Confusion_matrix for different scores\n",
    "    Args:\n",
    "        cnf (np.array): a confusion matrix.\n",
    "    Returns:\n",
    "        dict: a set of metrics for each class, indexed by the metric name.\n",
    "    \"\"\"\n",
    "    FP = cnf.sum(axis=0) - np.diag(cnf) + 1e-5\n",
    "    FN = cnf.sum(axis=1) - np.diag(cnf) + 1e-5\n",
    "    TP = np.diag(cnf) + 1e-5\n",
    "    TN = cnf.sum() - (FP + FN + TP) + 1e-5\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP / (TP + FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN / (TN + FP)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP / (TP + FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN / (TN + FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP / (FP + TN)\n",
    "    # False negative rate\n",
    "    FNR = FN / (TP + FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP / (TP + FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "    # Positive Prediction Rates\n",
    "    PPR = (TP + FP) / (TP + FP + FN + TN)\n",
    "\n",
    "    return {\n",
    "        \"TPR\": TPR,\n",
    "        \"TNR\": TNR,\n",
    "        \"PPV\": PPV,\n",
    "        \"NPV\": NPV,\n",
    "        \"FPR\": FPR,\n",
    "        \"FNR\": FNR,\n",
    "        \"FDR\": FDR,\n",
    "        \"ACC\": ACC,\n",
    "        \"PPR\": PPR,\n",
    "    }\n",
    "\n",
    "\n",
    "def power_mean(series, p, axis=0):\n",
    "    \"\"\"calculate the generalized mean of a given list.\n",
    "    Args:\n",
    "        series (list): a list of numbers.\n",
    "        p (int): power of the generalized mean aggregation\n",
    "        axis (int, optional): aggregation along which dim of the input. Defaults to 0.\n",
    "    Returns:\n",
    "        np.array: aggregated scores.\n",
    "    \"\"\"\n",
    "    if p > 50:\n",
    "        return np.max(series, axis=axis)\n",
    "    elif p < -50:\n",
    "        return np.min(series, axis=axis)\n",
    "    else:\n",
    "        total = np.mean(np.power(series, p), axis=axis)\n",
    "        return np.power(total, 1 / p)\n",
    "\n",
    "######## USED in (gap_eval_scores) Data Challenge ###########\n",
    "def aggregation_GAP(distinct_groups, all_scores, metric=\"TPR\", group_agg_power=None, class_agg_power=2):\n",
    "    \"\"\"Aggregate fairness metrics at the group level and class level.\n",
    "    Args:\n",
    "        distinct_groups (list): a list of distinc labels of protected groups.\n",
    "        all_scores (dict): confusion matrix based scores for each protected group and all.\n",
    "        metric (str, optional): fairness metric. Defaults to \"TPR\".\n",
    "        group_agg_power (int, optional): generalized mean aggregation power at the group level. Use absolute value aggregation if None. Defaults to None.\n",
    "        class_agg_power (int, optional): generalized mean aggregation power at the class level. Defaults to 2.\n",
    "    Returns:\n",
    "        np.array: aggregated fairness score.\n",
    "    \"\"\"\n",
    "    group_scores = []\n",
    "    for gid in distinct_groups:\n",
    "        # Save the TPR direct to the list \n",
    "        group_scores.append(all_scores[gid][metric])\n",
    "        # n_class * n_groups\n",
    "    scores = np.stack(group_scores, axis=1)\n",
    "    # Calculate GAP (n_class * n_groups) - (n_class * 1)\n",
    "    score_gaps = scores - all_scores[\"overall\"][metric].reshape(-1, 1)\n",
    "    # Sum over gaps of all protected groups within each class\n",
    "    if group_agg_power is None:\n",
    "        score_gaps = np.sum(abs(score_gaps), axis=1)\n",
    "    else:\n",
    "        score_gaps = power_mean(score_gaps, p=group_agg_power, axis=1)\n",
    "    # Aggregate gaps of each class, RMS by default\n",
    "    score_gaps = power_mean(score_gaps, class_agg_power)\n",
    "\n",
    "    return score_gaps\n",
    "\n",
    "######## Not USED in Data Challenge ###########\n",
    "def aggregation_Ratio(distinct_groups, all_scores, metric=\"TPR\", group_agg_power=None, class_agg_power=2):\n",
    "    \"\"\"Aggregate fairness metric ratios at the group level and class level.\n",
    "    Args:\n",
    "        distinct_groups (list): a list of distinc labels of protected groups.\n",
    "        all_scores (dict): confusion matrix based scores for each protected group and all.\n",
    "        metric (str, optional): fairness metric. Defaults to \"TPR\".\n",
    "        group_agg_power (int, optional): generalized mean aggregation power at the group level. Use absolute value aggregation if None. Defaults to None.\n",
    "        class_agg_power (int, optional): generalized mean aggregation power at the class level. Defaults to 2.\n",
    "    Returns:\n",
    "        np.array: aggregated fairness score.\n",
    "    \"\"\"\n",
    "    group_scores = []\n",
    "    for gid in distinct_groups:\n",
    "        # Save the TPR direct to the list \n",
    "        group_scores.append(all_scores[gid][metric])\n",
    "        # n_class * n_groups\n",
    "    scores = np.stack(group_scores, axis=1)\n",
    "    # Calculate GAP (n_class * n_groups) - (n_class * 1)\n",
    "    score_ratios = scores / all_scores[\"overall\"][metric].reshape(-1, 1)\n",
    "    # Sum over ratios of all protected groups within each class\n",
    "    if group_agg_power is None:\n",
    "        score_ratios = np.sum(abs(score_ratios), axis=1)\n",
    "    else:\n",
    "        score_ratios = power_mean(score_ratios, p=group_agg_power, axis=1)\n",
    "    # Aggregate ratios of each class, RMS by default\n",
    "    score_ratios = power_mean(score_ratios, class_agg_power)\n",
    "\n",
    "    return score_ratios\n",
    "\n",
    "\n",
    "def gap_eval_scores(y_pred, y_true, protected_attribute, metrics=[\"TPR\", \"FPR\", \"PPR\"], args=None):\n",
    "    \"\"\"fairness evaluation\n",
    "    Args:\n",
    "        y_pred (np.array): model predictions.\n",
    "        y_true (np.array): target labels.\n",
    "        protected_attribute (np.array): protected labels.\n",
    "        metrics (list, optional): a list of metric names that will be considered for fairness evaluation. Defaults to [\"TPR\",\"FPR\",\"PPR\"].\n",
    "    Returns:\n",
    "        tuple: (fairness evaluation results, confusion matrices)\n",
    "    \"\"\"\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    protected_attribute = np.array(protected_attribute)\n",
    "\n",
    "    if (args is not None) and args.regression:\n",
    "        eval_scores = {\n",
    "            \"mean_absolute_error\": mean_absolute_error(y_true, y_pred),\n",
    "            \"mean_squared_error\": mean_squared_error(y_true, y_pred),\n",
    "            \"r2_score\": r2_score(y_true, y_pred),\n",
    "        }\n",
    "        # Processing regression labels for fairness evaluation under the classification framework\n",
    "        y_true = pd.cut(np.squeeze(y_true), bins=args.regression_bins, labels=False, duplicates=\"drop\")\n",
    "        y_pred = pd.cut(np.squeeze(y_pred), bins=args.regression_bins, labels=False, duplicates=\"drop\")\n",
    "        y_true = np.nan_to_num(y_true, nan=0)\n",
    "        y_pred = np.nan_to_num(y_pred, nan=0)\n",
    "\n",
    "    else:\n",
    "        # performance evaluation\n",
    "        eval_scores = {\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"macro_fscore\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"micro_fscore\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "        }\n",
    "\n",
    "    all_scores = {}\n",
    "    confusion_matrices = {}\n",
    "    # Overall evaluation\n",
    "    distinct_labels = [i for i in range(len(set(y_true)))]\n",
    "    overall_confusion_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=distinct_labels)\n",
    "    confusion_matrices[\"overall\"] = overall_confusion_matrix\n",
    "    all_scores[\"overall\"] = confusion_matrix_based_scores(overall_confusion_matrix)\n",
    "\n",
    "    # Group scores\n",
    "    distinct_groups = [i for i in range(len(set(protected_attribute)))]\n",
    "    for gid in distinct_groups:\n",
    "        group_identifier = (protected_attribute == gid)\n",
    "        group_confusion_matrix = confusion_matrix(y_true=y_true[group_identifier], y_pred=y_pred[group_identifier],\n",
    "                                                  labels=distinct_labels)\n",
    "        confusion_matrices[gid] = group_confusion_matrix\n",
    "        all_scores[gid] = confusion_matrix_based_scores(group_confusion_matrix)\n",
    "\n",
    "    for _metric in metrics:\n",
    "        eval_scores[\"{}_GAP\".format(_metric)] = aggregation_GAP(distinct_groups=distinct_groups, all_scores=all_scores,\n",
    "                                                                metric=_metric)\n",
    "\n",
    "    return eval_scores, confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

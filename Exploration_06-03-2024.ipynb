{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is NOT available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluator_ANAELE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluator_ANAELE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m'''import tensorflow as tf\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mfrom tensorflow.keras.layers import Input, Dense, Dropout\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mfrom tensorflow.keras.models import Model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mfrom tensorflow.keras.optimizers import Adam\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mfrom tensorflow.keras.metrics import AUC, SparseCategoricalAccuracy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mfrom tensorflow.keras.callbacks import ReduceLROnPlateau'''\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluator_ANAELE'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from evaluator_ANAELE import *\n",
    "\n",
    "'''import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n",
    "\n",
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')\n",
    "\n",
    "# Split the data (final _ to keep split data untouched and be able to reload in file)\n",
    "X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_ = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "# Refresh training data\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classwise_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function that calculates the cross-entropy loss for each class\n",
    "    individually and then averages these losses.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted labels, as probabilities.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: A scalar tensor representing the average class-wise cross-entropy loss.\n",
    "    \"\"\"\n",
    "        # Ensure y_true and y_pred are both float32 for compatibility\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Ensure predictions sum to 1\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    y_pred = tf.math.divide(y_pred, tf.reduce_sum(y_pred, axis=-1, keepdims=True))\n",
    "    \n",
    "    # Calculate cross-entropy loss for each class\n",
    "    classwise_losses = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=0)\n",
    "    \n",
    "    # Average the class-wise losses\n",
    "    loss = tf.reduce_mean(classwise_losses)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class GradientReversalLayer(Layer):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, x):\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def grad_reverse(x, grad):\n",
    "        return x, -grad * self.alpha\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'alpha': self.alpha\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "243/243 [==============================] - 3s 8ms/step - loss: 2.2830 - main_output_loss: 2.2833 - adversary_output_loss: 3.1513 - main_output_accuracy: 0.4115 - adversary_output_auc: 0.4724 - val_loss: 1.6796 - val_main_output_loss: 1.6804 - val_adversary_output_loss: 7.5507 - val_main_output_accuracy: 0.5591 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 1.6250 - main_output_loss: 1.6265 - adversary_output_loss: 15.6673 - main_output_accuracy: 0.5599 - adversary_output_auc: 0.5000 - val_loss: 1.3585 - val_main_output_loss: 1.3612 - val_adversary_output_loss: 26.4842 - val_main_output_accuracy: 0.6147 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 1.3886 - main_output_loss: 1.3927 - adversary_output_loss: 40.6770 - main_output_accuracy: 0.6095 - adversary_output_auc: 0.5000 - val_loss: 1.1891 - val_main_output_loss: 1.1950 - val_adversary_output_loss: 58.2024 - val_main_output_accuracy: 0.6466 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 1.2575 - main_output_loss: 1.2652 - adversary_output_loss: 77.1387 - main_output_accuracy: 0.6436 - adversary_output_auc: 0.5000 - val_loss: 1.0773 - val_main_output_loss: 1.0875 - val_adversary_output_loss: 101.9729 - val_main_output_accuracy: 0.6816 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 1.1508 - main_output_loss: 1.1635 - adversary_output_loss: 127.5595 - main_output_accuracy: 0.6638 - adversary_output_auc: 0.5000 - val_loss: 0.9942 - val_main_output_loss: 1.0101 - val_adversary_output_loss: 158.8179 - val_main_output_accuracy: 0.7022 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 1.0755 - main_output_loss: 1.0944 - adversary_output_loss: 189.0694 - main_output_accuracy: 0.6797 - adversary_output_auc: 0.5000 - val_loss: 0.9361 - val_main_output_loss: 0.9593 - val_adversary_output_loss: 231.6077 - val_main_output_accuracy: 0.7135 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 1.0199 - main_output_loss: 1.0463 - adversary_output_loss: 264.1634 - main_output_accuracy: 0.6932 - adversary_output_auc: 0.5000 - val_loss: 0.8884 - val_main_output_loss: 0.9193 - val_adversary_output_loss: 308.5648 - val_main_output_accuracy: 0.7251 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "243/243 [==============================] - 2s 6ms/step - loss: 0.9719 - main_output_loss: 1.0073 - adversary_output_loss: 353.5570 - main_output_accuracy: 0.7002 - adversary_output_auc: 0.5000 - val_loss: 0.8498 - val_main_output_loss: 0.8904 - val_adversary_output_loss: 406.9194 - val_main_output_accuracy: 0.7277 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "243/243 [==============================] - 2s 7ms/step - loss: 0.9323 - main_output_loss: 0.9774 - adversary_output_loss: 451.4019 - main_output_accuracy: 0.7129 - adversary_output_auc: 0.5000 - val_loss: 0.8102 - val_main_output_loss: 0.8618 - val_adversary_output_loss: 516.4464 - val_main_output_accuracy: 0.7362 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 0.8811 - main_output_loss: 0.9386 - adversary_output_loss: 574.8298 - main_output_accuracy: 0.7208 - adversary_output_auc: 0.5000 - val_loss: 0.7724 - val_main_output_loss: 0.8372 - val_adversary_output_loss: 648.1527 - val_main_output_accuracy: 0.7465 - val_adversary_output_auc: 0.5000 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "input_layer = Input(shape=(768,))\n",
    "shared_layer = Dense(256, activation='relu', name='shared_representation')(input_layer)\n",
    "\n",
    "# Main task classifier\n",
    "#main_classifier = Dense(128, activation='relu')(shared_layer)\n",
    "main_classifier = Dropout(0.5)(shared_layer)#main_classifier)\n",
    "main_output = Dense(28, activation='softmax', name='main_output')(main_classifier)\n",
    "\n",
    "# Apply Gradient Reversal Layer before the adversary classifier\n",
    "grl = GradientReversalLayer(alpha=0.1)(shared_layer)\n",
    "\n",
    "# Adversary classifier (Adjusted for binary classification)\n",
    "adversary_classifier = Dense(128, activation='relu')(grl)\n",
    "adversary_classifier = Dropout(0.5)(adversary_classifier)\n",
    "adversary_output = Dense(1, activation='sigmoid', name='adversary_output')(adversary_classifier)  # Adjusted\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=input_layer, outputs=[main_output, adversary_output])\n",
    "\n",
    "# Compile model with adjusted loss and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss={'main_output': 'sparse_categorical_crossentropy', 'adversary_output': 'binary_crossentropy'},  # Adjusted classwise_loss / 'sparse_categorical_crossentropy'\n",
    "              loss_weights={'main_output': 1., 'adversary_output': -0.0001},  # Negative weight to adversary for debiasing\n",
    "              metrics={'main_output': SparseCategoricalAccuracy(name='accuracy'), 'adversary_output': AUC(name='auc')})\n",
    "\n",
    "# Callback to reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, {'main_output': Y_train, 'adversary_output': S_train},\n",
    "                    epochs=10, batch_size=64, validation_split=0.2, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 1s 3ms/step - loss: 0.0360 - main_output_loss: 0.8787 - adversary_output_loss: 1685.4054 - main_output_accuracy: 0.7356 - adversary_output_auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "evaluation = model.evaluate(X_test, {'main_output': Y_test, 'adversary_output': S_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/607 [==============================] - 1s 2ms/step\n",
      "261/261 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract shared representation model\n",
    "representation_model = Model(inputs=model.input, outputs=model.get_layer('shared_representation').output)\n",
    "\n",
    "# Transform X_train and X_test\n",
    "X_train_transformed_2 = representation_model.predict(X_train)\n",
    "X_test_transformed_2 = representation_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7577177177177177\n",
      "\n",
      "final 0.7319076217263948\n",
      "macro_fscore 0.6526436809324047\n",
      "1-eval_scores 0.8111715625203849\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train a new classifier on the transformed training data\n",
    "clf_2 = LogisticRegression(max_iter=5000)  # Increase max_iter if needed for convergence\n",
    "history_new_2 = clf_2.fit(X_train_transformed_2, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_2 = clf_2.predict(X_test_transformed_2)\n",
    "accuracy_2= accuracy_score(Y_test, Y_pred_2)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_2}\")\n",
    "\n",
    "# Step 4 : Predict with gloabl score\n",
    "eval_scores_2, confusion_matrices_eval_2 = gap_eval_scores(Y_pred_2, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_2 = (eval_scores_2['macro_fscore']+ (1-eval_scores_2['TPR_GAP']))/2\n",
    "print('\\nfinal',final_score_2)\n",
    "print('macro_fscore',eval_scores_2['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores_2['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 22:06:17.056611: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (0 for input with 0 dimension(s)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (0 for input with 0 dimension(s) [Op:Mean] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m classwise_loss(Y_test, Y_pred_2)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclasswise_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 24\u001b[0m, in \u001b[0;36mclasswise_accuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     20\u001b[0m class_accuracies \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mequal(y_true_classes, y_pred_classes), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Reduce across all dimensions but the first (batch dimension)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# to get accuracy per class, then take the mean across classes\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m classwise_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m overall_acc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(classwise_acc)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overall_acc\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (0 for input with 0 dimension(s) [Op:Mean] name: "
     ]
    }
   ],
   "source": [
    "classwise_loss(Y_test, Y_pred_2)\n",
    "classwise_accuracy(Y_test, Y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def classwise_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom metric function that calculates the accuracy for each class individually\n",
    "    and then averages these accuracies.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor of true labels, one-hot encoded.\n",
    "        y_pred: Tensor of predicted labels, as probabilities.\n",
    "\n",
    "    Returns:\n",
    "        A scalar tensor representing the average class-wise accuracy.\n",
    "    \"\"\"\n",
    "    # Convert probabilities to predicted class (highest probability)\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "    y_true_classes = tf.argmax(y_true, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy for each class\n",
    "    class_accuracies = tf.cast(tf.equal(y_true_classes, y_pred_classes), tf.float32)\n",
    "    \n",
    "    # Reduce across all dimensions but the first (batch dimension)\n",
    "    # to get accuracy per class, then take the mean across classes\n",
    "    classwise_acc = tf.reduce_mean(class_accuracies, axis=0)\n",
    "    overall_acc = tf.reduce_mean(classwise_acc)\n",
    "    \n",
    "    return overall_acc\n",
    "\n",
    "# Example usage with a model compilation in TensorFlow/Keras\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[classwise_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

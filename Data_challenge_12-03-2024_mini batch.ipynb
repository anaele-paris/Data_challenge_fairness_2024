{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model,name):\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the F1 score as a loss function.\n",
    "    \"\"\"\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    soft_f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    loss = 1 - soft_f1.mean()  # Mean to aggregate over all classes\n",
    "    return loss\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred_probs, dim=0)\n",
    "    pp = torch.sum(y_pred_probs, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)  # Average F1 score across all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "test_tensor: 2 1 torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN with customized loss function (final score)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.9841914176940918, Final Score Train: 0.501908004283905, Final Score Test: 0.4984510540962219, macro F1 Train: 0.008571775500201642, macro F1 Test: 0.007334987901510969, 1-TPR Gap Train: 0.9952442049980164, 1-TPR Gap Test: 0.9895671010017395\n",
      "Epoch 200, Loss: 0.9833416938781738, Final Score Train: 0.5067097544670105, Final Score Test: 0.5049751996994019, macro F1 Train: 0.019669357091836863, macro F1 Test: 0.01782821189219381, 1-TPR Gap Train: 0.9937501549720764, 1-TPR Gap Test: 0.992122232913971\n",
      "Epoch 300, Loss: 0.9840088486671448, Final Score Train: 0.5078456401824951, Final Score Test: 0.5077477693557739, macro F1 Train: 0.019754407542982855, macro F1 Test: 0.019433599351551208, 1-TPR Gap Train: 0.995936930179596, 1-TPR Gap Test: 0.9960619807243347\n",
      "Epoch 400, Loss: 0.985431432723999, Final Score Train: 0.5088541507720947, Final Score Test: 0.5081185102462769, macro F1 Train: 0.018309218934500774, macro F1 Test: 0.016256363347120585, 1-TPR Gap Train: 0.9993990659713745, 1-TPR Gap Test: 0.999980628490448\n",
      "Epoch 500, Loss: 0.9852122068405151, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 600, Loss: 0.9843538999557495, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 700, Loss: 0.983160138130188, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 800, Loss: 0.9821821451187134, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 900, Loss: 0.9814532995223999, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 1000, Loss: 0.980742871761322, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Final Evaluation Score: 0.5081309080123901 Macro F1: 0.0162617788557115 1-TPR_gap: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    #loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*10 + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'y_pred_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_Y_pred_tofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_true_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36msave_Y_pred_tofile\u001b[0;34m(X, model, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m probs\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(y_pred_probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), columns\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m28\u001b[39m)))\n\u001b[1;32m     30\u001b[0m file_name_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_probs/y_pred_probs_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(name)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# save predicted labels for each Xi (dim=1)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_pred_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/core/generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3962\u001b[0m )\n\u001b[0;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'y_pred_probs'"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "save_Y_pred_tofile(X_test_true_tensor, model,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        81\n",
      "           1       0.00      0.00      0.00       127\n",
      "           2       0.00      0.00      0.00       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.00      0.00      0.00        72\n",
      "           6       0.00      0.00      0.00       178\n",
      "           7       0.00      0.00      0.00        54\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00        91\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.00      0.00      0.00       286\n",
      "          12       0.00      0.00      0.00       110\n",
      "          13       0.00      0.00      0.00       258\n",
      "          14       0.00      0.00      0.00       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.00      0.00      0.00        33\n",
      "          17       0.00      0.00      0.00        26\n",
      "          18       0.00      0.00      0.00       383\n",
      "          19       0.00      0.00      0.00       611\n",
      "          20       0.00      0.00      0.00        98\n",
      "          21       0.29      1.00      0.46      1636\n",
      "          22       0.00      0.00      0.00       264\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.00      0.00      0.00        89\n",
      "          25       0.00      0.00      0.00       183\n",
      "          26       0.00      0.00      0.00       227\n",
      "          27       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.29      5550\n",
      "   macro avg       0.01      0.04      0.02      5550\n",
      "weighted avg       0.09      0.29      0.13      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_probs = model(X_test_true_tensor)\n",
    "Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)\n",
    "\n",
    "results=pd.DataFrame(Y_pred_tensor, columns= ['score'])\n",
    "name = 'NN_with_custom_loss'\n",
    "file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "results.to_csv(file_name, header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRET ANTICIPE DU NN (sans mini-batch)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.6180952787399292, Final Score Train: 0.8383526802062988, Final Score Test: 0.7733933925628662, macro F1 Train: 0.727813242975664, macro F1 Test: 0.6287169097435105, 1-TPR Gap Train: 0.9488921761512756, 1-TPR Gap Test: 0.9180698394775391\n",
      "Epoch 200, Loss: 0.5957030057907104, Final Score Train: 0.8599814772605896, Final Score Test: 0.7825250029563904, macro F1 Train: 0.7688718539032557, macro F1 Test: 0.6406831513698007, 1-TPR Gap Train: 0.9510911107063293, 1-TPR Gap Test: 0.92436683177948\n",
      "Epoch 300, Loss: 0.5893559455871582, Final Score Train: 0.8680845499038696, Final Score Test: 0.7799464464187622, macro F1 Train: 0.7814826795012992, macro F1 Test: 0.6342193246928722, 1-TPR Gap Train: 0.9546864628791809, 1-TPR Gap Test: 0.9256735444068909\n",
      "Epoch 400, Loss: 0.5860427618026733, Final Score Train: 0.8719273209571838, Final Score Test: 0.7806618213653564, macro F1 Train: 0.7879128081575509, macro F1 Test: 0.6328553288903299, 1-TPR Gap Train: 0.9559418559074402, 1-TPR Gap Test: 0.9284682273864746\n",
      "Epoch 500, Loss: 0.5838711261749268, Final Score Train: 0.873989462852478, Final Score Test: 0.7756043076515198, macro F1 Train: 0.7916538288078653, macro F1 Test: 0.6359021635570292, 1-TPR Gap Train: 0.9563250541687012, 1-TPR Gap Test: 0.9153064489364624\n",
      "Epoch 600, Loss: 0.582038402557373, Final Score Train: 0.874853253364563, Final Score Test: 0.7724635601043701, macro F1 Train: 0.7939487191221201, macro F1 Test: 0.6300677856073184, 1-TPR Gap Train: 0.955757737159729, 1-TPR Gap Test: 0.914859414100647\n",
      "Epoch 700, Loss: 0.5809394121170044, Final Score Train: 0.8754029870033264, Final Score Test: 0.7747632265090942, macro F1 Train: 0.7953585486340774, macro F1 Test: 0.6325119358405923, 1-TPR Gap Train: 0.9554474353790283, 1-TPR Gap Test: 0.9170145392417908\n",
      "Epoch 800, Loss: 0.579990029335022, Final Score Train: 0.8761406540870667, Final Score Test: 0.7721831798553467, macro F1 Train: 0.796795821382338, macro F1 Test: 0.6320952853052472, 1-TPR Gap Train: 0.955485463142395, 1-TPR Gap Test: 0.9122711420059204\n",
      "Epoch 900, Loss: 0.5790771245956421, Final Score Train: 0.8768042922019958, Final Score Test: 0.7738946080207825, macro F1 Train: 0.7983541643512864, macro F1 Test: 0.6342652527311115, 1-TPR Gap Train: 0.9552544355392456, 1-TPR Gap Test: 0.9135239720344543\n",
      "Epoch 1000, Loss: 0.5786576271057129, Final Score Train: 0.877066969871521, Final Score Test: 0.7720322608947754, macro F1 Train: 0.7988552052836821, macro F1 Test: 0.631037906782591, 1-TPR Gap Train: 0.9552788138389587, 1-TPR Gap Test: 0.9130265712738037\n",
      "Final Evaluation Score: 0.7720322608947754 Macro F1: 0.631037906782591 1-TPR_gap: 0.9130265712738037\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# 2. Paramètres pour l'arrêt précoce\n",
    "# -------------------------------\n",
    "patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "best_loss = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 3. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    # loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "\n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "            # Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "            if best_loss is None or final_score_test < best_loss:\n",
    "                best_loss = final_score_test\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                    break  # Arrêter l'entraînement\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANS MINI-BATCH\n",
    "def train_NN_with_custom_loss_no_mini_batch(model, optimizer, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs_train = model(X_train_tensor)\n",
    "    \n",
    "        loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "        #loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) \n",
    "        # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # 1. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "\n",
    "        # Evaluate predictions on test (validation) data\n",
    "        final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or final_score_test < best_loss:\n",
    "            best_loss = final_score_test\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "\n",
    "        # 2. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6730861067771912 Macro F1: 0.4465380708164603 1-TPR_gap: 0.8996341228485107\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "    )  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss_no_mini_batch(model,optim.Adam(model.parameters(), lr=learning_rate), X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "#save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            #if epoch==0 : print('dim de X_batch Y_batch et S_batch',X_batch.size(),Y_batch.size(),S_batch.size())\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_train = model(X_batch)\n",
    "            #print('Y_batch',Y_batch.size(),'outputs_train',outputs_train.size())\n",
    "            Y_batch_one_hot = torch.nn.functional.one_hot(Y_batch, num_classes=Y_train.nunique())\n",
    "            loss = soft_final_score_loss(Y_batch_one_hot, outputs_train, S_batch)\n",
    "            # loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) \n",
    "            # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print('boucle mini batch terminée')\n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test_one_hot, outputs_test, S_batch)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        print('fin boucle mini batch test')    \n",
    "        # Evaluate predictions on test (validation) data\n",
    "        #final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        #outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "        print('fin eval early ending') \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8391872644424438, Final Score Train: 0.6692754626274109, Final Score Test: 0.6604130268096924, macro F1 Train: 0.40932455512050653, macro F1 Test: 0.3926724804707982, 1-TPR Gap Train: 0.9292263984680176, 1-TPR Gap Test: 0.9281536340713501\n",
      "Epoch 10, Loss: 0.8653194904327393, Final Score Train: 0.7007002234458923, Final Score Test: 0.6837728023529053, macro F1 Train: 0.45899918405659335, macro F1 Test: 0.4340852817906672, 1-TPR Gap Train: 0.9424012303352356, 1-TPR Gap Test: 0.9334603548049927\n",
      "Epoch 20, Loss: 0.8109594583511353, Final Score Train: 0.7038607597351074, Final Score Test: 0.6880536079406738, macro F1 Train: 0.4490490433965809, macro F1 Test: 0.4292081148655373, 1-TPR Gap Train: 0.9586724638938904, 1-TPR Gap Test: 0.9468991160392761\n",
      "Arrêt précoce après 28 époques\n",
      "Final Evaluation Score: 0.6998753547668457 Macro F1: 0.4463147634331981 1-TPR_gap: 0.9534358978271484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([18, 21,  9,  ..., 21,  2, 19]),\n",
       " tensor([[ -75.7619,  -32.1508,  -78.6328,  ..., -130.1212,  -23.7126,\n",
       "           -36.0007],\n",
       "         [-227.8640,    6.3937, -171.8242,  ...,   91.4837,  -19.4240,\n",
       "           -38.7736],\n",
       "         [ -91.3719,   10.6754, -118.3706,  ...,   10.1084,   19.9372,\n",
       "           -25.6650],\n",
       "         ...,\n",
       "         [-143.4588, -166.5751, -129.7825,  ...,  -59.0934,   65.6600,\n",
       "           -28.9990],\n",
       "         [  63.7109,    1.6440,  283.0884,  ...,  -65.7813,  -27.1779,\n",
       "           -46.2642],\n",
       "         [ -79.4356,  -37.4708,  -85.4813,  ...,   45.8519,  -25.3591,\n",
       "           -22.8943]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    )  # Additional layer for complexity\n",
    "'''    model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "    )  # LogSoftmax for multi-class classification'''\n",
    "\n",
    "batch_size = 56\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.9854655265808105, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.8613970279693604, Final Score Train: 0.6198021173477173, Final Score Test: 0.6191191673278809, macro F1 Train: 0.28181842822967057, macro F1 Test: 0.27811187544601906, 1-TPR Gap Train: 0.9577857851982117, 1-TPR Gap Test: 0.9601263999938965\n",
      "Epoch 20, Loss: 0.9114627838134766, Final Score Train: 0.6807969212532043, Final Score Test: 0.669647753238678, macro F1 Train: 0.3988563700069408, macro F1 Test: 0.38250176499643374, 1-TPR Gap Train: 0.9627374410629272, 1-TPR Gap Test: 0.9567937850952148\n",
      "Epoch 30, Loss: 0.8214148283004761, Final Score Train: 0.6924355626106262, Final Score Test: 0.6773017048835754, macro F1 Train: 0.41669544805518666, macro F1 Test: 0.396144177853795, 1-TPR Gap Train: 0.9681757092475891, 1-TPR Gap Test: 0.9584592580795288\n",
      "Epoch 40, Loss: 0.8764626383781433, Final Score Train: 0.6979048848152161, Final Score Test: 0.6814781427383423, macro F1 Train: 0.4258304862235777, macro F1 Test: 0.40223285460660757, 1-TPR Gap Train: 0.9699792861938477, 1-TPR Gap Test: 0.9607234001159668\n",
      "Epoch 50, Loss: 0.8594169616699219, Final Score Train: 0.7084728479385376, Final Score Test: 0.6879000067710876, macro F1 Train: 0.44475154190982014, macro F1 Test: 0.41429896589133824, 1-TPR Gap Train: 0.9721941351890564, 1-TPR Gap Test: 0.9615010619163513\n",
      "Epoch 60, Loss: 0.8666553497314453, Final Score Train: 0.7131967544555664, Final Score Test: 0.691228449344635, macro F1 Train: 0.4537761282027168, macro F1 Test: 0.4227944395351141, 1-TPR Gap Train: 0.9726174473762512, 1-TPR Gap Test: 0.9596624374389648\n",
      "Epoch 70, Loss: 0.7222824096679688, Final Score Train: 0.7223688364028931, Final Score Test: 0.6967284679412842, macro F1 Train: 0.4744644066723677, macro F1 Test: 0.4358951361727141, 1-TPR Gap Train: 0.9702732563018799, 1-TPR Gap Test: 0.957561731338501\n",
      "Epoch 80, Loss: 0.8092069029808044, Final Score Train: 0.7307312488555908, Final Score Test: 0.7056361436843872, macro F1 Train: 0.48858226944686295, macro F1 Test: 0.4487619269007032, 1-TPR Gap Train: 0.9728802442550659, 1-TPR Gap Test: 0.9625102877616882\n",
      "Epoch 90, Loss: 0.8232433795928955, Final Score Train: 0.7338746786117554, Final Score Test: 0.7070397734642029, macro F1 Train: 0.4945836698776573, macro F1 Test: 0.44916221166835213, 1-TPR Gap Train: 0.97316575050354, 1-TPR Gap Test: 0.9649173617362976\n",
      "Arrêt précoce après 97 époques\n",
      "Final Evaluation Score: 0.7049031257629395 Macro F1: 0.44966418431103683 1-TPR_gap: 0.9601420164108276\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.7911274433135986, Final Score Train: 0.7382580041885376, Final Score Test: 0.7073420286178589, macro F1 Train: 0.5020104530716542, macro F1 Test: 0.4521434527401254, 1-TPR Gap Train: 0.9745055437088013, 1-TPR Gap Test: 0.9625405669212341\n",
      "Epoch 10, Loss: 0.7746959924697876, Final Score Train: 0.738828718662262, Final Score Test: 0.7083930373191833, macro F1 Train: 0.5044414518638131, macro F1 Test: 0.45318320183150673, 1-TPR Gap Train: 0.9732159972190857, 1-TPR Gap Test: 0.9636028409004211\n",
      "Epoch 20, Loss: 0.8098596334457397, Final Score Train: 0.7426031231880188, Final Score Test: 0.7086015343666077, macro F1 Train: 0.5105844846479582, macro F1 Test: 0.4542171382851494, 1-TPR Gap Train: 0.9746217727661133, 1-TPR Gap Test: 0.9629859328269958\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7090950012207031 Macro F1: 0.457528010050816 1-TPR_gap: 0.9606620073318481\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7408056855201721, Final Score Train: 0.7439122200012207, Final Score Test: 0.7093628644943237, macro F1 Train: 0.5135003067901218, macro F1 Test: 0.4580037848208996, 1-TPR Gap Train: 0.9743241667747498, 1-TPR Gap Test: 0.9607219099998474\n",
      "Epoch 10, Loss: 0.7332335710525513, Final Score Train: 0.7453566789627075, Final Score Test: 0.7107498645782471, macro F1 Train: 0.5157719434499944, macro F1 Test: 0.4595450030187068, 1-TPR Gap Train: 0.9749414920806885, 1-TPR Gap Test: 0.9619547128677368\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7090125679969788 Macro F1: 0.46053091811155195 1-TPR_gap: 0.9574942588806152\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.7309248447418213, Final Score Train: 0.7470274567604065, Final Score Test: 0.7098551988601685, macro F1 Train: 0.5192224067832895, macro F1 Test: 0.460497023452386, 1-TPR Gap Train: 0.9748325347900391, 1-TPR Gap Test: 0.959213376045227\n",
      "Epoch 10, Loss: 0.7267539501190186, Final Score Train: 0.7477626204490662, Final Score Test: 0.7093465328216553, macro F1 Train: 0.5203014812837881, macro F1 Test: 0.4606375776718789, 1-TPR Gap Train: 0.9752237796783447, 1-TPR Gap Test: 0.9580554366111755\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7089338898658752 Macro F1: 0.45999429108842615 1-TPR_gap: 0.957873523235321\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.7134681940078735, Final Score Train: 0.749473512172699, Final Score Test: 0.7099894881248474, macro F1 Train: 0.52313750994726, macro F1 Test: 0.45994539608162166, 1-TPR Gap Train: 0.9758095145225525, 1-TPR Gap Test: 0.9600335359573364\n",
      "Epoch 10, Loss: 0.7210210561752319, Final Score Train: 0.7496885657310486, Final Score Test: 0.7101234197616577, macro F1 Train: 0.5237507535067168, macro F1 Test: 0.46069322740471563, 1-TPR Gap Train: 0.9756263494491577, 1-TPR Gap Test: 0.9595535397529602\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7082864046096802 Macro F1: 0.4592653560179967 1-TPR_gap: 0.9573073983192444\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.725848913192749, Final Score Train: 0.7496387958526611, Final Score Test: 0.7088354825973511, macro F1 Train: 0.5240909392553192, macro F1 Test: 0.4601065964630597, 1-TPR Gap Train: 0.9751865863800049, 1-TPR Gap Test: 0.9575642943382263\n",
      "Epoch 10, Loss: 0.7270679473876953, Final Score Train: 0.7497438192367554, Final Score Test: 0.709250271320343, macro F1 Train: 0.5242189729436513, macro F1 Test: 0.46061331902107877, 1-TPR Gap Train: 0.9752687215805054, 1-TPR Gap Test: 0.9578872323036194\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7089999914169312 Macro F1: 0.4600543156829499 1-TPR_gap: 0.957945704460144\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.7226332426071167, Final Score Train: 0.749747097492218, Final Score Test: 0.710148274898529, macro F1 Train: 0.5241574154355416, macro F1 Test: 0.4607236600461248, 1-TPR Gap Train: 0.9753367900848389, 1-TPR Gap Test: 0.9595728516578674\n",
      "Epoch 10, Loss: 0.735853374004364, Final Score Train: 0.7497580051422119, Final Score Test: 0.709156334400177, macro F1 Train: 0.5243106169663958, macro F1 Test: 0.46008094247462133, 1-TPR Gap Train: 0.9752053618431091, 1-TPR Gap Test: 0.9582316875457764\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7090455293655396 Macro F1: 0.4600814287528957 1-TPR_gap: 0.9580096006393433\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.7536008358001709, Final Score Train: 0.7454742193222046, Final Score Test: 0.7084908485412598, macro F1 Train: 0.5155679390699702, macro F1 Test: 0.4557918284851043, 1-TPR Gap Train: 0.975380539894104, 1-TPR Gap Test: 0.961189866065979\n",
      "Epoch 10, Loss: 0.8335530161857605, Final Score Train: 0.7462158203125, Final Score Test: 0.7075332403182983, macro F1 Train: 0.5176815707338406, macro F1 Test: 0.45503563913479644, 1-TPR Gap Train: 0.9747499823570251, 1-TPR Gap Test: 0.9600307941436768\n",
      "Epoch 20, Loss: 0.7755122184753418, Final Score Train: 0.7480870485305786, Final Score Test: 0.7079979181289673, macro F1 Train: 0.519251698453678, macro F1 Test: 0.453908698233815, 1-TPR Gap Train: 0.9769224524497986, 1-TPR Gap Test: 0.962087094783783\n",
      "Epoch 30, Loss: 0.7652208805084229, Final Score Train: 0.748878538608551, Final Score Test: 0.7061081528663635, macro F1 Train: 0.5206884153478297, macro F1 Test: 0.4535517378071751, 1-TPR Gap Train: 0.9770686626434326, 1-TPR Gap Test: 0.9586645364761353\n",
      "Epoch 40, Loss: 0.8324977159500122, Final Score Train: 0.7485425472259521, Final Score Test: 0.7077415585517883, macro F1 Train: 0.5200469394230852, macro F1 Test: 0.45450383370732556, 1-TPR Gap Train: 0.9770382046699524, 1-TPR Gap Test: 0.9609792828559875\n",
      "Arrêt précoce après 50 époques\n",
      "Final Evaluation Score: 0.7067449688911438 Macro F1: 0.455464976711171 1-TPR_gap: 0.9580249786376953\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.863594651222229, Final Score Train: 0.7520016431808472, Final Score Test: 0.7065126895904541, macro F1 Train: 0.5264485397577452, macro F1 Test: 0.4552255354714953, 1-TPR Gap Train: 0.9775546789169312, 1-TPR Gap Test: 0.9577998518943787\n",
      "Epoch 10, Loss: 0.7981727123260498, Final Score Train: 0.7511641979217529, Final Score Test: 0.7058208584785461, macro F1 Train: 0.5264180428012512, macro F1 Test: 0.45665004098291007, 1-TPR Gap Train: 0.9759103655815125, 1-TPR Gap Test: 0.9549916982650757\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7060471773147583 Macro F1: 0.4530314038547313 1-TPR_gap: 0.9590629935264587\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.7282189130783081, Final Score Train: 0.7535597085952759, Final Score Test: 0.709255039691925, macro F1 Train: 0.5305755485742941, macro F1 Test: 0.45793109722885017, 1-TPR Gap Train: 0.9765439033508301, 1-TPR Gap Test: 0.960578978061676\n",
      "Epoch 10, Loss: 0.7097694873809814, Final Score Train: 0.754702091217041, Final Score Test: 0.7114014625549316, macro F1 Train: 0.5322307875600867, macro F1 Test: 0.46075510936534764, 1-TPR Gap Train: 0.9771734476089478, 1-TPR Gap Test: 0.9620478749275208\n",
      "Epoch 20, Loss: 0.7474105358123779, Final Score Train: 0.7566127777099609, Final Score Test: 0.710094690322876, macro F1 Train: 0.5360424584497417, macro F1 Test: 0.4617445427825874, 1-TPR Gap Train: 0.9771830439567566, 1-TPR Gap Test: 0.9584448337554932\n",
      "Epoch 30, Loss: 0.7740219831466675, Final Score Train: 0.7573230266571045, Final Score Test: 0.7087218165397644, macro F1 Train: 0.5373688544264584, macro F1 Test: 0.4603579977964087, 1-TPR Gap Train: 0.9772771596908569, 1-TPR Gap Test: 0.9570856094360352\n",
      "Arrêt précoce après 39 époques\n",
      "Final Evaluation Score: 0.7102843523025513 Macro F1: 0.4614336379346747 1-TPR_gap: 0.9591349959373474\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.6913158297538757, Final Score Train: 0.7580651044845581, Final Score Test: 0.7097057104110718, macro F1 Train: 0.5386294722632986, macro F1 Test: 0.46146680365613924, 1-TPR Gap Train: 0.9775007963180542, 1-TPR Gap Test: 0.9579446911811829\n",
      "Epoch 10, Loss: 0.7011332511901855, Final Score Train: 0.7584881782531738, Final Score Test: 0.711432695388794, macro F1 Train: 0.539193362807547, macro F1 Test: 0.461323275443565, 1-TPR Gap Train: 0.9777829647064209, 1-TPR Gap Test: 0.9615421295166016\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7094542980194092 Macro F1: 0.4592083764432463 1-TPR_gap: 0.9597002267837524\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7024892568588257, Final Score Train: 0.7584298253059387, Final Score Test: 0.7099186182022095, macro F1 Train: 0.539595781285903, macro F1 Test: 0.4609057502083166, 1-TPR Gap Train: 0.977263867855072, 1-TPR Gap Test: 0.9589315056800842\n",
      "Epoch 10, Loss: 0.7048856019973755, Final Score Train: 0.7585764527320862, Final Score Test: 0.7095624208450317, macro F1 Train: 0.5398607964355658, macro F1 Test: 0.4602749340182472, 1-TPR Gap Train: 0.9772921204566956, 1-TPR Gap Test: 0.9588499069213867\n",
      "Epoch 20, Loss: 0.7079142332077026, Final Score Train: 0.7587563395500183, Final Score Test: 0.7094445824623108, macro F1 Train: 0.5400869784985457, macro F1 Test: 0.46077723889991834, 1-TPR Gap Train: 0.9774256944656372, 1-TPR Gap Test: 0.9581119418144226\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7093308568000793 Macro F1: 0.4601204861526475 1-TPR_gap: 0.958541214466095\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7157978415489197, Final Score Train: 0.7586761713027954, Final Score Test: 0.7092905044555664, macro F1 Train: 0.540027742593519, macro F1 Test: 0.4594792127992609, 1-TPR Gap Train: 0.9773246049880981, 1-TPR Gap Test: 0.9591018557548523\n",
      "Epoch 10, Loss: 0.7177829742431641, Final Score Train: 0.7588290572166443, Final Score Test: 0.7092486619949341, macro F1 Train: 0.5403192318523616, macro F1 Test: 0.4598169884306455, 1-TPR Gap Train: 0.9773389101028442, 1-TPR Gap Test: 0.958680272102356\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7092085480690002 Macro F1: 0.4602608424620423 1-TPR_gap: 0.9581562280654907\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.7183718681335449, Final Score Train: 0.7588307857513428, Final Score Test: 0.7091774940490723, macro F1 Train: 0.5403330077974746, macro F1 Test: 0.4594759674883257, 1-TPR Gap Train: 0.9773285388946533, 1-TPR Gap Test: 0.9588789939880371\n",
      "Epoch 10, Loss: 0.7217234969139099, Final Score Train: 0.7588618397712708, Final Score Test: 0.7091424465179443, macro F1 Train: 0.5404128539197179, macro F1 Test: 0.45932988401668473, 1-TPR Gap Train: 0.977310836315155, 1-TPR Gap Test: 0.9589549899101257\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7091684341430664 Macro F1: 0.4599230372890989 1-TPR_gap: 0.9584138989448547\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.8405330181121826, Final Score Train: 0.7573999166488647, Final Score Test: 0.7084987163543701, macro F1 Train: 0.5375986360718951, macro F1 Test: 0.4581354007080577, 1-TPR Gap Train: 0.9772012829780579, 1-TPR Gap Test: 0.9588620662689209\n",
      "Epoch 10, Loss: 0.7350935935974121, Final Score Train: 0.756066083908081, Final Score Test: 0.7086451053619385, macro F1 Train: 0.5348154599916184, macro F1 Test: 0.45773932351028873, 1-TPR Gap Train: 0.9773167371749878, 1-TPR Gap Test: 0.9595509171485901\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7100188732147217 Macro F1: 0.45528737792859353 1-TPR_gap: 0.964750349521637\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.7835330963134766, Final Score Train: 0.7568469047546387, Final Score Test: 0.7093720436096191, macro F1 Train: 0.5363444479579906, macro F1 Test: 0.4581116532743889, 1-TPR Gap Train: 0.9773493409156799, 1-TPR Gap Test: 0.9606323838233948\n",
      "Epoch 10, Loss: 0.7683373689651489, Final Score Train: 0.7580597400665283, Final Score Test: 0.7095588445663452, macro F1 Train: 0.5386947646122987, macro F1 Test: 0.4591879140474305, 1-TPR Gap Train: 0.977424681186676, 1-TPR Gap Test: 0.9599297046661377\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7091858386993408 Macro F1: 0.4583610272157582 1-TPR_gap: 0.9600105881690979\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7581014037132263, Final Score Train: 0.7592066526412964, Final Score Test: 0.710861086845398, macro F1 Train: 0.5403903231656277, macro F1 Test: 0.46069187518813953, 1-TPR Gap Train: 0.9780229926109314, 1-TPR Gap Test: 0.9610303044319153\n",
      "Epoch 10, Loss: 0.7543653249740601, Final Score Train: 0.7600585222244263, Final Score Test: 0.710330605506897, macro F1 Train: 0.5423742238941661, macro F1 Test: 0.46024893879597534, 1-TPR Gap Train: 0.9777427315711975, 1-TPR Gap Test: 0.9604122042655945\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7095170021057129 Macro F1: 0.45907912271877466 1-TPR_gap: 0.9599548578262329\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.6577394008636475, Final Score Train: 0.7597298622131348, Final Score Test: 0.7085745930671692, macro F1 Train: 0.5421899529717562, macro F1 Test: 0.4597563959941823, 1-TPR Gap Train: 0.97726970911026, 1-TPR Gap Test: 0.9573928117752075\n",
      "Epoch 10, Loss: 0.688769519329071, Final Score Train: 0.7604321241378784, Final Score Test: 0.7098867297172546, macro F1 Train: 0.5431945038367175, macro F1 Test: 0.46064554969576804, 1-TPR Gap Train: 0.9776697158813477, 1-TPR Gap Test: 0.9591279029846191\n",
      "Epoch 20, Loss: 0.7025096416473389, Final Score Train: 0.7608016729354858, Final Score Test: 0.7084749937057495, macro F1 Train: 0.5441511424549899, macro F1 Test: 0.4608182841746145, 1-TPR Gap Train: 0.9774521589279175, 1-TPR Gap Test: 0.9561316967010498\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.7110536098480225 Macro F1: 0.46159023564697704 1-TPR_gap: 0.9605169892311096\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.7082992196083069, Final Score Train: 0.7609051465988159, Final Score Test: 0.7106800675392151, macro F1 Train: 0.5443883826142377, macro F1 Test: 0.4617851678937316, 1-TPR Gap Train: 0.9774219393730164, 1-TPR Gap Test: 0.9595749974250793\n",
      "Epoch 10, Loss: 0.7059975862503052, Final Score Train: 0.7609785795211792, Final Score Test: 0.7091915607452393, macro F1 Train: 0.5445458294768846, macro F1 Test: 0.4598364400789525, 1-TPR Gap Train: 0.9774113297462463, 1-TPR Gap Test: 0.9585466384887695\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7101156711578369 Macro F1: 0.4601852781137402 1-TPR_gap: 0.9600460529327393\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7179244160652161, Final Score Train: 0.7609467506408691, Final Score Test: 0.7092313766479492, macro F1 Train: 0.5446190939638264, macro F1 Test: 0.45975337192796356, 1-TPR Gap Train: 0.9772744178771973, 1-TPR Gap Test: 0.9587094187736511\n",
      "Epoch 10, Loss: 0.717235803604126, Final Score Train: 0.7610703110694885, Final Score Test: 0.7114853858947754, macro F1 Train: 0.5446192605945216, macro F1 Test: 0.4608125039806598, 1-TPR Gap Train: 0.9775213599205017, 1-TPR Gap Test: 0.962158203125\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7100774049758911 Macro F1: 0.46010881820818017 1-TPR_gap: 0.9600460529327393\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.7247369289398193, Final Score Train: 0.7610292434692383, Final Score Test: 0.7100774049758911, macro F1 Train: 0.5446849613051965, macro F1 Test: 0.46010881820818017, 1-TPR Gap Train: 0.977373480796814, 1-TPR Gap Test: 0.9600460529327393\n",
      "Epoch 10, Loss: 0.7071633338928223, Final Score Train: 0.7610260248184204, Final Score Test: 0.7109176516532898, macro F1 Train: 0.5446632009844674, macro F1 Test: 0.46048799109601696, 1-TPR Gap Train: 0.9773887991905212, 1-TPR Gap Test: 0.9613473415374756\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7098861932754517 Macro F1: 0.4598939301675549 1-TPR_gap: 0.9598783850669861\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.798873245716095, Final Score Train: 0.7593948841094971, Final Score Test: 0.7110615968704224, macro F1 Train: 0.5415819216816676, macro F1 Test: 0.4586405546312889, 1-TPR Gap Train: 0.9772077798843384, 1-TPR Gap Test: 0.9634826183319092\n",
      "Epoch 10, Loss: 0.8520179986953735, Final Score Train: 0.7580857276916504, Final Score Test: 0.7072775363922119, macro F1 Train: 0.5391387052315082, macro F1 Test: 0.4556724404419977, 1-TPR Gap Train: 0.9770327210426331, 1-TPR Gap Test: 0.9588826894760132\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7102058529853821 Macro F1: 0.45851411911717216 1-TPR_gap: 0.961897611618042\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.8354414701461792, Final Score Train: 0.7593805193901062, Final Score Test: 0.7122411131858826, macro F1 Train: 0.541539061837841, macro F1 Test: 0.46125682481431535, 1-TPR Gap Train: 0.9772219657897949, 1-TPR Gap Test: 0.9632253646850586\n",
      "Epoch 10, Loss: 0.8372490406036377, Final Score Train: 0.7602297067642212, Final Score Test: 0.7127934098243713, macro F1 Train: 0.5428475755181464, macro F1 Test: 0.46173662093971723, 1-TPR Gap Train: 0.977611780166626, 1-TPR Gap Test: 0.963850200176239\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7113609910011292 Macro F1: 0.4597499655137097 1-TPR_gap: 0.9629720449447632\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.6979804635047913, Final Score Train: 0.7610146403312683, Final Score Test: 0.7117316126823425, macro F1 Train: 0.544430583320849, macro F1 Test: 0.46083328886330455, 1-TPR Gap Train: 0.9775987267494202, 1-TPR Gap Test: 0.9626299142837524\n",
      "Epoch 10, Loss: 0.6655058860778809, Final Score Train: 0.761457085609436, Final Score Test: 0.7078415155410767, macro F1 Train: 0.5454349937341715, macro F1 Test: 0.4603218339114073, 1-TPR Gap Train: 0.9774792194366455, 1-TPR Gap Test: 0.9553611278533936\n",
      "Epoch 20, Loss: 0.7069857120513916, Final Score Train: 0.7617799639701843, Final Score Test: 0.7105292081832886, macro F1 Train: 0.5462567675232399, macro F1 Test: 0.4610149905168757, 1-TPR Gap Train: 0.977303147315979, 1-TPR Gap Test: 0.9600434303283691\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.708905816078186 Macro F1: 0.458985633267504 1-TPR_gap: 0.9588260650634766\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.6510841250419617, Final Score Train: 0.761926531791687, Final Score Test: 0.7104769945144653, macro F1 Train: 0.5464275836147533, macro F1 Test: 0.46015850780263745, 1-TPR Gap Train: 0.9774255156517029, 1-TPR Gap Test: 0.960795521736145\n",
      "Epoch 10, Loss: 0.6783273220062256, Final Score Train: 0.7621688842773438, Final Score Test: 0.7099651098251343, macro F1 Train: 0.547038770357377, macro F1 Test: 0.4613323406129589, 1-TPR Gap Train: 0.9772990345954895, 1-TPR Gap Test: 0.9585978984832764\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7096270322799683 Macro F1: 0.4604342331085949 1-TPR_gap: 0.9588198065757751\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7225661277770996, Final Score Train: 0.7621113061904907, Final Score Test: 0.7097169160842896, macro F1 Train: 0.5468589179880361, macro F1 Test: 0.4603059344061397, 1-TPR Gap Train: 0.9773637652397156, 1-TPR Gap Test: 0.9591279029846191\n",
      "Epoch 10, Loss: 0.7005184888839722, Final Score Train: 0.7622830867767334, Final Score Test: 0.7107800841331482, macro F1 Train: 0.5472659751728459, macro F1 Test: 0.4604934020007803, 1-TPR Gap Train: 0.977300226688385, 1-TPR Gap Test: 0.9610667824745178\n",
      "Epoch 20, Loss: 0.6950759887695312, Final Score Train: 0.7622383832931519, Final Score Test: 0.7101292014122009, macro F1 Train: 0.5471544258316415, macro F1 Test: 0.460382078224755, 1-TPR Gap Train: 0.9773222804069519, 1-TPR Gap Test: 0.959876298904419\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7102075815200806 Macro F1: 0.46059550057541315 1-TPR_gap: 0.9598197340965271\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7086965441703796, Final Score Train: 0.7623239755630493, Final Score Test: 0.709841251373291, macro F1 Train: 0.5472310804466839, macro F1 Test: 0.4607746601986532, 1-TPR Gap Train: 0.9774168729782104, 1-TPR Gap Test: 0.9589079022407532\n",
      "Epoch 10, Loss: 0.7057703137397766, Final Score Train: 0.7624062299728394, Final Score Test: 0.7094503045082092, macro F1 Train: 0.5474442996626773, macro F1 Test: 0.45972862520439667, 1-TPR Gap Train: 0.977368175983429, 1-TPR Gap Test: 0.9591720104217529\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7094452977180481 Macro F1: 0.4595239421321395 1-TPR_gap: 0.9593666791915894\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7109062671661377, Final Score Train: 0.7624504566192627, Final Score Test: 0.7097555994987488, macro F1 Train: 0.5474858678709825, macro F1 Test: 0.4598775373438108, 1-TPR Gap Train: 0.9774149656295776, 1-TPR Gap Test: 0.9596336483955383\n",
      "Epoch 10, Loss: 0.7072182893753052, Final Score Train: 0.7623999714851379, Final Score Test: 0.710192859172821, macro F1 Train: 0.5474555620714396, macro F1 Test: 0.4602301029244436, 1-TPR Gap Train: 0.9773443937301636, 1-TPR Gap Test: 0.9601556062698364\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7101786732673645 Macro F1: 0.46001139765553317 1-TPR_gap: 0.9603459239006042\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.7443463802337646, Final Score Train: 0.7616525292396545, Final Score Test: 0.7129011154174805, macro F1 Train: 0.5460889683361464, macro F1 Test: 0.4604269452905432, 1-TPR Gap Train: 0.9772160649299622, 1-TPR Gap Test: 0.9653752446174622\n",
      "Epoch 10, Loss: 0.8152541518211365, Final Score Train: 0.7605737447738647, Final Score Test: 0.7115660905838013, macro F1 Train: 0.5439159952443534, macro F1 Test: 0.4593071973060167, 1-TPR Gap Train: 0.9772314429283142, 1-TPR Gap Test: 0.9638250470161438\n",
      "Epoch 20, Loss: 0.7657618522644043, Final Score Train: 0.7603881359100342, Final Score Test: 0.7116849422454834, macro F1 Train: 0.543628548850275, macro F1 Test: 0.4600922064550354, 1-TPR Gap Train: 0.9771477580070496, 1-TPR Gap Test: 0.9632776975631714\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7087172865867615 Macro F1: 0.45756451590591063 1-TPR_gap: 0.9598701000213623\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.7903434038162231, Final Score Train: 0.7611033320426941, Final Score Test: 0.7090986371040344, macro F1 Train: 0.5447097645717741, macro F1 Test: 0.4584985152011122, 1-TPR Gap Train: 0.9774969220161438, 1-TPR Gap Test: 0.9596987962722778\n",
      "Epoch 10, Loss: 0.742424726486206, Final Score Train: 0.7616024613380432, Final Score Test: 0.7083020210266113, macro F1 Train: 0.5459143126666121, macro F1 Test: 0.4597656730338536, 1-TPR Gap Train: 0.9772906303405762, 1-TPR Gap Test: 0.9568384289741516\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7114012837409973 Macro F1: 0.4603681391355052 1-TPR_gap: 0.9624344110488892\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.7346068620681763, Final Score Train: 0.7619916200637817, Final Score Test: 0.7107036113739014, macro F1 Train: 0.5468125796664288, macro F1 Test: 0.46104300043676494, 1-TPR Gap Train: 0.9771706461906433, 1-TPR Gap Test: 0.9603642225265503\n",
      "Epoch 10, Loss: 0.6940925717353821, Final Score Train: 0.7627665996551514, Final Score Test: 0.7096716165542603, macro F1 Train: 0.5482618764212519, macro F1 Test: 0.46010954198343634, 1-TPR Gap Train: 0.9772713780403137, 1-TPR Gap Test: 0.9592336416244507\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7087025046348572 Macro F1: 0.45922686899182735 1-TPR_gap: 0.9581781625747681\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.640624463558197, Final Score Train: 0.7628875970840454, Final Score Test: 0.7096081972122192, macro F1 Train: 0.548532893205732, macro F1 Test: 0.46011318409884877, 1-TPR Gap Train: 0.9772423505783081, 1-TPR Gap Test: 0.9591031670570374\n",
      "Epoch 10, Loss: 0.6950331926345825, Final Score Train: 0.763035774230957, Final Score Test: 0.7096468210220337, macro F1 Train: 0.5489255456099161, macro F1 Test: 0.4606953914594873, 1-TPR Gap Train: 0.9771460294723511, 1-TPR Gap Test: 0.9585983157157898\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7090213894844055 Macro F1: 0.46044019577991085 1-TPR_gap: 0.9576025605201721\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.6921062469482422, Final Score Train: 0.7633757591247559, Final Score Test: 0.709621012210846, macro F1 Train: 0.5493870106256511, macro F1 Test: 0.46062374294674574, 1-TPR Gap Train: 0.9773645401000977, 1-TPR Gap Test: 0.9586182832717896\n",
      "Epoch 10, Loss: 0.7029499411582947, Final Score Train: 0.7634192705154419, Final Score Test: 0.7096384167671204, macro F1 Train: 0.5495304342765482, macro F1 Test: 0.4607582136936177, 1-TPR Gap Train: 0.9773080348968506, 1-TPR Gap Test: 0.9585186243057251\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7095921039581299 Macro F1: 0.4605601695787818 1-TPR_gap: 0.958624005317688\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7132301330566406, Final Score Train: 0.7634007930755615, Final Score Test: 0.709659993648529, macro F1 Train: 0.5494502908368303, macro F1 Test: 0.4607316471841693, 1-TPR Gap Train: 0.9773513078689575, 1-TPR Gap Test: 0.9585883617401123\n",
      "Epoch 10, Loss: 0.7084242701530457, Final Score Train: 0.7634048461914062, Final Score Test: 0.7095181345939636, macro F1 Train: 0.5495046824386149, macro F1 Test: 0.46064081457620387, 1-TPR Gap Train: 0.9773050546646118, 1-TPR Gap Test: 0.9583954811096191\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7095022797584534 Macro F1: 0.4606091105157713 1-TPR_gap: 0.9583954811096191\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7002461552619934, Final Score Train: 0.763451099395752, Final Score Test: 0.7095173001289368, macro F1 Train: 0.5496800770998649, macro F1 Test: 0.46063908060594894, 1-TPR Gap Train: 0.9772221446037292, 1-TPR Gap Test: 0.9583954811096191\n",
      "Epoch 10, Loss: 0.7125818729400635, Final Score Train: 0.7634197473526001, Final Score Test: 0.7095779776573181, macro F1 Train: 0.549573223573056, macro F1 Test: 0.46023543438913334, 1-TPR Gap Train: 0.9772661924362183, 1-TPR Gap Test: 0.9589205384254456\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7095723152160645 Macro F1: 0.46027971904840476 1-TPR_gap: 0.9588649272918701\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.8133668899536133, Final Score Train: 0.762692928314209, Final Score Test: 0.7111731767654419, macro F1 Train: 0.5479733143741823, macro F1 Test: 0.4618650085370866, 1-TPR Gap Train: 0.9774125814437866, 1-TPR Gap Test: 0.9604814052581787\n",
      "Epoch 10, Loss: 0.8060895204544067, Final Score Train: 0.7612121105194092, Final Score Test: 0.710096538066864, macro F1 Train: 0.5453760192436636, macro F1 Test: 0.457822854764847, 1-TPR Gap Train: 0.9770481586456299, 1-TPR Gap Test: 0.9623702168464661\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7121022343635559 Macro F1: 0.45960984102583646 1-TPR_gap: 0.9645946025848389\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.762863278388977, Final Score Train: 0.7624403238296509, Final Score Test: 0.7101004123687744, macro F1 Train: 0.5479387662617119, macro F1 Test: 0.45933388896021726, 1-TPR Gap Train: 0.9769418835639954, 1-TPR Gap Test: 0.9608668684959412\n",
      "Epoch 10, Loss: 0.7035292387008667, Final Score Train: 0.7628045082092285, Final Score Test: 0.7099311351776123, macro F1 Train: 0.5487473381059038, macro F1 Test: 0.46018536307353536, 1-TPR Gap Train: 0.9768615961074829, 1-TPR Gap Test: 0.95967698097229\n",
      "Epoch 20, Loss: 0.7435394525527954, Final Score Train: 0.7626292705535889, Final Score Test: 0.7102665305137634, macro F1 Train: 0.5483813396926454, macro F1 Test: 0.45894508797749756, 1-TPR Gap Train: 0.9768772721290588, 1-TPR Gap Test: 0.9615879654884338\n",
      "Epoch 30, Loss: 0.8731775283813477, Final Score Train: 0.7627731561660767, Final Score Test: 0.7091545462608337, macro F1 Train: 0.5488670368113672, macro F1 Test: 0.4581150203707795, 1-TPR Gap Train: 0.9766792058944702, 1-TPR Gap Test: 0.9601941108703613\n",
      "Epoch 40, Loss: 0.8390703201293945, Final Score Train: 0.763037383556366, Final Score Test: 0.7085319757461548, macro F1 Train: 0.5493607751901471, macro F1 Test: 0.4590389421502653, 1-TPR Gap Train: 0.976714015007019, 1-TPR Gap Test: 0.9580250382423401\n",
      "Arrêt précoce après 47 époques\n",
      "Final Evaluation Score: 0.7101057171821594 Macro F1: 0.46012473521901504 1-TPR_gap: 0.9600867033004761\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.6645087599754333, Final Score Train: 0.7634955048561096, Final Score Test: 0.7090973854064941, macro F1 Train: 0.550219089250669, macro F1 Test: 0.459655928143201, 1-TPR Gap Train: 0.976771891117096, 1-TPR Gap Test: 0.9585387706756592\n",
      "Epoch 10, Loss: 0.683075487613678, Final Score Train: 0.7635930180549622, Final Score Test: 0.7094554901123047, macro F1 Train: 0.5504492602862608, macro F1 Test: 0.45934099146974855, 1-TPR Gap Train: 0.9767367839813232, 1-TPR Gap Test: 0.959570050239563\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7093436121940613 Macro F1: 0.45995406157777596 1-TPR_gap: 0.9587332010269165\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.6924717426300049, Final Score Train: 0.7639015316963196, Final Score Test: 0.7104686498641968, macro F1 Train: 0.550968509498211, macro F1 Test: 0.46086866735948745, 1-TPR Gap Train: 0.9768345355987549, 1-TPR Gap Test: 0.9600687026977539\n",
      "Epoch 10, Loss: 0.6955791711807251, Final Score Train: 0.7638157606124878, Final Score Test: 0.7099891901016235, macro F1 Train: 0.550902728063046, macro F1 Test: 0.4600927532235227, 1-TPR Gap Train: 0.9767287969589233, 1-TPR Gap Test: 0.9598855972290039\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7111445069313049 Macro F1: 0.4616988585896194 1-TPR_gap: 0.960590124130249\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.6939222812652588, Final Score Train: 0.764107882976532, Final Score Test: 0.7107213735580444, macro F1 Train: 0.5514572843650574, macro F1 Test: 0.4616201599451629, 1-TPR Gap Train: 0.9767584800720215, 1-TPR Gap Test: 0.9598226547241211\n",
      "Epoch 10, Loss: 0.6817401647567749, Final Score Train: 0.7640841007232666, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5513890966674093, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.960029125213623\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7108665704727173 Macro F1: 0.46180539127692705 1-TPR_gap: 0.9599277973175049\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7110913991928101, Final Score Train: 0.7640784978866577, Final Score Test: 0.7109078764915466, macro F1 Train: 0.5513779038338418, macro F1 Test: 0.46207518904343486, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.9597405195236206\n",
      "Epoch 10, Loss: 0.7071220278739929, Final Score Train: 0.764079213142395, Final Score Test: 0.7109007835388184, macro F1 Train: 0.5513793273180126, macro F1 Test: 0.46181119509915375, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.959990382194519\n",
      "Epoch 20, Loss: 0.710746169090271, Final Score Train: 0.7641359567642212, Final Score Test: 0.7111122012138367, macro F1 Train: 0.5514868270764436, macro F1 Test: 0.46220088905294393, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.9600235223770142\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7110104560852051 Macro F1: 0.4622155891026907 1-TPR_gap: 0.9598053097724915\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7137851715087891, Final Score Train: 0.7641341686248779, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5514831655218142, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.960029125213623\n",
      "Epoch 10, Loss: 0.7124017477035522, Final Score Train: 0.7641377449035645, Final Score Test: 0.7110580801963806, macro F1 Train: 0.5514904394818089, macro F1 Test: 0.46211200404219577, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.9600041508674622\n",
      "Epoch 20, Loss: 0.6988430619239807, Final Score Train: 0.7641359567642212, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5514867867771821, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.960029125213623\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7109786868095398 Macro F1: 0.46179094131287146 1-TPR_gap: 0.9601663947105408\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.7776163816452026, Final Score Train: 0.7633556723594666, Final Score Test: 0.7104685306549072, macro F1 Train: 0.5501396041649859, macro F1 Test: 0.45950210384097406, 1-TPR Gap Train: 0.9765717387199402, 1-TPR Gap Test: 0.9614350199699402\n",
      "Epoch 10, Loss: 0.7658340930938721, Final Score Train: 0.7635006904602051, Final Score Test: 0.7085611820220947, macro F1 Train: 0.549599788799594, macro F1 Test: 0.4585115725505911, 1-TPR Gap Train: 0.9774015545845032, 1-TPR Gap Test: 0.9586108326911926\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7089000940322876 Macro F1: 0.45893181867004745 1-TPR_gap: 0.9588683247566223\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.8345272541046143, Final Score Train: 0.7635828852653503, Final Score Test: 0.709423303604126, macro F1 Train: 0.5504469886680162, macro F1 Test: 0.4586634305395264, 1-TPR Gap Train: 0.9767187833786011, 1-TPR Gap Test: 0.9601831436157227\n",
      "Epoch 10, Loss: 0.8279632329940796, Final Score Train: 0.7634963989257812, Final Score Test: 0.7088677287101746, macro F1 Train: 0.5504253156974703, macro F1 Test: 0.4588433818123828, 1-TPR Gap Train: 0.9765674471855164, 1-TPR Gap Test: 0.9588921070098877\n",
      "Epoch 20, Loss: 0.7264643907546997, Final Score Train: 0.7633972764015198, Final Score Test: 0.7102397680282593, macro F1 Train: 0.5502467109916475, macro F1 Test: 0.4600018539998801, 1-TPR Gap Train: 0.9765478372573853, 1-TPR Gap Test: 0.9604776501655579\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7093523740768433 Macro F1: 0.46007485043086455 1-TPR_gap: 0.958629846572876\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.6601825952529907, Final Score Train: 0.7643130421638489, Final Score Test: 0.7088170051574707, macro F1 Train: 0.5518634679144628, macro F1 Test: 0.4588515129671772, 1-TPR Gap Train: 0.976762592792511, 1-TPR Gap Test: 0.958782434463501\n",
      "Epoch 10, Loss: 0.7434941530227661, Final Score Train: 0.7644243836402893, Final Score Test: 0.7103109359741211, macro F1 Train: 0.5521716838775547, macro F1 Test: 0.460559759065226, 1-TPR Gap Train: 0.9766770601272583, 1-TPR Gap Test: 0.9600620865821838\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7101020812988281 Macro F1: 0.4600107215389255 1-TPR_gap: 0.9601935148239136\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.6879298686981201, Final Score Train: 0.7646681666374207, Final Score Test: 0.7095276117324829, macro F1 Train: 0.5526527298896375, macro F1 Test: 0.4604570723379298, 1-TPR Gap Train: 0.9766836166381836, 1-TPR Gap Test: 0.9585981369018555\n",
      "Epoch 10, Loss: 0.6648935675621033, Final Score Train: 0.7647291421890259, Final Score Test: 0.7095547318458557, macro F1 Train: 0.5527792081488145, macro F1 Test: 0.46069558101684865, 1-TPR Gap Train: 0.9766790866851807, 1-TPR Gap Test: 0.95841383934021\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7092775702476501 Macro F1: 0.46059442848356974 1-TPR_gap: 0.9579607248306274\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7125214338302612, Final Score Train: 0.7648409605026245, Final Score Test: 0.7092995047569275, macro F1 Train: 0.5529948048887907, macro F1 Test: 0.46092692151641673, 1-TPR Gap Train: 0.9766870737075806, 1-TPR Gap Test: 0.957672119140625\n",
      "Epoch 10, Loss: 0.6950925588607788, Final Score Train: 0.7648287415504456, Final Score Test: 0.7093514204025269, macro F1 Train: 0.5530024332993253, macro F1 Test: 0.46013715375160436, 1-TPR Gap Train: 0.9766550660133362, 1-TPR Gap Test: 0.9585656523704529\n",
      "Epoch 20, Loss: 0.6926829218864441, Final Score Train: 0.764888346195221, Final Score Test: 0.7089998126029968, macro F1 Train: 0.5531058839912023, macro F1 Test: 0.46022382619067326, 1-TPR Gap Train: 0.9766708016395569, 1-TPR Gap Test: 0.9577757716178894\n",
      "Arrêt précoce après 30 époques\n",
      "Final Evaluation Score: 0.7093287706375122 Macro F1: 0.46008627396583907 1-TPR_gap: 0.958571195602417\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.705488920211792, Final Score Train: 0.7649445533752441, Final Score Test: 0.709405779838562, macro F1 Train: 0.5532124799140652, macro F1 Test: 0.4603668952303427, 1-TPR Gap Train: 0.9766767024993896, 1-TPR Gap Test: 0.9584447145462036\n",
      "Epoch 10, Loss: 0.7147866487503052, Final Score Train: 0.7649369239807129, Final Score Test: 0.7093387842178345, macro F1 Train: 0.553206910845016, macro F1 Test: 0.4602315604465031, 1-TPR Gap Train: 0.9766669273376465, 1-TPR Gap Test: 0.9584459662437439\n",
      "Epoch 20, Loss: 0.7144156694412231, Final Score Train: 0.7649496793746948, Final Score Test: 0.7092416286468506, macro F1 Train: 0.5532170903186933, macro F1 Test: 0.4599889491252855, 1-TPR Gap Train: 0.976682186126709, 1-TPR Gap Test: 0.9584943652153015\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7094192504882812 Macro F1: 0.4602608895360185 1-TPR_gap: 0.9585776329040527\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7086201310157776, Final Score Train: 0.7649757862091064, Final Score Test: 0.7092170119285583, macro F1 Train: 0.5532881302994975, macro F1 Test: 0.4600690471282715, 1-TPR Gap Train: 0.9766634702682495, 1-TPR Gap Test: 0.9583649635314941\n",
      "Epoch 10, Loss: 0.7135108709335327, Final Score Train: 0.7650065422058105, Final Score Test: 0.7092220783233643, macro F1 Train: 0.553330932841278, macro F1 Test: 0.46007919414374826, 1-TPR Gap Train: 0.976682186126709, 1-TPR Gap Test: 0.9583649635314941\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7093677520751953 Macro F1: 0.46016989082456156 1-TPR_gap: 0.9585656523704529\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.6973056197166443, Final Score Train: 0.7642636299133301, Final Score Test: 0.7101004123687744, macro F1 Train: 0.5519317804831931, macro F1 Test: 0.4588491990135021, 1-TPR Gap Train: 0.976595401763916, 1-TPR Gap Test: 0.9613516330718994\n",
      "Epoch 10, Loss: 0.7781487703323364, Final Score Train: 0.7638533115386963, Final Score Test: 0.7108557820320129, macro F1 Train: 0.5509951688719331, macro F1 Test: 0.4595456120665099, 1-TPR Gap Train: 0.9767114520072937, 1-TPR Gap Test: 0.9621659517288208\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7096161842346191 Macro F1: 0.4590424950519566 1-TPR_gap: 0.9601898193359375\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.7940989136695862, Final Score Train: 0.7643692493438721, Final Score Test: 0.7105710506439209, macro F1 Train: 0.5520933852122684, macro F1 Test: 0.4603951212894911, 1-TPR Gap Train: 0.9766450524330139, 1-TPR Gap Test: 0.9607469439506531\n",
      "Epoch 10, Loss: 0.6406720280647278, Final Score Train: 0.7647448182106018, Final Score Test: 0.7097187042236328, macro F1 Train: 0.5526894520809422, macro F1 Test: 0.4605680589267795, 1-TPR Gap Train: 0.9768002033233643, 1-TPR Gap Test: 0.9588693976402283\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7094919681549072 Macro F1: 0.4595975753579293 1-TPR_gap: 0.95938640832901\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7107357382774353, Final Score Train: 0.7649226188659668, Final Score Test: 0.7086682319641113, macro F1 Train: 0.5530850982365759, macro F1 Test: 0.46038413356864344, 1-TPR Gap Train: 0.9767600893974304, 1-TPR Gap Test: 0.9569522738456726\n",
      "Epoch 10, Loss: 0.6850690841674805, Final Score Train: 0.7651122808456421, Final Score Test: 0.7089362740516663, macro F1 Train: 0.5535092352796901, macro F1 Test: 0.45906339082819725, 1-TPR Gap Train: 0.9767153859138489, 1-TPR Gap Test: 0.9588091969490051\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7098397016525269 Macro F1: 0.46099179689304304 1-TPR_gap: 0.9586876630783081\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.6999490261077881, Final Score Train: 0.7652946710586548, Final Score Test: 0.7092453241348267, macro F1 Train: 0.5538315309510071, macro F1 Test: 0.46026046863845105, 1-TPR Gap Train: 0.976757824420929, 1-TPR Gap Test: 0.9582301378250122\n",
      "Epoch 10, Loss: 0.7011361122131348, Final Score Train: 0.7654101848602295, Final Score Test: 0.7086979150772095, macro F1 Train: 0.5539076731021997, macro F1 Test: 0.4596245430161866, 1-TPR Gap Train: 0.9769127368927002, 1-TPR Gap Test: 0.9577713012695312\n",
      "Epoch 20, Loss: 0.6451625823974609, Final Score Train: 0.7655270099639893, Final Score Test: 0.7085428237915039, macro F1 Train: 0.5541364303113261, macro F1 Test: 0.4596006052895527, 1-TPR Gap Train: 0.9769176244735718, 1-TPR Gap Test: 0.9574850797653198\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7091134190559387 Macro F1: 0.4610011925444239 1-TPR_gap: 0.9572256803512573\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.710567831993103, Final Score Train: 0.7656173706054688, Final Score Test: 0.709415078163147, macro F1 Train: 0.55432490307293, macro F1 Test: 0.4608784168671605, 1-TPR Gap Train: 0.976909875869751, 1-TPR Gap Test: 0.9579517841339111\n",
      "Epoch 10, Loss: 0.6960428357124329, Final Score Train: 0.765684962272644, Final Score Test: 0.7095619440078735, macro F1 Train: 0.554479590446613, macro F1 Test: 0.4609763982771154, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581474661827087\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7089600563049316 Macro F1: 0.4601201389653577 1-TPR_gap: 0.9577999114990234\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.7062394618988037, Final Score Train: 0.7656655311584473, Final Score Test: 0.7095123529434204, macro F1 Train: 0.5544359127958653, macro F1 Test: 0.46091695257480286, 1-TPR Gap Train: 0.9768951535224915, 1-TPR Gap Test: 0.9581077098846436\n",
      "Epoch 10, Loss: 0.7112011909484863, Final Score Train: 0.7656747102737427, Final Score Test: 0.7095186710357666, macro F1 Train: 0.5544541936856288, macro F1 Test: 0.4609296985682538, 1-TPR Gap Train: 0.9768951535224915, 1-TPR Gap Test: 0.9581077098846436\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7094597220420837 Macro F1: 0.46070286747563005 1-TPR_gap: 0.9582166075706482\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.706257164478302, Final Score Train: 0.7657082676887512, Final Score Test: 0.7095663547515869, macro F1 Train: 0.5545262678703194, macro F1 Test: 0.461030566309837, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581021666526794\n",
      "Epoch 10, Loss: 0.7101259231567383, Final Score Train: 0.7657210826873779, Final Score Test: 0.7095106840133667, macro F1 Train: 0.5545519313545421, macro F1 Test: 0.4609136075283989, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581077098846436\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7095573544502258 Macro F1: 0.4610069746112832 1-TPR_gap: 0.9581077098846436\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.7987650632858276, Final Score Train: 0.7652987241744995, Final Score Test: 0.7097739577293396, macro F1 Train: 0.5537053319376785, macro F1 Test: 0.4605070071620018, 1-TPR Gap Train: 0.9768921136856079, 1-TPR Gap Test: 0.9590408802032471\n",
      "Epoch 10, Loss: 0.7860246896743774, Final Score Train: 0.7652720212936401, Final Score Test: 0.7094895839691162, macro F1 Train: 0.5537068233881272, macro F1 Test: 0.45966078102056945, 1-TPR Gap Train: 0.9768372178077698, 1-TPR Gap Test: 0.9593183398246765\n",
      "Epoch 20, Loss: 0.8937048316001892, Final Score Train: 0.7650575637817383, Final Score Test: 0.7089897394180298, macro F1 Train: 0.5532666847184441, macro F1 Test: 0.45839505526171537, 1-TPR Gap Train: 0.9768484234809875, 1-TPR Gap Test: 0.9595844149589539\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7094175219535828 Macro F1: 0.4574686968962024 1-TPR_gap: 0.9613663554191589\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.8632200956344604, Final Score Train: 0.7652406692504883, Final Score Test: 0.7094039916992188, macro F1 Train: 0.5536155346973458, macro F1 Test: 0.45824896350793204, 1-TPR Gap Train: 0.9768658876419067, 1-TPR Gap Test: 0.9605590105056763\n",
      "Epoch 10, Loss: 0.8169986009597778, Final Score Train: 0.7654800415039062, Final Score Test: 0.709585964679718, macro F1 Train: 0.5541720927734595, macro F1 Test: 0.4589943525496968, 1-TPR Gap Train: 0.9767879843711853, 1-TPR Gap Test: 0.9601775407791138\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7089213132858276 Macro F1: 0.45917087913236576 1-TPR_gap: 0.9586716890335083\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.715619683265686, Final Score Train: 0.7656483054161072, Final Score Test: 0.710224986076355, macro F1 Train: 0.5544584790614379, macro F1 Test: 0.4597757551085147, 1-TPR Gap Train: 0.9768381118774414, 1-TPR Gap Test: 0.9606741666793823\n",
      "Epoch 10, Loss: 0.7034143209457397, Final Score Train: 0.765833854675293, Final Score Test: 0.7114391326904297, macro F1 Train: 0.5548604180625321, macro F1 Test: 0.4614271582746702, 1-TPR Gap Train: 0.9768072366714478, 1-TPR Gap Test: 0.9614511132240295\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7093814015388489 Macro F1: 0.4587039691839041 1-TPR_gap: 0.960058867931366\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.6452302932739258, Final Score Train: 0.7658973932266235, Final Score Test: 0.7100988626480103, macro F1 Train: 0.5549546719034127, macro F1 Test: 0.4595692914016922, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9606283903121948\n",
      "Epoch 10, Loss: 0.6756788492202759, Final Score Train: 0.765946626663208, Final Score Test: 0.7090388536453247, macro F1 Train: 0.5550309422151425, macro F1 Test: 0.45843533079416615, 1-TPR Gap Train: 0.9768622517585754, 1-TPR Gap Test: 0.9596423506736755\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7096307873725891 Macro F1: 0.45895543690809254 1-TPR_gap: 0.9603061676025391\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7040132284164429, Final Score Train: 0.7659262418746948, Final Score Test: 0.7099044919013977, macro F1 Train: 0.5550122879060227, macro F1 Test: 0.45938642645920724, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9604225754737854\n",
      "Epoch 10, Loss: 0.6891756653785706, Final Score Train: 0.7659302949905396, Final Score Test: 0.7086065411567688, macro F1 Train: 0.5550204101838344, macro F1 Test: 0.45823117788781825, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9589819312095642\n",
      "Epoch 20, Loss: 0.7068285942077637, Final Score Train: 0.7659302949905396, Final Score Test: 0.7090679407119751, macro F1 Train: 0.5550204101838344, macro F1 Test: 0.45848887379163766, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.959646999835968\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.708700954914093 Macro F1: 0.4577370240092354 1-TPR_gap: 0.9596648812294006\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7131639719009399, Final Score Train: 0.7659556865692139, Final Score Test: 0.7094560265541077, macro F1 Train: 0.555071319653188, macro F1 Test: 0.4582847375856734, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9606273174285889\n",
      "Epoch 10, Loss: 0.7127952575683594, Final Score Train: 0.7659533023834229, Final Score Test: 0.7093682885169983, macro F1 Train: 0.5550664191847249, macro F1 Test: 0.45850630711331164, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.96023029088974\n",
      "Epoch 20, Loss: 0.7056879997253418, Final Score Train: 0.7659973502159119, Final Score Test: 0.7092061042785645, macro F1 Train: 0.5551162276759735, macro F1 Test: 0.4586762421851591, 1-TPR Gap Train: 0.9768784642219543, 1-TPR Gap Test: 0.9597359299659729\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7093443274497986 Macro F1: 0.45843335703584565 1-TPR_gap: 0.9602552652359009\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.6978440284729004, Final Score Train: 0.7659666538238525, Final Score Test: 0.7089684009552002, macro F1 Train: 0.555098131778289, macro F1 Test: 0.4583248643695466, 1-TPR Gap Train: 0.9768351912498474, 1-TPR Gap Test: 0.9596118927001953\n",
      "Epoch 10, Loss: 0.7079520225524902, Final Score Train: 0.7660000324249268, Final Score Test: 0.7090199589729309, macro F1 Train: 0.5551215860787092, macro F1 Test: 0.45831032991829196, 1-TPR Gap Train: 0.9768784642219543, 1-TPR Gap Test: 0.9597295522689819\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7089272737503052 Macro F1: 0.4584007062871834 1-TPR_gap: 0.9594538807868958\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.7043732404708862, Final Score Train: 0.7652570009231567, Final Score Test: 0.7088682055473328, macro F1 Train: 0.5536116611110158, macro F1 Test: 0.45852024357408966, 1-TPR Gap Train: 0.9769023060798645, 1-TPR Gap Test: 0.9592161774635315\n",
      "Epoch 10, Loss: 0.8313915729522705, Final Score Train: 0.7650586366653442, Final Score Test: 0.7103180289268494, macro F1 Train: 0.5534466903434876, macro F1 Test: 0.45829933302578046, 1-TPR Gap Train: 0.9766706228256226, 1-TPR Gap Test: 0.9623367190361023\n",
      "Epoch 20, Loss: 0.7552738189697266, Final Score Train: 0.7645407319068909, Final Score Test: 0.708991289138794, macro F1 Train: 0.5524507672085768, macro F1 Test: 0.457887957849887, 1-TPR Gap Train: 0.976630687713623, 1-TPR Gap Test: 0.9600945711135864\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.7092119455337524 Macro F1: 0.4560528558081817 1-TPR_gap: 0.9623710513114929\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8500080704689026, Final Score Train: 0.7654128670692444, Final Score Test: 0.7091126441955566, macro F1 Train: 0.5540211120481661, macro F1 Test: 0.45739878121650074, 1-TPR Gap Train: 0.9768046140670776, 1-TPR Gap Test: 0.9608264565467834\n",
      "Epoch 10, Loss: 0.8460057377815247, Final Score Train: 0.7656903266906738, Final Score Test: 0.7107878923416138, macro F1 Train: 0.5546178932109591, macro F1 Test: 0.45887036837747364, 1-TPR Gap Train: 0.9767628312110901, 1-TPR Gap Test: 0.9627053737640381\n",
      "Epoch 20, Loss: 0.8196902275085449, Final Score Train: 0.7659379243850708, Final Score Test: 0.7090548276901245, macro F1 Train: 0.5551699076280219, macro F1 Test: 0.45726346115686656, 1-TPR Gap Train: 0.9767059087753296, 1-TPR Gap Test: 0.9608462452888489\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.711036205291748 Macro F1: 0.4591907794067544 1-TPR_gap: 0.9628816246986389\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7378625869750977, Final Score Train: 0.7660890221595764, Final Score Test: 0.709060549736023, macro F1 Train: 0.555416065019181, macro F1 Test: 0.45855982553587715, 1-TPR Gap Train: 0.9767619967460632, 1-TPR Gap Test: 0.959561288356781\n",
      "Epoch 10, Loss: 0.6836721897125244, Final Score Train: 0.7661726474761963, Final Score Test: 0.7105035185813904, macro F1 Train: 0.5556034628384754, macro F1 Test: 0.458950213841081, 1-TPR Gap Train: 0.9767418503761292, 1-TPR Gap Test: 0.9620568156242371\n",
      "Epoch 20, Loss: 0.7268312573432922, Final Score Train: 0.7662531137466431, Final Score Test: 0.7092527151107788, macro F1 Train: 0.555622187542418, macro F1 Test: 0.4583636648202206, 1-TPR Gap Train: 0.9768840074539185, 1-TPR Gap Test: 0.9601417183876038\n",
      "Arrêt précoce après 24 époques\n",
      "Final Evaluation Score: 0.7095433473587036 Macro F1: 0.45848334611860825 1-TPR_gap: 0.9606034159660339\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.6629295349121094, Final Score Train: 0.7662070989608765, Final Score Test: 0.7099195718765259, macro F1 Train: 0.5556733381448942, macro F1 Test: 0.4586760300521234, 1-TPR Gap Train: 0.976740837097168, 1-TPR Gap Test: 0.9611631631851196\n",
      "Epoch 10, Loss: 0.6629542112350464, Final Score Train: 0.7663533091545105, Final Score Test: 0.7094942927360535, macro F1 Train: 0.5558226364439638, macro F1 Test: 0.4587311468948801, 1-TPR Gap Train: 0.9768840074539185, 1-TPR Gap Test: 0.9602574110031128\n",
      "Epoch 20, Loss: 0.6677641868591309, Final Score Train: 0.7663529515266418, Final Score Test: 0.7090997099876404, macro F1 Train: 0.5559616242294261, macro F1 Test: 0.4588620334455443, 1-TPR Gap Train: 0.9767442941665649, 1-TPR Gap Test: 0.9593373537063599\n",
      "Epoch 30, Loss: 0.6672694683074951, Final Score Train: 0.7665009498596191, Final Score Test: 0.7092292308807373, macro F1 Train: 0.556072212115164, macro F1 Test: 0.4574518660384426, 1-TPR Gap Train: 0.9769297242164612, 1-TPR Gap Test: 0.9610066413879395\n",
      "Arrêt précoce après 32 époques\n",
      "Final Evaluation Score: 0.7091791033744812 Macro F1: 0.45847369804173743 1-TPR_gap: 0.959884524345398\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.6964743733406067, Final Score Train: 0.7664636373519897, Final Score Test: 0.7089089751243591, macro F1 Train: 0.5560482043394149, macro F1 Test: 0.4574844308316936, 1-TPR Gap Train: 0.9768791198730469, 1-TPR Gap Test: 0.960333526134491\n",
      "Epoch 10, Loss: 0.6924798488616943, Final Score Train: 0.7664679884910583, Final Score Test: 0.7089381217956543, macro F1 Train: 0.5560568506593838, macro F1 Test: 0.45787404853839214, 1-TPR Gap Train: 0.9768791198730469, 1-TPR Gap Test: 0.9600022435188293\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7092539668083191 Macro F1: 0.4578252013939574 1-TPR_gap: 0.96068274974823\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7139667272567749, Final Score Train: 0.7664797306060791, Final Score Test: 0.7094332575798035, macro F1 Train: 0.5560646009592497, macro F1 Test: 0.45822347987652, 1-TPR Gap Train: 0.9768948554992676, 1-TPR Gap Test: 0.9606429934501648\n",
      "Epoch 10, Loss: 0.7099629044532776, Final Score Train: 0.7664990425109863, Final Score Test: 0.7094894647598267, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.45825297390584374, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9607259631156921\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7093132734298706 Macro F1: 0.45801985959167624 1-TPR_gap: 0.9606067538261414\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.6998120546340942, Final Score Train: 0.7664990425109863, Final Score Test: 0.709478497505188, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.458389095781179, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9605679512023926\n",
      "Epoch 10, Loss: 0.7040873169898987, Final Score Train: 0.7664990425109863, Final Score Test: 0.7093324065208435, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.45807746162696966, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9605873227119446\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7094164490699768 Macro F1: 0.45830796586199146 1-TPR_gap: 0.9605249166488647\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.7936120629310608, Final Score Train: 0.7662976384162903, Final Score Test: 0.7101032733917236, macro F1 Train: 0.5558431410150108, macro F1 Test: 0.45884400514966917, 1-TPR Gap Train: 0.9767521619796753, 1-TPR Gap Test: 0.9613624811172485\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005, 0.001,0.0005,0.0001]\n",
    "batch_size_list = [28,56,128,256,1024,2056,4112]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-28-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending= train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,optimizer,learning_rate,batch_size,early_ending,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>26</td>\n",
       "      <td>tensor(0.7079)</td>\n",
       "      <td>0.467709</td>\n",
       "      <td>tensor(0.9480)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7082)</td>\n",
       "      <td>0.468484</td>\n",
       "      <td>tensor(0.9479)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>16</td>\n",
       "      <td>tensor(0.7079)</td>\n",
       "      <td>0.467390</td>\n",
       "      <td>tensor(0.9484)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>24</td>\n",
       "      <td>tensor(0.7066)</td>\n",
       "      <td>0.466057</td>\n",
       "      <td>tensor(0.9472)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>25</td>\n",
       "      <td>tensor(0.7084)</td>\n",
       "      <td>0.467715</td>\n",
       "      <td>tensor(0.9490)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>30</td>\n",
       "      <td>tensor(0.7054)</td>\n",
       "      <td>0.465088</td>\n",
       "      <td>tensor(0.9458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.7070)</td>\n",
       "      <td>0.466672</td>\n",
       "      <td>tensor(0.9474)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN-28-28_NAG_lr_0.1_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>24</td>\n",
       "      <td>tensor(0.7074)</td>\n",
       "      <td>0.466449</td>\n",
       "      <td>tensor(0.9484)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN-28-28_NAG_lr_0.05_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7070)</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>tensor(0.9486)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN-28-28_NAG_lr_0.01_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7087)</td>\n",
       "      <td>0.468469</td>\n",
       "      <td>tensor(0.9489)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN-28-28_NAG_lr_0.005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>21</td>\n",
       "      <td>tensor(0.7087)</td>\n",
       "      <td>0.466815</td>\n",
       "      <td>tensor(0.9505)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN-28-28_NAG_lr_0.001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.7079)</td>\n",
       "      <td>0.467912</td>\n",
       "      <td>tensor(0.9479)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN-28-28_NAG_lr_0.0005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>16</td>\n",
       "      <td>tensor(0.7081)</td>\n",
       "      <td>0.469238</td>\n",
       "      <td>tensor(0.9469)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN-28-28_NAG_lr_0.0001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>32</td>\n",
       "      <td>tensor(0.7064)</td>\n",
       "      <td>0.465253</td>\n",
       "      <td>tensor(0.9476)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NN-28-28_Adam_lr_0.1_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NN-28-28_Adam_lr_0.05_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN-28-28_Adam_lr_0.005_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN-28-28_Adam_lr_0.001_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NN-28-28_Adam_lr_0.0005_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN-28-28_Adam_lr_0.0001_batch_size_4112</td>\n",
       "      <td>Adam (\\nParameter Group 0\\n    amsgrad: False\\...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.1_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.05_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.01_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.005_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.001_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.0005_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NN-28-28_Adagrad_lr_0.0001_batch_size_4112</td>\n",
       "      <td>Adagrad (\\nParameter Group 0\\n    differentiab...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NN-28-28_SGD_lr_0.1_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NN-28-28_SGD_lr_0.05_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NN-28-28_SGD_lr_0.01_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NN-28-28_SGD_lr_0.005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NN-28-28_SGD_lr_0.001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NN-28-28_SGD_lr_0.0005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NN-28-28_SGD_lr_0.0001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.5014)</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  \\\n",
       "0      NN-28-28_Momentum_lr_0.1_batch_size_4112   \n",
       "1     NN-28-28_Momentum_lr_0.05_batch_size_4112   \n",
       "2     NN-28-28_Momentum_lr_0.01_batch_size_4112   \n",
       "3    NN-28-28_Momentum_lr_0.005_batch_size_4112   \n",
       "4    NN-28-28_Momentum_lr_0.001_batch_size_4112   \n",
       "5   NN-28-28_Momentum_lr_0.0005_batch_size_4112   \n",
       "6   NN-28-28_Momentum_lr_0.0001_batch_size_4112   \n",
       "7           NN-28-28_NAG_lr_0.1_batch_size_4112   \n",
       "8          NN-28-28_NAG_lr_0.05_batch_size_4112   \n",
       "9          NN-28-28_NAG_lr_0.01_batch_size_4112   \n",
       "10        NN-28-28_NAG_lr_0.005_batch_size_4112   \n",
       "11        NN-28-28_NAG_lr_0.001_batch_size_4112   \n",
       "12       NN-28-28_NAG_lr_0.0005_batch_size_4112   \n",
       "13       NN-28-28_NAG_lr_0.0001_batch_size_4112   \n",
       "14         NN-28-28_Adam_lr_0.1_batch_size_4112   \n",
       "15        NN-28-28_Adam_lr_0.05_batch_size_4112   \n",
       "16        NN-28-28_Adam_lr_0.01_batch_size_4112   \n",
       "17       NN-28-28_Adam_lr_0.005_batch_size_4112   \n",
       "18       NN-28-28_Adam_lr_0.001_batch_size_4112   \n",
       "19      NN-28-28_Adam_lr_0.0005_batch_size_4112   \n",
       "20      NN-28-28_Adam_lr_0.0001_batch_size_4112   \n",
       "21      NN-28-28_Adagrad_lr_0.1_batch_size_4112   \n",
       "22     NN-28-28_Adagrad_lr_0.05_batch_size_4112   \n",
       "23     NN-28-28_Adagrad_lr_0.01_batch_size_4112   \n",
       "24    NN-28-28_Adagrad_lr_0.005_batch_size_4112   \n",
       "25    NN-28-28_Adagrad_lr_0.001_batch_size_4112   \n",
       "26   NN-28-28_Adagrad_lr_0.0005_batch_size_4112   \n",
       "27   NN-28-28_Adagrad_lr_0.0001_batch_size_4112   \n",
       "28          NN-28-28_SGD_lr_0.1_batch_size_4112   \n",
       "29         NN-28-28_SGD_lr_0.05_batch_size_4112   \n",
       "30         NN-28-28_SGD_lr_0.01_batch_size_4112   \n",
       "31        NN-28-28_SGD_lr_0.005_batch_size_4112   \n",
       "32        NN-28-28_SGD_lr_0.001_batch_size_4112   \n",
       "33       NN-28-28_SGD_lr_0.0005_batch_size_4112   \n",
       "34       NN-28-28_SGD_lr_0.0001_batch_size_4112   \n",
       "\n",
       "                                            optimizer      lr  batch_size  \\\n",
       "0   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        4112   \n",
       "1   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        4112   \n",
       "2   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        4112   \n",
       "3   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        4112   \n",
       "4   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        4112   \n",
       "5   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        4112   \n",
       "6   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        4112   \n",
       "7   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        4112   \n",
       "8   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        4112   \n",
       "9   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        4112   \n",
       "10  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        4112   \n",
       "11  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        4112   \n",
       "12  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        4112   \n",
       "13  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        4112   \n",
       "14  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.1000        4112   \n",
       "15  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0500        4112   \n",
       "16  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0100        4112   \n",
       "17  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0050        4112   \n",
       "18  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0010        4112   \n",
       "19  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0005        4112   \n",
       "20  Adam (\\nParameter Group 0\\n    amsgrad: False\\...  0.0001        4112   \n",
       "21  Adagrad (\\nParameter Group 0\\n    differentiab...  0.1000        4112   \n",
       "22  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0500        4112   \n",
       "23  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0100        4112   \n",
       "24  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0050        4112   \n",
       "25  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0010        4112   \n",
       "26  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0005        4112   \n",
       "27  Adagrad (\\nParameter Group 0\\n    differentiab...  0.0001        4112   \n",
       "28  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        4112   \n",
       "29  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        4112   \n",
       "30  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        4112   \n",
       "31  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        4112   \n",
       "32  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        4112   \n",
       "33  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        4112   \n",
       "34  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        4112   \n",
       "\n",
       "    early_ending     final_score  macro_f1   macro_tpr_gap  \n",
       "0             26  tensor(0.7079)  0.467709  tensor(0.9480)  \n",
       "1             14  tensor(0.7082)  0.468484  tensor(0.9479)  \n",
       "2             16  tensor(0.7079)  0.467390  tensor(0.9484)  \n",
       "3             24  tensor(0.7066)  0.466057  tensor(0.9472)  \n",
       "4             25  tensor(0.7084)  0.467715  tensor(0.9490)  \n",
       "5             30  tensor(0.7054)  0.465088  tensor(0.9458)  \n",
       "6             13  tensor(0.7070)  0.466672  tensor(0.9474)  \n",
       "7             24  tensor(0.7074)  0.466449  tensor(0.9484)  \n",
       "8             11  tensor(0.7070)  0.465390  tensor(0.9486)  \n",
       "9             18  tensor(0.7087)  0.468469  tensor(0.9489)  \n",
       "10            21  tensor(0.7087)  0.466815  tensor(0.9505)  \n",
       "11            13  tensor(0.7079)  0.467912  tensor(0.9479)  \n",
       "12            16  tensor(0.7081)  0.469238  tensor(0.9469)  \n",
       "13            32  tensor(0.7064)  0.465253  tensor(0.9476)  \n",
       "14            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "15            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "16            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "17            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "18            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "19            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "20            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "21            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "22            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "23            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "24            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "25            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "26            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "27            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "28            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "29            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "30            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "31            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "32            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "33            11  tensor(0.5014)  0.002807      tensor(1.)  \n",
       "34            11  tensor(0.5014)  0.002807      tensor(1.)  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.5)\n",
    "    nn.Linear(2048, 256),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.3)\n",
    "    nn.Linear(256, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005, 0.001,0.0005,0.0001]\n",
    "batch_size_list = [28,56,128,256,1024,2056,4112]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending= train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,optimizer,learning_rate,batch_size,early_ending,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res_2, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINEMENT SUR TOUT LE MODELE\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for i in range(1,10):\n",
    "            alpha=i\n",
    "            name = 'all'+opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "            print('\\n\\n Starting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_tensor, Y_tensor, S_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Couche d'entrée à la première couche cachée\n",
    "    nn.ReLU(),  # Fonction d'activation ReLU\n",
    "    nn.Dropout(p=0.5),  # Dropout avec une probabilité de désactivation de 50%\n",
    "    nn.Linear(2048, 512),  # De la première couche cachée à la deuxième couche cachée\n",
    "    nn.ReLU(),  # Une autre fonction d'activation ReLU après la deuxième couche cachée\n",
    "    nn.Dropout(p=0.5),  # Un autre dropout après la deuxième couche cachée\n",
    "    nn.Linear(512, 28),  # De la deuxième couche cachée à la couche de sortie\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour la classification multiclasse\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28-dropout_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(2048,512),  # Assuming 768 input features and 28 classes\n",
    "    nn.Linear(512,28),  \n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. REGRESSION WITH CUSTOM LOSS macro F1**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, optimizer, X_train_tensor, Y_train_one_hot, X_test_tensor, Y_test are already defined\n",
    "\n",
    "# Convert Y_test to one-hot encoding if it's not already one-hot encoded\n",
    "# This is necessary for consistency in our loss function calculations\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.int64) if isinstance(Y_test, pd.Series) else torch.from_numpy(Y_test).long()\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    # Forward pass on the training data\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = macro_soft_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No gradient computation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Forward pass on the validation data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        # Calculate the exact macro F1 score for both training and validation data\n",
    "        f1_train = calculate_exact_macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "        f1_test = calculate_exact_macro_f1(Y_test_one_hot.float(), outputs_test)\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    \n",
    "    # Print loss and F1 score\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train.item():.4f}, macro F1 Train: {f1_train:.4f}, macro F1 Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUSTON LOSS FUNCTION TRP GAP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gap_TPR(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap for each class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Initialize TPR storage\n",
    "    tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        # Calculate TPR for the current class across all groups\n",
    "        tpr_list = []\n",
    "        \n",
    "        # Calculate overall TPR for the current class\n",
    "        overall_mask = y_true_indices == class_idx\n",
    "        overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / torch.sum(overall_mask).float()\n",
    "        \n",
    "        # Calculate TPR for each protected group\n",
    "        for group_val in protected_attribute.unique():\n",
    "            group_mask = (protected_attribute == group_val) & overall_mask\n",
    "            group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / torch.sum(group_mask).float()\n",
    "            tpr_list.append(group_tpr)\n",
    "        \n",
    "        # Calculate TPR gap for the current class and store it\n",
    "        tpr_gaps.append(torch.abs(torch.tensor(tpr_list) - overall_tpr))\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> (5550,)\n",
      "<class 'torch.Tensor'> torch.Size([5550, 28])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_test),Y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_pred_probs),Y_pred_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_macro_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mget_macro_tpr_gap\u001b[0;34m(y_true, y_pred, protected_attribute)\u001b[0m\n\u001b[1;32m    110\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m--> 112\u001b[0m     class_tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mget_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotected_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     class_tpr_gaps\u001b[38;5;241m.\u001b[39mappend(class_tpr_gap)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Calculate the average TPR gap across all classes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mget_tpr_gap\u001b[0;34m(y_true, y_pred_probs, protected_attribute, class_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mCalculate the TPR gap for a specific class across protected groups.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m- TPR gap for the specified class.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert one-hot labels to class indices for gathering\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m y_true_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate overall TPR for the current class\u001b[39;00m\n\u001b[1;32m     74\u001b[0m overall_mask \u001b[38;5;241m=\u001b[39m y_true_indices \u001b[38;5;241m==\u001b[39m class_idx\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not Series"
     ]
    }
   ],
   "source": [
    "print(type(Y_test),Y_test.shape)\n",
    "print(type(Y_pred_probs),Y_pred_probs.shape)\n",
    "get_macro_tpr_gap(Y_test,Y_pred_probs,S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

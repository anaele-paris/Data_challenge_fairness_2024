{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model,name):\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the F1 score as a loss function.\n",
    "    \"\"\"\n",
    "    #y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    soft_f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    loss = 1 - soft_f1.mean()  # Mean to aggregate over all classes\n",
    "    return loss\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    #y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)  # Average F1 score across all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "test_tensor: 2 1 torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split with Y56 (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NN with customized loss function (final score)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.9841914176940918, Final Score Train: 0.501908004283905, Final Score Test: 0.4984510540962219, macro F1 Train: 0.008571775500201642, macro F1 Test: 0.007334987901510969, 1-TPR Gap Train: 0.9952442049980164, 1-TPR Gap Test: 0.9895671010017395\n",
      "Epoch 200, Loss: 0.9833416938781738, Final Score Train: 0.5067097544670105, Final Score Test: 0.5049751996994019, macro F1 Train: 0.019669357091836863, macro F1 Test: 0.01782821189219381, 1-TPR Gap Train: 0.9937501549720764, 1-TPR Gap Test: 0.992122232913971\n",
      "Epoch 300, Loss: 0.9840088486671448, Final Score Train: 0.5078456401824951, Final Score Test: 0.5077477693557739, macro F1 Train: 0.019754407542982855, macro F1 Test: 0.019433599351551208, 1-TPR Gap Train: 0.995936930179596, 1-TPR Gap Test: 0.9960619807243347\n",
      "Epoch 400, Loss: 0.985431432723999, Final Score Train: 0.5088541507720947, Final Score Test: 0.5081185102462769, macro F1 Train: 0.018309218934500774, macro F1 Test: 0.016256363347120585, 1-TPR Gap Train: 0.9993990659713745, 1-TPR Gap Test: 0.999980628490448\n",
      "Epoch 500, Loss: 0.9852122068405151, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 600, Loss: 0.9843538999557495, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 700, Loss: 0.983160138130188, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 800, Loss: 0.9821821451187134, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 900, Loss: 0.9814532995223999, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 1000, Loss: 0.980742871761322, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.0162617788557115, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Final Evaluation Score: 0.5081309080123901 Macro F1: 0.0162617788557115 1-TPR_gap: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    #loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)*10 + get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            \n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'y_pred_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msave_Y_pred_tofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_true_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36msave_Y_pred_tofile\u001b[0;34m(X, model, name)\u001b[0m\n\u001b[1;32m     29\u001b[0m probs\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(y_pred_probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), columns\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m28\u001b[39m)))\n\u001b[1;32m     30\u001b[0m file_name_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_probs/y_pred_probs_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(name)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# save predicted labels for each Xi (dim=1)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_pred_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/core/generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3962\u001b[0m )\n\u001b[0;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'y_pred_probs'"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "save_Y_pred_tofile(X_test_true_tensor, model,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        81\n",
      "           1       0.00      0.00      0.00       127\n",
      "           2       0.00      0.00      0.00       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.00      0.00      0.00        72\n",
      "           6       0.00      0.00      0.00       178\n",
      "           7       0.00      0.00      0.00        54\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00        91\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.00      0.00      0.00       286\n",
      "          12       0.00      0.00      0.00       110\n",
      "          13       0.00      0.00      0.00       258\n",
      "          14       0.00      0.00      0.00       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.00      0.00      0.00        33\n",
      "          17       0.00      0.00      0.00        26\n",
      "          18       0.00      0.00      0.00       383\n",
      "          19       0.00      0.00      0.00       611\n",
      "          20       0.00      0.00      0.00        98\n",
      "          21       0.29      1.00      0.46      1636\n",
      "          22       0.00      0.00      0.00       264\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.00      0.00      0.00        89\n",
      "          25       0.00      0.00      0.00       183\n",
      "          26       0.00      0.00      0.00       227\n",
      "          27       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.29      5550\n",
      "   macro avg       0.01      0.04      0.02      5550\n",
      "weighted avg       0.09      0.29      0.13      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_probs = model(X_test_true_tensor)\n",
    "Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)\n",
    "\n",
    "results=pd.DataFrame(Y_pred_tensor, columns= ['score'])\n",
    "name = 'NN_with_custom_loss'\n",
    "file_name = \"Data_Challenge_MDI_341_\"+str(name)+\".csv\"\n",
    "results.to_csv(file_name, header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRET ANTICIPE DU NN (sans mini-batch)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.6180952787399292, Final Score Train: 0.8383526802062988, Final Score Test: 0.7733933925628662, macro F1 Train: 0.727813242975664, macro F1 Test: 0.6287169097435105, 1-TPR Gap Train: 0.9488921761512756, 1-TPR Gap Test: 0.9180698394775391\n",
      "Epoch 200, Loss: 0.5957030057907104, Final Score Train: 0.8599814772605896, Final Score Test: 0.7825250029563904, macro F1 Train: 0.7688718539032557, macro F1 Test: 0.6406831513698007, 1-TPR Gap Train: 0.9510911107063293, 1-TPR Gap Test: 0.92436683177948\n",
      "Epoch 300, Loss: 0.5893559455871582, Final Score Train: 0.8680845499038696, Final Score Test: 0.7799464464187622, macro F1 Train: 0.7814826795012992, macro F1 Test: 0.6342193246928722, 1-TPR Gap Train: 0.9546864628791809, 1-TPR Gap Test: 0.9256735444068909\n",
      "Epoch 400, Loss: 0.5860427618026733, Final Score Train: 0.8719273209571838, Final Score Test: 0.7806618213653564, macro F1 Train: 0.7879128081575509, macro F1 Test: 0.6328553288903299, 1-TPR Gap Train: 0.9559418559074402, 1-TPR Gap Test: 0.9284682273864746\n",
      "Epoch 500, Loss: 0.5838711261749268, Final Score Train: 0.873989462852478, Final Score Test: 0.7756043076515198, macro F1 Train: 0.7916538288078653, macro F1 Test: 0.6359021635570292, 1-TPR Gap Train: 0.9563250541687012, 1-TPR Gap Test: 0.9153064489364624\n",
      "Epoch 600, Loss: 0.582038402557373, Final Score Train: 0.874853253364563, Final Score Test: 0.7724635601043701, macro F1 Train: 0.7939487191221201, macro F1 Test: 0.6300677856073184, 1-TPR Gap Train: 0.955757737159729, 1-TPR Gap Test: 0.914859414100647\n",
      "Epoch 700, Loss: 0.5809394121170044, Final Score Train: 0.8754029870033264, Final Score Test: 0.7747632265090942, macro F1 Train: 0.7953585486340774, macro F1 Test: 0.6325119358405923, 1-TPR Gap Train: 0.9554474353790283, 1-TPR Gap Test: 0.9170145392417908\n",
      "Epoch 800, Loss: 0.579990029335022, Final Score Train: 0.8761406540870667, Final Score Test: 0.7721831798553467, macro F1 Train: 0.796795821382338, macro F1 Test: 0.6320952853052472, 1-TPR Gap Train: 0.955485463142395, 1-TPR Gap Test: 0.9122711420059204\n",
      "Epoch 900, Loss: 0.5790771245956421, Final Score Train: 0.8768042922019958, Final Score Test: 0.7738946080207825, macro F1 Train: 0.7983541643512864, macro F1 Test: 0.6342652527311115, 1-TPR Gap Train: 0.9552544355392456, 1-TPR Gap Test: 0.9135239720344543\n",
      "Epoch 1000, Loss: 0.5786576271057129, Final Score Train: 0.877066969871521, Final Score Test: 0.7720322608947754, macro F1 Train: 0.7988552052836821, macro F1 Test: 0.631037906782591, 1-TPR Gap Train: 0.9552788138389587, 1-TPR Gap Test: 0.9130265712738037\n",
      "Final Evaluation Score: 0.7720322608947754 Macro F1: 0.631037906782591 1-TPR_gap: 0.9130265712738037\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the model and optimizer\n",
    "# ---------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# 2. Paramètres pour l'arrêt précoce\n",
    "# -------------------------------\n",
    "patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "best_loss = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# 3. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "num_epochs = 1000  # Adjust as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs_train = model(X_train_tensor)\n",
    "    \n",
    "    loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "    # loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate metrics for training data\n",
    "            outputs_train = model(X_train_tensor) # probabilities\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "            inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "\n",
    "            # Calculate metrics for test data\n",
    "            outputs_test = model(X_test_tensor)\n",
    "\n",
    "            # Evaluate predictions on training data\n",
    "            final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "            macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "            inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "            # Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "            if best_loss is None or final_score_test < best_loss:\n",
    "                best_loss = final_score_test\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                    break  # Arrêter l'entraînement\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            # print(f'Epoch {epoch + 1}, Loss: {loss.item()},  macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}')# Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "\n",
    "\n",
    "# 4. Make Predictions and Evaluate with final_score\n",
    "# -------------------------------------------------\n",
    "            \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    Y_pred_probs = model(X_test_tensor) # dim = 28\n",
    "    Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "    Y_pred_one_hot = torch.nn.functional.one_hot(Y_pred_tensor, num_classes=28)  # dim = 28\n",
    " \n",
    "    macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "    inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "    print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANS MINI-BATCH\n",
    "def train_NN_with_custom_loss_no_mini_batch(model, optimizer, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs_train = model(X_train_tensor)\n",
    "    \n",
    "        loss = soft_final_score_loss(Y_train_one_hot.float(), outputs_train, S_train_tensor)\n",
    "        #loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) \n",
    "        # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        # 1. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Calculate metrics for test data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "\n",
    "        # Evaluate predictions on test (validation) data\n",
    "        final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or final_score_test < best_loss:\n",
    "            best_loss = final_score_test\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "\n",
    "        # 2. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6730861067771912 Macro F1: 0.4465380708164603 1-TPR_gap: 0.8996341228485107\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "    )  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss_no_mini_batch(model,optim.Adam(model.parameters(), lr=learning_rate), X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "#save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            #if epoch==0 : print('dim de X_batch Y_batch et S_batch',X_batch.size(),Y_batch.size(),S_batch.size())\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_train = model(X_batch)\n",
    "            #print('Y_batch',Y_batch.size(),'outputs_train',outputs_train.size())\n",
    "            Y_batch_one_hot = torch.nn.functional.one_hot(Y_batch, num_classes=Y_train.nunique())\n",
    "            loss = soft_final_score_loss(Y_batch_one_hot, outputs_train, S_batch)\n",
    "            # loss = soft_macro_f1_loss(Y_train_one_hot.float(), outputs_train) \n",
    "            # loss = get_macro_tpr_gap(Y_train_one_hot.float(), outputs_train, S_train_tensor )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print('boucle mini batch terminée')\n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test_one_hot, outputs_test, S_batch)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        print('fin boucle mini batch test')    \n",
    "        # Evaluate predictions on test (validation) data\n",
    "        #final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "        #outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                break  # Arrêter l'entraînement\n",
    "        print('fin eval early ending') \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()}, macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    " \n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        print(f'Final Evaluation Score: {final_score.item()} Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 10  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train.item()}, Final Score Test: {final_score_test.item()} (gap {final_score_test.item()-final_score_train.item()}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1_train = get_macro_f1(Y_train_tensor, Y_train_pred_tensor)\n",
    "        #inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        #final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.9672987461090088, Final Score Train: 0.5082401633262634, Final Score Test: 0.5081490278244019 (gap -9.113550186157227e-05) macro F1 Train: 0.016485233218439258, macro F1 Test: 0.01629806734409245, 1-TPR Gap Train: 0.9999951124191284, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9381933808326721, Final Score Train: 0.5082521438598633, Final Score Test: 0.5081501603126526 (gap -0.00010198354721069336) macro F1 Train: 0.016504329004329004, macro F1 Test: 0.01630034075284459, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9442574977874756, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.5015599727630615 gap -0.006671607494354248 || Macro F1: 0.0062470945825138565 1-TPR_gap: 0.9968729019165039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 2, 27, 21,  ..., 27, 21,  2]),\n",
       " tensor([[-8.0931e+02, -9.5521e+02, -8.2858e-03,  ..., -5.8070e+02,\n",
       "          -3.4660e+02, -2.7036e+01],\n",
       "         [-6.3604e+02, -7.4950e+02, -2.5137e+01,  ..., -4.6625e+02,\n",
       "          -1.9792e+02, -2.2650e-06],\n",
       "         [-5.5730e+02, -6.4171e+02, -1.7020e+01,  ..., -3.9428e+02,\n",
       "          -1.1912e+03, -4.3805e+02],\n",
       "         ...,\n",
       "         [-6.1900e+02, -7.3155e+02, -1.8086e+01,  ..., -4.5194e+02,\n",
       "          -1.9343e+02, -1.5497e-06],\n",
       "         [-2.8359e+03, -9.4456e+02, -8.7078e+03,  ..., -3.4690e+02,\n",
       "          -2.2788e+02, -1.6803e+02],\n",
       "         [-5.3348e+02, -6.2752e+02, -2.1443e-01,  ..., -3.7338e+02,\n",
       "          -1.4681e+02, -3.4281e+00]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    nn.LogSoftmax(dim=1),  # LogSoftmax for multi-class classification\n",
    "    )  \n",
    "\n",
    "batch_size = 128\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending, final_score_train = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending,final_score_train, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.9854655265808105, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901, macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.8613970279693604, Final Score Train: 0.6198021173477173, Final Score Test: 0.6191191673278809, macro F1 Train: 0.28181842822967057, macro F1 Test: 0.27811187544601906, 1-TPR Gap Train: 0.9577857851982117, 1-TPR Gap Test: 0.9601263999938965\n",
      "Epoch 20, Loss: 0.9114627838134766, Final Score Train: 0.6807969212532043, Final Score Test: 0.669647753238678, macro F1 Train: 0.3988563700069408, macro F1 Test: 0.38250176499643374, 1-TPR Gap Train: 0.9627374410629272, 1-TPR Gap Test: 0.9567937850952148\n",
      "Epoch 30, Loss: 0.8214148283004761, Final Score Train: 0.6924355626106262, Final Score Test: 0.6773017048835754, macro F1 Train: 0.41669544805518666, macro F1 Test: 0.396144177853795, 1-TPR Gap Train: 0.9681757092475891, 1-TPR Gap Test: 0.9584592580795288\n",
      "Epoch 40, Loss: 0.8764626383781433, Final Score Train: 0.6979048848152161, Final Score Test: 0.6814781427383423, macro F1 Train: 0.4258304862235777, macro F1 Test: 0.40223285460660757, 1-TPR Gap Train: 0.9699792861938477, 1-TPR Gap Test: 0.9607234001159668\n",
      "Epoch 50, Loss: 0.8594169616699219, Final Score Train: 0.7084728479385376, Final Score Test: 0.6879000067710876, macro F1 Train: 0.44475154190982014, macro F1 Test: 0.41429896589133824, 1-TPR Gap Train: 0.9721941351890564, 1-TPR Gap Test: 0.9615010619163513\n",
      "Epoch 60, Loss: 0.8666553497314453, Final Score Train: 0.7131967544555664, Final Score Test: 0.691228449344635, macro F1 Train: 0.4537761282027168, macro F1 Test: 0.4227944395351141, 1-TPR Gap Train: 0.9726174473762512, 1-TPR Gap Test: 0.9596624374389648\n",
      "Epoch 70, Loss: 0.7222824096679688, Final Score Train: 0.7223688364028931, Final Score Test: 0.6967284679412842, macro F1 Train: 0.4744644066723677, macro F1 Test: 0.4358951361727141, 1-TPR Gap Train: 0.9702732563018799, 1-TPR Gap Test: 0.957561731338501\n",
      "Epoch 80, Loss: 0.8092069029808044, Final Score Train: 0.7307312488555908, Final Score Test: 0.7056361436843872, macro F1 Train: 0.48858226944686295, macro F1 Test: 0.4487619269007032, 1-TPR Gap Train: 0.9728802442550659, 1-TPR Gap Test: 0.9625102877616882\n",
      "Epoch 90, Loss: 0.8232433795928955, Final Score Train: 0.7338746786117554, Final Score Test: 0.7070397734642029, macro F1 Train: 0.4945836698776573, macro F1 Test: 0.44916221166835213, 1-TPR Gap Train: 0.97316575050354, 1-TPR Gap Test: 0.9649173617362976\n",
      "Arrêt précoce après 97 époques\n",
      "Final Evaluation Score: 0.7049031257629395 Macro F1: 0.44966418431103683 1-TPR_gap: 0.9601420164108276\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.7911274433135986, Final Score Train: 0.7382580041885376, Final Score Test: 0.7073420286178589, macro F1 Train: 0.5020104530716542, macro F1 Test: 0.4521434527401254, 1-TPR Gap Train: 0.9745055437088013, 1-TPR Gap Test: 0.9625405669212341\n",
      "Epoch 10, Loss: 0.7746959924697876, Final Score Train: 0.738828718662262, Final Score Test: 0.7083930373191833, macro F1 Train: 0.5044414518638131, macro F1 Test: 0.45318320183150673, 1-TPR Gap Train: 0.9732159972190857, 1-TPR Gap Test: 0.9636028409004211\n",
      "Epoch 20, Loss: 0.8098596334457397, Final Score Train: 0.7426031231880188, Final Score Test: 0.7086015343666077, macro F1 Train: 0.5105844846479582, macro F1 Test: 0.4542171382851494, 1-TPR Gap Train: 0.9746217727661133, 1-TPR Gap Test: 0.9629859328269958\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7090950012207031 Macro F1: 0.457528010050816 1-TPR_gap: 0.9606620073318481\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7408056855201721, Final Score Train: 0.7439122200012207, Final Score Test: 0.7093628644943237, macro F1 Train: 0.5135003067901218, macro F1 Test: 0.4580037848208996, 1-TPR Gap Train: 0.9743241667747498, 1-TPR Gap Test: 0.9607219099998474\n",
      "Epoch 10, Loss: 0.7332335710525513, Final Score Train: 0.7453566789627075, Final Score Test: 0.7107498645782471, macro F1 Train: 0.5157719434499944, macro F1 Test: 0.4595450030187068, 1-TPR Gap Train: 0.9749414920806885, 1-TPR Gap Test: 0.9619547128677368\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7090125679969788 Macro F1: 0.46053091811155195 1-TPR_gap: 0.9574942588806152\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.7309248447418213, Final Score Train: 0.7470274567604065, Final Score Test: 0.7098551988601685, macro F1 Train: 0.5192224067832895, macro F1 Test: 0.460497023452386, 1-TPR Gap Train: 0.9748325347900391, 1-TPR Gap Test: 0.959213376045227\n",
      "Epoch 10, Loss: 0.7267539501190186, Final Score Train: 0.7477626204490662, Final Score Test: 0.7093465328216553, macro F1 Train: 0.5203014812837881, macro F1 Test: 0.4606375776718789, 1-TPR Gap Train: 0.9752237796783447, 1-TPR Gap Test: 0.9580554366111755\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7089338898658752 Macro F1: 0.45999429108842615 1-TPR_gap: 0.957873523235321\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.7134681940078735, Final Score Train: 0.749473512172699, Final Score Test: 0.7099894881248474, macro F1 Train: 0.52313750994726, macro F1 Test: 0.45994539608162166, 1-TPR Gap Train: 0.9758095145225525, 1-TPR Gap Test: 0.9600335359573364\n",
      "Epoch 10, Loss: 0.7210210561752319, Final Score Train: 0.7496885657310486, Final Score Test: 0.7101234197616577, macro F1 Train: 0.5237507535067168, macro F1 Test: 0.46069322740471563, 1-TPR Gap Train: 0.9756263494491577, 1-TPR Gap Test: 0.9595535397529602\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7082864046096802 Macro F1: 0.4592653560179967 1-TPR_gap: 0.9573073983192444\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.725848913192749, Final Score Train: 0.7496387958526611, Final Score Test: 0.7088354825973511, macro F1 Train: 0.5240909392553192, macro F1 Test: 0.4601065964630597, 1-TPR Gap Train: 0.9751865863800049, 1-TPR Gap Test: 0.9575642943382263\n",
      "Epoch 10, Loss: 0.7270679473876953, Final Score Train: 0.7497438192367554, Final Score Test: 0.709250271320343, macro F1 Train: 0.5242189729436513, macro F1 Test: 0.46061331902107877, 1-TPR Gap Train: 0.9752687215805054, 1-TPR Gap Test: 0.9578872323036194\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7089999914169312 Macro F1: 0.4600543156829499 1-TPR_gap: 0.957945704460144\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.7226332426071167, Final Score Train: 0.749747097492218, Final Score Test: 0.710148274898529, macro F1 Train: 0.5241574154355416, macro F1 Test: 0.4607236600461248, 1-TPR Gap Train: 0.9753367900848389, 1-TPR Gap Test: 0.9595728516578674\n",
      "Epoch 10, Loss: 0.735853374004364, Final Score Train: 0.7497580051422119, Final Score Test: 0.709156334400177, macro F1 Train: 0.5243106169663958, macro F1 Test: 0.46008094247462133, 1-TPR Gap Train: 0.9752053618431091, 1-TPR Gap Test: 0.9582316875457764\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7090455293655396 Macro F1: 0.4600814287528957 1-TPR_gap: 0.9580096006393433\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.7536008358001709, Final Score Train: 0.7454742193222046, Final Score Test: 0.7084908485412598, macro F1 Train: 0.5155679390699702, macro F1 Test: 0.4557918284851043, 1-TPR Gap Train: 0.975380539894104, 1-TPR Gap Test: 0.961189866065979\n",
      "Epoch 10, Loss: 0.8335530161857605, Final Score Train: 0.7462158203125, Final Score Test: 0.7075332403182983, macro F1 Train: 0.5176815707338406, macro F1 Test: 0.45503563913479644, 1-TPR Gap Train: 0.9747499823570251, 1-TPR Gap Test: 0.9600307941436768\n",
      "Epoch 20, Loss: 0.7755122184753418, Final Score Train: 0.7480870485305786, Final Score Test: 0.7079979181289673, macro F1 Train: 0.519251698453678, macro F1 Test: 0.453908698233815, 1-TPR Gap Train: 0.9769224524497986, 1-TPR Gap Test: 0.962087094783783\n",
      "Epoch 30, Loss: 0.7652208805084229, Final Score Train: 0.748878538608551, Final Score Test: 0.7061081528663635, macro F1 Train: 0.5206884153478297, macro F1 Test: 0.4535517378071751, 1-TPR Gap Train: 0.9770686626434326, 1-TPR Gap Test: 0.9586645364761353\n",
      "Epoch 40, Loss: 0.8324977159500122, Final Score Train: 0.7485425472259521, Final Score Test: 0.7077415585517883, macro F1 Train: 0.5200469394230852, macro F1 Test: 0.45450383370732556, 1-TPR Gap Train: 0.9770382046699524, 1-TPR Gap Test: 0.9609792828559875\n",
      "Arrêt précoce après 50 époques\n",
      "Final Evaluation Score: 0.7067449688911438 Macro F1: 0.455464976711171 1-TPR_gap: 0.9580249786376953\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.863594651222229, Final Score Train: 0.7520016431808472, Final Score Test: 0.7065126895904541, macro F1 Train: 0.5264485397577452, macro F1 Test: 0.4552255354714953, 1-TPR Gap Train: 0.9775546789169312, 1-TPR Gap Test: 0.9577998518943787\n",
      "Epoch 10, Loss: 0.7981727123260498, Final Score Train: 0.7511641979217529, Final Score Test: 0.7058208584785461, macro F1 Train: 0.5264180428012512, macro F1 Test: 0.45665004098291007, 1-TPR Gap Train: 0.9759103655815125, 1-TPR Gap Test: 0.9549916982650757\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7060471773147583 Macro F1: 0.4530314038547313 1-TPR_gap: 0.9590629935264587\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.7282189130783081, Final Score Train: 0.7535597085952759, Final Score Test: 0.709255039691925, macro F1 Train: 0.5305755485742941, macro F1 Test: 0.45793109722885017, 1-TPR Gap Train: 0.9765439033508301, 1-TPR Gap Test: 0.960578978061676\n",
      "Epoch 10, Loss: 0.7097694873809814, Final Score Train: 0.754702091217041, Final Score Test: 0.7114014625549316, macro F1 Train: 0.5322307875600867, macro F1 Test: 0.46075510936534764, 1-TPR Gap Train: 0.9771734476089478, 1-TPR Gap Test: 0.9620478749275208\n",
      "Epoch 20, Loss: 0.7474105358123779, Final Score Train: 0.7566127777099609, Final Score Test: 0.710094690322876, macro F1 Train: 0.5360424584497417, macro F1 Test: 0.4617445427825874, 1-TPR Gap Train: 0.9771830439567566, 1-TPR Gap Test: 0.9584448337554932\n",
      "Epoch 30, Loss: 0.7740219831466675, Final Score Train: 0.7573230266571045, Final Score Test: 0.7087218165397644, macro F1 Train: 0.5373688544264584, macro F1 Test: 0.4603579977964087, 1-TPR Gap Train: 0.9772771596908569, 1-TPR Gap Test: 0.9570856094360352\n",
      "Arrêt précoce après 39 époques\n",
      "Final Evaluation Score: 0.7102843523025513 Macro F1: 0.4614336379346747 1-TPR_gap: 0.9591349959373474\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.6913158297538757, Final Score Train: 0.7580651044845581, Final Score Test: 0.7097057104110718, macro F1 Train: 0.5386294722632986, macro F1 Test: 0.46146680365613924, 1-TPR Gap Train: 0.9775007963180542, 1-TPR Gap Test: 0.9579446911811829\n",
      "Epoch 10, Loss: 0.7011332511901855, Final Score Train: 0.7584881782531738, Final Score Test: 0.711432695388794, macro F1 Train: 0.539193362807547, macro F1 Test: 0.461323275443565, 1-TPR Gap Train: 0.9777829647064209, 1-TPR Gap Test: 0.9615421295166016\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7094542980194092 Macro F1: 0.4592083764432463 1-TPR_gap: 0.9597002267837524\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7024892568588257, Final Score Train: 0.7584298253059387, Final Score Test: 0.7099186182022095, macro F1 Train: 0.539595781285903, macro F1 Test: 0.4609057502083166, 1-TPR Gap Train: 0.977263867855072, 1-TPR Gap Test: 0.9589315056800842\n",
      "Epoch 10, Loss: 0.7048856019973755, Final Score Train: 0.7585764527320862, Final Score Test: 0.7095624208450317, macro F1 Train: 0.5398607964355658, macro F1 Test: 0.4602749340182472, 1-TPR Gap Train: 0.9772921204566956, 1-TPR Gap Test: 0.9588499069213867\n",
      "Epoch 20, Loss: 0.7079142332077026, Final Score Train: 0.7587563395500183, Final Score Test: 0.7094445824623108, macro F1 Train: 0.5400869784985457, macro F1 Test: 0.46077723889991834, 1-TPR Gap Train: 0.9774256944656372, 1-TPR Gap Test: 0.9581119418144226\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7093308568000793 Macro F1: 0.4601204861526475 1-TPR_gap: 0.958541214466095\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7157978415489197, Final Score Train: 0.7586761713027954, Final Score Test: 0.7092905044555664, macro F1 Train: 0.540027742593519, macro F1 Test: 0.4594792127992609, 1-TPR Gap Train: 0.9773246049880981, 1-TPR Gap Test: 0.9591018557548523\n",
      "Epoch 10, Loss: 0.7177829742431641, Final Score Train: 0.7588290572166443, Final Score Test: 0.7092486619949341, macro F1 Train: 0.5403192318523616, macro F1 Test: 0.4598169884306455, 1-TPR Gap Train: 0.9773389101028442, 1-TPR Gap Test: 0.958680272102356\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7092085480690002 Macro F1: 0.4602608424620423 1-TPR_gap: 0.9581562280654907\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.7183718681335449, Final Score Train: 0.7588307857513428, Final Score Test: 0.7091774940490723, macro F1 Train: 0.5403330077974746, macro F1 Test: 0.4594759674883257, 1-TPR Gap Train: 0.9773285388946533, 1-TPR Gap Test: 0.9588789939880371\n",
      "Epoch 10, Loss: 0.7217234969139099, Final Score Train: 0.7588618397712708, Final Score Test: 0.7091424465179443, macro F1 Train: 0.5404128539197179, macro F1 Test: 0.45932988401668473, 1-TPR Gap Train: 0.977310836315155, 1-TPR Gap Test: 0.9589549899101257\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7091684341430664 Macro F1: 0.4599230372890989 1-TPR_gap: 0.9584138989448547\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.8405330181121826, Final Score Train: 0.7573999166488647, Final Score Test: 0.7084987163543701, macro F1 Train: 0.5375986360718951, macro F1 Test: 0.4581354007080577, 1-TPR Gap Train: 0.9772012829780579, 1-TPR Gap Test: 0.9588620662689209\n",
      "Epoch 10, Loss: 0.7350935935974121, Final Score Train: 0.756066083908081, Final Score Test: 0.7086451053619385, macro F1 Train: 0.5348154599916184, macro F1 Test: 0.45773932351028873, 1-TPR Gap Train: 0.9773167371749878, 1-TPR Gap Test: 0.9595509171485901\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7100188732147217 Macro F1: 0.45528737792859353 1-TPR_gap: 0.964750349521637\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.7835330963134766, Final Score Train: 0.7568469047546387, Final Score Test: 0.7093720436096191, macro F1 Train: 0.5363444479579906, macro F1 Test: 0.4581116532743889, 1-TPR Gap Train: 0.9773493409156799, 1-TPR Gap Test: 0.9606323838233948\n",
      "Epoch 10, Loss: 0.7683373689651489, Final Score Train: 0.7580597400665283, Final Score Test: 0.7095588445663452, macro F1 Train: 0.5386947646122987, macro F1 Test: 0.4591879140474305, 1-TPR Gap Train: 0.977424681186676, 1-TPR Gap Test: 0.9599297046661377\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7091858386993408 Macro F1: 0.4583610272157582 1-TPR_gap: 0.9600105881690979\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7581014037132263, Final Score Train: 0.7592066526412964, Final Score Test: 0.710861086845398, macro F1 Train: 0.5403903231656277, macro F1 Test: 0.46069187518813953, 1-TPR Gap Train: 0.9780229926109314, 1-TPR Gap Test: 0.9610303044319153\n",
      "Epoch 10, Loss: 0.7543653249740601, Final Score Train: 0.7600585222244263, Final Score Test: 0.710330605506897, macro F1 Train: 0.5423742238941661, macro F1 Test: 0.46024893879597534, 1-TPR Gap Train: 0.9777427315711975, 1-TPR Gap Test: 0.9604122042655945\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7095170021057129 Macro F1: 0.45907912271877466 1-TPR_gap: 0.9599548578262329\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.6577394008636475, Final Score Train: 0.7597298622131348, Final Score Test: 0.7085745930671692, macro F1 Train: 0.5421899529717562, macro F1 Test: 0.4597563959941823, 1-TPR Gap Train: 0.97726970911026, 1-TPR Gap Test: 0.9573928117752075\n",
      "Epoch 10, Loss: 0.688769519329071, Final Score Train: 0.7604321241378784, Final Score Test: 0.7098867297172546, macro F1 Train: 0.5431945038367175, macro F1 Test: 0.46064554969576804, 1-TPR Gap Train: 0.9776697158813477, 1-TPR Gap Test: 0.9591279029846191\n",
      "Epoch 20, Loss: 0.7025096416473389, Final Score Train: 0.7608016729354858, Final Score Test: 0.7084749937057495, macro F1 Train: 0.5441511424549899, macro F1 Test: 0.4608182841746145, 1-TPR Gap Train: 0.9774521589279175, 1-TPR Gap Test: 0.9561316967010498\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.7110536098480225 Macro F1: 0.46159023564697704 1-TPR_gap: 0.9605169892311096\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.7082992196083069, Final Score Train: 0.7609051465988159, Final Score Test: 0.7106800675392151, macro F1 Train: 0.5443883826142377, macro F1 Test: 0.4617851678937316, 1-TPR Gap Train: 0.9774219393730164, 1-TPR Gap Test: 0.9595749974250793\n",
      "Epoch 10, Loss: 0.7059975862503052, Final Score Train: 0.7609785795211792, Final Score Test: 0.7091915607452393, macro F1 Train: 0.5445458294768846, macro F1 Test: 0.4598364400789525, 1-TPR Gap Train: 0.9774113297462463, 1-TPR Gap Test: 0.9585466384887695\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7101156711578369 Macro F1: 0.4601852781137402 1-TPR_gap: 0.9600460529327393\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7179244160652161, Final Score Train: 0.7609467506408691, Final Score Test: 0.7092313766479492, macro F1 Train: 0.5446190939638264, macro F1 Test: 0.45975337192796356, 1-TPR Gap Train: 0.9772744178771973, 1-TPR Gap Test: 0.9587094187736511\n",
      "Epoch 10, Loss: 0.717235803604126, Final Score Train: 0.7610703110694885, Final Score Test: 0.7114853858947754, macro F1 Train: 0.5446192605945216, macro F1 Test: 0.4608125039806598, 1-TPR Gap Train: 0.9775213599205017, 1-TPR Gap Test: 0.962158203125\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7100774049758911 Macro F1: 0.46010881820818017 1-TPR_gap: 0.9600460529327393\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.7247369289398193, Final Score Train: 0.7610292434692383, Final Score Test: 0.7100774049758911, macro F1 Train: 0.5446849613051965, macro F1 Test: 0.46010881820818017, 1-TPR Gap Train: 0.977373480796814, 1-TPR Gap Test: 0.9600460529327393\n",
      "Epoch 10, Loss: 0.7071633338928223, Final Score Train: 0.7610260248184204, Final Score Test: 0.7109176516532898, macro F1 Train: 0.5446632009844674, macro F1 Test: 0.46048799109601696, 1-TPR Gap Train: 0.9773887991905212, 1-TPR Gap Test: 0.9613473415374756\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7098861932754517 Macro F1: 0.4598939301675549 1-TPR_gap: 0.9598783850669861\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.798873245716095, Final Score Train: 0.7593948841094971, Final Score Test: 0.7110615968704224, macro F1 Train: 0.5415819216816676, macro F1 Test: 0.4586405546312889, 1-TPR Gap Train: 0.9772077798843384, 1-TPR Gap Test: 0.9634826183319092\n",
      "Epoch 10, Loss: 0.8520179986953735, Final Score Train: 0.7580857276916504, Final Score Test: 0.7072775363922119, macro F1 Train: 0.5391387052315082, macro F1 Test: 0.4556724404419977, 1-TPR Gap Train: 0.9770327210426331, 1-TPR Gap Test: 0.9588826894760132\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7102058529853821 Macro F1: 0.45851411911717216 1-TPR_gap: 0.961897611618042\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.8354414701461792, Final Score Train: 0.7593805193901062, Final Score Test: 0.7122411131858826, macro F1 Train: 0.541539061837841, macro F1 Test: 0.46125682481431535, 1-TPR Gap Train: 0.9772219657897949, 1-TPR Gap Test: 0.9632253646850586\n",
      "Epoch 10, Loss: 0.8372490406036377, Final Score Train: 0.7602297067642212, Final Score Test: 0.7127934098243713, macro F1 Train: 0.5428475755181464, macro F1 Test: 0.46173662093971723, 1-TPR Gap Train: 0.977611780166626, 1-TPR Gap Test: 0.963850200176239\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7113609910011292 Macro F1: 0.4597499655137097 1-TPR_gap: 0.9629720449447632\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.6979804635047913, Final Score Train: 0.7610146403312683, Final Score Test: 0.7117316126823425, macro F1 Train: 0.544430583320849, macro F1 Test: 0.46083328886330455, 1-TPR Gap Train: 0.9775987267494202, 1-TPR Gap Test: 0.9626299142837524\n",
      "Epoch 10, Loss: 0.6655058860778809, Final Score Train: 0.761457085609436, Final Score Test: 0.7078415155410767, macro F1 Train: 0.5454349937341715, macro F1 Test: 0.4603218339114073, 1-TPR Gap Train: 0.9774792194366455, 1-TPR Gap Test: 0.9553611278533936\n",
      "Epoch 20, Loss: 0.7069857120513916, Final Score Train: 0.7617799639701843, Final Score Test: 0.7105292081832886, macro F1 Train: 0.5462567675232399, macro F1 Test: 0.4610149905168757, 1-TPR Gap Train: 0.977303147315979, 1-TPR Gap Test: 0.9600434303283691\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.708905816078186 Macro F1: 0.458985633267504 1-TPR_gap: 0.9588260650634766\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.6510841250419617, Final Score Train: 0.761926531791687, Final Score Test: 0.7104769945144653, macro F1 Train: 0.5464275836147533, macro F1 Test: 0.46015850780263745, 1-TPR Gap Train: 0.9774255156517029, 1-TPR Gap Test: 0.960795521736145\n",
      "Epoch 10, Loss: 0.6783273220062256, Final Score Train: 0.7621688842773438, Final Score Test: 0.7099651098251343, macro F1 Train: 0.547038770357377, macro F1 Test: 0.4613323406129589, 1-TPR Gap Train: 0.9772990345954895, 1-TPR Gap Test: 0.9585978984832764\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7096270322799683 Macro F1: 0.4604342331085949 1-TPR_gap: 0.9588198065757751\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7225661277770996, Final Score Train: 0.7621113061904907, Final Score Test: 0.7097169160842896, macro F1 Train: 0.5468589179880361, macro F1 Test: 0.4603059344061397, 1-TPR Gap Train: 0.9773637652397156, 1-TPR Gap Test: 0.9591279029846191\n",
      "Epoch 10, Loss: 0.7005184888839722, Final Score Train: 0.7622830867767334, Final Score Test: 0.7107800841331482, macro F1 Train: 0.5472659751728459, macro F1 Test: 0.4604934020007803, 1-TPR Gap Train: 0.977300226688385, 1-TPR Gap Test: 0.9610667824745178\n",
      "Epoch 20, Loss: 0.6950759887695312, Final Score Train: 0.7622383832931519, Final Score Test: 0.7101292014122009, macro F1 Train: 0.5471544258316415, macro F1 Test: 0.460382078224755, 1-TPR Gap Train: 0.9773222804069519, 1-TPR Gap Test: 0.959876298904419\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7102075815200806 Macro F1: 0.46059550057541315 1-TPR_gap: 0.9598197340965271\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7086965441703796, Final Score Train: 0.7623239755630493, Final Score Test: 0.709841251373291, macro F1 Train: 0.5472310804466839, macro F1 Test: 0.4607746601986532, 1-TPR Gap Train: 0.9774168729782104, 1-TPR Gap Test: 0.9589079022407532\n",
      "Epoch 10, Loss: 0.7057703137397766, Final Score Train: 0.7624062299728394, Final Score Test: 0.7094503045082092, macro F1 Train: 0.5474442996626773, macro F1 Test: 0.45972862520439667, 1-TPR Gap Train: 0.977368175983429, 1-TPR Gap Test: 0.9591720104217529\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7094452977180481 Macro F1: 0.4595239421321395 1-TPR_gap: 0.9593666791915894\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7109062671661377, Final Score Train: 0.7624504566192627, Final Score Test: 0.7097555994987488, macro F1 Train: 0.5474858678709825, macro F1 Test: 0.4598775373438108, 1-TPR Gap Train: 0.9774149656295776, 1-TPR Gap Test: 0.9596336483955383\n",
      "Epoch 10, Loss: 0.7072182893753052, Final Score Train: 0.7623999714851379, Final Score Test: 0.710192859172821, macro F1 Train: 0.5474555620714396, macro F1 Test: 0.4602301029244436, 1-TPR Gap Train: 0.9773443937301636, 1-TPR Gap Test: 0.9601556062698364\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7101786732673645 Macro F1: 0.46001139765553317 1-TPR_gap: 0.9603459239006042\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.7443463802337646, Final Score Train: 0.7616525292396545, Final Score Test: 0.7129011154174805, macro F1 Train: 0.5460889683361464, macro F1 Test: 0.4604269452905432, 1-TPR Gap Train: 0.9772160649299622, 1-TPR Gap Test: 0.9653752446174622\n",
      "Epoch 10, Loss: 0.8152541518211365, Final Score Train: 0.7605737447738647, Final Score Test: 0.7115660905838013, macro F1 Train: 0.5439159952443534, macro F1 Test: 0.4593071973060167, 1-TPR Gap Train: 0.9772314429283142, 1-TPR Gap Test: 0.9638250470161438\n",
      "Epoch 20, Loss: 0.7657618522644043, Final Score Train: 0.7603881359100342, Final Score Test: 0.7116849422454834, macro F1 Train: 0.543628548850275, macro F1 Test: 0.4600922064550354, 1-TPR Gap Train: 0.9771477580070496, 1-TPR Gap Test: 0.9632776975631714\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7087172865867615 Macro F1: 0.45756451590591063 1-TPR_gap: 0.9598701000213623\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.7903434038162231, Final Score Train: 0.7611033320426941, Final Score Test: 0.7090986371040344, macro F1 Train: 0.5447097645717741, macro F1 Test: 0.4584985152011122, 1-TPR Gap Train: 0.9774969220161438, 1-TPR Gap Test: 0.9596987962722778\n",
      "Epoch 10, Loss: 0.742424726486206, Final Score Train: 0.7616024613380432, Final Score Test: 0.7083020210266113, macro F1 Train: 0.5459143126666121, macro F1 Test: 0.4597656730338536, 1-TPR Gap Train: 0.9772906303405762, 1-TPR Gap Test: 0.9568384289741516\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7114012837409973 Macro F1: 0.4603681391355052 1-TPR_gap: 0.9624344110488892\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.7346068620681763, Final Score Train: 0.7619916200637817, Final Score Test: 0.7107036113739014, macro F1 Train: 0.5468125796664288, macro F1 Test: 0.46104300043676494, 1-TPR Gap Train: 0.9771706461906433, 1-TPR Gap Test: 0.9603642225265503\n",
      "Epoch 10, Loss: 0.6940925717353821, Final Score Train: 0.7627665996551514, Final Score Test: 0.7096716165542603, macro F1 Train: 0.5482618764212519, macro F1 Test: 0.46010954198343634, 1-TPR Gap Train: 0.9772713780403137, 1-TPR Gap Test: 0.9592336416244507\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7087025046348572 Macro F1: 0.45922686899182735 1-TPR_gap: 0.9581781625747681\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.640624463558197, Final Score Train: 0.7628875970840454, Final Score Test: 0.7096081972122192, macro F1 Train: 0.548532893205732, macro F1 Test: 0.46011318409884877, 1-TPR Gap Train: 0.9772423505783081, 1-TPR Gap Test: 0.9591031670570374\n",
      "Epoch 10, Loss: 0.6950331926345825, Final Score Train: 0.763035774230957, Final Score Test: 0.7096468210220337, macro F1 Train: 0.5489255456099161, macro F1 Test: 0.4606953914594873, 1-TPR Gap Train: 0.9771460294723511, 1-TPR Gap Test: 0.9585983157157898\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7090213894844055 Macro F1: 0.46044019577991085 1-TPR_gap: 0.9576025605201721\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.6921062469482422, Final Score Train: 0.7633757591247559, Final Score Test: 0.709621012210846, macro F1 Train: 0.5493870106256511, macro F1 Test: 0.46062374294674574, 1-TPR Gap Train: 0.9773645401000977, 1-TPR Gap Test: 0.9586182832717896\n",
      "Epoch 10, Loss: 0.7029499411582947, Final Score Train: 0.7634192705154419, Final Score Test: 0.7096384167671204, macro F1 Train: 0.5495304342765482, macro F1 Test: 0.4607582136936177, 1-TPR Gap Train: 0.9773080348968506, 1-TPR Gap Test: 0.9585186243057251\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7095921039581299 Macro F1: 0.4605601695787818 1-TPR_gap: 0.958624005317688\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7132301330566406, Final Score Train: 0.7634007930755615, Final Score Test: 0.709659993648529, macro F1 Train: 0.5494502908368303, macro F1 Test: 0.4607316471841693, 1-TPR Gap Train: 0.9773513078689575, 1-TPR Gap Test: 0.9585883617401123\n",
      "Epoch 10, Loss: 0.7084242701530457, Final Score Train: 0.7634048461914062, Final Score Test: 0.7095181345939636, macro F1 Train: 0.5495046824386149, macro F1 Test: 0.46064081457620387, 1-TPR Gap Train: 0.9773050546646118, 1-TPR Gap Test: 0.9583954811096191\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7095022797584534 Macro F1: 0.4606091105157713 1-TPR_gap: 0.9583954811096191\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7002461552619934, Final Score Train: 0.763451099395752, Final Score Test: 0.7095173001289368, macro F1 Train: 0.5496800770998649, macro F1 Test: 0.46063908060594894, 1-TPR Gap Train: 0.9772221446037292, 1-TPR Gap Test: 0.9583954811096191\n",
      "Epoch 10, Loss: 0.7125818729400635, Final Score Train: 0.7634197473526001, Final Score Test: 0.7095779776573181, macro F1 Train: 0.549573223573056, macro F1 Test: 0.46023543438913334, 1-TPR Gap Train: 0.9772661924362183, 1-TPR Gap Test: 0.9589205384254456\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7095723152160645 Macro F1: 0.46027971904840476 1-TPR_gap: 0.9588649272918701\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.8133668899536133, Final Score Train: 0.762692928314209, Final Score Test: 0.7111731767654419, macro F1 Train: 0.5479733143741823, macro F1 Test: 0.4618650085370866, 1-TPR Gap Train: 0.9774125814437866, 1-TPR Gap Test: 0.9604814052581787\n",
      "Epoch 10, Loss: 0.8060895204544067, Final Score Train: 0.7612121105194092, Final Score Test: 0.710096538066864, macro F1 Train: 0.5453760192436636, macro F1 Test: 0.457822854764847, 1-TPR Gap Train: 0.9770481586456299, 1-TPR Gap Test: 0.9623702168464661\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7121022343635559 Macro F1: 0.45960984102583646 1-TPR_gap: 0.9645946025848389\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.762863278388977, Final Score Train: 0.7624403238296509, Final Score Test: 0.7101004123687744, macro F1 Train: 0.5479387662617119, macro F1 Test: 0.45933388896021726, 1-TPR Gap Train: 0.9769418835639954, 1-TPR Gap Test: 0.9608668684959412\n",
      "Epoch 10, Loss: 0.7035292387008667, Final Score Train: 0.7628045082092285, Final Score Test: 0.7099311351776123, macro F1 Train: 0.5487473381059038, macro F1 Test: 0.46018536307353536, 1-TPR Gap Train: 0.9768615961074829, 1-TPR Gap Test: 0.95967698097229\n",
      "Epoch 20, Loss: 0.7435394525527954, Final Score Train: 0.7626292705535889, Final Score Test: 0.7102665305137634, macro F1 Train: 0.5483813396926454, macro F1 Test: 0.45894508797749756, 1-TPR Gap Train: 0.9768772721290588, 1-TPR Gap Test: 0.9615879654884338\n",
      "Epoch 30, Loss: 0.8731775283813477, Final Score Train: 0.7627731561660767, Final Score Test: 0.7091545462608337, macro F1 Train: 0.5488670368113672, macro F1 Test: 0.4581150203707795, 1-TPR Gap Train: 0.9766792058944702, 1-TPR Gap Test: 0.9601941108703613\n",
      "Epoch 40, Loss: 0.8390703201293945, Final Score Train: 0.763037383556366, Final Score Test: 0.7085319757461548, macro F1 Train: 0.5493607751901471, macro F1 Test: 0.4590389421502653, 1-TPR Gap Train: 0.976714015007019, 1-TPR Gap Test: 0.9580250382423401\n",
      "Arrêt précoce après 47 époques\n",
      "Final Evaluation Score: 0.7101057171821594 Macro F1: 0.46012473521901504 1-TPR_gap: 0.9600867033004761\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.6645087599754333, Final Score Train: 0.7634955048561096, Final Score Test: 0.7090973854064941, macro F1 Train: 0.550219089250669, macro F1 Test: 0.459655928143201, 1-TPR Gap Train: 0.976771891117096, 1-TPR Gap Test: 0.9585387706756592\n",
      "Epoch 10, Loss: 0.683075487613678, Final Score Train: 0.7635930180549622, Final Score Test: 0.7094554901123047, macro F1 Train: 0.5504492602862608, macro F1 Test: 0.45934099146974855, 1-TPR Gap Train: 0.9767367839813232, 1-TPR Gap Test: 0.959570050239563\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7093436121940613 Macro F1: 0.45995406157777596 1-TPR_gap: 0.9587332010269165\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.6924717426300049, Final Score Train: 0.7639015316963196, Final Score Test: 0.7104686498641968, macro F1 Train: 0.550968509498211, macro F1 Test: 0.46086866735948745, 1-TPR Gap Train: 0.9768345355987549, 1-TPR Gap Test: 0.9600687026977539\n",
      "Epoch 10, Loss: 0.6955791711807251, Final Score Train: 0.7638157606124878, Final Score Test: 0.7099891901016235, macro F1 Train: 0.550902728063046, macro F1 Test: 0.4600927532235227, 1-TPR Gap Train: 0.9767287969589233, 1-TPR Gap Test: 0.9598855972290039\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7111445069313049 Macro F1: 0.4616988585896194 1-TPR_gap: 0.960590124130249\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.6939222812652588, Final Score Train: 0.764107882976532, Final Score Test: 0.7107213735580444, macro F1 Train: 0.5514572843650574, macro F1 Test: 0.4616201599451629, 1-TPR Gap Train: 0.9767584800720215, 1-TPR Gap Test: 0.9598226547241211\n",
      "Epoch 10, Loss: 0.6817401647567749, Final Score Train: 0.7640841007232666, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5513890966674093, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.960029125213623\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7108665704727173 Macro F1: 0.46180539127692705 1-TPR_gap: 0.9599277973175049\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7110913991928101, Final Score Train: 0.7640784978866577, Final Score Test: 0.7109078764915466, macro F1 Train: 0.5513779038338418, macro F1 Test: 0.46207518904343486, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.9597405195236206\n",
      "Epoch 10, Loss: 0.7071220278739929, Final Score Train: 0.764079213142395, Final Score Test: 0.7109007835388184, macro F1 Train: 0.5513793273180126, macro F1 Test: 0.46181119509915375, 1-TPR Gap Train: 0.9767791628837585, 1-TPR Gap Test: 0.959990382194519\n",
      "Epoch 20, Loss: 0.710746169090271, Final Score Train: 0.7641359567642212, Final Score Test: 0.7111122012138367, macro F1 Train: 0.5514868270764436, macro F1 Test: 0.46220088905294393, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.9600235223770142\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7110104560852051 Macro F1: 0.4622155891026907 1-TPR_gap: 0.9598053097724915\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7137851715087891, Final Score Train: 0.7641341686248779, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5514831655218142, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.960029125213623\n",
      "Epoch 10, Loss: 0.7124017477035522, Final Score Train: 0.7641377449035645, Final Score Test: 0.7110580801963806, macro F1 Train: 0.5514904394818089, macro F1 Test: 0.46211200404219577, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.9600041508674622\n",
      "Epoch 20, Loss: 0.6988430619239807, Final Score Train: 0.7641359567642212, Final Score Test: 0.7110110521316528, macro F1 Train: 0.5514867867771821, macro F1 Test: 0.46199298024763785, 1-TPR Gap Train: 0.9767851233482361, 1-TPR Gap Test: 0.960029125213623\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7109786868095398 Macro F1: 0.46179094131287146 1-TPR_gap: 0.9601663947105408\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.7776163816452026, Final Score Train: 0.7633556723594666, Final Score Test: 0.7104685306549072, macro F1 Train: 0.5501396041649859, macro F1 Test: 0.45950210384097406, 1-TPR Gap Train: 0.9765717387199402, 1-TPR Gap Test: 0.9614350199699402\n",
      "Epoch 10, Loss: 0.7658340930938721, Final Score Train: 0.7635006904602051, Final Score Test: 0.7085611820220947, macro F1 Train: 0.549599788799594, macro F1 Test: 0.4585115725505911, 1-TPR Gap Train: 0.9774015545845032, 1-TPR Gap Test: 0.9586108326911926\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7089000940322876 Macro F1: 0.45893181867004745 1-TPR_gap: 0.9588683247566223\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.8345272541046143, Final Score Train: 0.7635828852653503, Final Score Test: 0.709423303604126, macro F1 Train: 0.5504469886680162, macro F1 Test: 0.4586634305395264, 1-TPR Gap Train: 0.9767187833786011, 1-TPR Gap Test: 0.9601831436157227\n",
      "Epoch 10, Loss: 0.8279632329940796, Final Score Train: 0.7634963989257812, Final Score Test: 0.7088677287101746, macro F1 Train: 0.5504253156974703, macro F1 Test: 0.4588433818123828, 1-TPR Gap Train: 0.9765674471855164, 1-TPR Gap Test: 0.9588921070098877\n",
      "Epoch 20, Loss: 0.7264643907546997, Final Score Train: 0.7633972764015198, Final Score Test: 0.7102397680282593, macro F1 Train: 0.5502467109916475, macro F1 Test: 0.4600018539998801, 1-TPR Gap Train: 0.9765478372573853, 1-TPR Gap Test: 0.9604776501655579\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7093523740768433 Macro F1: 0.46007485043086455 1-TPR_gap: 0.958629846572876\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.6601825952529907, Final Score Train: 0.7643130421638489, Final Score Test: 0.7088170051574707, macro F1 Train: 0.5518634679144628, macro F1 Test: 0.4588515129671772, 1-TPR Gap Train: 0.976762592792511, 1-TPR Gap Test: 0.958782434463501\n",
      "Epoch 10, Loss: 0.7434941530227661, Final Score Train: 0.7644243836402893, Final Score Test: 0.7103109359741211, macro F1 Train: 0.5521716838775547, macro F1 Test: 0.460559759065226, 1-TPR Gap Train: 0.9766770601272583, 1-TPR Gap Test: 0.9600620865821838\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7101020812988281 Macro F1: 0.4600107215389255 1-TPR_gap: 0.9601935148239136\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.6879298686981201, Final Score Train: 0.7646681666374207, Final Score Test: 0.7095276117324829, macro F1 Train: 0.5526527298896375, macro F1 Test: 0.4604570723379298, 1-TPR Gap Train: 0.9766836166381836, 1-TPR Gap Test: 0.9585981369018555\n",
      "Epoch 10, Loss: 0.6648935675621033, Final Score Train: 0.7647291421890259, Final Score Test: 0.7095547318458557, macro F1 Train: 0.5527792081488145, macro F1 Test: 0.46069558101684865, 1-TPR Gap Train: 0.9766790866851807, 1-TPR Gap Test: 0.95841383934021\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7092775702476501 Macro F1: 0.46059442848356974 1-TPR_gap: 0.9579607248306274\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7125214338302612, Final Score Train: 0.7648409605026245, Final Score Test: 0.7092995047569275, macro F1 Train: 0.5529948048887907, macro F1 Test: 0.46092692151641673, 1-TPR Gap Train: 0.9766870737075806, 1-TPR Gap Test: 0.957672119140625\n",
      "Epoch 10, Loss: 0.6950925588607788, Final Score Train: 0.7648287415504456, Final Score Test: 0.7093514204025269, macro F1 Train: 0.5530024332993253, macro F1 Test: 0.46013715375160436, 1-TPR Gap Train: 0.9766550660133362, 1-TPR Gap Test: 0.9585656523704529\n",
      "Epoch 20, Loss: 0.6926829218864441, Final Score Train: 0.764888346195221, Final Score Test: 0.7089998126029968, macro F1 Train: 0.5531058839912023, macro F1 Test: 0.46022382619067326, 1-TPR Gap Train: 0.9766708016395569, 1-TPR Gap Test: 0.9577757716178894\n",
      "Arrêt précoce après 30 époques\n",
      "Final Evaluation Score: 0.7093287706375122 Macro F1: 0.46008627396583907 1-TPR_gap: 0.958571195602417\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.705488920211792, Final Score Train: 0.7649445533752441, Final Score Test: 0.709405779838562, macro F1 Train: 0.5532124799140652, macro F1 Test: 0.4603668952303427, 1-TPR Gap Train: 0.9766767024993896, 1-TPR Gap Test: 0.9584447145462036\n",
      "Epoch 10, Loss: 0.7147866487503052, Final Score Train: 0.7649369239807129, Final Score Test: 0.7093387842178345, macro F1 Train: 0.553206910845016, macro F1 Test: 0.4602315604465031, 1-TPR Gap Train: 0.9766669273376465, 1-TPR Gap Test: 0.9584459662437439\n",
      "Epoch 20, Loss: 0.7144156694412231, Final Score Train: 0.7649496793746948, Final Score Test: 0.7092416286468506, macro F1 Train: 0.5532170903186933, macro F1 Test: 0.4599889491252855, 1-TPR Gap Train: 0.976682186126709, 1-TPR Gap Test: 0.9584943652153015\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7094192504882812 Macro F1: 0.4602608895360185 1-TPR_gap: 0.9585776329040527\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7086201310157776, Final Score Train: 0.7649757862091064, Final Score Test: 0.7092170119285583, macro F1 Train: 0.5532881302994975, macro F1 Test: 0.4600690471282715, 1-TPR Gap Train: 0.9766634702682495, 1-TPR Gap Test: 0.9583649635314941\n",
      "Epoch 10, Loss: 0.7135108709335327, Final Score Train: 0.7650065422058105, Final Score Test: 0.7092220783233643, macro F1 Train: 0.553330932841278, macro F1 Test: 0.46007919414374826, 1-TPR Gap Train: 0.976682186126709, 1-TPR Gap Test: 0.9583649635314941\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7093677520751953 Macro F1: 0.46016989082456156 1-TPR_gap: 0.9585656523704529\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.6973056197166443, Final Score Train: 0.7642636299133301, Final Score Test: 0.7101004123687744, macro F1 Train: 0.5519317804831931, macro F1 Test: 0.4588491990135021, 1-TPR Gap Train: 0.976595401763916, 1-TPR Gap Test: 0.9613516330718994\n",
      "Epoch 10, Loss: 0.7781487703323364, Final Score Train: 0.7638533115386963, Final Score Test: 0.7108557820320129, macro F1 Train: 0.5509951688719331, macro F1 Test: 0.4595456120665099, 1-TPR Gap Train: 0.9767114520072937, 1-TPR Gap Test: 0.9621659517288208\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7096161842346191 Macro F1: 0.4590424950519566 1-TPR_gap: 0.9601898193359375\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.7940989136695862, Final Score Train: 0.7643692493438721, Final Score Test: 0.7105710506439209, macro F1 Train: 0.5520933852122684, macro F1 Test: 0.4603951212894911, 1-TPR Gap Train: 0.9766450524330139, 1-TPR Gap Test: 0.9607469439506531\n",
      "Epoch 10, Loss: 0.6406720280647278, Final Score Train: 0.7647448182106018, Final Score Test: 0.7097187042236328, macro F1 Train: 0.5526894520809422, macro F1 Test: 0.4605680589267795, 1-TPR Gap Train: 0.9768002033233643, 1-TPR Gap Test: 0.9588693976402283\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7094919681549072 Macro F1: 0.4595975753579293 1-TPR_gap: 0.95938640832901\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7107357382774353, Final Score Train: 0.7649226188659668, Final Score Test: 0.7086682319641113, macro F1 Train: 0.5530850982365759, macro F1 Test: 0.46038413356864344, 1-TPR Gap Train: 0.9767600893974304, 1-TPR Gap Test: 0.9569522738456726\n",
      "Epoch 10, Loss: 0.6850690841674805, Final Score Train: 0.7651122808456421, Final Score Test: 0.7089362740516663, macro F1 Train: 0.5535092352796901, macro F1 Test: 0.45906339082819725, 1-TPR Gap Train: 0.9767153859138489, 1-TPR Gap Test: 0.9588091969490051\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7098397016525269 Macro F1: 0.46099179689304304 1-TPR_gap: 0.9586876630783081\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.6999490261077881, Final Score Train: 0.7652946710586548, Final Score Test: 0.7092453241348267, macro F1 Train: 0.5538315309510071, macro F1 Test: 0.46026046863845105, 1-TPR Gap Train: 0.976757824420929, 1-TPR Gap Test: 0.9582301378250122\n",
      "Epoch 10, Loss: 0.7011361122131348, Final Score Train: 0.7654101848602295, Final Score Test: 0.7086979150772095, macro F1 Train: 0.5539076731021997, macro F1 Test: 0.4596245430161866, 1-TPR Gap Train: 0.9769127368927002, 1-TPR Gap Test: 0.9577713012695312\n",
      "Epoch 20, Loss: 0.6451625823974609, Final Score Train: 0.7655270099639893, Final Score Test: 0.7085428237915039, macro F1 Train: 0.5541364303113261, macro F1 Test: 0.4596006052895527, 1-TPR Gap Train: 0.9769176244735718, 1-TPR Gap Test: 0.9574850797653198\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7091134190559387 Macro F1: 0.4610011925444239 1-TPR_gap: 0.9572256803512573\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.710567831993103, Final Score Train: 0.7656173706054688, Final Score Test: 0.709415078163147, macro F1 Train: 0.55432490307293, macro F1 Test: 0.4608784168671605, 1-TPR Gap Train: 0.976909875869751, 1-TPR Gap Test: 0.9579517841339111\n",
      "Epoch 10, Loss: 0.6960428357124329, Final Score Train: 0.765684962272644, Final Score Test: 0.7095619440078735, macro F1 Train: 0.554479590446613, macro F1 Test: 0.4609763982771154, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581474661827087\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7089600563049316 Macro F1: 0.4601201389653577 1-TPR_gap: 0.9577999114990234\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.7062394618988037, Final Score Train: 0.7656655311584473, Final Score Test: 0.7095123529434204, macro F1 Train: 0.5544359127958653, macro F1 Test: 0.46091695257480286, 1-TPR Gap Train: 0.9768951535224915, 1-TPR Gap Test: 0.9581077098846436\n",
      "Epoch 10, Loss: 0.7112011909484863, Final Score Train: 0.7656747102737427, Final Score Test: 0.7095186710357666, macro F1 Train: 0.5544541936856288, macro F1 Test: 0.4609296985682538, 1-TPR Gap Train: 0.9768951535224915, 1-TPR Gap Test: 0.9581077098846436\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7094597220420837 Macro F1: 0.46070286747563005 1-TPR_gap: 0.9582166075706482\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.706257164478302, Final Score Train: 0.7657082676887512, Final Score Test: 0.7095663547515869, macro F1 Train: 0.5545262678703194, macro F1 Test: 0.461030566309837, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581021666526794\n",
      "Epoch 10, Loss: 0.7101259231567383, Final Score Train: 0.7657210826873779, Final Score Test: 0.7095106840133667, macro F1 Train: 0.5545519313545421, macro F1 Test: 0.4609136075283989, 1-TPR Gap Train: 0.9768902659416199, 1-TPR Gap Test: 0.9581077098846436\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7095573544502258 Macro F1: 0.4610069746112832 1-TPR_gap: 0.9581077098846436\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.7987650632858276, Final Score Train: 0.7652987241744995, Final Score Test: 0.7097739577293396, macro F1 Train: 0.5537053319376785, macro F1 Test: 0.4605070071620018, 1-TPR Gap Train: 0.9768921136856079, 1-TPR Gap Test: 0.9590408802032471\n",
      "Epoch 10, Loss: 0.7860246896743774, Final Score Train: 0.7652720212936401, Final Score Test: 0.7094895839691162, macro F1 Train: 0.5537068233881272, macro F1 Test: 0.45966078102056945, 1-TPR Gap Train: 0.9768372178077698, 1-TPR Gap Test: 0.9593183398246765\n",
      "Epoch 20, Loss: 0.8937048316001892, Final Score Train: 0.7650575637817383, Final Score Test: 0.7089897394180298, macro F1 Train: 0.5532666847184441, macro F1 Test: 0.45839505526171537, 1-TPR Gap Train: 0.9768484234809875, 1-TPR Gap Test: 0.9595844149589539\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7094175219535828 Macro F1: 0.4574686968962024 1-TPR_gap: 0.9613663554191589\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.8632200956344604, Final Score Train: 0.7652406692504883, Final Score Test: 0.7094039916992188, macro F1 Train: 0.5536155346973458, macro F1 Test: 0.45824896350793204, 1-TPR Gap Train: 0.9768658876419067, 1-TPR Gap Test: 0.9605590105056763\n",
      "Epoch 10, Loss: 0.8169986009597778, Final Score Train: 0.7654800415039062, Final Score Test: 0.709585964679718, macro F1 Train: 0.5541720927734595, macro F1 Test: 0.4589943525496968, 1-TPR Gap Train: 0.9767879843711853, 1-TPR Gap Test: 0.9601775407791138\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7089213132858276 Macro F1: 0.45917087913236576 1-TPR_gap: 0.9586716890335083\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.715619683265686, Final Score Train: 0.7656483054161072, Final Score Test: 0.710224986076355, macro F1 Train: 0.5544584790614379, macro F1 Test: 0.4597757551085147, 1-TPR Gap Train: 0.9768381118774414, 1-TPR Gap Test: 0.9606741666793823\n",
      "Epoch 10, Loss: 0.7034143209457397, Final Score Train: 0.765833854675293, Final Score Test: 0.7114391326904297, macro F1 Train: 0.5548604180625321, macro F1 Test: 0.4614271582746702, 1-TPR Gap Train: 0.9768072366714478, 1-TPR Gap Test: 0.9614511132240295\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7093814015388489 Macro F1: 0.4587039691839041 1-TPR_gap: 0.960058867931366\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.6452302932739258, Final Score Train: 0.7658973932266235, Final Score Test: 0.7100988626480103, macro F1 Train: 0.5549546719034127, macro F1 Test: 0.4595692914016922, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9606283903121948\n",
      "Epoch 10, Loss: 0.6756788492202759, Final Score Train: 0.765946626663208, Final Score Test: 0.7090388536453247, macro F1 Train: 0.5550309422151425, macro F1 Test: 0.45843533079416615, 1-TPR Gap Train: 0.9768622517585754, 1-TPR Gap Test: 0.9596423506736755\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7096307873725891 Macro F1: 0.45895543690809254 1-TPR_gap: 0.9603061676025391\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7040132284164429, Final Score Train: 0.7659262418746948, Final Score Test: 0.7099044919013977, macro F1 Train: 0.5550122879060227, macro F1 Test: 0.45938642645920724, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9604225754737854\n",
      "Epoch 10, Loss: 0.6891756653785706, Final Score Train: 0.7659302949905396, Final Score Test: 0.7086065411567688, macro F1 Train: 0.5550204101838344, macro F1 Test: 0.45823117788781825, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9589819312095642\n",
      "Epoch 20, Loss: 0.7068285942077637, Final Score Train: 0.7659302949905396, Final Score Test: 0.7090679407119751, macro F1 Train: 0.5550204101838344, macro F1 Test: 0.45848887379163766, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.959646999835968\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.708700954914093 Macro F1: 0.4577370240092354 1-TPR_gap: 0.9596648812294006\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7131639719009399, Final Score Train: 0.7659556865692139, Final Score Test: 0.7094560265541077, macro F1 Train: 0.555071319653188, macro F1 Test: 0.4582847375856734, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.9606273174285889\n",
      "Epoch 10, Loss: 0.7127952575683594, Final Score Train: 0.7659533023834229, Final Score Test: 0.7093682885169983, macro F1 Train: 0.5550664191847249, macro F1 Test: 0.45850630711331164, 1-TPR Gap Train: 0.9768401384353638, 1-TPR Gap Test: 0.96023029088974\n",
      "Epoch 20, Loss: 0.7056879997253418, Final Score Train: 0.7659973502159119, Final Score Test: 0.7092061042785645, macro F1 Train: 0.5551162276759735, macro F1 Test: 0.4586762421851591, 1-TPR Gap Train: 0.9768784642219543, 1-TPR Gap Test: 0.9597359299659729\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7093443274497986 Macro F1: 0.45843335703584565 1-TPR_gap: 0.9602552652359009\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.6978440284729004, Final Score Train: 0.7659666538238525, Final Score Test: 0.7089684009552002, macro F1 Train: 0.555098131778289, macro F1 Test: 0.4583248643695466, 1-TPR Gap Train: 0.9768351912498474, 1-TPR Gap Test: 0.9596118927001953\n",
      "Epoch 10, Loss: 0.7079520225524902, Final Score Train: 0.7660000324249268, Final Score Test: 0.7090199589729309, macro F1 Train: 0.5551215860787092, macro F1 Test: 0.45831032991829196, 1-TPR Gap Train: 0.9768784642219543, 1-TPR Gap Test: 0.9597295522689819\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7089272737503052 Macro F1: 0.4584007062871834 1-TPR_gap: 0.9594538807868958\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.7043732404708862, Final Score Train: 0.7652570009231567, Final Score Test: 0.7088682055473328, macro F1 Train: 0.5536116611110158, macro F1 Test: 0.45852024357408966, 1-TPR Gap Train: 0.9769023060798645, 1-TPR Gap Test: 0.9592161774635315\n",
      "Epoch 10, Loss: 0.8313915729522705, Final Score Train: 0.7650586366653442, Final Score Test: 0.7103180289268494, macro F1 Train: 0.5534466903434876, macro F1 Test: 0.45829933302578046, 1-TPR Gap Train: 0.9766706228256226, 1-TPR Gap Test: 0.9623367190361023\n",
      "Epoch 20, Loss: 0.7552738189697266, Final Score Train: 0.7645407319068909, Final Score Test: 0.708991289138794, macro F1 Train: 0.5524507672085768, macro F1 Test: 0.457887957849887, 1-TPR Gap Train: 0.976630687713623, 1-TPR Gap Test: 0.9600945711135864\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.7092119455337524 Macro F1: 0.4560528558081817 1-TPR_gap: 0.9623710513114929\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8500080704689026, Final Score Train: 0.7654128670692444, Final Score Test: 0.7091126441955566, macro F1 Train: 0.5540211120481661, macro F1 Test: 0.45739878121650074, 1-TPR Gap Train: 0.9768046140670776, 1-TPR Gap Test: 0.9608264565467834\n",
      "Epoch 10, Loss: 0.8460057377815247, Final Score Train: 0.7656903266906738, Final Score Test: 0.7107878923416138, macro F1 Train: 0.5546178932109591, macro F1 Test: 0.45887036837747364, 1-TPR Gap Train: 0.9767628312110901, 1-TPR Gap Test: 0.9627053737640381\n",
      "Epoch 20, Loss: 0.8196902275085449, Final Score Train: 0.7659379243850708, Final Score Test: 0.7090548276901245, macro F1 Train: 0.5551699076280219, macro F1 Test: 0.45726346115686656, 1-TPR Gap Train: 0.9767059087753296, 1-TPR Gap Test: 0.9608462452888489\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.711036205291748 Macro F1: 0.4591907794067544 1-TPR_gap: 0.9628816246986389\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7378625869750977, Final Score Train: 0.7660890221595764, Final Score Test: 0.709060549736023, macro F1 Train: 0.555416065019181, macro F1 Test: 0.45855982553587715, 1-TPR Gap Train: 0.9767619967460632, 1-TPR Gap Test: 0.959561288356781\n",
      "Epoch 10, Loss: 0.6836721897125244, Final Score Train: 0.7661726474761963, Final Score Test: 0.7105035185813904, macro F1 Train: 0.5556034628384754, macro F1 Test: 0.458950213841081, 1-TPR Gap Train: 0.9767418503761292, 1-TPR Gap Test: 0.9620568156242371\n",
      "Epoch 20, Loss: 0.7268312573432922, Final Score Train: 0.7662531137466431, Final Score Test: 0.7092527151107788, macro F1 Train: 0.555622187542418, macro F1 Test: 0.4583636648202206, 1-TPR Gap Train: 0.9768840074539185, 1-TPR Gap Test: 0.9601417183876038\n",
      "Arrêt précoce après 24 époques\n",
      "Final Evaluation Score: 0.7095433473587036 Macro F1: 0.45848334611860825 1-TPR_gap: 0.9606034159660339\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.6629295349121094, Final Score Train: 0.7662070989608765, Final Score Test: 0.7099195718765259, macro F1 Train: 0.5556733381448942, macro F1 Test: 0.4586760300521234, 1-TPR Gap Train: 0.976740837097168, 1-TPR Gap Test: 0.9611631631851196\n",
      "Epoch 10, Loss: 0.6629542112350464, Final Score Train: 0.7663533091545105, Final Score Test: 0.7094942927360535, macro F1 Train: 0.5558226364439638, macro F1 Test: 0.4587311468948801, 1-TPR Gap Train: 0.9768840074539185, 1-TPR Gap Test: 0.9602574110031128\n",
      "Epoch 20, Loss: 0.6677641868591309, Final Score Train: 0.7663529515266418, Final Score Test: 0.7090997099876404, macro F1 Train: 0.5559616242294261, macro F1 Test: 0.4588620334455443, 1-TPR Gap Train: 0.9767442941665649, 1-TPR Gap Test: 0.9593373537063599\n",
      "Epoch 30, Loss: 0.6672694683074951, Final Score Train: 0.7665009498596191, Final Score Test: 0.7092292308807373, macro F1 Train: 0.556072212115164, macro F1 Test: 0.4574518660384426, 1-TPR Gap Train: 0.9769297242164612, 1-TPR Gap Test: 0.9610066413879395\n",
      "Arrêt précoce après 32 époques\n",
      "Final Evaluation Score: 0.7091791033744812 Macro F1: 0.45847369804173743 1-TPR_gap: 0.959884524345398\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.6964743733406067, Final Score Train: 0.7664636373519897, Final Score Test: 0.7089089751243591, macro F1 Train: 0.5560482043394149, macro F1 Test: 0.4574844308316936, 1-TPR Gap Train: 0.9768791198730469, 1-TPR Gap Test: 0.960333526134491\n",
      "Epoch 10, Loss: 0.6924798488616943, Final Score Train: 0.7664679884910583, Final Score Test: 0.7089381217956543, macro F1 Train: 0.5560568506593838, macro F1 Test: 0.45787404853839214, 1-TPR Gap Train: 0.9768791198730469, 1-TPR Gap Test: 0.9600022435188293\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7092539668083191 Macro F1: 0.4578252013939574 1-TPR_gap: 0.96068274974823\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7139667272567749, Final Score Train: 0.7664797306060791, Final Score Test: 0.7094332575798035, macro F1 Train: 0.5560646009592497, macro F1 Test: 0.45822347987652, 1-TPR Gap Train: 0.9768948554992676, 1-TPR Gap Test: 0.9606429934501648\n",
      "Epoch 10, Loss: 0.7099629044532776, Final Score Train: 0.7664990425109863, Final Score Test: 0.7094894647598267, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.45825297390584374, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9607259631156921\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7093132734298706 Macro F1: 0.45801985959167624 1-TPR_gap: 0.9606067538261414\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.6998120546340942, Final Score Train: 0.7664990425109863, Final Score Test: 0.709478497505188, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.458389095781179, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9605679512023926\n",
      "Epoch 10, Loss: 0.7040873169898987, Final Score Train: 0.7664990425109863, Final Score Test: 0.7093324065208435, macro F1 Train: 0.5561081977242606, macro F1 Test: 0.45807746162696966, 1-TPR Gap Train: 0.9768899083137512, 1-TPR Gap Test: 0.9605873227119446\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7094164490699768 Macro F1: 0.45830796586199146 1-TPR_gap: 0.9605249166488647\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.7936120629310608, Final Score Train: 0.7662976384162903, Final Score Test: 0.7101032733917236, macro F1 Train: 0.5558431410150108, macro F1 Test: 0.45884400514966917, 1-TPR Gap Train: 0.9767521619796753, 1-TPR Gap Test: 0.9613624811172485\n",
      "Epoch 10, Loss: 0.8235973119735718, Final Score Train: 0.765984833240509, Final Score Test: 0.7105951309204102, macro F1 Train: 0.554763910804134, macro F1 Test: 0.45859237562526334, 1-TPR Gap Train: 0.977205753326416, 1-TPR Gap Test: 0.9625978469848633\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7099618911743164 Macro F1: 0.4581583819302272 1-TPR_gap: 0.9617653489112854\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.842221736907959, Final Score Train: 0.7664124369621277, Final Score Test: 0.7119945883750916, macro F1 Train: 0.5557691069268866, macro F1 Test: 0.46075692490779485, 1-TPR Gap Train: 0.9770557880401611, 1-TPR Gap Test: 0.9632322192192078\n",
      "Epoch 10, Loss: 0.8023384809494019, Final Score Train: 0.7664259076118469, Final Score Test: 0.7103863954544067, macro F1 Train: 0.5557995472993044, macro F1 Test: 0.4592214704583436, 1-TPR Gap Train: 0.9770522713661194, 1-TPR Gap Test: 0.9615513682365417\n",
      "Epoch 20, Loss: 0.8361706733703613, Final Score Train: 0.7665995359420776, Final Score Test: 0.7090833783149719, macro F1 Train: 0.5561064738427361, macro F1 Test: 0.45776020951269086, 1-TPR Gap Train: 0.9770926833152771, 1-TPR Gap Test: 0.9604065418243408\n",
      "Arrêt précoce après 27 époques\n",
      "Final Evaluation Score: 0.7099382281303406 Macro F1: 0.46001301407461836 1-TPR_gap: 0.9598634243011475\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.7016222476959229, Final Score Train: 0.7668010592460632, Final Score Test: 0.7109034657478333, macro F1 Train: 0.5563613284123959, macro F1 Test: 0.4603546510074922, 1-TPR Gap Train: 0.977240800857544, 1-TPR Gap Test: 0.961452305316925\n",
      "Epoch 10, Loss: 0.7908712029457092, Final Score Train: 0.7668625116348267, Final Score Test: 0.7094393372535706, macro F1 Train: 0.5565010752906082, macro F1 Test: 0.46018446427496595, 1-TPR Gap Train: 0.9772239923477173, 1-TPR Gap Test: 0.9586942195892334\n",
      "Epoch 20, Loss: 0.7987620830535889, Final Score Train: 0.7669350504875183, Final Score Test: 0.7087582349777222, macro F1 Train: 0.5566509706312696, macro F1 Test: 0.4588960182875059, 1-TPR Gap Train: 0.9772191047668457, 1-TPR Gap Test: 0.9586204290390015\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7093897461891174 Macro F1: 0.4598037377058845 1-TPR_gap: 0.9589757919311523\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.6689091324806213, Final Score Train: 0.7668598890304565, Final Score Test: 0.7094907164573669, macro F1 Train: 0.5566566823742456, macro F1 Test: 0.45898458232616657, 1-TPR Gap Train: 0.9770631194114685, 1-TPR Gap Test: 0.9599968194961548\n",
      "Epoch 10, Loss: 0.6680995225906372, Final Score Train: 0.7669514417648315, Final Score Test: 0.7087864875793457, macro F1 Train: 0.5567004670446762, macro F1 Test: 0.4584232488506114, 1-TPR Gap Train: 0.9772023558616638, 1-TPR Gap Test: 0.9591497182846069\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.709419310092926 Macro F1: 0.4594723314429143 1-TPR_gap: 0.9593663215637207\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7108616828918457, Final Score Train: 0.7670499086380005, Final Score Test: 0.7095319032669067, macro F1 Train: 0.556707458731707, macro F1 Test: 0.46002740739469433, 1-TPR Gap Train: 0.977392315864563, 1-TPR Gap Test: 0.9590364694595337\n",
      "Epoch 10, Loss: 0.7061588764190674, Final Score Train: 0.7670546770095825, Final Score Test: 0.7096606492996216, macro F1 Train: 0.5567122398588676, macro F1 Test: 0.4592383664899146, 1-TPR Gap Train: 0.9773972034454346, 1-TPR Gap Test: 0.9600828886032104\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7094579935073853 Macro F1: 0.45958775701207255 1-TPR_gap: 0.959328293800354\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7058290243148804, Final Score Train: 0.7669628858566284, Final Score Test: 0.7094374895095825, macro F1 Train: 0.5567052609714904, macro F1 Test: 0.4596340438752797, 1-TPR Gap Train: 0.9772205948829651, 1-TPR Gap Test: 0.9592409729957581\n",
      "Epoch 10, Loss: 0.7033321857452393, Final Score Train: 0.7670526504516602, Final Score Test: 0.7096896171569824, macro F1 Train: 0.5567081246683347, macro F1 Test: 0.4593562099100604, 1-TPR Gap Train: 0.9773972034454346, 1-TPR Gap Test: 0.9600229859352112\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.709358274936676 Macro F1: 0.4594506143660345 1-TPR_gap: 0.959265947341919\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7131592035293579, Final Score Train: 0.7670662999153137, Final Score Test: 0.7096966505050659, macro F1 Train: 0.5567354117409491, macro F1 Test: 0.45937027961773574, 1-TPR Gap Train: 0.9773972034454346, 1-TPR Gap Test: 0.9600229859352112\n",
      "Epoch 10, Loss: 0.7124313712120056, Final Score Train: 0.7670753002166748, Final Score Test: 0.7092488408088684, macro F1 Train: 0.5567583473498182, macro F1 Test: 0.45925107691261174, 1-TPR Gap Train: 0.977392315864563, 1-TPR Gap Test: 0.9592465758323669\n",
      "Epoch 20, Loss: 0.6972710490226746, Final Score Train: 0.7670753002166748, Final Score Test: 0.7092558741569519, macro F1 Train: 0.5567583473498182, macro F1 Test: 0.45926514662028717, 1-TPR Gap Train: 0.977392315864563, 1-TPR Gap Test: 0.9592465758323669\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7096896171569824 Macro F1: 0.4593562099100604 1-TPR_gap: 0.9600229859352112\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.8609318733215332, Final Score Train: 0.766600489616394, Final Score Test: 0.7099987864494324, macro F1 Train: 0.5560506093195522, macro F1 Test: 0.4576816328381915, 1-TPR Gap Train: 0.9771504402160645, 1-TPR Gap Test: 0.9623159766197205\n",
      "Epoch 10, Loss: 0.7734427452087402, Final Score Train: 0.7666634917259216, Final Score Test: 0.7101282477378845, macro F1 Train: 0.5559065802025027, macro F1 Test: 0.4578869324121844, 1-TPR Gap Train: 0.9774203896522522, 1-TPR Gap Test: 0.9623695611953735\n",
      "Epoch 20, Loss: 0.7642423510551453, Final Score Train: 0.7667385339736938, Final Score Test: 0.7107571363449097, macro F1 Train: 0.5560708486248753, macro F1 Test: 0.45873947223269057, 1-TPR Gap Train: 0.9774061441421509, 1-TPR Gap Test: 0.9627748727798462\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7097390294075012 Macro F1: 0.456938296866197 1-TPR_gap: 0.962539792060852\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.8417109251022339, Final Score Train: 0.7669320702552795, Final Score Test: 0.71168053150177, macro F1 Train: 0.5564540478087712, macro F1 Test: 0.4585573028173352, 1-TPR Gap Train: 0.977410078048706, 1-TPR Gap Test: 0.9648038148880005\n",
      "Epoch 10, Loss: 0.8317211866378784, Final Score Train: 0.7669534087181091, Final Score Test: 0.7091361284255981, macro F1 Train: 0.5564954732494536, macro F1 Test: 0.4582684326842576, 1-TPR Gap Train: 0.9774113297462463, 1-TPR Gap Test: 0.9600038528442383\n",
      "Epoch 20, Loss: 0.7912202477455139, Final Score Train: 0.767156720161438, Final Score Test: 0.7104788422584534, macro F1 Train: 0.5568504475694522, macro F1 Test: 0.46026444135900774, 1-TPR Gap Train: 0.9774630069732666, 1-TPR Gap Test: 0.9606932401657104\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.710506796836853 Macro F1: 0.46008371414987675 1-TPR_gap: 0.9609298706054688\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.7178761959075928, Final Score Train: 0.7672322988510132, Final Score Test: 0.7086344361305237, macro F1 Train: 0.5570114544730067, macro F1 Test: 0.45860498266424127, 1-TPR Gap Train: 0.9774532318115234, 1-TPR Gap Test: 0.9586638808250427\n",
      "Epoch 10, Loss: 0.7533817291259766, Final Score Train: 0.7673789262771606, Final Score Test: 0.7091913223266602, macro F1 Train: 0.5573096448840019, macro F1 Test: 0.45961414992746763, 1-TPR Gap Train: 0.9774482846260071, 1-TPR Gap Test: 0.9587684273719788\n",
      "Epoch 20, Loss: 0.6729438304901123, Final Score Train: 0.7674918174743652, Final Score Test: 0.7092812061309814, macro F1 Train: 0.5575560260554547, macro F1 Test: 0.45880032256661357, 1-TPR Gap Train: 0.9774276614189148, 1-TPR Gap Test: 0.9597621560096741\n",
      "Epoch 30, Loss: 0.7226791381835938, Final Score Train: 0.7675278782844543, Final Score Test: 0.7080879211425781, macro F1 Train: 0.5576177610088698, macro F1 Test: 0.45786636734955394, 1-TPR Gap Train: 0.9774379730224609, 1-TPR Gap Test: 0.958309531211853\n",
      "Arrêt précoce après 35 époques\n",
      "Final Evaluation Score: 0.7095379829406738 Macro F1: 0.45884827389899596 1-TPR_gap: 0.9602277278900146\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.6575847864151001, Final Score Train: 0.7675804495811462, Final Score Test: 0.7083045244216919, macro F1 Train: 0.5577179558186552, macro F1 Test: 0.45853499371953704, 1-TPR Gap Train: 0.9774429202079773, 1-TPR Gap Test: 0.9580740928649902\n",
      "Epoch 10, Loss: 0.696689784526825, Final Score Train: 0.7676117420196533, Final Score Test: 0.7088091373443604, macro F1 Train: 0.5577796248572374, macro F1 Test: 0.45952357399483035, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9580946564674377\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7091478705406189 Macro F1: 0.4591081339758882 1-TPR_gap: 0.9591876268386841\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.68854820728302, Final Score Train: 0.7676244974136353, Final Score Test: 0.7091631889343262, macro F1 Train: 0.5578049865892429, macro F1 Test: 0.4594730942586766, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9588532447814941\n",
      "Epoch 10, Loss: 0.7102219462394714, Final Score Train: 0.7676451802253723, Final Score Test: 0.7089998126029968, macro F1 Train: 0.5578464146658771, macro F1 Test: 0.45910901266908244, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9588906168937683\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.708975076675415 Macro F1: 0.458928261716789 1-TPR_gap: 0.9590219259262085\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7074850797653198, Final Score Train: 0.7676341533660889, Final Score Test: 0.708992600440979, macro F1 Train: 0.5578244056969464, macro F1 Test: 0.45928075703498067, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9587044715881348\n",
      "Epoch 10, Loss: 0.705653727054596, Final Score Train: 0.7676402926445007, Final Score Test: 0.7088624238967896, macro F1 Train: 0.5578366604491013, macro F1 Test: 0.45878462060406594, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9589402079582214\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7089638113975525 Macro F1: 0.4588578551265779 1-TPR_gap: 0.9590697884559631\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7054237127304077, Final Score Train: 0.7676443457603455, Final Score Test: 0.708893895149231, macro F1 Train: 0.5578447479436462, macro F1 Test: 0.45882826947167615, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9589595794677734\n",
      "Epoch 10, Loss: 0.70428466796875, Final Score Train: 0.7676443457603455, Final Score Test: 0.7089092135429382, macro F1 Train: 0.5578447479436462, macro F1 Test: 0.4588759350115294, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9589424729347229\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7088711857795715 Macro F1: 0.4588216336668351 1-TPR_gap: 0.9589207768440247\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.8387954235076904, Final Score Train: 0.7675145864486694, Final Score Test: 0.7091226577758789, macro F1 Train: 0.5575857876850243, macro F1 Test: 0.4575193595383325, 1-TPR Gap Train: 0.9774433970451355, 1-TPR Gap Test: 0.9607260227203369\n",
      "Epoch 10, Loss: 0.7675787210464478, Final Score Train: 0.7674949169158936, Final Score Test: 0.7112077474594116, macro F1 Train: 0.5575458949475002, macro F1 Test: 0.4586758131763063, 1-TPR Gap Train: 0.9774439334869385, 1-TPR Gap Test: 0.9637396335601807\n",
      "Epoch 20, Loss: 0.8187081813812256, Final Score Train: 0.7671611309051514, Final Score Test: 0.7105737924575806, macro F1 Train: 0.5568733434886964, macro F1 Test: 0.45811041998331375, 1-TPR Gap Train: 0.9774488806724548, 1-TPR Gap Test: 0.9630371332168579\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7116193771362305 Macro F1: 0.4592959078460897 1-TPR_gap: 0.9639428853988647\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.7275074124336243, Final Score Train: 0.7674124240875244, Final Score Test: 0.7105206847190857, macro F1 Train: 0.557413930430978, macro F1 Test: 0.45952909100141615, 1-TPR Gap Train: 0.9774109721183777, 1-TPR Gap Test: 0.9615122675895691\n",
      "Epoch 10, Loss: 0.836423397064209, Final Score Train: 0.767498254776001, Final Score Test: 0.7098256349563599, macro F1 Train: 0.5576151045843682, macro F1 Test: 0.4598172686970711, 1-TPR Gap Train: 0.9773814678192139, 1-TPR Gap Test: 0.9598339796066284\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7091920375823975 Macro F1: 0.4595637528182329 1-TPR_gap: 0.9588202834129333\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.7179664373397827, Final Score Train: 0.7675484418869019, Final Score Test: 0.7108911275863647, macro F1 Train: 0.5576937850463325, macro F1 Test: 0.46054424212401596, 1-TPR Gap Train: 0.9774031043052673, 1-TPR Gap Test: 0.9612379670143127\n",
      "Epoch 10, Loss: 0.8405085802078247, Final Score Train: 0.7676479816436768, Final Score Test: 0.7101329565048218, macro F1 Train: 0.5579136216214144, macro F1 Test: 0.4602497955677, 1-TPR Gap Train: 0.9773824214935303, 1-TPR Gap Test: 0.9600161910057068\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.708703339099884 Macro F1: 0.4583478302981982 1-TPR_gap: 0.9590588212013245\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.6866039037704468, Final Score Train: 0.767686128616333, Final Score Test: 0.7090330123901367, macro F1 Train: 0.558005489184402, macro F1 Test: 0.4594428961604177, 1-TPR Gap Train: 0.9773666858673096, 1-TPR Gap Test: 0.9586231708526611\n",
      "Epoch 10, Loss: 0.718670129776001, Final Score Train: 0.7677333950996399, Final Score Test: 0.7095301747322083, macro F1 Train: 0.5581001192608528, macro F1 Test: 0.45973863343662874, 1-TPR Gap Train: 0.9773666858673096, 1-TPR Gap Test: 0.9593216776847839\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7093139886856079 Macro F1: 0.45934390225117816 1-TPR_gap: 0.9592841267585754\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.6949367523193359, Final Score Train: 0.7677140235900879, Final Score Test: 0.7090438604354858, macro F1 Train: 0.5580612952261002, macro F1 Test: 0.4585908100379828, 1-TPR Gap Train: 0.9773666858673096, 1-TPR Gap Test: 0.9594969153404236\n",
      "Epoch 10, Loss: 0.6980526447296143, Final Score Train: 0.7677140235900879, Final Score Test: 0.709220290184021, macro F1 Train: 0.5580612952261002, macro F1 Test: 0.4588235520328513, 1-TPR Gap Train: 0.9773666858673096, 1-TPR Gap Test: 0.959617018699646\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7092651128768921 Macro F1: 0.45914095308979164 1-TPR_gap: 0.959389328956604\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7054551839828491, Final Score Train: 0.7677488327026367, Final Score Test: 0.709220290184021, macro F1 Train: 0.5581358186967967, macro F1 Test: 0.4588235520328513, 1-TPR Gap Train: 0.977361798286438, 1-TPR Gap Test: 0.959617018699646\n",
      "Epoch 10, Loss: 0.7097402215003967, Final Score Train: 0.7677488327026367, Final Score Test: 0.7092152833938599, macro F1 Train: 0.5581358186967967, macro F1 Test: 0.4588634326533271, 1-TPR Gap Train: 0.977361798286438, 1-TPR Gap Test: 0.9595670700073242\n",
      "Epoch 20, Loss: 0.7013399600982666, Final Score Train: 0.7677513957023621, Final Score Test: 0.7092152833938599, macro F1 Train: 0.5581409714539812, macro F1 Test: 0.4588634326533271, 1-TPR Gap Train: 0.977361798286438, 1-TPR Gap Test: 0.9595670700073242\n",
      "Arrêt précoce après 30 époques\n",
      "Final Evaluation Score: 0.7093650102615356 Macro F1: 0.458847120085138 1-TPR_gap: 0.9598829746246338\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7042436599731445, Final Score Train: 0.7677621841430664, Final Score Test: 0.7093122005462646, macro F1 Train: 0.5581624992910612, macro F1 Test: 0.4590380175496708, 1-TPR Gap Train: 0.977361798286438, 1-TPR Gap Test: 0.9595864415168762\n",
      "Epoch 10, Loss: 0.7181731462478638, Final Score Train: 0.7677595615386963, Final Score Test: 0.7092297077178955, macro F1 Train: 0.5581573465338767, macro F1 Test: 0.45880583641067924, 1-TPR Gap Train: 0.977361798286438, 1-TPR Gap Test: 0.9596536159515381\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7092553377151489 Macro F1: 0.45888205215461264 1-TPR_gap: 0.9596286416053772\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.7541669607162476, Final Score Train: 0.7677547931671143, Final Score Test: 0.7103393077850342, macro F1 Train: 0.5578869183604336, macro F1 Test: 0.4588864263075399, 1-TPR Gap Train: 0.9776226878166199, 1-TPR Gap Test: 0.9617922306060791\n",
      "Epoch 10, Loss: 0.8837100267410278, Final Score Train: 0.7676998376846313, Final Score Test: 0.7102460265159607, macro F1 Train: 0.5577750494970176, macro F1 Test: 0.46013403231669614, 1-TPR Gap Train: 0.9776247143745422, 1-TPR Gap Test: 0.9603580236434937\n",
      "Epoch 20, Loss: 0.8694798350334167, Final Score Train: 0.7677332162857056, Final Score Test: 0.7095581293106079, macro F1 Train: 0.5578921356917524, macro F1 Test: 0.4585832959767367, 1-TPR Gap Train: 0.977574348449707, 1-TPR Gap Test: 0.9605329036712646\n",
      "Arrêt précoce après 27 époques\n",
      "Final Evaluation Score: 0.7110577821731567 Macro F1: 0.4604378103214111 1-TPR_gap: 0.9616777300834656\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.8953695297241211, Final Score Train: 0.7676926255226135, Final Score Test: 0.7108595967292786, macro F1 Train: 0.5578781359004775, macro F1 Test: 0.46094681954855005, 1-TPR Gap Train: 0.9775071144104004, 1-TPR Gap Test: 0.9607723951339722\n",
      "Epoch 10, Loss: 0.7193601131439209, Final Score Train: 0.7677547931671143, Final Score Test: 0.7096662521362305, macro F1 Train: 0.5580192604656746, macro F1 Test: 0.45803125689101115, 1-TPR Gap Train: 0.9774903655052185, 1-TPR Gap Test: 0.9613012671470642\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7103123664855957 Macro F1: 0.4597515281978798 1-TPR_gap: 0.9608731865882874\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.6457584500312805, Final Score Train: 0.7678500413894653, Final Score Test: 0.7103427052497864, macro F1 Train: 0.5582490731595121, macro F1 Test: 0.4605306718108844, 1-TPR Gap Train: 0.9774510264396667, 1-TPR Gap Test: 0.9601547122001648\n",
      "Epoch 10, Loss: 0.7718937397003174, Final Score Train: 0.767980694770813, Final Score Test: 0.7103930711746216, macro F1 Train: 0.5584951263995633, macro F1 Test: 0.46036138725788217, 1-TPR Gap Train: 0.9774662852287292, 1-TPR Gap Test: 0.9604247212409973\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7091373205184937 Macro F1: 0.4588243212055061 1-TPR_gap: 0.9594503045082092\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.6620851755142212, Final Score Train: 0.7680511474609375, Final Score Test: 0.7101869583129883, macro F1 Train: 0.5586207025295934, macro F1 Test: 0.46009434438154745, 1-TPR Gap Train: 0.9774815440177917, 1-TPR Gap Test: 0.9602795839309692\n",
      "Epoch 10, Loss: 0.6753655076026917, Final Score Train: 0.7681018114089966, Final Score Test: 0.7092460989952087, macro F1 Train: 0.5587220979769907, macro F1 Test: 0.45889530534204637, 1-TPR Gap Train: 0.9774815440177917, 1-TPR Gap Test: 0.9595969319343567\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7097859978675842 Macro F1: 0.45903559142891365 1-TPR_gap: 0.9605363607406616\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.6906489133834839, Final Score Train: 0.7681245803833008, Final Score Test: 0.709997832775116, macro F1 Train: 0.5587725825508791, macro F1 Test: 0.45926068477397697, 1-TPR Gap Train: 0.9774765968322754, 1-TPR Gap Test: 0.960735023021698\n",
      "Epoch 10, Loss: 0.6898782253265381, Final Score Train: 0.7682220339775085, Final Score Test: 0.7104778289794922, macro F1 Train: 0.5589713573176406, macro F1 Test: 0.4602308703538254, 1-TPR Gap Train: 0.977472722530365, 1-TPR Gap Test: 0.9607248306274414\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.709708034992218 Macro F1: 0.459656547441274 1-TPR_gap: 0.959759533405304\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7043713927268982, Final Score Train: 0.7682390213012695, Final Score Test: 0.7101293206214905, macro F1 Train: 0.5590101721202199, macro F1 Test: 0.4597118294135799, 1-TPR Gap Train: 0.9774678349494934, 1-TPR Gap Test: 0.9605467915534973\n",
      "Epoch 10, Loss: 0.7093440890312195, Final Score Train: 0.7682621479034424, Final Score Test: 0.710151731967926, macro F1 Train: 0.5590564799930691, macro F1 Test: 0.45975665684122485, 1-TPR Gap Train: 0.9774678349494934, 1-TPR Gap Test: 0.9605467915534973\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7100309729576111 Macro F1: 0.45970275272292177 1-TPR_gap: 0.9603591561317444\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7034038305282593, Final Score Train: 0.7682621479034424, Final Score Test: 0.710083544254303, macro F1 Train: 0.5590564799930691, macro F1 Test: 0.45980678162904115, 1-TPR Gap Train: 0.9774678349494934, 1-TPR Gap Test: 0.9603603482246399\n",
      "Epoch 10, Loss: 0.7084628343582153, Final Score Train: 0.7682621479034424, Final Score Test: 0.7101300954818726, macro F1 Train: 0.5590564799930691, macro F1 Test: 0.4597894106450509, 1-TPR Gap Train: 0.9774678349494934, 1-TPR Gap Test: 0.9604707956314087\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7100600004196167 Macro F1: 0.4596685721505704 1-TPR_gap: 0.9604514241218567\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.8712490797042847, Final Score Train: 0.567874550819397, Final Score Test: 0.5616454482078552, macro F1 Train: 0.1752888570326747, macro F1 Test: 0.1635435042998349, 1-TPR Gap Train: 0.9604602456092834, 1-TPR Gap Test: 0.9597474336624146\n",
      "Epoch 10, Loss: 0.7886906862258911, Final Score Train: 0.6504388451576233, Final Score Test: 0.6361653208732605, macro F1 Train: 0.34006244672322866, macro F1 Test: 0.32683388514901723, 1-TPR Gap Train: 0.9608152508735657, 1-TPR Gap Test: 0.9454967379570007\n",
      "Epoch 20, Loss: 0.8591095805168152, Final Score Train: 0.6666697263717651, Final Score Test: 0.658926784992218, macro F1 Train: 0.3811472977312489, macro F1 Test: 0.37284889609817407, 1-TPR Gap Train: 0.9521921277046204, 1-TPR Gap Test: 0.9450047016143799\n",
      "Epoch 30, Loss: 0.8675596714019775, Final Score Train: 0.6693974137306213, Final Score Test: 0.6568872928619385, macro F1 Train: 0.3867760534273573, macro F1 Test: 0.3682100686048126, 1-TPR Gap Train: 0.9520187973976135, 1-TPR Gap Test: 0.9455644488334656\n",
      "Epoch 40, Loss: 0.7633161544799805, Final Score Train: 0.6841496825218201, Final Score Test: 0.6731427311897278, macro F1 Train: 0.41895259647065775, macro F1 Test: 0.40299070096098916, 1-TPR Gap Train: 0.9493467807769775, 1-TPR Gap Test: 0.9432947635650635\n",
      "Epoch 50, Loss: 0.8016583919525146, Final Score Train: 0.6720083951950073, Final Score Test: 0.6652178168296814, macro F1 Train: 0.3845232689518176, macro F1 Test: 0.36984607356464266, 1-TPR Gap Train: 0.9594935178756714, 1-TPR Gap Test: 0.9605895280838013\n",
      "Epoch 60, Loss: 0.840933084487915, Final Score Train: 0.6748789548873901, Final Score Test: 0.6616033315658569, macro F1 Train: 0.39971681735869596, macro F1 Test: 0.37881283433995466, 1-TPR Gap Train: 0.9500410556793213, 1-TPR Gap Test: 0.9443938732147217\n",
      "Arrêt précoce après 61 époques\n",
      "Final Evaluation Score: 0.6646485328674316 Macro F1: 0.37482656255364305 1-TPR_gap: 0.9544704556465149\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.8477892875671387, Final Score Train: 0.6867016553878784, Final Score Test: 0.6774441003799438, macro F1 Train: 0.4114911922886087, macro F1 Test: 0.40157140400617447, 1-TPR Gap Train: 0.9619120955467224, 1-TPR Gap Test: 0.9533167481422424\n",
      "Epoch 10, Loss: 0.8051067590713501, Final Score Train: 0.6920483112335205, Final Score Test: 0.6829401254653931, macro F1 Train: 0.42353336096669253, macro F1 Test: 0.4059511378845935, 1-TPR Gap Train: 0.9605632424354553, 1-TPR Gap Test: 0.9599290490150452\n",
      "Epoch 20, Loss: 0.7973518371582031, Final Score Train: 0.6863165497779846, Final Score Test: 0.6738550662994385, macro F1 Train: 0.4054140314461819, macro F1 Test: 0.3877967466084723, 1-TPR Gap Train: 0.9672190546989441, 1-TPR Gap Test: 0.9599133729934692\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.6516790390014648 Macro F1: 0.3440497160756642 1-TPR_gap: 0.9593083262443542\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7947958111763, Final Score Train: 0.7004508972167969, Final Score Test: 0.6879391670227051, macro F1 Train: 0.4340768853912812, macro F1 Test: 0.4155768263777778, 1-TPR Gap Train: 0.9668249487876892, 1-TPR Gap Test: 0.9603014588356018\n",
      "Epoch 10, Loss: 0.6851968169212341, Final Score Train: 0.7028179168701172, Final Score Test: 0.6905076503753662, macro F1 Train: 0.44413450788340514, macro F1 Test: 0.4287140662541278, 1-TPR Gap Train: 0.9615013003349304, 1-TPR Gap Test: 0.9523012638092041\n",
      "Epoch 20, Loss: 0.7004536390304565, Final Score Train: 0.69580078125, Final Score Test: 0.6822687387466431, macro F1 Train: 0.4338480752315072, macro F1 Test: 0.4139785367847065, 1-TPR Gap Train: 0.9577534794807434, 1-TPR Gap Test: 0.9505589008331299\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.6891729831695557 Macro F1: 0.4230692837718732 1-TPR_gap: 0.955276608467102\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.7075366377830505, Final Score Train: 0.7075757384300232, Final Score Test: 0.6937369108200073, macro F1 Train: 0.450014120588002, macro F1 Test: 0.428120077216144, 1-TPR Gap Train: 0.9651373624801636, 1-TPR Gap Test: 0.9593538045883179\n",
      "Epoch 10, Loss: 0.732858419418335, Final Score Train: 0.7077199220657349, Final Score Test: 0.6969730854034424, macro F1 Train: 0.4509209428391215, macro F1 Test: 0.4371768851701084, 1-TPR Gap Train: 0.9645188450813293, 1-TPR Gap Test: 0.9567693471908569\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.6974863409996033 Macro F1: 0.442606969548324 1-TPR_gap: 0.9523657560348511\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.7271312475204468, Final Score Train: 0.7106645703315735, Final Score Test: 0.6984151005744934, macro F1 Train: 0.45914634015617095, macro F1 Test: 0.4409375874646049, 1-TPR Gap Train: 0.9621828198432922, 1-TPR Gap Test: 0.9558926224708557\n",
      "Epoch 10, Loss: 0.7229649424552917, Final Score Train: 0.7172862887382507, Final Score Test: 0.7049073576927185, macro F1 Train: 0.4702590437703205, macro F1 Test: 0.4509856125091148, 1-TPR Gap Train: 0.9643135070800781, 1-TPR Gap Test: 0.9588291049003601\n",
      "Epoch 20, Loss: 0.7253013849258423, Final Score Train: 0.7205798029899597, Final Score Test: 0.7069335579872131, macro F1 Train: 0.47284156419776296, macro F1 Test: 0.45184804863624367, 1-TPR Gap Train: 0.9683180451393127, 1-TPR Gap Test: 0.9620190262794495\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7047184705734253 Macro F1: 0.448718187313364 1-TPR_gap: 0.9607188105583191\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.7347244620323181, Final Score Train: 0.7182634472846985, Final Score Test: 0.7038025856018066, macro F1 Train: 0.4698298931665384, macro F1 Test: 0.44623200728255347, 1-TPR Gap Train: 0.9666969776153564, 1-TPR Gap Test: 0.9613731503486633\n",
      "Epoch 10, Loss: 0.7429177761077881, Final Score Train: 0.7211899757385254, Final Score Test: 0.7047803997993469, macro F1 Train: 0.47420735266534975, macro F1 Test: 0.45014390277186717, 1-TPR Gap Train: 0.968172550201416, 1-TPR Gap Test: 0.959416925907135\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7044206857681274 Macro F1: 0.449694525974758 1-TPR_gap: 0.9591467976570129\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.7422782778739929, Final Score Train: 0.7219882011413574, Final Score Test: 0.7055597901344299, macro F1 Train: 0.4761079096770399, macro F1 Test: 0.45063004373744536, 1-TPR Gap Train: 0.967868447303772, 1-TPR Gap Test: 0.9604895710945129\n",
      "Epoch 10, Loss: 0.7401504516601562, Final Score Train: 0.7234054803848267, Final Score Test: 0.7057672739028931, macro F1 Train: 0.4781496733746522, macro F1 Test: 0.45261194274599914, 1-TPR Gap Train: 0.9686612486839294, 1-TPR Gap Test: 0.9589226245880127\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7056417465209961 Macro F1: 0.45099501058794206 1-TPR_gap: 0.9602885246276855\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.8517858386039734, Final Score Train: 0.6816504001617432, Final Score Test: 0.6758610010147095, macro F1 Train: 0.3964447617295696, macro F1 Test: 0.3869350421447151, 1-TPR Gap Train: 0.966856062412262, 1-TPR Gap Test: 0.964786946773529\n",
      "Epoch 10, Loss: 0.7440478801727295, Final Score Train: 0.6962791085243225, Final Score Test: 0.684820294380188, macro F1 Train: 0.4322696178980499, macro F1 Test: 0.41043221122325185, 1-TPR Gap Train: 0.9602885842323303, 1-TPR Gap Test: 0.9592083692550659\n",
      "Epoch 20, Loss: 0.9132035970687866, Final Score Train: 0.6895992159843445, Final Score Test: 0.6747196316719055, macro F1 Train: 0.4211782115889521, macro F1 Test: 0.4059079033491009, 1-TPR Gap Train: 0.9580202102661133, 1-TPR Gap Test: 0.9435313940048218\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.6682100892066956 Macro F1: 0.38583312145942017 1-TPR_gap: 0.9505870342254639\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.8491773009300232, Final Score Train: 0.7051730751991272, Final Score Test: 0.6950187683105469, macro F1 Train: 0.44950586480217686, macro F1 Test: 0.43300780937082456, 1-TPR Gap Train: 0.9608402848243713, 1-TPR Gap Test: 0.9570297598838806\n",
      "Epoch 10, Loss: 0.8053032159805298, Final Score Train: 0.7026724815368652, Final Score Test: 0.6884883642196655, macro F1 Train: 0.4429478145899151, macro F1 Test: 0.4226897337879844, 1-TPR Gap Train: 0.9623970985412598, 1-TPR Gap Test: 0.9542869329452515\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.6654761433601379 Macro F1: 0.39229376711770675 1-TPR_gap: 0.9386584758758545\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.771295964717865, Final Score Train: 0.6982748508453369, Final Score Test: 0.6881347298622131, macro F1 Train: 0.43714933487723145, macro F1 Test: 0.4259414024717217, 1-TPR Gap Train: 0.9594002962112427, 1-TPR Gap Test: 0.9503280520439148\n",
      "Epoch 10, Loss: 0.7633167505264282, Final Score Train: 0.6983131170272827, Final Score Test: 0.6806381344795227, macro F1 Train: 0.44507775897722074, macro F1 Test: 0.4243241369684956, 1-TPR Gap Train: 0.9515484571456909, 1-TPR Gap Test: 0.9369521737098694\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6915616393089294 Macro F1: 0.43505255309862384 1-TPR_gap: 0.9480707049369812\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.706494927406311, Final Score Train: 0.7046974897384644, Final Score Test: 0.6927241683006287, macro F1 Train: 0.4574287882089016, macro F1 Test: 0.4360610979408831, 1-TPR Gap Train: 0.9519661664962769, 1-TPR Gap Test: 0.94938725233078\n",
      "Epoch 10, Loss: 0.7513523697853088, Final Score Train: 0.706963300704956, Final Score Test: 0.6945354342460632, macro F1 Train: 0.46209313772713323, macro F1 Test: 0.4388472618589595, 1-TPR Gap Train: 0.9518334865570068, 1-TPR Gap Test: 0.9502235651016235\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.6968507170677185 Macro F1: 0.4407262500583777 1-TPR_gap: 0.9529751539230347\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7436087727546692, Final Score Train: 0.7107243537902832, Final Score Test: 0.6982883810997009, macro F1 Train: 0.4610160546227591, macro F1 Test: 0.44362308445724746, 1-TPR Gap Train: 0.9604327082633972, 1-TPR Gap Test: 0.9529536366462708\n",
      "Epoch 10, Loss: 0.7294087409973145, Final Score Train: 0.714966356754303, Final Score Test: 0.7010484337806702, macro F1 Train: 0.4696100170383356, macro F1 Test: 0.4461872204974198, 1-TPR Gap Train: 0.9603227376937866, 1-TPR Gap Test: 0.9559096097946167\n",
      "Epoch 20, Loss: 0.7287954092025757, Final Score Train: 0.7169384360313416, Final Score Test: 0.705004096031189, macro F1 Train: 0.4727739671544184, macro F1 Test: 0.4504572063580753, 1-TPR Gap Train: 0.9611029028892517, 1-TPR Gap Test: 0.9595510363578796\n",
      "Arrêt précoce après 27 époques\n",
      "Final Evaluation Score: 0.7023705244064331 Macro F1: 0.4494550956954084 1-TPR_gap: 0.9552859663963318\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7389034032821655, Final Score Train: 0.7173635363578796, Final Score Test: 0.7029141187667847, macro F1 Train: 0.47324021863039756, macro F1 Test: 0.44890147142521275, 1-TPR Gap Train: 0.96148681640625, 1-TPR Gap Test: 0.9569267630577087\n",
      "Epoch 10, Loss: 0.7211781144142151, Final Score Train: 0.7182219624519348, Final Score Test: 0.7035000324249268, macro F1 Train: 0.4754305529204728, macro F1 Test: 0.4514814433904406, 1-TPR Gap Train: 0.9610133767127991, 1-TPR Gap Test: 0.9555186033248901\n",
      "Epoch 20, Loss: 0.7451207637786865, Final Score Train: 0.7190326452255249, Final Score Test: 0.7024935483932495, macro F1 Train: 0.4766211606025375, macro F1 Test: 0.44898864583845394, 1-TPR Gap Train: 0.9614441394805908, 1-TPR Gap Test: 0.9559984803199768\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7039266228675842 Macro F1: 0.44966003428528095 1-TPR_gap: 0.9581932425498962\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.7353835701942444, Final Score Train: 0.7197394371032715, Final Score Test: 0.70415198802948, macro F1 Train: 0.4766488374352639, macro F1 Test: 0.4499646822845305, 1-TPR Gap Train: 0.9628300666809082, 1-TPR Gap Test: 0.9583393335342407\n",
      "Epoch 10, Loss: 0.7354295253753662, Final Score Train: 0.7209886312484741, Final Score Test: 0.7052526473999023, macro F1 Train: 0.47890775912472866, macro F1 Test: 0.4518489725675114, 1-TPR Gap Train: 0.963069498538971, 1-TPR Gap Test: 0.9586562514305115\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7056608200073242 Macro F1: 0.45236672582652954 1-TPR_gap: 0.958954930305481\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.7887990474700928, Final Score Train: 0.6964982151985168, Final Score Test: 0.6813943982124329, macro F1 Train: 0.4332387277189816, macro F1 Test: 0.4163413206091357, 1-TPR Gap Train: 0.9597577452659607, 1-TPR Gap Test: 0.9464474320411682\n",
      "Epoch 10, Loss: 0.8740476965904236, Final Score Train: 0.7079774141311646, Final Score Test: 0.6937035322189331, macro F1 Train: 0.44976132555647447, macro F1 Test: 0.42926203153758696, 1-TPR Gap Train: 0.9661935567855835, 1-TPR Gap Test: 0.9581449627876282\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.6714408993721008 Macro F1: 0.3923769885980454 1-TPR_gap: 0.9505047798156738\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8009742498397827, Final Score Train: 0.6899045705795288, Final Score Test: 0.672742486000061, macro F1 Train: 0.4286644904828573, macro F1 Test: 0.4051218601682974, 1-TPR Gap Train: 0.9511446356773376, 1-TPR Gap Test: 0.9403630495071411\n",
      "Epoch 10, Loss: 0.6950761079788208, Final Score Train: 0.6978658437728882, Final Score Test: 0.6888989210128784, macro F1 Train: 0.44080566391121917, macro F1 Test: 0.4236430307389419, 1-TPR Gap Train: 0.9549260139465332, 1-TPR Gap Test: 0.9541548490524292\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.6791797280311584 Macro F1: 0.41149365124146986 1-TPR_gap: 0.9468657970428467\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.8349214196205139, Final Score Train: 0.6987232565879822, Final Score Test: 0.6896520853042603, macro F1 Train: 0.4401059235700989, macro F1 Test: 0.4273667243943364, 1-TPR Gap Train: 0.9573405981063843, 1-TPR Gap Test: 0.9519374966621399\n",
      "Epoch 10, Loss: 0.7621459364891052, Final Score Train: 0.709715723991394, Final Score Test: 0.6973155736923218, macro F1 Train: 0.46000238303953717, macro F1 Test: 0.4366188678864772, 1-TPR Gap Train: 0.9594290256500244, 1-TPR Gap Test: 0.9580122232437134\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.6894155740737915 Macro F1: 0.42742353306692943 1-TPR_gap: 0.9514076113700867\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.7238296270370483, Final Score Train: 0.705981433391571, Final Score Test: 0.6918324828147888, macro F1 Train: 0.450827154034941, macro F1 Test: 0.4296251317557357, 1-TPR Gap Train: 0.9611356854438782, 1-TPR Gap Test: 0.9540398716926575\n",
      "Epoch 10, Loss: 0.7461251020431519, Final Score Train: 0.7067804336547852, Final Score Test: 0.6918070912361145, macro F1 Train: 0.4501200415223942, macro F1 Test: 0.4300957830692747, 1-TPR Gap Train: 0.9634408354759216, 1-TPR Gap Test: 0.9535183906555176\n",
      "Epoch 20, Loss: 0.7398774027824402, Final Score Train: 0.7165482640266418, Final Score Test: 0.699917733669281, macro F1 Train: 0.4674525699400068, macro F1 Test: 0.4457918160941263, 1-TPR Gap Train: 0.9656440019607544, 1-TPR Gap Test: 0.9540436267852783\n",
      "Epoch 30, Loss: 0.727620005607605, Final Score Train: 0.7146497368812561, Final Score Test: 0.6952933073043823, macro F1 Train: 0.4673002838589292, macro F1 Test: 0.44292342296914905, 1-TPR Gap Train: 0.9619991779327393, 1-TPR Gap Test: 0.9476631879806519\n",
      "Arrêt précoce après 38 époques\n",
      "Final Evaluation Score: 0.6996085047721863 Macro F1: 0.4423913667553921 1-TPR_gap: 0.9568256139755249\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.727895975112915, Final Score Train: 0.7171040773391724, Final Score Test: 0.7002291679382324, macro F1 Train: 0.46890197402337713, macro F1 Test: 0.44289499270797705, 1-TPR Gap Train: 0.9653062224388123, 1-TPR Gap Test: 0.9575633406639099\n",
      "Epoch 10, Loss: 0.742956280708313, Final Score Train: 0.7215911746025085, Final Score Test: 0.7025858163833618, macro F1 Train: 0.4747011650453608, macro F1 Test: 0.45121441032721926, 1-TPR Gap Train: 0.968481183052063, 1-TPR Gap Test: 0.953957200050354\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7018455266952515 Macro F1: 0.4480492873611141 1-TPR_gap: 0.9556418061256409\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7394804954528809, Final Score Train: 0.7230830788612366, Final Score Test: 0.7012284398078918, macro F1 Train: 0.47900403700180577, macro F1 Test: 0.44772764084730843, 1-TPR Gap Train: 0.9671621322631836, 1-TPR Gap Test: 0.9547292590141296\n",
      "Epoch 10, Loss: 0.7364869713783264, Final Score Train: 0.7242475748062134, Final Score Test: 0.7049446702003479, macro F1 Train: 0.48185429843173616, macro F1 Test: 0.45318381595870677, 1-TPR Gap Train: 0.9666408896446228, 1-TPR Gap Test: 0.9567055106163025\n",
      "Epoch 20, Loss: 0.7317507266998291, Final Score Train: 0.7252944707870483, Final Score Test: 0.70393967628479, macro F1 Train: 0.4827219464729867, macro F1 Test: 0.4508465268814582, 1-TPR Gap Train: 0.9678670167922974, 1-TPR Gap Test: 0.9570328593254089\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7037736773490906 Macro F1: 0.4505066524511228 1-TPR_gap: 0.9570406675338745\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.7360951900482178, Final Score Train: 0.7256515026092529, Final Score Test: 0.7035191059112549, macro F1 Train: 0.48310012804519376, macro F1 Test: 0.45078654018949205, 1-TPR Gap Train: 0.9682029485702515, 1-TPR Gap Test: 0.9562516808509827\n",
      "Epoch 10, Loss: 0.7353456020355225, Final Score Train: 0.7261935472488403, Final Score Test: 0.7056097388267517, macro F1 Train: 0.4839164870572654, macro F1 Test: 0.4523865740189001, 1-TPR Gap Train: 0.9684706330299377, 1-TPR Gap Test: 0.958832859992981\n",
      "Epoch 20, Loss: 0.7324604392051697, Final Score Train: 0.726981520652771, Final Score Test: 0.7051914930343628, macro F1 Train: 0.48517632645120023, macro F1 Test: 0.45237138158376405, 1-TPR Gap Train: 0.9687867760658264, 1-TPR Gap Test: 0.9580116271972656\n",
      "Epoch 30, Loss: 0.7416761517524719, Final Score Train: 0.7269797921180725, Final Score Test: 0.704993724822998, macro F1 Train: 0.48567793666468295, macro F1 Test: 0.45292508534384324, 1-TPR Gap Train: 0.968281626701355, 1-TPR Gap Test: 0.9570624232292175\n",
      "Arrêt précoce après 31 époques\n",
      "Final Evaluation Score: 0.7049434185028076 Macro F1: 0.45227942460710774 1-TPR_gap: 0.9576073884963989\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.8687971234321594, Final Score Train: 0.7005125880241394, Final Score Test: 0.6850216388702393, macro F1 Train: 0.43684946638598354, macro F1 Test: 0.41850733352356484, 1-TPR Gap Train: 0.9641757011413574, 1-TPR Gap Test: 0.9515359997749329\n",
      "Epoch 10, Loss: 0.824028491973877, Final Score Train: 0.6964031457901001, Final Score Test: 0.6817259192466736, macro F1 Train: 0.4289198559274309, macro F1 Test: 0.40628031739796, 1-TPR Gap Train: 0.9638864994049072, 1-TPR Gap Test: 0.9571714997291565\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.6930220127105713 Macro F1: 0.4282766950962764 1-TPR_gap: 0.9577673673629761\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.874946117401123, Final Score Train: 0.6945512294769287, Final Score Test: 0.6808537244796753, macro F1 Train: 0.4245751771750087, macro F1 Test: 0.40743273929484053, 1-TPR Gap Train: 0.9645272493362427, 1-TPR Gap Test: 0.9542747139930725\n",
      "Epoch 10, Loss: 0.8102816343307495, Final Score Train: 0.699458122253418, Final Score Test: 0.6880195140838623, macro F1 Train: 0.4390178445505248, macro F1 Test: 0.41566584378459515, 1-TPR Gap Train: 0.9598984122276306, 1-TPR Gap Test: 0.9603732228279114\n",
      "Epoch 20, Loss: 0.7478024959564209, Final Score Train: 0.708229660987854, Final Score Test: 0.6922327280044556, macro F1 Train: 0.45423911260706057, macro F1 Test: 0.4282575888505171, 1-TPR Gap Train: 0.9622201919555664, 1-TPR Gap Test: 0.9562078714370728\n",
      "Epoch 30, Loss: 0.8329950571060181, Final Score Train: 0.709604024887085, Final Score Test: 0.6943352222442627, macro F1 Train: 0.4548532981574489, macro F1 Test: 0.4309598123883624, 1-TPR Gap Train: 0.9643548130989075, 1-TPR Gap Test: 0.9577106237411499\n",
      "Arrêt précoce après 33 époques\n",
      "Final Evaluation Score: 0.690910279750824 Macro F1: 0.42671111887211904 1-TPR_gap: 0.9551094174385071\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.7351633310317993, Final Score Train: 0.7020504474639893, Final Score Test: 0.688761830329895, macro F1 Train: 0.4461642580720141, macro F1 Test: 0.42490694185688965, 1-TPR Gap Train: 0.9579366445541382, 1-TPR Gap Test: 0.9526166915893555\n",
      "Epoch 10, Loss: 0.8161653280258179, Final Score Train: 0.7119131684303284, Final Score Test: 0.6971654295921326, macro F1 Train: 0.46009032691777224, macro F1 Test: 0.44169205624222485, 1-TPR Gap Train: 0.9637359976768494, 1-TPR Gap Test: 0.9526388049125671\n",
      "Epoch 20, Loss: 0.8004897832870483, Final Score Train: 0.7088520526885986, Final Score Test: 0.6935222148895264, macro F1 Train: 0.4536950439220253, macro F1 Test: 0.4291597835440207, 1-TPR Gap Train: 0.9640090465545654, 1-TPR Gap Test: 0.957884669303894\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.6920027732849121 Macro F1: 0.43080812013988334 1-TPR_gap: 0.9531974196434021\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.7371783256530762, Final Score Train: 0.7085385322570801, Final Score Test: 0.6924008131027222, macro F1 Train: 0.4533975041362602, macro F1 Test: 0.42813366041769346, 1-TPR Gap Train: 0.9636796116828918, 1-TPR Gap Test: 0.956667959690094\n",
      "Epoch 10, Loss: 0.7243044972419739, Final Score Train: 0.7148008346557617, Final Score Test: 0.6996139883995056, macro F1 Train: 0.4671260611433327, macro F1 Test: 0.44597475036615014, 1-TPR Gap Train: 0.9624755382537842, 1-TPR Gap Test: 0.9532532691955566\n",
      "Epoch 20, Loss: 0.6869957447052002, Final Score Train: 0.7174259424209595, Final Score Test: 0.7043906450271606, macro F1 Train: 0.4689576763409358, macro F1 Test: 0.44660417098767213, 1-TPR Gap Train: 0.9658941626548767, 1-TPR Gap Test: 0.9621770977973938\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.7062975168228149 Macro F1: 0.45168838837502073 1-TPR_gap: 0.9609066247940063\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7181400060653687, Final Score Train: 0.720281183719635, Final Score Test: 0.7041320204734802, macro F1 Train: 0.4739889215179609, macro F1 Test: 0.4514239861651492, 1-TPR Gap Train: 0.9665734171867371, 1-TPR Gap Test: 0.9568400382995605\n",
      "Epoch 10, Loss: 0.720468282699585, Final Score Train: 0.7224080562591553, Final Score Test: 0.7060521841049194, macro F1 Train: 0.47874092061724877, macro F1 Test: 0.4546216678776572, 1-TPR Gap Train: 0.9660751819610596, 1-TPR Gap Test: 0.9574826955795288\n",
      "Epoch 20, Loss: 0.7216082811355591, Final Score Train: 0.7229398488998413, Final Score Test: 0.7036968469619751, macro F1 Train: 0.47878813148090826, macro F1 Test: 0.4503884014588708, 1-TPR Gap Train: 0.9670915007591248, 1-TPR Gap Test: 0.9570052623748779\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7049720883369446 Macro F1: 0.45201824137671887 1-TPR_gap: 0.9579259753227234\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7259839773178101, Final Score Train: 0.7232975363731384, Final Score Test: 0.7062009572982788, macro F1 Train: 0.48060514238306595, macro F1 Test: 0.45406334995438524, 1-TPR Gap Train: 0.965989887714386, 1-TPR Gap Test: 0.9583386182785034\n",
      "Epoch 10, Loss: 0.7373715043067932, Final Score Train: 0.7240340113639832, Final Score Test: 0.7058207392692566, macro F1 Train: 0.4812639214498898, macro F1 Test: 0.4537857063064341, 1-TPR Gap Train: 0.9668040871620178, 1-TPR Gap Test: 0.957855761051178\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7063705921173096 Macro F1: 0.4543637940635717 1-TPR_gap: 0.9583773612976074\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.725938618183136, Final Score Train: 0.7239788770675659, Final Score Test: 0.7059528827667236, macro F1 Train: 0.4811160075777009, macro F1 Test: 0.45432691090966176, 1-TPR Gap Train: 0.9668418169021606, 1-TPR Gap Test: 0.9575788378715515\n",
      "Epoch 10, Loss: 0.7492651343345642, Final Score Train: 0.7241902351379395, Final Score Test: 0.7042955160140991, macro F1 Train: 0.48279689565189304, macro F1 Test: 0.4518716982596409, 1-TPR Gap Train: 0.9655835032463074, 1-TPR Gap Test: 0.9567192792892456\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7039287090301514 Macro F1: 0.4516748524292398 1-TPR_gap: 0.956182599067688\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.7855237722396851, Final Score Train: 0.7033472657203674, Final Score Test: 0.6859050393104553, macro F1 Train: 0.44273108333095257, macro F1 Test: 0.4151418465761721, 1-TPR Gap Train: 0.9639634490013123, 1-TPR Gap Test: 0.9566682577133179\n",
      "Epoch 10, Loss: 0.816666841506958, Final Score Train: 0.6986594200134277, Final Score Test: 0.6874592304229736, macro F1 Train: 0.44004081536552153, macro F1 Test: 0.4230658371824362, 1-TPR Gap Train: 0.9572779536247253, 1-TPR Gap Test: 0.951852560043335\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6862998008728027 Macro F1: 0.4185997812207764 1-TPR_gap: 0.9539998173713684\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.8964647054672241, Final Score Train: 0.7133389115333557, Final Score Test: 0.6969366073608398, macro F1 Train: 0.46585479592215406, macro F1 Test: 0.44266277632153744, 1-TPR Gap Train: 0.9608230590820312, 1-TPR Gap Test: 0.9512104392051697\n",
      "Epoch 10, Loss: 0.8811275959014893, Final Score Train: 0.7026793360710144, Final Score Test: 0.6828858256340027, macro F1 Train: 0.44595190794322964, macro F1 Test: 0.42087919179457767, 1-TPR Gap Train: 0.9594067931175232, 1-TPR Gap Test: 0.9448924660682678\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.6782519221305847 Macro F1: 0.40770726233991067 1-TPR_gap: 0.9487965703010559\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.7288872599601746, Final Score Train: 0.706024706363678, Final Score Test: 0.6881056427955627, macro F1 Train: 0.4507228133164583, macro F1 Test: 0.4280879941062744, 1-TPR Gap Train: 0.9613265991210938, 1-TPR Gap Test: 0.9481232762336731\n",
      "Epoch 10, Loss: 0.659028172492981, Final Score Train: 0.7056615948677063, Final Score Test: 0.6873959898948669, macro F1 Train: 0.445261041778554, macro F1 Test: 0.4234707599611366, 1-TPR Gap Train: 0.9660621881484985, 1-TPR Gap Test: 0.9513211846351624\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.689608097076416 Macro F1: 0.4309648640922304 1-TPR_gap: 0.9482513666152954\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.75059974193573, Final Score Train: 0.7133105993270874, Final Score Test: 0.6958926916122437, macro F1 Train: 0.46349918426277714, macro F1 Test: 0.4394329875844484, 1-TPR Gap Train: 0.9631219506263733, 1-TPR Gap Test: 0.9523524641990662\n",
      "Epoch 10, Loss: 0.7119119763374329, Final Score Train: 0.7177167534828186, Final Score Test: 0.6984128355979919, macro F1 Train: 0.47208692499519084, macro F1 Test: 0.4465669161387247, 1-TPR Gap Train: 0.9633466005325317, 1-TPR Gap Test: 0.9502587914466858\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.6980171799659729 Macro F1: 0.43946087861997674 1-TPR_gap: 0.956573486328125\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7421231269836426, Final Score Train: 0.7150170803070068, Final Score Test: 0.6986780762672424, macro F1 Train: 0.4663549210753061, macro F1 Test: 0.44028545437666883, 1-TPR Gap Train: 0.963679313659668, 1-TPR Gap Test: 0.9570707082748413\n",
      "Epoch 10, Loss: 0.7231157422065735, Final Score Train: 0.7200496792793274, Final Score Test: 0.7028279304504395, macro F1 Train: 0.4784026579905731, macro F1 Test: 0.4524557209544972, 1-TPR Gap Train: 0.9616967439651489, 1-TPR Gap Test: 0.953200101852417\n",
      "Epoch 20, Loss: 0.7162647247314453, Final Score Train: 0.7219634652137756, Final Score Test: 0.7036148309707642, macro F1 Train: 0.4797675845906154, macro F1 Test: 0.4517388487478309, 1-TPR Gap Train: 0.9641593098640442, 1-TPR Gap Test: 0.9554908871650696\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.706537127494812 Macro F1: 0.4541560847114821 1-TPR_gap: 0.9589182138442993\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7277300357818604, Final Score Train: 0.7214097380638123, Final Score Test: 0.7063672542572021, macro F1 Train: 0.48093702801799093, macro F1 Test: 0.4545585376184866, 1-TPR Gap Train: 0.961882472038269, 1-TPR Gap Test: 0.9581758975982666\n",
      "Epoch 10, Loss: 0.7365180253982544, Final Score Train: 0.7230907678604126, Final Score Test: 0.7059561014175415, macro F1 Train: 0.4836280700334137, macro F1 Test: 0.45497388157319135, 1-TPR Gap Train: 0.9625535011291504, 1-TPR Gap Test: 0.9569383859634399\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7042970657348633 Macro F1: 0.4542580938392412 1-TPR_gap: 0.9543360471725464\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7263470888137817, Final Score Train: 0.7236628532409668, Final Score Test: 0.7041230201721191, macro F1 Train: 0.48526948447220075, macro F1 Test: 0.454361861336169, 1-TPR Gap Train: 0.9620562791824341, 1-TPR Gap Test: 0.9538841247558594\n",
      "Epoch 10, Loss: 0.7174222469329834, Final Score Train: 0.7242567539215088, Final Score Test: 0.7033662796020508, macro F1 Train: 0.48560459756723223, macro F1 Test: 0.45439634915484495, 1-TPR Gap Train: 0.962908923625946, 1-TPR Gap Test: 0.9523361921310425\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7049111723899841 Macro F1: 0.4548044595911059 1-TPR_gap: 0.9550179243087769\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.8983844518661499, Final Score Train: 0.6713408827781677, Final Score Test: 0.6578554511070251, macro F1 Train: 0.379751073464862, macro F1 Test: 0.36446334729566315, 1-TPR Gap Train: 0.9629306793212891, 1-TPR Gap Test: 0.95124751329422\n",
      "Epoch 10, Loss: 0.8772895932197571, Final Score Train: 0.696969211101532, Final Score Test: 0.6813440322875977, macro F1 Train: 0.42866321312534594, macro F1 Test: 0.41071974407163603, 1-TPR Gap Train: 0.965275228023529, 1-TPR Gap Test: 0.9519683718681335\n",
      "Epoch 20, Loss: 0.884081244468689, Final Score Train: 0.6915457844734192, Final Score Test: 0.6792922019958496, macro F1 Train: 0.43078128683135747, macro F1 Test: 0.41349105538950603, 1-TPR Gap Train: 0.9523102641105652, 1-TPR Gap Test: 0.9450932741165161\n",
      "Arrêt précoce après 22 époques\n",
      "Final Evaluation Score: 0.682266354560852 Macro F1: 0.4172921926982401 1-TPR_gap: 0.9472404718399048\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.8462018966674805, Final Score Train: 0.7030628323554993, Final Score Test: 0.6876987218856812, macro F1 Train: 0.44136418382265735, macro F1 Test: 0.4250873191478857, 1-TPR Gap Train: 0.96476149559021, 1-TPR Gap Test: 0.9503101706504822\n",
      "Epoch 10, Loss: 0.918254017829895, Final Score Train: 0.7092113494873047, Final Score Test: 0.6936073303222656, macro F1 Train: 0.45472930291110314, macro F1 Test: 0.4353624799308978, 1-TPR Gap Train: 0.963693380355835, 1-TPR Gap Test: 0.9518521428108215\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.6956223249435425 Macro F1: 0.43995399586502043 1-TPR_gap: 0.9512906670570374\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.729412317276001, Final Score Train: 0.7164044380187988, Final Score Test: 0.6959258913993835, macro F1 Train: 0.46566922317059906, macro F1 Test: 0.4375543827849664, 1-TPR Gap Train: 0.9671396613121033, 1-TPR Gap Test: 0.9542974233627319\n",
      "Epoch 10, Loss: 0.7540403604507446, Final Score Train: 0.7227203845977783, Final Score Test: 0.7038886547088623, macro F1 Train: 0.4715796381035394, macro F1 Test: 0.4482167191710275, 1-TPR Gap Train: 0.9738611578941345, 1-TPR Gap Test: 0.9595606327056885\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7030009627342224 Macro F1: 0.4428913782110145 1-TPR_gap: 0.9631105661392212\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.716865062713623, Final Score Train: 0.7238085269927979, Final Score Test: 0.7062466144561768, macro F1 Train: 0.474130157155351, macro F1 Test: 0.4477989955453821, 1-TPR Gap Train: 0.9734868407249451, 1-TPR Gap Test: 0.9646942019462585\n",
      "Epoch 10, Loss: 0.7021962404251099, Final Score Train: 0.7225556969642639, Final Score Test: 0.702994704246521, macro F1 Train: 0.47394370992079743, macro F1 Test: 0.4481588557698641, 1-TPR Gap Train: 0.9711676836013794, 1-TPR Gap Test: 0.9578304886817932\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7016722559928894 Macro F1: 0.44491998059994653 1-TPR_gap: 0.9584245681762695\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7332926988601685, Final Score Train: 0.7211476564407349, Final Score Test: 0.701377272605896, macro F1 Train: 0.4698698954707841, macro F1 Test: 0.44349573069301557, 1-TPR Gap Train: 0.9724254608154297, 1-TPR Gap Test: 0.9592587947845459\n",
      "Epoch 10, Loss: 0.7267805337905884, Final Score Train: 0.7242938280105591, Final Score Test: 0.7082090377807617, macro F1 Train: 0.4770900844861221, macro F1 Test: 0.45029848573208453, 1-TPR Gap Train: 0.9714975357055664, 1-TPR Gap Test: 0.9661195278167725\n",
      "Epoch 20, Loss: 0.7434921264648438, Final Score Train: 0.726459264755249, Final Score Test: 0.7070955634117126, macro F1 Train: 0.4828392793464806, macro F1 Test: 0.4518370215010669, 1-TPR Gap Train: 0.970079243183136, 1-TPR Gap Test: 0.9623541235923767\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7056649923324585 Macro F1: 0.44935233406475245 1-TPR_gap: 0.9619777202606201\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7400025129318237, Final Score Train: 0.7246959805488586, Final Score Test: 0.7053712606430054, macro F1 Train: 0.4797147238901064, macro F1 Test: 0.4492711484430806, 1-TPR Gap Train: 0.969677209854126, 1-TPR Gap Test: 0.961471438407898\n",
      "Epoch 10, Loss: 0.7294131517410278, Final Score Train: 0.7266132235527039, Final Score Test: 0.7074934840202332, macro F1 Train: 0.4840280235281625, macro F1 Test: 0.4540504842403528, 1-TPR Gap Train: 0.96919846534729, 1-TPR Gap Test: 0.9609364867210388\n",
      "Epoch 20, Loss: 0.7444946765899658, Final Score Train: 0.7271969318389893, Final Score Test: 0.7076878547668457, macro F1 Train: 0.48424921158873635, macro F1 Test: 0.45441136839609614, 1-TPR Gap Train: 0.9701446890830994, 1-TPR Gap Test: 0.9609643220901489\n",
      "Epoch 30, Loss: 0.736989438533783, Final Score Train: 0.7273366451263428, Final Score Test: 0.7063430547714233, macro F1 Train: 0.4860662397668281, macro F1 Test: 0.4539214346899601, 1-TPR Gap Train: 0.9686070084571838, 1-TPR Gap Test: 0.9587646126747131\n",
      "Arrêt précoce après 35 époques\n",
      "Final Evaluation Score: 0.7053438425064087 Macro F1: 0.4526998568712897 1-TPR_gap: 0.9579878449440002\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7400965690612793, Final Score Train: 0.7283167839050293, Final Score Test: 0.7054775357246399, macro F1 Train: 0.4865695629923419, macro F1 Test: 0.45308039090015423, 1-TPR Gap Train: 0.9700639843940735, 1-TPR Gap Test: 0.9578747153282166\n",
      "Epoch 10, Loss: 0.7390457391738892, Final Score Train: 0.7289948463439941, Final Score Test: 0.7056930065155029, macro F1 Train: 0.48739976193973567, macro F1 Test: 0.4532585775272365, 1-TPR Gap Train: 0.9705899953842163, 1-TPR Gap Test: 0.958127498626709\n",
      "Epoch 20, Loss: 0.7338263988494873, Final Score Train: 0.7291027903556824, Final Score Test: 0.7053049206733704, macro F1 Train: 0.4880629640438499, macro F1 Test: 0.4524038543612413, 1-TPR Gap Train: 0.9701426029205322, 1-TPR Gap Test: 0.9582059979438782\n",
      "Arrêt précoce après 26 époques\n",
      "Final Evaluation Score: 0.7056281566619873 Macro F1: 0.45309236373981143 1-TPR_gap: 0.9581639766693115\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.878942608833313, Final Score Train: 0.7075294256210327, Final Score Test: 0.688839852809906, macro F1 Train: 0.44578441253205653, macro F1 Test: 0.4196071611375395, 1-TPR Gap Train: 0.9692744016647339, 1-TPR Gap Test: 0.9580725431442261\n",
      "Epoch 10, Loss: 0.7257656455039978, Final Score Train: 0.7152199149131775, Final Score Test: 0.6977362632751465, macro F1 Train: 0.46395065969692334, macro F1 Test: 0.4412669099756808, 1-TPR Gap Train: 0.9664891958236694, 1-TPR Gap Test: 0.9542056322097778\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6674479246139526 Macro F1: 0.3820092137308005 1-TPR_gap: 0.952886700630188\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.8688312768936157, Final Score Train: 0.7007142901420593, Final Score Test: 0.6839715242385864, macro F1 Train: 0.4387806494631944, macro F1 Test: 0.41750276399887015, 1-TPR Gap Train: 0.9626479148864746, 1-TPR Gap Test: 0.9504402279853821\n",
      "Epoch 10, Loss: 0.9422622919082642, Final Score Train: 0.7041578888893127, Final Score Test: 0.6879572868347168, macro F1 Train: 0.4477020332921769, macro F1 Test: 0.4261292794336251, 1-TPR Gap Train: 0.9606137871742249, 1-TPR Gap Test: 0.9497853517532349\n",
      "Epoch 20, Loss: 0.8029404282569885, Final Score Train: 0.6987730264663696, Final Score Test: 0.6818004846572876, macro F1 Train: 0.4367017439797634, macro F1 Test: 0.409402476794678, 1-TPR Gap Train: 0.9608442783355713, 1-TPR Gap Test: 0.95419842004776\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.6813134551048279 Macro F1: 0.40654073506825517 1-TPR_gap: 0.9560862183570862\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.7548691034317017, Final Score Train: 0.6986743807792664, Final Score Test: 0.6802757978439331, macro F1 Train: 0.4408431994329815, macro F1 Test: 0.4129967468515896, 1-TPR Gap Train: 0.956505537033081, 1-TPR Gap Test: 0.9475548267364502\n",
      "Epoch 10, Loss: 0.6797808408737183, Final Score Train: 0.7135007977485657, Final Score Test: 0.694771945476532, macro F1 Train: 0.46496168919308095, macro F1 Test: 0.44011734435687594, 1-TPR Gap Train: 0.9620398879051208, 1-TPR Gap Test: 0.949426531791687\n",
      "Epoch 20, Loss: 0.7533867955207825, Final Score Train: 0.7173502445220947, Final Score Test: 0.6997988224029541, macro F1 Train: 0.46787213470244365, macro F1 Test: 0.4379234007618776, 1-TPR Gap Train: 0.9668282866477966, 1-TPR Gap Test: 0.9616742730140686\n",
      "Arrêt précoce après 28 époques\n",
      "Final Evaluation Score: 0.6980993151664734 Macro F1: 0.4415510606386445 1-TPR_gap: 0.9546475410461426\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.6976606845855713, Final Score Train: 0.7172286510467529, Final Score Test: 0.7020277380943298, macro F1 Train: 0.4698227110996146, macro F1 Test: 0.44597064937543834, 1-TPR Gap Train: 0.9646345376968384, 1-TPR Gap Test: 0.9580848217010498\n",
      "Epoch 10, Loss: 0.7349712252616882, Final Score Train: 0.7197108268737793, Final Score Test: 0.7015699148178101, macro F1 Train: 0.4734715104063469, macro F1 Test: 0.4490289990775792, 1-TPR Gap Train: 0.9659501314163208, 1-TPR Gap Test: 0.954110860824585\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7000201344490051 Macro F1: 0.44432559083670997 1-TPR_gap: 0.9557146430015564\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7212713956832886, Final Score Train: 0.7192392349243164, Final Score Test: 0.7009305953979492, macro F1 Train: 0.4747491866248959, macro F1 Test: 0.445094688596644, 1-TPR Gap Train: 0.9637292623519897, 1-TPR Gap Test: 0.9567665457725525\n",
      "Epoch 10, Loss: 0.7282568216323853, Final Score Train: 0.724978506565094, Final Score Test: 0.705557107925415, macro F1 Train: 0.4831696579122185, macro F1 Test: 0.4547371008788167, 1-TPR Gap Train: 0.9667873978614807, 1-TPR Gap Test: 0.9563771486282349\n",
      "Epoch 20, Loss: 0.7302603721618652, Final Score Train: 0.7262388467788696, Final Score Test: 0.7047731280326843, macro F1 Train: 0.48486024799888033, macro F1 Test: 0.45062348006819314, 1-TPR Gap Train: 0.9676175117492676, 1-TPR Gap Test: 0.9589227437973022\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7056227326393127 Macro F1: 0.45073773328825056 1-TPR_gap: 0.9605077505111694\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7367768883705139, Final Score Train: 0.7255798578262329, Final Score Test: 0.7050007581710815, macro F1 Train: 0.4832206694855727, macro F1 Test: 0.4501256672288491, 1-TPR Gap Train: 0.9679390788078308, 1-TPR Gap Test: 0.9598758816719055\n",
      "Epoch 10, Loss: 0.7354114055633545, Final Score Train: 0.7275893092155457, Final Score Test: 0.7063947319984436, macro F1 Train: 0.48636909834176484, macro F1 Test: 0.4528699730279238, 1-TPR Gap Train: 0.9688095450401306, 1-TPR Gap Test: 0.9599194526672363\n",
      "Epoch 20, Loss: 0.732371985912323, Final Score Train: 0.7279072999954224, Final Score Test: 0.7054260969161987, macro F1 Train: 0.4879180835100924, macro F1 Test: 0.4522848163560558, 1-TPR Gap Train: 0.9678965210914612, 1-TPR Gap Test: 0.9585673809051514\n",
      "Arrêt précoce après 24 époques\n",
      "Final Evaluation Score: 0.7061417102813721 Macro F1: 0.4542592105545226 1-TPR_gap: 0.9580241441726685\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7278861403465271, Final Score Train: 0.728557825088501, Final Score Test: 0.7064313888549805, macro F1 Train: 0.4887249280722084, macro F1 Test: 0.4545303529708062, 1-TPR Gap Train: 0.968390703201294, 1-TPR Gap Test: 0.9583324193954468\n",
      "Epoch 10, Loss: 0.739508867263794, Final Score Train: 0.729289710521698, Final Score Test: 0.705971896648407, macro F1 Train: 0.4897361882946468, macro F1 Test: 0.45370990975035835, 1-TPR Gap Train: 0.9688432216644287, 1-TPR Gap Test: 0.9582338929176331\n",
      "Epoch 20, Loss: 0.732986569404602, Final Score Train: 0.729213535785675, Final Score Test: 0.7055087089538574, macro F1 Train: 0.49003927731544666, macro F1 Test: 0.45334545326980547, 1-TPR Gap Train: 0.9683877825737, 1-TPR Gap Test: 0.9576719403266907\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7060162425041199 Macro F1: 0.4536597390816706 1-TPR_gap: 0.9583727717399597\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.846031904220581, Final Score Train: 0.7293732762336731, Final Score Test: 0.7052103281021118, macro F1 Train: 0.4899411599330335, macro F1 Test: 0.4535238178616466, 1-TPR Gap Train: 0.9688054323196411, 1-TPR Gap Test: 0.9568967819213867\n",
      "Epoch 10, Loss: 0.8779762983322144, Final Score Train: 0.7293546199798584, Final Score Test: 0.7065062522888184, macro F1 Train: 0.49033517658064557, macro F1 Test: 0.45468010817968013, 1-TPR Gap Train: 0.9683740139007568, 1-TPR Gap Test: 0.9583324193954468\n",
      "Epoch 20, Loss: 0.7585320472717285, Final Score Train: 0.7294949889183044, Final Score Test: 0.7060359716415405, macro F1 Train: 0.4908454591888036, macro F1 Test: 0.45503474650188286, 1-TPR Gap Train: 0.9681445360183716, 1-TPR Gap Test: 0.9570371508598328\n",
      "Epoch 30, Loss: 0.8246033191680908, Final Score Train: 0.7295089364051819, Final Score Test: 0.7060487270355225, macro F1 Train: 0.4909524423846703, macro F1 Test: 0.45518851588110026, 1-TPR Gap Train: 0.9680654406547546, 1-TPR Gap Test: 0.9569090008735657\n",
      "Arrêt précoce après 35 époques\n",
      "Final Evaluation Score: 0.7060487270355225 Macro F1: 0.45518851588110026 1-TPR_gap: 0.9569090008735657\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.873041033744812, Final Score Train: 0.7296258211135864, Final Score Test: 0.7060487270355225, macro F1 Train: 0.4910373033952405, macro F1 Test: 0.45518851588110026, 1-TPR Gap Train: 0.9682142734527588, 1-TPR Gap Test: 0.9569090008735657\n",
      "Epoch 10, Loss: 0.7688526511192322, Final Score Train: 0.7296258211135864, Final Score Test: 0.7060487270355225, macro F1 Train: 0.4910373033952405, macro F1 Test: 0.45518851588110026, 1-TPR Gap Train: 0.9682142734527588, 1-TPR Gap Test: 0.9569090008735657\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7060487270355225 Macro F1: 0.45518851588110026 1-TPR_gap: 0.9569090008735657\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7699956297874451, Final Score Train: 0.7296258211135864, Final Score Test: 0.7060487270355225, macro F1 Train: 0.4910373033952405, macro F1 Test: 0.45518851588110026, 1-TPR Gap Train: 0.9682142734527588, 1-TPR Gap Test: 0.9569090008735657\n",
      "Epoch 10, Loss: 0.74004065990448, Final Score Train: 0.729636549949646, Final Score Test: 0.7060562968254089, macro F1 Train: 0.49105289876593583, macro F1 Test: 0.4552035495696474, 1-TPR Gap Train: 0.9682201743125916, 1-TPR Gap Test: 0.9569090008735657\n",
      "Epoch 20, Loss: 0.645039975643158, Final Score Train: 0.729753315448761, Final Score Test: 0.7059755325317383, macro F1 Train: 0.49115112224598184, macro F1 Test: 0.4551836995514916, 1-TPR Gap Train: 0.9683555364608765, 1-TPR Gap Test: 0.9567673802375793\n",
      "Epoch 30, Loss: 0.7560657262802124, Final Score Train: 0.7297651767730713, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49119544275168375, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 34 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.7658965587615967, Final Score Train: 0.7297651767730713, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49119544275168375, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.7370656728744507, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.7268136739730835, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.7046884298324585, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 20, Loss: 0.7142587900161743, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 24 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.726230263710022, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.7272903323173523, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.7186232805252075, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.7393841743469238, Final Score Train: 0.7297736406326294, Final Score Test: 0.7059302926063538, macro F1 Train: 0.49121243640524054, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.7758872509002686, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.795706033706665, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.7615082263946533, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.8364797830581665, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 20, Loss: 0.9016776084899902, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Arrêt précoce après 21 époques\n",
      "Final Evaluation Score: 0.7059302926063538 Macro F1: 0.45514884606324596 1-TPR_gap: 0.9567117691040039\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.7133930921554565, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 10, Loss: 0.6506527662277222, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 20, Loss: 0.7144078016281128, Final Score Train: 0.7297685742378235, Final Score Test: 0.7059302926063538, macro F1 Train: 0.4912022729518735, macro F1 Test: 0.45514884606324596, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9567117691040039\n",
      "Epoch 30, Loss: 0.7154884338378906, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059048414230347, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498563190718766, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Arrêt précoce après 37 époques\n",
      "Final Evaluation Score: 0.7059048414230347 Macro F1: 0.45498563190718766 1-TPR_gap: 0.9568240642547607\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.6716130971908569, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059048414230347, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498563190718766, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 10, Loss: 0.7528818249702454, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059048414230347, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498563190718766, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 20, Loss: 0.6652013063430786, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059048414230347, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498563190718766, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 30, Loss: 0.7085793018341064, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 40, Loss: 0.7083251476287842, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 50, Loss: 0.6995809674263, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 60, Loss: 0.7194068431854248, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 70, Loss: 0.6972372531890869, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 80, Loss: 0.7031211256980896, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 90, Loss: 0.6939455270767212, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 100, Loss: 0.6623746752738953, Final Score Train: 0.7297993302345276, Final Score Test: 0.7059024572372437, macro F1 Train: 0.49124846298503877, macro F1 Test: 0.45498089201711317, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9568240642547607\n",
      "Epoch 110, Loss: 0.7029153108596802, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Arrêt précoce après 111 époques\n",
      "Final Evaluation Score: 0.706512451171875 Macro F1: 0.45558612607948284 1-TPR_gap: 0.9574387073516846\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7288252115249634, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 10, Loss: 0.7144739031791687, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 20, Loss: 0.6998273134231567, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 30, Loss: 0.7191789746284485, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 40, Loss: 0.7388824224472046, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 50, Loss: 0.7228860855102539, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 60, Loss: 0.7269846200942993, Final Score Train: 0.7298123836517334, Final Score Test: 0.706512451171875, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.45558612607948284, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574387073516846\n",
      "Epoch 70, Loss: 0.7076688408851624, Final Score Train: 0.7298123836517334, Final Score Test: 0.7065068483352661, macro F1 Train: 0.49127465589472996, macro F1 Test: 0.4556306223206902, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573830366134644\n",
      "Epoch 80, Loss: 0.7245571613311768, Final Score Train: 0.7298175096511841, Final Score Test: 0.7065068483352661, macro F1 Train: 0.4912848926488253, macro F1 Test: 0.4556306223206902, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573830366134644\n",
      "Epoch 90, Loss: 0.7394448518753052, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Arrêt précoce après 91 époques\n",
      "Final Evaluation Score: 0.7063878774642944 Macro F1: 0.455412105070875 1-TPR_gap: 0.9573636651039124\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7367141246795654, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Epoch 10, Loss: 0.7311038970947266, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7063878774642944 Macro F1: 0.455412105070875 1-TPR_gap: 0.9573636651039124\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.7276911735534668, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Epoch 10, Loss: 0.7318246364593506, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7063878774642944 Macro F1: 0.455412105070875 1-TPR_gap: 0.9573636651039124\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.8412416577339172, Final Score Train: 0.7298226356506348, Final Score Test: 0.7063878774642944, macro F1 Train: 0.4912951550681516, macro F1 Test: 0.455412105070875, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9573636651039124\n",
      "Epoch 10, Loss: 0.7978711128234863, Final Score Train: 0.7298175096511841, Final Score Test: 0.7064023017883301, macro F1 Train: 0.4912848926488253, macro F1 Test: 0.455340651993012, 1-TPR Gap Train: 0.9683501720428467, 1-TPR Gap Test: 0.9574639797210693\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7064624428749084 Macro F1: 0.4555090230574693 1-TPR_gap: 0.9574158787727356\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8390501141548157, Final Score Train: 0.7297816872596741, Final Score Test: 0.7064624428749084, macro F1 Train: 0.49122846586156477, macro F1 Test: 0.4555090230574693, 1-TPR Gap Train: 0.9683349132537842, 1-TPR Gap Test: 0.9574158787727356\n",
      "Epoch 10, Loss: 0.7972163558006287, Final Score Train: 0.7298244833946228, Final Score Test: 0.7064589858055115, macro F1 Train: 0.4913037085322297, macro F1 Test: 0.45543113715521766, 1-TPR Gap Train: 0.9683452248573303, 1-TPR Gap Test: 0.9574868679046631\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7064045071601868 Macro F1: 0.455213218364475 1-TPR_gap: 0.9575957655906677\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7245249152183533, Final Score Train: 0.729826807975769, Final Score Test: 0.7064045071601868, macro F1 Train: 0.4913054837070124, macro F1 Test: 0.455213218364475, 1-TPR Gap Train: 0.9683482050895691, 1-TPR Gap Test: 0.9575957655906677\n",
      "Epoch 10, Loss: 0.8080723285675049, Final Score Train: 0.7298353910446167, Final Score Test: 0.7064524292945862, macro F1 Train: 0.491322565657477, macro F1 Test: 0.45530569857661557, 1-TPR Gap Train: 0.9683482050895691, 1-TPR Gap Test: 0.9575991630554199\n",
      "Epoch 20, Loss: 0.7369718551635742, Final Score Train: 0.7298647165298462, Final Score Test: 0.7068006992340088, macro F1 Train: 0.4913753304644905, macro F1 Test: 0.45560176784377165, 1-TPR Gap Train: 0.9683541655540466, 1-TPR Gap Test: 0.9579996466636658\n",
      "Epoch 30, Loss: 0.7219716310501099, Final Score Train: 0.72987300157547, Final Score Test: 0.7072855830192566, macro F1 Train: 0.49139182286503946, macro F1 Test: 0.45582744036676653, 1-TPR Gap Train: 0.9683541655540466, 1-TPR Gap Test: 0.958743691444397\n",
      "Arrêt précoce après 34 époques\n",
      "Final Evaluation Score: 0.7072855830192566 Macro F1: 0.45582744036676653 1-TPR_gap: 0.958743691444397\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.703239917755127, Final Score Train: 0.72987300157547, Final Score Test: 0.7072855830192566, macro F1 Train: 0.49139182286503946, macro F1 Test: 0.45582744036676653, 1-TPR Gap Train: 0.9683541655540466, 1-TPR Gap Test: 0.958743691444397\n",
      "Epoch 10, Loss: 0.6833095550537109, Final Score Train: 0.7298763990402222, Final Score Test: 0.7072855830192566, macro F1 Train: 0.49139870504171373, macro F1 Test: 0.45582744036676653, 1-TPR Gap Train: 0.9683541655540466, 1-TPR Gap Test: 0.958743691444397\n",
      "Epoch 20, Loss: 0.7226437330245972, Final Score Train: 0.7298847436904907, Final Score Test: 0.7070152759552002, macro F1 Train: 0.49141526807110736, macro F1 Test: 0.4555659290810235, 1-TPR Gap Train: 0.9683541655540466, 1-TPR Gap Test: 0.9584646821022034\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.7070474624633789 Macro F1: 0.45570251629988123 1-TPR_gap: 0.9583923816680908\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.7295751571655273, Final Score Train: 0.7299667596817017, Final Score Test: 0.7070474624633789, macro F1 Train: 0.49154095634656986, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Epoch 10, Loss: 0.716538667678833, Final Score Train: 0.7299718856811523, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4915513518885489, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Epoch 20, Loss: 0.7213577032089233, Final Score Train: 0.7299718856811523, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4915513518885489, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7070474624633789 Macro F1: 0.45570251629988123 1-TPR_gap: 0.9583923816680908\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7336465716362, Final Score Train: 0.7299718856811523, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4915513518885489, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Epoch 10, Loss: 0.7364359498023987, Final Score Train: 0.729983925819397, Final Score Test: 0.7070474624633789, macro F1 Train: 0.49157539053344895, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7070474624633789 Macro F1: 0.45570251629988123 1-TPR_gap: 0.9583923816680908\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.7300997972488403, Final Score Train: 0.7300069332122803, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4916213671420188, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Epoch 10, Loss: 0.7362352609634399, Final Score Train: 0.7300069332122803, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4916213671420188, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7070474624633789 Macro F1: 0.45570251629988123 1-TPR_gap: 0.9583923816680908\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.7546629905700684, Final Score Train: 0.7300056219100952, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4916186955056503, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Epoch 10, Loss: 0.8119474649429321, Final Score Train: 0.7300056219100952, Final Score Test: 0.7070474624633789, macro F1 Train: 0.4916186955056503, macro F1 Test: 0.45570251629988123, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583923816680908\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7070474624633789 Macro F1: 0.45570251629988123 1-TPR_gap: 0.9583923816680908\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.7982144951820374, Final Score Train: 0.7300058603286743, Final Score Test: 0.706936240196228, macro F1 Train: 0.49161926386341753, macro F1 Test: 0.45549950195717653, 1-TPR Gap Train: 0.9683924913406372, 1-TPR Gap Test: 0.9583730101585388\n",
      "Epoch 10, Loss: 0.7708662748336792, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.7821516990661621, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7975006699562073, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.6791908740997314, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.6792027950286865, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7254283428192139, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7161953449249268, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7305668592453003, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7409277558326721, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7211781740188599, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7271891832351685, Final Score Train: 0.7300044298171997, Final Score Test: 0.7068662643432617, macro F1 Train: 0.491717272507452, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.759053111076355, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.8007171750068665, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.7829615473747253, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 24 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.8563188314437866, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7424453496932983, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.7507104873657227, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7922059297561646, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.6790696382522583, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7718504071235657, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7313122153282166, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.736957311630249, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.7068179249763489, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.7292417883872986, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 40, Loss: 0.7073325514793396, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 50, Loss: 0.7243227362632751, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 60, Loss: 0.7278505563735962, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 70, Loss: 0.7220991849899292, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 80, Loss: 0.7214707732200623, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 90, Loss: 0.7143570780754089, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 100, Loss: 0.7305430173873901, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 110, Loss: 0.7194443941116333, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 120, Loss: 0.7271653413772583, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 130, Loss: 0.7222627401351929, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 140, Loss: 0.7225689888000488, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 150, Loss: 0.7406803965568542, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 160, Loss: 0.7118454575538635, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 170, Loss: 0.7171164751052856, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 180, Loss: 0.7243343591690063, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 190, Loss: 0.7184334993362427, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 200, Loss: 0.7219382524490356, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 210, Loss: 0.7367003560066223, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 220, Loss: 0.7329088449478149, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 230, Loss: 0.7268603444099426, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 240, Loss: 0.7261286973953247, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 250, Loss: 0.7111126184463501, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 260, Loss: 0.7234148979187012, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 263 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7301377654075623, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7309446930885315, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7245794534683228, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7218371629714966, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.7068662643432617 Macro F1: 0.4554659632225171 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.812946617603302, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.8719156980514526, Final Score Train: 0.7300030589103699, Final Score Test: 0.7068662643432617, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4554659632225171, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.8062897324562073, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.8176031112670898, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 33 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.8558481335639954, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.8175410032272339, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.7207562327384949, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.6958543062210083, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.7218515276908875, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7146195769309998, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7264481782913208, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7229502201080322, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7241406440734863, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7281181812286377, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7294502258300781, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7377532720565796, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.8122296333312988, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.8392444849014282, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.739163875579834, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 27 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.8666210174560547, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7929925918579102, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.7620900869369507, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7779914140701294, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.7568743228912354, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7091488838195801, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7179530262947083, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7410407066345215, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.7101197838783264, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.7063500285148621, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 40, Loss: 0.7254195213317871, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 50, Loss: 0.7189054489135742, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 58 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7303705215454102, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7313296794891357, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.7304343581199646, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.735542893409729, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 31 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.739619255065918, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7329237461090088, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_28\n",
      "Epoch 1, Loss: 0.8258930444717407, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7990648746490479, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.68724524974823, Final Score Train: 0.7300030589103699, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49171457447344885, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682915210723877, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.7727081775665283, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Arrêt précoce après 31 époques\n",
      "Final Evaluation Score: 0.7067049145698547 Macro F1: 0.4554178943563606 1-TPR_gap: 0.957991898059845\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_56\n",
      "Epoch 1, Loss: 0.8409633636474609, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Epoch 10, Loss: 0.8141158223152161, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7067049145698547 Macro F1: 0.4554178943563606 1-TPR_gap: 0.957991898059845\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_128\n",
      "Epoch 1, Loss: 0.7326197624206543, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Epoch 10, Loss: 0.7528389692306519, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7067049145698547 Macro F1: 0.4554178943563606 1-TPR_gap: 0.957991898059845\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_256\n",
      "Epoch 1, Loss: 0.6835188865661621, Final Score Train: 0.7300006151199341, Final Score Test: 0.7067049145698547, macro F1 Train: 0.4917283810291293, macro F1 Test: 0.4554178943563606, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.957991898059845\n",
      "Epoch 10, Loss: 0.6864069700241089, Final Score Train: 0.7300040125846863, Final Score Test: 0.7069263458251953, macro F1 Train: 0.49173524895332343, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 20, Loss: 0.7314746379852295, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 30, Loss: 0.6784178018569946, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 36 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_1024\n",
      "Epoch 1, Loss: 0.7339972257614136, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7176107168197632, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_2056\n",
      "Epoch 1, Loss: 0.7358877658843994, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7239980697631836, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.1_batch_size_4112\n",
      "Epoch 1, Loss: 0.7357497215270996, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7296872138977051, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7069263458251953 Macro F1: 0.4555860054416216 1-TPR_gap: 0.9582666158676147\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_28\n",
      "Epoch 1, Loss: 0.6994050741195679, Final Score Train: 0.7300074696540833, Final Score Test: 0.7069263458251953, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.4555860054416216, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582666158676147\n",
      "Epoch 10, Loss: 0.7944884896278381, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_56\n",
      "Epoch 1, Loss: 0.8613908290863037, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7841993570327759, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.8755953311920166, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 30 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_128\n",
      "Epoch 1, Loss: 0.6770371198654175, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7478015422821045, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_256\n",
      "Epoch 1, Loss: 0.7285192608833313, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.6680648326873779, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_1024\n",
      "Epoch 1, Loss: 0.7288705110549927, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7262614965438843, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_2056\n",
      "Epoch 1, Loss: 0.7278086543083191, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.736885130405426, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.05_batch_size_4112\n",
      "Epoch 1, Loss: 0.7360458970069885, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7323218584060669, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_28\n",
      "Epoch 1, Loss: 0.7901787757873535, Final Score Train: 0.7300074696540833, Final Score Test: 0.7068067193031311, macro F1 Train: 0.4917421323022849, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682728052139282, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7886906862258911, Final Score Train: 0.7300093173980713, Final Score Test: 0.7068067193031311, macro F1 Train: 0.49176650858102583, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682521820068359, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7067341208457947 Macro F1: 0.45528338326637846 1-TPR_gap: 0.9581848382949829\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_56\n",
      "Epoch 1, Loss: 0.8184525966644287, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 10, Loss: 0.7754580974578857, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 20, Loss: 0.8467738628387451, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 30, Loss: 0.8596940040588379, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 40, Loss: 0.7876986265182495, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 50, Loss: 0.8031595349311829, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 60, Loss: 0.825992226600647, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 70, Loss: 0.759556770324707, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 80, Loss: 0.7994126677513123, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 90, Loss: 0.7552086114883423, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 100, Loss: 0.7622712850570679, Final Score Train: 0.7300373315811157, Final Score Test: 0.7067341208457947, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45528338326637846, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9581848382949829\n",
      "Epoch 110, Loss: 0.800123929977417, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068067193031311, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 117 époques\n",
      "Final Evaluation Score: 0.7068067193031311 Macro F1: 0.45536622818761197 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7964752912521362, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068067193031311, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536622818761197, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7458868622779846, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.722625732421875, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 30, Loss: 0.7192268371582031, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 40, Loss: 0.6841112971305847, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 43 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_256\n",
      "Epoch 1, Loss: 0.7105560302734375, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7307950258255005, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.6961372494697571, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 30, Loss: 0.6798868179321289, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 40, Loss: 0.6940811276435852, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 50, Loss: 0.7121442556381226, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 60, Loss: 0.6916840076446533, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 70, Loss: 0.6746039390563965, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 80, Loss: 0.717538058757782, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 90, Loss: 0.6744098663330078, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 100, Loss: 0.7106966972351074, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 110, Loss: 0.6666015386581421, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 117 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_1024\n",
      "Epoch 1, Loss: 0.723800778388977, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7119026184082031, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.7156369686126709, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 30, Loss: 0.7199323177337646, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 40, Loss: 0.7042173743247986, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 50, Loss: 0.7252084016799927, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 59 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_2056\n",
      "Epoch 1, Loss: 0.7287049293518066, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7407705783843994, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_4112\n",
      "Epoch 1, Loss: 0.7268232107162476, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7238931655883789, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_28\n",
      "Epoch 1, Loss: 0.8713011145591736, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7603744268417358, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_56\n",
      "Epoch 1, Loss: 0.8078218698501587, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.8101192712783813, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_128\n",
      "Epoch 1, Loss: 0.8127515316009521, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7736546397209167, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_256\n",
      "Epoch 1, Loss: 0.6900660395622253, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7033506631851196, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.6855927109718323, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 30, Loss: 0.7075484395027161, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 40, Loss: 0.7083367705345154, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 50, Loss: 0.6725739240646362, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 60, Loss: 0.7353556156158447, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 70, Loss: 0.7094855904579163, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 80, Loss: 0.6913705468177795, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 90, Loss: 0.6998564600944519, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 100, Loss: 0.7009274363517761, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 110, Loss: 0.705095648765564, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 120, Loss: 0.6967799663543701, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 130, Loss: 0.6793652772903442, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 135 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_1024\n",
      "Epoch 1, Loss: 0.7160510420799255, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7093240022659302, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7203797101974487, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7261483669281006, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7376927733421326, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7360477447509766, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_28\n",
      "Epoch 1, Loss: 0.829613208770752, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.8144146203994751, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.7424652576446533, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 30, Loss: 0.8596940040588379, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 40, Loss: 0.8136906623840332, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 50, Loss: 0.8451237678527832, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 60, Loss: 0.8032108545303345, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 70, Loss: 0.8124151825904846, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 74 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_56\n",
      "Epoch 1, Loss: 0.7939698696136475, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7827538251876831, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_128\n",
      "Epoch 1, Loss: 0.67647784948349, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7687328457832336, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_256\n",
      "Epoch 1, Loss: 0.6802021265029907, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.708885908126831, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7157033681869507, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7310042977333069, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7261917591094971, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7256320714950562, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_4112\n",
      "Epoch 1, Loss: 0.72542405128479, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7262569665908813, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_28\n",
      "Epoch 1, Loss: 0.8422620296478271, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7690478563308716, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_56\n",
      "Epoch 1, Loss: 0.8172698020935059, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7813623547554016, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_128\n",
      "Epoch 1, Loss: 0.714971125125885, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7169908285140991, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_256\n",
      "Epoch 1, Loss: 0.679215669631958, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7360563278198242, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_1024\n",
      "Epoch 1, Loss: 0.711034893989563, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7231457233428955, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_2056\n",
      "Epoch 1, Loss: 0.7224752902984619, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.737051248550415, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0005_batch_size_4112\n",
      "Epoch 1, Loss: 0.7240160703659058, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7303915023803711, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_28\n",
      "Epoch 1, Loss: 0.8386317491531372, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.8064790964126587, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 20, Loss: 0.7951301336288452, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 29 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_56\n",
      "Epoch 1, Loss: 0.7762801647186279, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7903488874435425, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_128\n",
      "Epoch 1, Loss: 0.7347726821899414, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7640221118927002, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_256\n",
      "Epoch 1, Loss: 0.764925479888916, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7156410217285156, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_1024\n",
      "Epoch 1, Loss: 0.7230262160301208, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7256869673728943, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_2056\n",
      "Epoch 1, Loss: 0.7210993766784668, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.7313593029975891, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_4112\n",
      "Epoch 1, Loss: 0.7290759682655334, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Epoch 10, Loss: 0.732785165309906, Final Score Train: 0.7300373315811157, Final Score Test: 0.7068073749542236, macro F1 Train: 0.49178412230529794, macro F1 Test: 0.45536748924201953, 1-TPR Gap Train: 0.9682905077934265, 1-TPR Gap Test: 0.9582472443580627\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7068073749542236 Macro F1: 0.45536748924201953 1-TPR_gap: 0.9582472443580627\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [0.1, 0.05, 0.01, 0.005, 0.001,0.0005,0.0001]\n",
    "batch_size_list = [28,56,128,256,1024,2056,4112]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score_train','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-28-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending, final_score_train= train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_NN-28-28_12-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>28</td>\n",
       "      <td>97</td>\n",
       "      <td>tensor(0.7049)</td>\n",
       "      <td>0.449664</td>\n",
       "      <td>tensor(0.9601)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "      <td>tensor(0.7067)</td>\n",
       "      <td>0.455465</td>\n",
       "      <td>tensor(0.9580)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.7100)</td>\n",
       "      <td>0.455287</td>\n",
       "      <td>tensor(0.9648)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7102)</td>\n",
       "      <td>0.458514</td>\n",
       "      <td>tensor(0.9619)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>tensor(0.7087)</td>\n",
       "      <td>0.457565</td>\n",
       "      <td>tensor(0.9599)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7121)</td>\n",
       "      <td>0.459610</td>\n",
       "      <td>tensor(0.9646)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_28</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>tensor(0.7089)</td>\n",
       "      <td>0.458932</td>\n",
       "      <td>tensor(0.9589)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7092)</td>\n",
       "      <td>0.458361</td>\n",
       "      <td>tensor(0.9600)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7114)</td>\n",
       "      <td>0.459750</td>\n",
       "      <td>tensor(0.9630)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7114)</td>\n",
       "      <td>0.460368</td>\n",
       "      <td>tensor(0.9624)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "      <td>tensor(0.7060)</td>\n",
       "      <td>0.453031</td>\n",
       "      <td>tensor(0.9591)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>tensor(0.7091)</td>\n",
       "      <td>0.457528</td>\n",
       "      <td>tensor(0.9607)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>tensor(0.7101)</td>\n",
       "      <td>0.460125</td>\n",
       "      <td>tensor(0.9601)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_56</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>56</td>\n",
       "      <td>25</td>\n",
       "      <td>tensor(0.7094)</td>\n",
       "      <td>0.460075</td>\n",
       "      <td>tensor(0.9586)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>15</td>\n",
       "      <td>tensor(0.7087)</td>\n",
       "      <td>0.459227</td>\n",
       "      <td>tensor(0.9582)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7095)</td>\n",
       "      <td>0.459079</td>\n",
       "      <td>tensor(0.9600)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>128</td>\n",
       "      <td>39</td>\n",
       "      <td>tensor(0.7103)</td>\n",
       "      <td>0.461434</td>\n",
       "      <td>tensor(0.9591)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>128</td>\n",
       "      <td>21</td>\n",
       "      <td>tensor(0.7089)</td>\n",
       "      <td>0.458986</td>\n",
       "      <td>tensor(0.9588)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7101)</td>\n",
       "      <td>0.460011</td>\n",
       "      <td>tensor(0.9602)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>tensor(0.7093)</td>\n",
       "      <td>0.459954</td>\n",
       "      <td>tensor(0.9587)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_128</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>128</td>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.7090)</td>\n",
       "      <td>0.460531</td>\n",
       "      <td>tensor(0.9575)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>256</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7096)</td>\n",
       "      <td>0.460434</td>\n",
       "      <td>tensor(0.9588)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7090)</td>\n",
       "      <td>0.460440</td>\n",
       "      <td>tensor(0.9576)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>tensor(0.7095)</td>\n",
       "      <td>0.459208</td>\n",
       "      <td>tensor(0.9597)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>20</td>\n",
       "      <td>tensor(0.7111)</td>\n",
       "      <td>0.461699</td>\n",
       "      <td>tensor(0.9606)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>256</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7093)</td>\n",
       "      <td>0.460594</td>\n",
       "      <td>tensor(0.9580)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>256</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7089)</td>\n",
       "      <td>0.459994</td>\n",
       "      <td>tensor(0.9579)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_256</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>256</td>\n",
       "      <td>26</td>\n",
       "      <td>tensor(0.7111)</td>\n",
       "      <td>0.461590</td>\n",
       "      <td>tensor(0.9605)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>1024</td>\n",
       "      <td>22</td>\n",
       "      <td>tensor(0.7093)</td>\n",
       "      <td>0.460120</td>\n",
       "      <td>tensor(0.9585)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1024</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7083)</td>\n",
       "      <td>0.459265</td>\n",
       "      <td>tensor(0.9573)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1024</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7101)</td>\n",
       "      <td>0.460185</td>\n",
       "      <td>tensor(0.9600)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>1024</td>\n",
       "      <td>23</td>\n",
       "      <td>tensor(0.7102)</td>\n",
       "      <td>0.460596</td>\n",
       "      <td>tensor(0.9598)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1024</td>\n",
       "      <td>30</td>\n",
       "      <td>tensor(0.7093)</td>\n",
       "      <td>0.460086</td>\n",
       "      <td>tensor(0.9586)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1024</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7096)</td>\n",
       "      <td>0.460560</td>\n",
       "      <td>tensor(0.9586)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_1024</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1024</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7109)</td>\n",
       "      <td>0.461805</td>\n",
       "      <td>tensor(0.9599)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>2056</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7095)</td>\n",
       "      <td>0.460609</td>\n",
       "      <td>tensor(0.9584)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>2056</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7092)</td>\n",
       "      <td>0.460261</td>\n",
       "      <td>tensor(0.9582)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>2056</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7090)</td>\n",
       "      <td>0.460054</td>\n",
       "      <td>tensor(0.9579)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>2056</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7101)</td>\n",
       "      <td>0.460109</td>\n",
       "      <td>tensor(0.9600)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>2056</td>\n",
       "      <td>19</td>\n",
       "      <td>tensor(0.7094)</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>tensor(0.9594)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2056</td>\n",
       "      <td>22</td>\n",
       "      <td>tensor(0.7094)</td>\n",
       "      <td>0.460261</td>\n",
       "      <td>tensor(0.9586)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_2056</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2056</td>\n",
       "      <td>21</td>\n",
       "      <td>tensor(0.7110)</td>\n",
       "      <td>0.462216</td>\n",
       "      <td>tensor(0.9598)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.1_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>4112</td>\n",
       "      <td>18</td>\n",
       "      <td>tensor(0.7090)</td>\n",
       "      <td>0.460081</td>\n",
       "      <td>tensor(0.9580)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.05_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>4112</td>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.7092)</td>\n",
       "      <td>0.459923</td>\n",
       "      <td>tensor(0.9584)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>4112</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7102)</td>\n",
       "      <td>0.460011</td>\n",
       "      <td>tensor(0.9603)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.01_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>4112</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7099)</td>\n",
       "      <td>0.459894</td>\n",
       "      <td>tensor(0.9599)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>4112</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.7096)</td>\n",
       "      <td>0.460280</td>\n",
       "      <td>tensor(0.9589)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0005_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4112</td>\n",
       "      <td>21</td>\n",
       "      <td>tensor(0.7110)</td>\n",
       "      <td>0.461791</td>\n",
       "      <td>tensor(0.9602)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NN-28-28_Momentum_lr_0.0001_batch_size_4112</td>\n",
       "      <td>SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4112</td>\n",
       "      <td>14</td>\n",
       "      <td>tensor(0.7094)</td>\n",
       "      <td>0.460170</td>\n",
       "      <td>tensor(0.9586)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model  \\\n",
       "0        NN-28-28_Momentum_lr_0.1_batch_size_28   \n",
       "7       NN-28-28_Momentum_lr_0.05_batch_size_28   \n",
       "14      NN-28-28_Momentum_lr_0.01_batch_size_28   \n",
       "21     NN-28-28_Momentum_lr_0.005_batch_size_28   \n",
       "28     NN-28-28_Momentum_lr_0.001_batch_size_28   \n",
       "35    NN-28-28_Momentum_lr_0.0005_batch_size_28   \n",
       "42    NN-28-28_Momentum_lr_0.0001_batch_size_28   \n",
       "15      NN-28-28_Momentum_lr_0.01_batch_size_56   \n",
       "22     NN-28-28_Momentum_lr_0.005_batch_size_56   \n",
       "29     NN-28-28_Momentum_lr_0.001_batch_size_56   \n",
       "8       NN-28-28_Momentum_lr_0.05_batch_size_56   \n",
       "1        NN-28-28_Momentum_lr_0.1_batch_size_56   \n",
       "36    NN-28-28_Momentum_lr_0.0005_batch_size_56   \n",
       "43    NN-28-28_Momentum_lr_0.0001_batch_size_56   \n",
       "30    NN-28-28_Momentum_lr_0.001_batch_size_128   \n",
       "16     NN-28-28_Momentum_lr_0.01_batch_size_128   \n",
       "9      NN-28-28_Momentum_lr_0.05_batch_size_128   \n",
       "23    NN-28-28_Momentum_lr_0.005_batch_size_128   \n",
       "44   NN-28-28_Momentum_lr_0.0001_batch_size_128   \n",
       "37   NN-28-28_Momentum_lr_0.0005_batch_size_128   \n",
       "2       NN-28-28_Momentum_lr_0.1_batch_size_128   \n",
       "24    NN-28-28_Momentum_lr_0.005_batch_size_256   \n",
       "31    NN-28-28_Momentum_lr_0.001_batch_size_256   \n",
       "10     NN-28-28_Momentum_lr_0.05_batch_size_256   \n",
       "38   NN-28-28_Momentum_lr_0.0005_batch_size_256   \n",
       "45   NN-28-28_Momentum_lr_0.0001_batch_size_256   \n",
       "3       NN-28-28_Momentum_lr_0.1_batch_size_256   \n",
       "17     NN-28-28_Momentum_lr_0.01_batch_size_256   \n",
       "11    NN-28-28_Momentum_lr_0.05_batch_size_1024   \n",
       "4      NN-28-28_Momentum_lr_0.1_batch_size_1024   \n",
       "18    NN-28-28_Momentum_lr_0.01_batch_size_1024   \n",
       "25   NN-28-28_Momentum_lr_0.005_batch_size_1024   \n",
       "46  NN-28-28_Momentum_lr_0.0001_batch_size_1024   \n",
       "32   NN-28-28_Momentum_lr_0.001_batch_size_1024   \n",
       "39  NN-28-28_Momentum_lr_0.0005_batch_size_1024   \n",
       "33   NN-28-28_Momentum_lr_0.001_batch_size_2056   \n",
       "12    NN-28-28_Momentum_lr_0.05_batch_size_2056   \n",
       "5      NN-28-28_Momentum_lr_0.1_batch_size_2056   \n",
       "19    NN-28-28_Momentum_lr_0.01_batch_size_2056   \n",
       "26   NN-28-28_Momentum_lr_0.005_batch_size_2056   \n",
       "47  NN-28-28_Momentum_lr_0.0001_batch_size_2056   \n",
       "40  NN-28-28_Momentum_lr_0.0005_batch_size_2056   \n",
       "6      NN-28-28_Momentum_lr_0.1_batch_size_4112   \n",
       "13    NN-28-28_Momentum_lr_0.05_batch_size_4112   \n",
       "27   NN-28-28_Momentum_lr_0.005_batch_size_4112   \n",
       "20    NN-28-28_Momentum_lr_0.01_batch_size_4112   \n",
       "34   NN-28-28_Momentum_lr_0.001_batch_size_4112   \n",
       "41  NN-28-28_Momentum_lr_0.0005_batch_size_4112   \n",
       "48  NN-28-28_Momentum_lr_0.0001_batch_size_4112   \n",
       "\n",
       "                                            optimizer      lr  batch_size  \\\n",
       "0   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000          28   \n",
       "7   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500          28   \n",
       "14  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100          28   \n",
       "21  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050          28   \n",
       "28  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010          28   \n",
       "35  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005          28   \n",
       "42  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001          28   \n",
       "15  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100          56   \n",
       "22  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050          56   \n",
       "29  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010          56   \n",
       "8   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500          56   \n",
       "1   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000          56   \n",
       "36  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005          56   \n",
       "43  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001          56   \n",
       "30  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010         128   \n",
       "16  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100         128   \n",
       "9   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500         128   \n",
       "23  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050         128   \n",
       "44  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001         128   \n",
       "37  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005         128   \n",
       "2   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000         128   \n",
       "24  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050         256   \n",
       "31  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010         256   \n",
       "10  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500         256   \n",
       "38  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005         256   \n",
       "45  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001         256   \n",
       "3   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000         256   \n",
       "17  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100         256   \n",
       "11  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        1024   \n",
       "4   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        1024   \n",
       "18  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        1024   \n",
       "25  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        1024   \n",
       "46  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        1024   \n",
       "32  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        1024   \n",
       "39  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        1024   \n",
       "33  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        2056   \n",
       "12  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        2056   \n",
       "5   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        2056   \n",
       "19  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        2056   \n",
       "26  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        2056   \n",
       "47  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        2056   \n",
       "40  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        2056   \n",
       "6   SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.1000        4112   \n",
       "13  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0500        4112   \n",
       "27  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0050        4112   \n",
       "20  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0100        4112   \n",
       "34  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0010        4112   \n",
       "41  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0005        4112   \n",
       "48  SGD (\\nParameter Group 0\\n    dampening: 0\\n  ...  0.0001        4112   \n",
       "\n",
       "    early_ending     final_score  macro_f1   macro_tpr_gap  \n",
       "0             97  tensor(0.7049)  0.449664  tensor(0.9601)  \n",
       "7             50  tensor(0.7067)  0.455465  tensor(0.9580)  \n",
       "14            13  tensor(0.7100)  0.455287  tensor(0.9648)  \n",
       "21            12  tensor(0.7102)  0.458514  tensor(0.9619)  \n",
       "28            21  tensor(0.7087)  0.457565  tensor(0.9599)  \n",
       "35            12  tensor(0.7121)  0.459610  tensor(0.9646)  \n",
       "42            17  tensor(0.7089)  0.458932  tensor(0.9589)  \n",
       "15            18  tensor(0.7092)  0.458361  tensor(0.9600)  \n",
       "22            14  tensor(0.7114)  0.459750  tensor(0.9630)  \n",
       "29            12  tensor(0.7114)  0.460368  tensor(0.9624)  \n",
       "8             17  tensor(0.7060)  0.453031  tensor(0.9591)  \n",
       "1             22  tensor(0.7091)  0.457528  tensor(0.9607)  \n",
       "36            47  tensor(0.7101)  0.460125  tensor(0.9601)  \n",
       "43            25  tensor(0.7094)  0.460075  tensor(0.9586)  \n",
       "30            15  tensor(0.7087)  0.459227  tensor(0.9582)  \n",
       "16            12  tensor(0.7095)  0.459079  tensor(0.9600)  \n",
       "9             39  tensor(0.7103)  0.461434  tensor(0.9591)  \n",
       "23            21  tensor(0.7089)  0.458986  tensor(0.9588)  \n",
       "44            18  tensor(0.7101)  0.460011  tensor(0.9602)  \n",
       "37            16  tensor(0.7093)  0.459954  tensor(0.9587)  \n",
       "2             13  tensor(0.7090)  0.460531  tensor(0.9575)  \n",
       "24            12  tensor(0.7096)  0.460434  tensor(0.9588)  \n",
       "31            14  tensor(0.7090)  0.460440  tensor(0.9576)  \n",
       "10            16  tensor(0.7095)  0.459208  tensor(0.9597)  \n",
       "38            20  tensor(0.7111)  0.461699  tensor(0.9606)  \n",
       "45            18  tensor(0.7093)  0.460594  tensor(0.9580)  \n",
       "3             18  tensor(0.7089)  0.459994  tensor(0.9579)  \n",
       "17            26  tensor(0.7111)  0.461590  tensor(0.9605)  \n",
       "11            22  tensor(0.7093)  0.460120  tensor(0.9585)  \n",
       "4             18  tensor(0.7083)  0.459265  tensor(0.9573)  \n",
       "18            14  tensor(0.7101)  0.460185  tensor(0.9600)  \n",
       "25            23  tensor(0.7102)  0.460596  tensor(0.9598)  \n",
       "46            30  tensor(0.7093)  0.460086  tensor(0.9586)  \n",
       "32            11  tensor(0.7096)  0.460560  tensor(0.9586)  \n",
       "39            11  tensor(0.7109)  0.461805  tensor(0.9599)  \n",
       "33            12  tensor(0.7095)  0.460609  tensor(0.9584)  \n",
       "12            14  tensor(0.7092)  0.460261  tensor(0.9582)  \n",
       "5             11  tensor(0.7090)  0.460054  tensor(0.9579)  \n",
       "19            11  tensor(0.7101)  0.460109  tensor(0.9600)  \n",
       "26            19  tensor(0.7094)  0.459524  tensor(0.9594)  \n",
       "47            22  tensor(0.7094)  0.460261  tensor(0.9586)  \n",
       "40            21  tensor(0.7110)  0.462216  tensor(0.9598)  \n",
       "6             18  tensor(0.7090)  0.460081  tensor(0.9580)  \n",
       "13            13  tensor(0.7092)  0.459923  tensor(0.9584)  \n",
       "27            12  tensor(0.7102)  0.460011  tensor(0.9603)  \n",
       "20            12  tensor(0.7099)  0.459894  tensor(0.9599)  \n",
       "34            11  tensor(0.7096)  0.460280  tensor(0.9589)  \n",
       "41            21  tensor(0.7110)  0.461791  tensor(0.9602)  \n",
       "48            14  tensor(0.7094)  0.460170  tensor(0.9586)  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 leanring rate x 7 batch size = 49 combinaisons par optimizer\n",
    "# 5 optimizer x 49 combinaison = 245\n",
    "Res.iloc[0:49,:].sort_values(by='batch_size').head(49)  #'batch_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Momentum'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(Res[Res['optimizer']==list(optimizer_dict.keys())[2]])\n",
    "list(optimizer_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_56_0\n",
      "Epoch 1, Loss: 0.9619801044464111, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9627382755279541, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_128_1\n",
      "Epoch 1, Loss: 0.9475432634353638, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9473494291305542, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_256_2\n",
      "Epoch 1, Loss: 0.962433397769928, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9569849371910095, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_512_3\n",
      "Epoch 1, Loss: 0.9505218267440796, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9441454410552979, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_1024_4\n",
      "Epoch 1, Loss: 0.9555106163024902, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.959869384765625, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_56_5\n",
      "Epoch 1, Loss: 0.962881326675415, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9613195657730103, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_128_6\n",
      "Epoch 1, Loss: 0.9631079435348511, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9628171920776367, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_256_7\n",
      "Epoch 1, Loss: 0.9629205465316772, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.954416036605835, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_512_8\n",
      "Epoch 1, Loss: 0.9593644142150879, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9582729339599609, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_1024_9\n",
      "Epoch 1, Loss: 0.9548274278640747, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9449251890182495, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_56_10\n",
      "Epoch 1, Loss: 0.9620548486709595, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9421370625495911, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_128_11\n",
      "Epoch 1, Loss: 0.9632900953292847, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9623175859451294, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_256_12\n",
      "Epoch 1, Loss: 0.956383228302002, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9546757936477661, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_512_13\n",
      "Epoch 1, Loss: 0.961563766002655, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9494081735610962, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_1024_14\n",
      "Epoch 1, Loss: 0.9486672878265381, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9465586543083191, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_56_15\n",
      "Epoch 1, Loss: 0.9628152847290039, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9626616835594177, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_128_16\n",
      "Epoch 1, Loss: 0.9479560852050781, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9622167348861694, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_256_17\n",
      "Epoch 1, Loss: 0.9595986604690552, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9493604302406311, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_512_18\n",
      "Epoch 1, Loss: 0.9588119983673096, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9437597990036011, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_1024_19\n",
      "Epoch 1, Loss: 0.9340413212776184, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9554332494735718, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_56_20\n",
      "Epoch 1, Loss: 0.9624695777893066, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9627238512039185, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_128_21\n",
      "Epoch 1, Loss: 0.9627935290336609, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9624748229980469, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_256_22\n",
      "Epoch 1, Loss: 0.9564724564552307, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9567225575447083, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_512_23\n",
      "Epoch 1, Loss: 0.9617987871170044, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9578564763069153, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_1024_24\n",
      "Epoch 1, Loss: 0.9438228607177734, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9533267021179199, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_56_25\n",
      "Epoch 1, Loss: 0.9440799951553345, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9631694555282593, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_128_26\n",
      "Epoch 1, Loss: 0.947606086730957, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9622937440872192, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_256_27\n",
      "Epoch 1, Loss: 0.9558461904525757, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9580962061882019, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_512_28\n",
      "Epoch 1, Loss: 0.9613135457038879, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9484223127365112, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_1024_29\n",
      "Epoch 1, Loss: 0.9551681280136108, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9515268802642822, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_56_30\n",
      "Epoch 1, Loss: 0.9633320569992065, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9619581699371338, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_128_31\n",
      "Epoch 1, Loss: 0.9622271060943604, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9502660036087036, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_256_32\n",
      "Epoch 1, Loss: 0.9477075934410095, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9600913524627686, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_512_33\n",
      "Epoch 1, Loss: 0.9597756862640381, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9580205082893372, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_1024_34\n",
      "Epoch 1, Loss: 0.9417344927787781, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Epoch 10, Loss: 0.9450589418411255, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49283674359321594 gap -0.005672961473464966 || Macro F1: 0.002946626944716261 1-TPR_gap: 0.9827268719673157\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_56_35\n",
      "Epoch 1, Loss: 0.9617259502410889, Final Score Train: 0.4985097050666809, Final Score Test: 0.49283674359321594 (gap -0.005672961473464966) macro F1 Train: 0.003291196758290755, macro F1 Test: 0.002946626944716261, 1-TPR Gap Train: 0.9937282204627991, 1-TPR Gap Test: 0.9827268719673157\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 256),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(256, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','gap','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+'_'+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending , final_score_train = train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train, final_score_train - final_score, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_NN-2048-256-28_12-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res_2, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINEMENT SUR TOUT LE MODELE\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','alpha','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for i in range(1,10):\n",
    "            alpha=i\n",
    "            name = 'all'+opt_name+'_lr_'+str(learning_rate)+'_alpha_'+str(i)\n",
    "            print('\\n\\n Starting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model, optimizer, alpha, X_tensor, Y_tensor, S_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Couche d'entrée à la première couche cachée\n",
    "    nn.ReLU(),  # Fonction d'activation ReLU\n",
    "    nn.Dropout(p=0.5),  # Dropout avec une probabilité de désactivation de 50%\n",
    "    nn.Linear(2048, 512),  # De la première couche cachée à la deuxième couche cachée\n",
    "    nn.ReLU(),  # Une autre fonction d'activation ReLU après la deuxième couche cachée\n",
    "    nn.Dropout(p=0.5),  # Un autre dropout après la deuxième couche cachée\n",
    "    nn.Linear(512, 28),  # De la deuxième couche cachée à la couche de sortie\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour la classification multiclasse\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28-dropout_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(2048,512),  # Assuming 768 input features and 28 classes\n",
    "    nn.Linear(512,28),  \n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. REGRESSION WITH CUSTOM LOSS macro F1**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming model, optimizer, X_train_tensor, Y_train_one_hot, X_test_tensor, Y_test are already defined\n",
    "\n",
    "# Convert Y_test to one-hot encoding if it's not already one-hot encoded\n",
    "# This is necessary for consistency in our loss function calculations\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.int64) if isinstance(Y_test, pd.Series) else torch.from_numpy(Y_test).long()\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=28)\n",
    "\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 10000  # Example number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    \n",
    "    # Forward pass on the training data\n",
    "    outputs_train = model(X_train_tensor)\n",
    "    loss_train = macro_soft_f1_loss(Y_train_one_hot.float(), outputs_train)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No gradient computation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Forward pass on the validation data\n",
    "        outputs_test = model(X_test_tensor)\n",
    "        \n",
    "        # Calculate the exact macro F1 score for both training and validation data\n",
    "        f1_train = calculate_exact_macro_f1(Y_train_one_hot.float(), outputs_train)\n",
    "        f1_test = calculate_exact_macro_f1(Y_test_one_hot.float(), outputs_test)\n",
    "        \n",
    "        model.train()  # Set the model back to training mode\n",
    "    \n",
    "    # Print loss and F1 score\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_train.item():.4f}, macro F1 Train: {f1_train:.4f}, macro F1 Test: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# Assuming model is already trained and X_test is a DataFrame\n",
    "\n",
    "# Convert X_test to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n",
    "\n",
    "# If you want to use the exact F1 score for evaluation, you can directly use it from sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"Exact F1 Score (micro):\", f1_score(Y_test, Y_pred_df['Predicted'],average = 'micro'))  # 'weighted' for multi-class\n",
    "print(\"Exact F1 Score (macro):\", f1_score(Y_test, Y_pred_df['Predicted'], average='macro'))  # 'weighted' for multi-class\n",
    "\n",
    "# Returning Y_pred as a DataFrame makes sense for further analysis or submission\n",
    "#return Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUSTON LOSS FUNCTION TRP GAP**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def gap_TPR(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap for each class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    # Apply softmax to get probabilities\n",
    "    y_pred_probs = torch.softmax(y_pred, dim=1)\n",
    "    \n",
    "    # Convert one-hot labels to class indices for gathering\n",
    "    y_true_indices = torch.argmax(y_true, dim=1)\n",
    "    \n",
    "    # Initialize TPR storage\n",
    "    tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = y_true.shape[1]\n",
    "    for class_idx in range(num_classes):\n",
    "        # Calculate TPR for the current class across all groups\n",
    "        tpr_list = []\n",
    "        \n",
    "        # Calculate overall TPR for the current class\n",
    "        overall_mask = y_true_indices == class_idx\n",
    "        overall_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & overall_mask).float() / torch.sum(overall_mask).float()\n",
    "        \n",
    "        # Calculate TPR for each protected group\n",
    "        for group_val in protected_attribute.unique():\n",
    "            group_mask = (protected_attribute == group_val) & overall_mask\n",
    "            group_tpr = torch.sum((y_pred_probs[:, class_idx] > 0.5) & group_mask).float() / torch.sum(group_mask).float()\n",
    "            tpr_list.append(group_tpr)\n",
    "        \n",
    "        # Calculate TPR gap for the current class and store it\n",
    "        tpr_gaps.append(torch.abs(torch.tensor(tpr_list) - overall_tpr))\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> (5550,)\n",
      "<class 'torch.Tensor'> torch.Size([5550, 28])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_test),Y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(Y_pred_probs),Y_pred_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_macro_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mS_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mget_macro_tpr_gap\u001b[0;34m(y_true, y_pred, protected_attribute)\u001b[0m\n\u001b[1;32m    110\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m--> 112\u001b[0m     class_tpr_gap \u001b[38;5;241m=\u001b[39m \u001b[43mget_tpr_gap\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotected_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     class_tpr_gaps\u001b[38;5;241m.\u001b[39mappend(class_tpr_gap)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Calculate the average TPR gap across all classes\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mget_tpr_gap\u001b[0;34m(y_true, y_pred_probs, protected_attribute, class_idx)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mCalculate the TPR gap for a specific class across protected groups.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m- TPR gap for the specified class.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert one-hot labels to class indices for gathering\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m y_true_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate overall TPR for the current class\u001b[39;00m\n\u001b[1;32m     74\u001b[0m overall_mask \u001b[38;5;241m=\u001b[39m y_true_indices \u001b[38;5;241m==\u001b[39m class_idx\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not Series"
     ]
    }
   ],
   "source": [
    "print(type(Y_test),Y_test.shape)\n",
    "print(type(Y_pred_probs),Y_pred_probs.shape)\n",
    "get_macro_tpr_gap(Y_test,Y_pred_probs,S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

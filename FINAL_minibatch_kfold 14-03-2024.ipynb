{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTIONS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    #y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)   # Mean to aggregate over all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model, name): # adapted to torch\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "    \n",
    "\n",
    "def print_cassif_report(Y_pred,Y_test):\n",
    "    # Convert Y_pred to a DataFrame\n",
    "    Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "    # Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "    table = classification_report(Y_test, Y_pred_df['Predicted'])\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD AND PREPARE**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS (INITIAL)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7309491038322449, Final Score Train: 0.7176300883293152, Final Score Test: 0.7077692747116089 (gap -0.009860813617706299) macro F1 Train: 0.4957828848553226, macro F1 Test: 0.4736360103458466, 1-TPR Gap Train: 0.939477264881134, 1-TPR Gap Test: 0.9419025182723999\n",
      "Epoch 10, Loss: 0.6423549652099609, Final Score Train: 0.7820804119110107, Final Score Test: 0.7496452331542969 (gap -0.03243517875671387) macro F1 Train: 0.6145531215331849, macro F1 Test: 0.5740386880851179, 1-TPR Gap Train: 0.9496077299118042, 1-TPR Gap Test: 0.9252517223358154\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7380497455596924 gap -0.039074718952178955 || Macro F1: 0.5445659286814128 1-TPR_gap: 0.9315335750579834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([18, 21, 18,  ..., 21,  2, 19]),\n",
       " tensor([[2.9876e-42, 2.5927e-30, 7.0065e-45,  ..., 3.7275e-43, 1.0006e-22,\n",
       "          1.7011e-34],\n",
       "         [0.0000e+00, 0.0000e+00, 1.8467e-37,  ..., 2.2881e-28, 1.4531e-40,\n",
       "          7.5907e-41],\n",
       "         [1.1310e-19, 4.2508e-19, 1.1708e-24,  ..., 2.7557e-25, 6.9932e-16,\n",
       "          2.5076e-21],\n",
       "         ...,\n",
       "         [5.8958e-36, 5.4925e-38, 3.1634e-21,  ..., 9.5636e-25, 1.8379e-28,\n",
       "          2.4237e-23],\n",
       "         [4.7296e-23, 1.0162e-41, 1.0000e+00,  ..., 7.9883e-35, 1.5204e-42,\n",
       "          2.2813e-42],\n",
       "         [2.2516e-16, 1.7949e-30, 4.3142e-16,  ..., 5.2620e-16, 3.9775e-32,\n",
       "          5.1686e-21]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#       TEST D'UN MODEL (ET DU CODE)\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    nn.Softmax(dim=1),  # LogSoftmax for multi-class classification\n",
    "    )  \n",
    "\n",
    "batch_size = 128\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=0.001)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending, final_score_train = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending,final_score_train, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_560\n",
      "Epoch 1, Loss: 0.9608454704284668, Final Score Train: 0.5081643462181091, Final Score Test: 0.5080236792564392 (gap -0.00014066696166992188) macro F1 Train: 0.01637493922344933, macro F1 Test: 0.016166304759248563, 1-TPR Gap Train: 0.9999538064002991, 1-TPR Gap Test: 0.9998810291290283\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5081309080123901 gap -0.00010067224502563477 || Macro F1: 0.016261778855711503 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_1281\n",
      "Epoch 1, Loss: 0.9748392105102539, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9754658937454224, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9735351800918579, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9746513962745667, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9718295931816101, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9743116497993469, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9771167039871216, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 70, Loss: 0.9745029211044312, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 80, Loss: 0.9776250123977661, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 90, Loss: 0.9731876850128174, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 100, Loss: 0.9752118587493896, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 110, Loss: 0.979990541934967, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 120, Loss: 0.975550651550293, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 130, Loss: 0.9708739519119263, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 140, Loss: 0.9715965986251831, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 150, Loss: 0.9774422645568848, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 160, Loss: 0.9707463979721069, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 170, Loss: 0.9709086418151855, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 180, Loss: 0.9751284122467041, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 190, Loss: 0.973737359046936, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9), #, weight_decay=0.0001),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True), #,weight_decay=0.0001),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate),#, weight_decay=0.0000),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, initial_accumulator_value=0, eps=1e-10), #, weight_decay=0.0000),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)#,weight_decay=0.0001)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score_train','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-28-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending, final_score_train= train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open('RESULTS_NN-28-28_12-03-2024_decay.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "path_pkl = 'pkl_files/'\n",
    "Res = pd.read_pickle('RESULTS_NN-28-28_12-03-2024_decay.pkl')\n",
    "   \n",
    "Res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 leanring rate x 7 batch size = 49 combinaisons par optimizer\n",
    "# 5 optimizer x 49 combinaison = 245\n",
    "Res.iloc[97:144,:].sort_values(by='batch_size').head(49)  #'batch_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Res[Res['optimizer']==list(optimizer_dict.keys())[2]])\n",
    "list(optimizer_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 256),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(256, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9), #, weight_decay=0.0001),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True), #,weight_decay=0.0001),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=0.0000),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, initial_accumulator_value=0, eps=1e-10) #, weight_decay=0.0000),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate)#,weight_decay=0.0001)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','gap','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+'_'+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending , final_score_train = train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train, final_score_train - final_score, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_NN-2048-256-28_12-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res_2, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification_report\n",
    "\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZING TRAINING FUNCTION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        # Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

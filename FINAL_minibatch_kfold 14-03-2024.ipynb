{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    #y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)   # Mean to aggregate over all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD AND PREPARE**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation functions**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        81\n",
      "           1       0.00      0.00      0.00       127\n",
      "           2       0.00      0.00      0.00       458\n",
      "           3       0.00      0.00      0.00        36\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.00      0.00      0.00        72\n",
      "           6       0.00      0.00      0.00       178\n",
      "           7       0.00      0.00      0.00        54\n",
      "           8       0.00      0.00      0.00        18\n",
      "           9       0.00      0.00      0.00        91\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.00      0.00      0.00       286\n",
      "          12       0.00      0.00      0.00       110\n",
      "          13       0.00      0.00      0.00       258\n",
      "          14       0.00      0.00      0.00       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.00      0.00      0.00        33\n",
      "          17       0.00      0.00      0.00        26\n",
      "          18       0.00      0.00      0.00       383\n",
      "          19       0.00      0.00      0.00       611\n",
      "          20       0.00      0.00      0.00        98\n",
      "          21       0.29      1.00      0.46      1636\n",
      "          22       0.00      0.00      0.00       264\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.00      0.00      0.00        89\n",
      "          25       0.00      0.00      0.00       183\n",
      "          26       0.00      0.00      0.00       227\n",
      "          27       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.29      5550\n",
      "   macro avg       0.01      0.04      0.02      5550\n",
      "weighted avg       0.09      0.29      0.13      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model, name): # adapted to torch\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "    \n",
    "\n",
    "def print_cassif_report(Y_pred,Y_test):\n",
    "    # Convert Y_pred to a DataFrame\n",
    "    Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "    # Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "    table = classification_report(Y_test, Y_pred_df['Predicted'])\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARRET ANTICIPE DU NN (sans mini-batch)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.6730861067771912 Macro F1: 0.4465380708164603 1-TPR_gap: 0.8996341228485107\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    #nn.LogSoftmax(dim=1)\n",
    "    )  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss_no_mini_batch(model,optim.Adam(model.parameters(), lr=learning_rate), X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "#save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS (INITIAL)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        # Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.9728055000305176, Final Score Train: 0.5082315802574158, Final Score Test: 0.5081309080123901 (gap -0.00010067224502563477) macro F1 Train: 0.016463136835433007, macro F1 Test: 0.016261778855711503, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9646603465080261, Final Score Train: 0.5012662410736084, Final Score Test: 0.5011098384857178 (gap -0.000156402587890625) macro F1 Train: 0.0025325098538220418, macro F1 Test: 0.002219672785315243, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.5017502307891846 gap 4.07099723815918e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([11, 11, 11,  ..., 11, 11, 11]),\n",
       " tensor([[-1019.4708, -1053.7491,  -901.7321,  ...,  -405.2040,  -953.3958,\n",
       "          -1096.1006],\n",
       "         [-1084.8196, -1147.7852, -1005.8746,  ...,  -570.9733, -1041.9598,\n",
       "          -1120.8088],\n",
       "         [ -808.4445,  -846.9246,  -729.4754,  ...,  -329.8860,  -764.5796,\n",
       "           -864.8046],\n",
       "         ...,\n",
       "         [-1098.2998, -1161.5825, -1016.0308,  ...,  -549.1259, -1055.2952,\n",
       "          -1133.0596],\n",
       "         [ -954.2846,  -952.4446,  -838.9060,  ...,  -361.5235,  -856.6089,\n",
       "           -929.9841],\n",
       "         [-1117.0275, -1160.0225, -1040.0208,  ..., -1203.8389, -1157.7285,\n",
       "          -1280.2397]], grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#          TEST DES PARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    nn.LogSoftmax(dim=1),  # LogSoftmax for multi-class classification\n",
    "    )  \n",
    "\n",
    "batch_size = 128\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=0.001)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending, final_score_train = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending,final_score_train, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_560\n",
      "Epoch 1, Loss: 0.9643118381500244, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.503541886806488 gap 0.00017970800399780273 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_1281\n",
      "Epoch 1, Loss: 0.9643909931182861, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9644181728363037, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9642853736877441, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.964311957359314, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9642802476882935, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9643017053604126, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9643335342407227, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 70, Loss: 0.964246392250061, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 80, Loss: 0.9642714858055115, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 83 époques\n",
      "Final Evaluation Score: 0.503541886806488 gap 0.00017970800399780273 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_2562\n",
      "Epoch 1, Loss: 0.96431565284729, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9642858505249023, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.503541886806488 gap 0.00017970800399780273 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_5123\n",
      "Epoch 1, Loss: 0.9643263220787048, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9643139243125916, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9643220901489258, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9643056392669678, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9643369913101196, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9643373489379883, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 53 époques\n",
      "Final Evaluation Score: 0.503541886806488 gap 0.00017970800399780273 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_10244\n",
      "Epoch 1, Loss: 0.9643274545669556, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.964327335357666, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9643164873123169, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9643298387527466, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9643301963806152, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9643298387527466, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9643203020095825, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 70, Loss: 0.9643193483352661, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 80, Loss: 0.9643177390098572, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 90, Loss: 0.9643211960792542, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 98 époques\n",
      "Final Evaluation Score: 0.503541886806488 gap 0.00017970800399780273 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_565\n",
      "Epoch 1, Loss: 0.9643322825431824, Final Score Train: 0.5033621788024902, Final Score Test: 0.503541886806488 (gap 0.00017970800399780273) macro F1 Train: 0.006724300754334216, macro F1 Test: 0.007083729450228395, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_1286\n",
      "Epoch 1, Loss: 0.9643436670303345, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9643234014511108, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9642903804779053, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9642811417579651, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9642861485481262, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9642724990844727, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9643185138702393, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 66 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_2567\n",
      "Epoch 1, Loss: 0.9642919301986694, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9642968773841858, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9642828702926636, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9642836451530457, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9643027782440186, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9642820358276367, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 60 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_5128\n",
      "Epoch 1, Loss: 0.96428382396698, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.964281439781189, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_10249\n",
      "Epoch 1, Loss: 0.96427983045578, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9642883539199829, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9642796516418457, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9642810225486755, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9642879962921143, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9642731547355652, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9642744064331055, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 70, Loss: 0.9642709493637085, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 80, Loss: 0.9642857313156128, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 90, Loss: 0.9642742276191711, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 100, Loss: 0.9642648696899414, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 110, Loss: 0.9642647504806519, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 120, Loss: 0.9642416834831238, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 127 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_5610\n",
      "Epoch 1, Loss: 0.9642602205276489, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9643341302871704, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_12811\n",
      "Epoch 1, Loss: 0.9642912745475769, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9643161296844482, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9643054008483887, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 30, Loss: 0.9642479419708252, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 40, Loss: 0.9642782211303711, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 50, Loss: 0.9642859697341919, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 60, Loss: 0.9642521142959595, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 70, Loss: 0.9641954302787781, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 75 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_25612\n",
      "Epoch 1, Loss: 0.9642699956893921, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9642908573150635, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_51213\n",
      "Epoch 1, Loss: 0.9642740488052368, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9643433094024658, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Arrêt précoce après 20 époques\n",
      "Final Evaluation Score: 0.5011399984359741 gap -4.976987838745117e-05 || Macro F1: 0.0162663060769965 1-TPR_gap: 1.0\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_102414\n",
      "Epoch 1, Loss: 0.9642981886863708, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 10, Loss: 0.9642771482467651, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n",
      "Epoch 20, Loss: 0.9642690420150757, Final Score Train: 0.5011897683143616, Final Score Test: 0.5011399984359741 (gap -4.976987838745117e-05) macro F1 Train: 0.0023795008336029064, macro F1 Test: 0.0022800328922777903, 1-TPR Gap Train: 1.0, 1-TPR Gap Test: 1.0\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    #nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True,weight_decay=0.0001),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.0001),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0.0001, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate,weight_decay=0.0001)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score_train','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-28-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending, final_score_train= train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score_train</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, optimizer, lr, batch_size, early_ending, final_score_train, final_score, macro_f1, macro_tpr_gap]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open('RESULTS_NN-28-28_12-03-2024_decay.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "path_pkl = 'pkl_files/'\n",
    "Res = pd.read_pickle('RESULTS_NN-28-28_12-03-2024_decay.pkl')\n",
    "   \n",
    "Res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score_train</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, optimizer, lr, batch_size, early_ending, final_score_train, final_score, macro_f1, macro_tpr_gap]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 leanring rate x 7 batch size = 49 combinaisons par optimizer\n",
    "# 5 optimizer x 49 combinaison = 245\n",
    "Res.iloc[97:144,:].sort_values(by='batch_size').head(49)  #'batch_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Momentum'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(Res[Res['optimizer']==list(optimizer_dict.keys())[2]])\n",
    "list(optimizer_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_56_0\n",
      "Epoch 1, Loss: 0.9622682332992554, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Epoch 10, Loss: 0.9625880718231201, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49868887662887573 gap -0.0004298686981201172 || Macro F1: 0.008741462217204633 1-TPR_gap: 0.988636314868927\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_56_1\n",
      "Epoch 1, Loss: 0.9614269733428955, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Epoch 10, Loss: 0.9614413976669312, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49868887662887573 gap -0.0004298686981201172 || Macro F1: 0.008741462217204633 1-TPR_gap: 0.988636314868927\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_56_2\n",
      "Epoch 1, Loss: 0.9483728408813477, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Epoch 10, Loss: 0.9618397355079651, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49868887662887573 gap -0.0004298686981201172 || Macro F1: 0.008741462217204633 1-TPR_gap: 0.988636314868927\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_56_3\n",
      "Epoch 1, Loss: 0.9532654285430908, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Epoch 10, Loss: 0.9624555706977844, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49868887662887573 gap -0.0004298686981201172 || Macro F1: 0.008741462217204633 1-TPR_gap: 0.988636314868927\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_56_4\n",
      "Epoch 1, Loss: 0.9616221189498901, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Epoch 10, Loss: 0.962959885597229, Final Score Train: 0.49911874532699585, Final Score Test: 0.49868887662887573 (gap -0.0004298686981201172) macro F1 Train: 0.00910291207674675, macro F1 Test: 0.008741462217204633, 1-TPR Gap Train: 0.9891345500946045, 1-TPR Gap Test: 0.988636314868927\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.49868887662887573 gap -0.0004298686981201172 || Macro F1: 0.008741462217204633 1-TPR_gap: 0.988636314868927\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 256),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(256, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True,weight_decay=0.0001),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.0001),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0.0001, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate,weight_decay=0.0001)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','gap','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+'_'+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending , final_score_train = train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train, final_score_train - final_score, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_NN-2048-256-28_12-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res_2, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score</th>\n",
       "      <th>gap</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN-2048-256-28_Adam_lr_0.01_batch_size_56_2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>tensor(0.4991)</td>\n",
       "      <td>tensor(0.0004)</td>\n",
       "      <td>tensor(0.4987)</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>tensor(0.9886)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model optimizer    lr  batch_size  \\\n",
       "2  NN-2048-256-28_Adam_lr_0.01_batch_size_56_2      Adam  0.01          56   \n",
       "\n",
       "   early_ending     final_score             gap     final_score  macro_f1  \\\n",
       "2            11  tensor(0.4991)  tensor(0.0004)  tensor(0.4987)  0.008741   \n",
       "\n",
       "    macro_tpr_gap  \n",
       "2  tensor(0.9886)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res_2[Res_2['optimizer']=='Adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINEMENT SUR TOUT X\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "optimizer_dict = {'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001),\n",
    "                'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True,weight_decay=0.0001),\n",
    "                'Adam': optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.0001),\n",
    "                'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, weight_decay=0.0001, initial_accumulator_value=0, eps=1e-10),\n",
    "                 'SGD': optim.SGD(model.parameters(), lr=learning_rate,weight_decay=0.0001)\n",
    "                }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2_all=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','gap','final_score','macro_f1','macro_tpr_gap'])\n",
    "\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list[:1]:\n",
    "        for batch_size in batch_size_list[:1]:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+'_'+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending , final_score_train = train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train, final_score_train - final_score, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Couche d'entrée à la première couche cachée\n",
    "    nn.ReLU(),  # Fonction d'activation ReLU\n",
    "    nn.Dropout(p=0.5),  # Dropout avec une probabilité de désactivation de 50%\n",
    "    nn.Linear(2048, 512),  # De la première couche cachée à la deuxième couche cachée\n",
    "    nn.ReLU(),  # Une autre fonction d'activation ReLU après la deuxième couche cachée\n",
    "    nn.Dropout(p=0.5),  # Un autre dropout après la deuxième couche cachée\n",
    "    nn.Linear(512, 28),  # De la deuxième couche cachée à la couche de sortie\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax pour la classification multiclasse\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28-dropout_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(2048,512),  # Assuming 768 input features and 28 classes\n",
    "    nn.Linear(512,28),  \n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.LogSoftmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20000 \n",
    "\n",
    "name = 'NN2048-512-28_Adam'+'_lr_'+str(learning_rate)+'_alpha_5'\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate) , 5, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "Res.loc[i]=[name,optimizer,learning_rate,alpha,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_11-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZING TRAINING FUNCTION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        # Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

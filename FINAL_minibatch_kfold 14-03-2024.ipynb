{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATACHALLENGE BDGIA DEBIASING MODEL**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTIONS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################\n",
    "#  DEFINE CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#   \n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   calculate_exact_macro_f1\n",
    "#   calculate_class_tpr_gap\n",
    "#   average_tpr_gap_per_class\n",
    "#   \n",
    "##############################################################\n",
    "\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    #y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)   # Mean to aggregate over all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# to show performance\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model, name): # adapted to torch\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "    \n",
    "\n",
    "def print_cassif_report(Y_pred,Y_test):\n",
    "    # Convert Y_pred to a DataFrame\n",
    "    Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "    # Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "    table = classification_report(Y_test, Y_pred_df['Predicted'])\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD AND PREPARE**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# LOAD DATA, \n",
    "#############################################################\n",
    "\n",
    "# Load pickle file and convert to numpy array\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "#create a label to distiguish 56 labels Y x 2 (man or woman)\n",
    "# 0 to 27 = non sensitive group | 28 + [0 , 27] = 28 to 55 = sensitive group\n",
    "Y56 = Y+28*S\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "# check size\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n",
      "train_tensor: torch.Size([22199, 768]) torch.Size([22199]) torch.Size([22199]) <class 'torch.Tensor'>\n",
      "test_tensor: torch.Size([5550, 768]) torch.Size([5550]) torch.Size([5550]) <class 'torch.Tensor'>\n",
      "Y_train_one_hot: torch.Size([22199, 28]) <class 'torch.Tensor'>\n",
      "X_test_true_tensor: torch.Size([11893, 768]) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCTION FOR NN WITH CUSTOM LOSS (INITIAL)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting to train model NN-28-28_Adam_lr_0.01_batch_size_128\n",
      "Epoch 1, Loss: 0.7309491038322449, Final Score Train: 0.7176300883293152, Final Score Test: 0.7077692747116089 (gap -0.009860813617706299) macro F1 Train: 0.4957828848553226, macro F1 Test: 0.4736360103458466, 1-TPR Gap Train: 0.939477264881134, 1-TPR Gap Test: 0.9419025182723999\n",
      "Epoch 10, Loss: 0.6423549652099609, Final Score Train: 0.7820804119110107, Final Score Test: 0.7496452331542969 (gap -0.03243517875671387) macro F1 Train: 0.6145531215331849, macro F1 Test: 0.5740386880851179, 1-TPR Gap Train: 0.9496077299118042, 1-TPR Gap Test: 0.9252517223358154\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7380497455596924 gap -0.039074718952178955 || Macro F1: 0.5445659286814128 1-TPR_gap: 0.9315335750579834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([18, 21, 18,  ..., 21,  2, 19]),\n",
       " tensor([[2.9876e-42, 2.5927e-30, 7.0065e-45,  ..., 3.7275e-43, 1.0006e-22,\n",
       "          1.7011e-34],\n",
       "         [0.0000e+00, 0.0000e+00, 1.8467e-37,  ..., 2.2881e-28, 1.4531e-40,\n",
       "          7.5907e-41],\n",
       "         [1.1310e-19, 4.2508e-19, 1.1708e-24,  ..., 2.7557e-25, 6.9932e-16,\n",
       "          2.5076e-21],\n",
       "         ...,\n",
       "         [5.8958e-36, 5.4925e-38, 3.1634e-21,  ..., 9.5636e-25, 1.8379e-28,\n",
       "          2.4237e-23],\n",
       "         [4.7296e-23, 1.0162e-41, 1.0000e+00,  ..., 7.9883e-35, 1.5204e-42,\n",
       "          2.2813e-42],\n",
       "         [2.2516e-16, 1.7949e-30, 4.3142e-16,  ..., 5.2620e-16, 3.9775e-32,\n",
       "          5.1686e-21]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################\n",
    "#       TEST D'UN MODEL (ET DU CODE)\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    nn.Softmax(dim=1),  # LogSoftmax for multi-class classification\n",
    "    )  \n",
    "\n",
    "batch_size = 128\n",
    "learning_rate=0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=0.001)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending, final_score_train = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "#Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending,final_score_train, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_560\n",
      "Epoch 1, Loss: 0.874840259552002, Final Score Train: 0.6685880422592163, Final Score Test: 0.6602449417114258 (gap -0.008343100547790527) macro F1 Train: 0.3871737839152913, macro F1 Test: 0.3692341311625453, 1-TPR Gap Train: 0.9500023126602173, 1-TPR Gap Test: 0.951255738735199\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.6803007125854492 gap -0.016678929328918457 || Macro F1: 0.43137779400128373 1-TPR_gap: 0.9292236566543579\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_1281\n",
      "Epoch 1, Loss: 0.6714240312576294, Final Score Train: 0.7201303839683533, Final Score Test: 0.6988503932952881 (gap -0.021279990673065186) macro F1 Train: 0.49332137688438754, macro F1 Test: 0.46348066860460474, 1-TPR Gap Train: 0.9469393491744995, 1-TPR Gap Test: 0.9342201352119446\n",
      "Epoch 10, Loss: 0.7563728094100952, Final Score Train: 0.7235465049743652, Final Score Test: 0.7072293162345886 (gap -0.01631718873977661) macro F1 Train: 0.5055187538863672, macro F1 Test: 0.4805934075050459, 1-TPR Gap Train: 0.9415743350982666, 1-TPR Gap Test: 0.9338651895523071\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7057598829269409 gap -0.025899291038513184 || Macro F1: 0.4719689380338429 1-TPR_gap: 0.9395508170127869\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_2562\n",
      "Epoch 1, Loss: 0.6954309940338135, Final Score Train: 0.7326699495315552, Final Score Test: 0.7077558040618896 (gap -0.024914145469665527) macro F1 Train: 0.5175272404804996, macro F1 Test: 0.4809860727310887, 1-TPR Gap Train: 0.9478126764297485, 1-TPR Gap Test: 0.9345255494117737\n",
      "Epoch 10, Loss: 0.6339743137359619, Final Score Train: 0.7393865585327148, Final Score Test: 0.7156922221183777 (gap -0.023694336414337158) macro F1 Train: 0.5315263206230408, macro F1 Test: 0.49294886992576437, 1-TPR Gap Train: 0.9472467303276062, 1-TPR Gap Test: 0.9384356141090393\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7163349390029907 gap -0.03229570388793945 || Macro F1: 0.49632282343093514 1-TPR_gap: 0.9363470673561096\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_5123\n",
      "Epoch 1, Loss: 0.6577661037445068, Final Score Train: 0.7556082010269165, Final Score Test: 0.7217410802841187 (gap -0.03386712074279785) macro F1 Train: 0.5598299403212617, macro F1 Test: 0.5058518974084746, 1-TPR Gap Train: 0.9513864517211914, 1-TPR Gap Test: 0.937630295753479\n",
      "Arrêt précoce après 7 époques\n",
      "Final Evaluation Score: 0.7194837331771851 gap -0.03724426031112671 || Macro F1: 0.4964190658868228 1-TPR_gap: 0.942548394203186\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.01_batch_size_10244\n",
      "Epoch 1, Loss: 0.6825166940689087, Final Score Train: 0.7623658180236816, Final Score Test: 0.7219544649124146 (gap -0.04041135311126709) macro F1 Train: 0.5693717329254554, macro F1 Test: 0.5063654557811167, 1-TPR Gap Train: 0.955359935760498, 1-TPR Gap Test: 0.9375434517860413\n",
      "Epoch 10, Loss: 0.6632565855979919, Final Score Train: 0.7696805596351624, Final Score Test: 0.7276933193206787 (gap -0.04198724031448364) macro F1 Train: 0.5818418175543721, macro F1 Test: 0.5078376409404287, 1-TPR Gap Train: 0.9575192928314209, 1-TPR Gap Test: 0.9475489854812622\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.727042555809021 gap -0.04367697238922119 || Macro F1: 0.5084073575246058 1-TPR_gap: 0.9456778168678284\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_565\n",
      "Epoch 1, Loss: 0.8620747327804565, Final Score Train: 0.7085718512535095, Final Score Test: 0.6877303719520569 (gap -0.020841479301452637) macro F1 Train: 0.48239221820180284, macro F1 Test: 0.44312588510498724, 1-TPR Gap Train: 0.9347514510154724, 1-TPR Gap Test: 0.9323348999023438\n",
      "Epoch 10, Loss: 0.7970952987670898, Final Score Train: 0.7403478622436523, Final Score Test: 0.7119687795639038 (gap -0.028379082679748535) macro F1 Train: 0.5230011111102245, macro F1 Test: 0.48073460387821554, 1-TPR Gap Train: 0.9576945304870605, 1-TPR Gap Test: 0.9432029128074646\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7047614455223083 gap -0.033315420150756836 || Macro F1: 0.4862059193206023 1-TPR_gap: 0.9233169555664062\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_1286\n",
      "Epoch 1, Loss: 0.7362677454948425, Final Score Train: 0.7437251806259155, Final Score Test: 0.7091494798660278 (gap -0.034575700759887695) macro F1 Train: 0.5338621306271439, macro F1 Test: 0.4841293955531199, 1-TPR Gap Train: 0.9535882472991943, 1-TPR Gap Test: 0.9341695308685303\n",
      "Epoch 10, Loss: 0.762660026550293, Final Score Train: 0.7551918029785156, Final Score Test: 0.7237687110900879 (gap -0.031423091888427734) macro F1 Train: 0.5661612419876413, macro F1 Test: 0.5246938208773586, 1-TPR Gap Train: 0.9442223310470581, 1-TPR Gap Test: 0.9228436350822449\n",
      "Epoch 20, Loss: 0.6721380949020386, Final Score Train: 0.7613589763641357, Final Score Test: 0.7210363149642944 (gap -0.04032266139984131) macro F1 Train: 0.568262257957235, macro F1 Test: 0.5166249615495067, 1-TPR Gap Train: 0.9544556140899658, 1-TPR Gap Test: 0.9254476428031921\n",
      "Arrêt précoce après 23 époques\n",
      "Final Evaluation Score: 0.7282117605209351 gap -0.03454482555389404 || Macro F1: 0.5185759368206742 1-TPR_gap: 0.9378475546836853\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_2567\n",
      "Epoch 1, Loss: 0.6736403703689575, Final Score Train: 0.7533913254737854, Final Score Test: 0.7198851108551025 (gap -0.03350621461868286) macro F1 Train: 0.5564371638834062, macro F1 Test: 0.5156854529345111, 1-TPR Gap Train: 0.950345516204834, 1-TPR Gap Test: 0.9240847826004028\n",
      "Epoch 10, Loss: 0.6728016138076782, Final Score Train: 0.7642827033996582, Final Score Test: 0.7261314392089844 (gap -0.03815126419067383) macro F1 Train: 0.5829602086652933, macro F1 Test: 0.542673382759012, 1-TPR Gap Train: 0.9456052780151367, 1-TPR Gap Test: 0.9095894694328308\n",
      "Arrêt précoce après 15 époques\n",
      "Final Evaluation Score: 0.7314606308937073 gap -0.042576372623443604 || Macro F1: 0.5393257223660498 1-TPR_gap: 0.9235955476760864\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_5128\n",
      "Epoch 1, Loss: 0.6612609624862671, Final Score Train: 0.777387261390686, Final Score Test: 0.7334542274475098 (gap -0.04393303394317627) macro F1 Train: 0.597845711770711, macro F1 Test: 0.5450491970016291, 1-TPR Gap Train: 0.9569288492202759, 1-TPR Gap Test: 0.9218592643737793\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.7379235625267029 gap -0.04848510026931763 || Macro F1: 0.5494223243508299 1-TPR_gap: 0.9264248013496399\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.001_batch_size_10249\n",
      "Epoch 1, Loss: 0.6498525142669678, Final Score Train: 0.788881242275238, Final Score Test: 0.7383702993392944 (gap -0.050510942935943604) macro F1 Train: 0.6232143865330355, macro F1 Test: 0.5520292935157607, 1-TPR Gap Train: 0.9545481204986572, 1-TPR Gap Test: 0.9247112274169922\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7398560047149658 gap -0.05074751377105713 || Macro F1: 0.553697547372129 1-TPR_gap: 0.9260144233703613\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_5610\n",
      "Epoch 1, Loss: 0.7588432431221008, Final Score Train: 0.7533203363418579, Final Score Test: 0.7139936089515686 (gap -0.03932672739028931) macro F1 Train: 0.5546010164084179, macro F1 Test: 0.5064886337848985, 1-TPR Gap Train: 0.9520395994186401, 1-TPR Gap Test: 0.9214985966682434\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.6988176107406616 gap -0.03335291147232056 || Macro F1: 0.48079715693559333 1-TPR_gap: 0.9168381094932556\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_12811\n",
      "Epoch 1, Loss: 0.7103326320648193, Final Score Train: 0.7554656267166138, Final Score Test: 0.7289799451828003 (gap -0.026485681533813477) macro F1 Train: 0.5644803294811379, macro F1 Test: 0.5237158839011236, 1-TPR Gap Train: 0.94645094871521, 1-TPR Gap Test: 0.9342440366744995\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.721603512763977 gap -0.04468822479248047 || Macro F1: 0.5155098883061477 1-TPR_gap: 0.9276971220970154\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_25612\n",
      "Epoch 1, Loss: 0.614031195640564, Final Score Train: 0.7717406749725342, Final Score Test: 0.7269086837768555 (gap -0.04483199119567871) macro F1 Train: 0.5880356707760351, macro F1 Test: 0.5245684254396339, 1-TPR Gap Train: 0.9554457068443298, 1-TPR Gap Test: 0.9292489886283875\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.7407068014144897 gap -0.040805816650390625 || Macro F1: 0.5466370085634478 1-TPR_gap: 0.9347766041755676\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_51213\n",
      "Epoch 1, Loss: 0.6660318970680237, Final Score Train: 0.787895917892456, Final Score Test: 0.7407259941101074 (gap -0.04716992378234863) macro F1 Train: 0.6169428106514098, macro F1 Test: 0.5453659782564005, 1-TPR Gap Train: 0.9588490128517151, 1-TPR Gap Test: 0.9360859990119934\n",
      "Epoch 10, Loss: 0.554824948310852, Final Score Train: 0.7779409289360046, Final Score Test: 0.7333840131759644 (gap -0.04455691576004028) macro F1 Train: 0.6007439485257613, macro F1 Test: 0.5435171971101623, 1-TPR Gap Train: 0.9551379084587097, 1-TPR Gap Test: 0.9232509136199951\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7383668422698975 gap -0.050921082496643066 || Macro F1: 0.5503726586971294 1-TPR_gap: 0.9263610243797302\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adam_lr_0.0001_batch_size_102414\n",
      "Epoch 1, Loss: 0.6285314559936523, Final Score Train: 0.7924944162368774, Final Score Test: 0.7399849891662598 (gap -0.052509427070617676) macro F1 Train: 0.630635449306097, macro F1 Test: 0.5515265700314194, 1-TPR Gap Train: 0.9543534517288208, 1-TPR Gap Test: 0.9284434914588928\n",
      "Epoch 10, Loss: 0.6260640621185303, Final Score Train: 0.7967871427536011, Final Score Test: 0.7412503957748413 (gap -0.055536746978759766) macro F1 Train: 0.6372544093128928, macro F1 Test: 0.5540154578647032, 1-TPR Gap Train: 0.9563198089599609, 1-TPR Gap Test: 0.9284853935241699\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7436093091964722 gap -0.053297996520996094 || Macro F1: 0.5585875032315067 1-TPR_gap: 0.9286311268806458\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_5615\n",
      "Epoch 1, Loss: 0.796585202217102, Final Score Train: 0.7960353493690491, Final Score Test: 0.7420668601989746 (gap -0.05396848917007446) macro F1 Train: 0.6366905887624162, macro F1 Test: 0.5580799360603838, 1-TPR Gap Train: 0.9553800821304321, 1-TPR Gap Test: 0.9260537028312683\n",
      "Arrêt précoce après 8 époques\n",
      "Final Evaluation Score: 0.7396546006202698 gap -0.0602724552154541 || Macro F1: 0.5569331265563233 1-TPR_gap: 0.9223760962486267\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_12816\n",
      "Epoch 1, Loss: 0.661085844039917, Final Score Train: 0.8000085353851318, Final Score Test: 0.7397836446762085 (gap -0.06022489070892334) macro F1 Train: 0.644499397106608, macro F1 Test: 0.5559930568480633, 1-TPR Gap Train: 0.955517590045929, 1-TPR Gap Test: 0.9235742092132568\n",
      "Arrêt précoce après 8 époques\n",
      "Final Evaluation Score: 0.7404773235321045 gap -0.06011533737182617 || Macro F1: 0.5548604799213234 1-TPR_gap: 0.9260941743850708\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_25617\n",
      "Epoch 1, Loss: 0.6091871857643127, Final Score Train: 0.8006830215454102, Final Score Test: 0.7409451007843018 (gap -0.0597379207611084) macro F1 Train: 0.6455953805823172, macro F1 Test: 0.555655488504914, 1-TPR Gap Train: 0.9557706117630005, 1-TPR Gap Test: 0.9262347221374512\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.7408217787742615 gap -0.060231149196624756 || Macro F1: 0.5566736906314419 1-TPR_gap: 0.9249698519706726\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_51218\n",
      "Epoch 1, Loss: 0.6105037927627563, Final Score Train: 0.801329493522644, Final Score Test: 0.7415413856506348 (gap -0.05978810787200928) macro F1 Train: 0.6467952017537962, macro F1 Test: 0.5567509295991445, 1-TPR Gap Train: 0.9558637142181396, 1-TPR Gap Test: 0.9263317584991455\n",
      "Epoch 10, Loss: 0.6433525085449219, Final Score Train: 0.8016315698623657, Final Score Test: 0.7415637969970703 (gap -0.06006777286529541) macro F1 Train: 0.6473113744284955, macro F1 Test: 0.5567968656408825, 1-TPR Gap Train: 0.9559517502784729, 1-TPR Gap Test: 0.9263308048248291\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7407361268997192 gap -0.06097221374511719 || Macro F1: 0.5566940690755765 1-TPR_gap: 0.9247782230377197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.01_batch_size_102419\n",
      "Epoch 1, Loss: 0.600393533706665, Final Score Train: 0.8017083406448364, Final Score Test: 0.7407361268997192 (gap -0.06097221374511719) macro F1 Train: 0.6474813450479308, macro F1 Test: 0.5566940690755765, 1-TPR Gap Train: 0.9559353590011597, 1-TPR Gap Test: 0.9247782230377197\n",
      "Epoch 10, Loss: 0.6578067541122437, Final Score Train: 0.8020482063293457, Final Score Test: 0.7408872842788696 (gap -0.061160922050476074) macro F1 Train: 0.6481364978478181, macro F1 Test: 0.5558366331948703, 1-TPR Gap Train: 0.9559599757194519, 1-TPR Gap Test: 0.9259379506111145\n",
      "Epoch 20, Loss: 0.637134850025177, Final Score Train: 0.8023864030838013, Final Score Test: 0.7409847378730774 (gap -0.06140166521072388) macro F1 Train: 0.6488497048829192, macro F1 Test: 0.556128774715532, 1-TPR Gap Train: 0.9559230804443359, 1-TPR Gap Test: 0.9258406758308411\n",
      "Arrêt précoce après 25 époques\n",
      "Final Evaluation Score: 0.7415899038314819 gap -0.060835957527160645 || Macro F1: 0.5569670995895076 1-TPR_gap: 0.9262126684188843\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_5620\n",
      "Epoch 1, Loss: 0.7398221492767334, Final Score Train: 0.8023096323013306, Final Score Test: 0.7408038377761841 (gap -0.061505794525146484) macro F1 Train: 0.648648527882935, macro F1 Test: 0.5570774117035124, 1-TPR Gap Train: 0.955970823764801, 1-TPR Gap Test: 0.9245302081108093\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.741470217704773 gap -0.060739219188690186 || Macro F1: 0.5586683470577379 1-TPR_gap: 0.9242720603942871\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_12821\n",
      "Epoch 1, Loss: 0.7005358934402466, Final Score Train: 0.802390456199646, Final Score Test: 0.7414140701293945 (gap -0.060976386070251465) macro F1 Train: 0.6485947934685355, macro F1 Test: 0.5587535714068971, 1-TPR Gap Train: 0.9561861753463745, 1-TPR Gap Test: 0.9240745902061462\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7412714958190918 gap -0.061218857765197754 || Macro F1: 0.5579562153095522 1-TPR_gap: 0.9245867729187012\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_25622\n",
      "Epoch 1, Loss: 0.6533230543136597, Final Score Train: 0.8024903535842896, Final Score Test: 0.7414273023605347 (gap -0.06106305122375488) macro F1 Train: 0.6488936285157373, macro F1 Test: 0.5580180406240127, 1-TPR Gap Train: 0.9560871124267578, 1-TPR Gap Test: 0.9248365163803101\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7413018941879272 gap -0.061358630657196045 || Macro F1: 0.5579864339280297 1-TPR_gap: 0.9246174097061157\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_51223\n",
      "Epoch 1, Loss: 0.6794260740280151, Final Score Train: 0.8026655316352844, Final Score Test: 0.741320013999939 (gap -0.06134551763534546) macro F1 Train: 0.6492537689314221, macro F1 Test: 0.5580226185272945, 1-TPR Gap Train: 0.9560772776603699, 1-TPR Gap Test: 0.9246174097061157\n",
      "Epoch 10, Loss: 0.5563656091690063, Final Score Train: 0.8029814958572388, Final Score Test: 0.7416707277297974 (gap -0.061310768127441406) macro F1 Train: 0.6498721748117989, macro F1 Test: 0.5586250584047494, 1-TPR Gap Train: 0.9560907483100891, 1-TPR Gap Test: 0.9247164726257324\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7418285012245178 gap -0.06120485067367554 || Macro F1: 0.5587398942238925 1-TPR_gap: 0.9249171018600464\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.001_batch_size_102424\n",
      "Epoch 1, Loss: 0.6323786377906799, Final Score Train: 0.8030333518981934, Final Score Test: 0.7415884733200073 (gap -0.061444878578186035) macro F1 Train: 0.6499760277167431, macro F1 Test: 0.5584604764429325, 1-TPR Gap Train: 0.9560907483100891, 1-TPR Gap Test: 0.9247164726257324\n",
      "Epoch 10, Loss: 0.6350300312042236, Final Score Train: 0.8030816316604614, Final Score Test: 0.7414436340332031 (gap -0.0616379976272583) macro F1 Train: 0.6500325401529452, macro F1 Test: 0.5573173822729576, 1-TPR Gap Train: 0.9561308026313782, 1-TPR Gap Test: 0.9255698323249817\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7414748668670654 gap -0.061606764793395996 || Macro F1: 0.5573798679030416 1-TPR_gap: 0.9255698323249817\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_5625\n",
      "Epoch 1, Loss: 0.8369861245155334, Final Score Train: 0.8030551671981812, Final Score Test: 0.742246150970459 (gap -0.06080901622772217) macro F1 Train: 0.6500112284871344, macro F1 Test: 0.5588817787394444, 1-TPR Gap Train: 0.9560990333557129, 1-TPR Gap Test: 0.9256105422973633\n",
      "Arrêt précoce après 8 époques\n",
      "Final Evaluation Score: 0.7419331073760986 gap -0.061005473136901855 || Macro F1: 0.5585615532798455 1-TPR_gap: 0.9253045916557312\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_12826\n",
      "Epoch 1, Loss: 0.6374574899673462, Final Score Train: 0.8029836416244507, Final Score Test: 0.7418237924575806 (gap -0.06115984916687012) macro F1 Train: 0.6498537687799484, macro F1 Test: 0.5584215258407262, 1-TPR Gap Train: 0.9561135172843933, 1-TPR Gap Test: 0.925226092338562\n",
      "Arrêt précoce après 7 époques\n",
      "Final Evaluation Score: 0.7417528629302979 gap -0.06131136417388916 || Macro F1: 0.55809134851887 1-TPR_gap: 0.9254143238067627\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_25627\n",
      "Epoch 1, Loss: 0.6046836972236633, Final Score Train: 0.8030600547790527, Final Score Test: 0.741779088973999 (gap -0.06128096580505371) macro F1 Train: 0.650073337333129, macro F1 Test: 0.5582948123409807, 1-TPR Gap Train: 0.9560467004776001, 1-TPR Gap Test: 0.9252634048461914\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7417465448379517 gap -0.06149989366531372 || Macro F1: 0.5580967477082459 1-TPR_gap: 0.9253963828086853\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_51228\n",
      "Epoch 1, Loss: 0.6620240211486816, Final Score Train: 0.8032464385032654, Final Score Test: 0.7418034076690674 (gap -0.061443030834198) macro F1 Train: 0.6504619051161332, macro F1 Test: 0.558191084994917, 1-TPR Gap Train: 0.9560309648513794, 1-TPR Gap Test: 0.9254157543182373\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.7416660189628601 gap -0.06166410446166992 || Macro F1: 0.5574689502803452 1-TPR_gap: 0.9258630871772766\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Adagrad_lr_0.0001_batch_size_102429\n",
      "Epoch 1, Loss: 0.6504766941070557, Final Score Train: 0.80333012342453, Final Score Test: 0.7417047023773193 (gap -0.06162542104721069) macro F1 Train: 0.6506293112670977, macro F1 Test: 0.5575269461921487, 1-TPR Gap Train: 0.9560309648513794, 1-TPR Gap Test: 0.9258825182914734\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7417047023773193 gap -0.06171607971191406 || Macro F1: 0.5575269461921487 1-TPR_gap: 0.9258825182914734\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_5630\n",
      "Epoch 1, Loss: 0.760114848613739, Final Score Train: 0.8034207820892334, Final Score Test: 0.7417130470275879 (gap -0.06170773506164551) macro F1 Train: 0.6507842822188621, macro F1 Test: 0.5574142685242346, 1-TPR Gap Train: 0.9560572504997253, 1-TPR Gap Test: 0.926011860370636\n",
      "Epoch 10, Loss: 0.8142625689506531, Final Score Train: 0.8033440709114075, Final Score Test: 0.7418872117996216 (gap -0.06145685911178589) macro F1 Train: 0.650646194921949, macro F1 Test: 0.558594780785958, 1-TPR Gap Train: 0.9560419321060181, 1-TPR Gap Test: 0.9251796007156372\n",
      "Epoch 20, Loss: 0.8448794484138489, Final Score Train: 0.803219199180603, Final Score Test: 0.7420694828033447 (gap -0.0611497163772583) macro F1 Train: 0.6504013241305158, macro F1 Test: 0.5587588237798145, 1-TPR Gap Train: 0.9560370445251465, 1-TPR Gap Test: 0.9253802299499512\n",
      "Epoch 30, Loss: 0.7116074562072754, Final Score Train: 0.8031798601150513, Final Score Test: 0.7421659231185913 (gap -0.06101393699645996) macro F1 Train: 0.6503414404067611, macro F1 Test: 0.5590072159971717, 1-TPR Gap Train: 0.956018328666687, 1-TPR Gap Test: 0.9253246188163757\n",
      "Arrêt précoce après 31 époques\n",
      "Final Evaluation Score: 0.741897463798523 gap -0.061305880546569824 || Macro F1: 0.5584147204151381 1-TPR_gap: 0.9253802299499512\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_12831\n",
      "Epoch 1, Loss: 0.6964395642280579, Final Score Train: 0.8032330274581909, Final Score Test: 0.7419655323028564 (gap -0.06126749515533447) macro F1 Train: 0.6504477601414536, macro F1 Test: 0.5585508049103728, 1-TPR Gap Train: 0.956018328666687, 1-TPR Gap Test: 0.9253802299499512\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7421282529830933 gap -0.061142921447753906 || Macro F1: 0.5588763380124239 1-TPR_gap: 0.9253802299499512\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_25632\n",
      "Epoch 1, Loss: 0.568324089050293, Final Score Train: 0.80333411693573, Final Score Test: 0.7422136068344116 (gap -0.06112051010131836) macro F1 Train: 0.650649871229317, macro F1 Test: 0.5590469815078091, 1-TPR Gap Train: 0.956018328666687, 1-TPR Gap Test: 0.9253802299499512\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7418385744094849 gap -0.06151527166366577 || Macro F1: 0.5578361336527329 1-TPR_gap: 0.9258409738540649\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_51233\n",
      "Epoch 1, Loss: 0.5942111015319824, Final Score Train: 0.8033589124679565, Final Score Test: 0.7417439222335815 (gap -0.061614990234375) macro F1 Train: 0.6506807192502713, macro F1 Test: 0.5576469198947223, 1-TPR Gap Train: 0.9560370445251465, 1-TPR Gap Test: 0.9258409738540649\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7418308854103088 gap -0.061614930629730225 || Macro F1: 0.5575075362182813 1-TPR_gap: 0.9261542558670044\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.01_batch_size_102434\n",
      "Epoch 1, Loss: 0.631103515625, Final Score Train: 0.8034464120864868, Final Score Test: 0.7418308854103088 (gap -0.06161552667617798) macro F1 Train: 0.6508715764513193, macro F1 Test: 0.5575075362182813, 1-TPR Gap Train: 0.9560213088989258, 1-TPR Gap Test: 0.9261542558670044\n",
      "Epoch 10, Loss: 0.6254764199256897, Final Score Train: 0.8034471273422241, Final Score Test: 0.7417372465133667 (gap -0.06170988082885742) macro F1 Train: 0.6508887028729177, macro F1 Test: 0.5574497196757834, 1-TPR Gap Train: 0.9560055732727051, 1-TPR Gap Test: 0.926024854183197\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7417372465133667 gap -0.06170988082885742 || Macro F1: 0.5574497196757834 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_5635\n",
      "Epoch 1, Loss: 0.777976393699646, Final Score Train: 0.8034452795982361, Final Score Test: 0.7417987585067749 (gap -0.06164652109146118) macro F1 Train: 0.6508849850440127, macro F1 Test: 0.5578860142724122, 1-TPR Gap Train: 0.9560055732727051, 1-TPR Gap Test: 0.9257115721702576\n",
      "Epoch 10, Loss: 0.754535436630249, Final Score Train: 0.80341637134552, Final Score Test: 0.741916298866272 (gap -0.06150007247924805) macro F1 Train: 0.6508119999992472, macro F1 Test: 0.5587823901380745, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.9250501990318298\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7420826554298401 gap -0.06123840808868408 || Macro F1: 0.5586012022921938 1-TPR_gap: 0.9255641102790833\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_12836\n",
      "Epoch 1, Loss: 0.6946436166763306, Final Score Train: 0.8033487796783447, Final Score Test: 0.7418716549873352 (gap -0.06147712469100952) macro F1 Train: 0.6506452408080846, macro F1 Test: 0.5586931221426852, 1-TPR Gap Train: 0.956052303314209, 1-TPR Gap Test: 0.9250501990318298\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7418732643127441 gap -0.06151175498962402 || Macro F1: 0.5583830895311783 1-TPR_gap: 0.9253634810447693\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_25637\n",
      "Epoch 1, Loss: 0.6570702791213989, Final Score Train: 0.8033888339996338, Final Score Test: 0.7418732643127441 (gap -0.06151556968688965) macro F1 Train: 0.6507568601573887, macro F1 Test: 0.5583830895311783, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.9253634810447693\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7416985630989075 gap -0.061751604080200195 || Macro F1: 0.5573722823067345 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_51238\n",
      "Epoch 1, Loss: 0.633571982383728, Final Score Train: 0.8034861087799072, Final Score Test: 0.7417510747909546 (gap -0.06173503398895264) macro F1 Train: 0.6509514035237431, macro F1 Test: 0.5574773334074014, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7417510747909546 gap -0.06173503398895264 || Macro F1: 0.5574773334074014 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.001_batch_size_102439\n",
      "Epoch 1, Loss: 0.64323890209198, Final Score Train: 0.8034861087799072, Final Score Test: 0.7417510747909546 (gap -0.06173503398895264) macro F1 Train: 0.6509514035237431, macro F1 Test: 0.5574773334074014, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Epoch 10, Loss: 0.6233685612678528, Final Score Train: 0.8034801483154297, Final Score Test: 0.7417510747909546 (gap -0.0617290735244751) macro F1 Train: 0.6509395122690138, macro F1 Test: 0.5574773334074014, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Arrêt précoce après 13 époques\n",
      "Final Evaluation Score: 0.7417510747909546 gap -0.0617290735244751 || Macro F1: 0.5574773334074014 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_5640\n",
      "Epoch 1, Loss: 0.7874714732170105, Final Score Train: 0.8034861087799072, Final Score Test: 0.7417510747909546 (gap -0.06173503398895264) macro F1 Train: 0.6509514035237431, macro F1 Test: 0.5574773334074014, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Epoch 10, Loss: 0.865851104259491, Final Score Train: 0.8034193515777588, Final Score Test: 0.7418716549873352 (gap -0.061547696590423584) macro F1 Train: 0.6508178849206393, macro F1 Test: 0.5586931221426852, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.9250501990318298\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7420810461044312 gap -0.061304450035095215 || Macro F1: 0.5589112349037006 1-TPR_gap: 0.9252508282661438\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_12841\n",
      "Epoch 1, Loss: 0.8124533891677856, Final Score Train: 0.8033854961395264, Final Score Test: 0.7420810461044312 (gap -0.061304450035095215) macro F1 Train: 0.6507374264480922, macro F1 Test: 0.5589112349037006, 1-TPR Gap Train: 0.9560335874557495, 1-TPR Gap Test: 0.9252508282661438\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7419063448905945 gap -0.06151241064071655 || Macro F1: 0.5579004276792568 1-TPR_gap: 0.9259122610092163\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_25642\n",
      "Epoch 1, Loss: 0.5987375974655151, Final Score Train: 0.803418755531311, Final Score Test: 0.7419063448905945 (gap -0.06151241064071655) macro F1 Train: 0.6508167106291142, macro F1 Test: 0.5579004276792568, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.9259122610092163\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7416985630989075 gap -0.06175100803375244 || Macro F1: 0.5573722823067345 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_51243\n",
      "Epoch 1, Loss: 0.6576511859893799, Final Score Train: 0.8034495711326599, Final Score Test: 0.7416985630989075 (gap -0.06175100803375244) macro F1 Train: 0.650878308852609, macro F1 Test: 0.5573722823067345, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7416985630989075 gap -0.061787545680999756 || Macro F1: 0.5573722823067345 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_SGD_lr_0.0001_batch_size_102444\n",
      "Epoch 1, Loss: 0.6074264645576477, Final Score Train: 0.8034861087799072, Final Score Test: 0.7416985630989075 (gap -0.061787545680999756) macro F1 Train: 0.6509514035237431, macro F1 Test: 0.5573722823067345, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Epoch 10, Loss: 0.6557978391647339, Final Score Train: 0.8034861087799072, Final Score Test: 0.7416985630989075 (gap -0.061787545680999756) macro F1 Train: 0.6509514035237431, macro F1 Test: 0.5573722823067345, 1-TPR Gap Train: 0.9560208320617676, 1-TPR Gap Test: 0.926024854183197\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7417625188827515 gap -0.06172358989715576 || Macro F1: 0.5575000972202598 1-TPR_gap: 0.926024854183197\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_5645\n",
      "Epoch 1, Loss: 0.835564136505127, Final Score Train: 0.8033241033554077, Final Score Test: 0.7419666051864624 (gap -0.06135749816894531) macro F1 Train: 0.6505414321200468, macro F1 Test: 0.5569770753880117, 1-TPR Gap Train: 0.9561067819595337, 1-TPR Gap Test: 0.926956057548523\n",
      "Epoch 10, Loss: 0.7690467834472656, Final Score Train: 0.8029482364654541, Final Score Test: 0.7413959503173828 (gap -0.06155228614807129) macro F1 Train: 0.6498699294943461, macro F1 Test: 0.5574577187895251, 1-TPR Gap Train: 0.9560266137123108, 1-TPR Gap Test: 0.9253342151641846\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7416008710861206 gap -0.06123960018157959 || Macro F1: 0.5580541470264867 1-TPR_gap: 0.9251475930213928\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_12846\n",
      "Epoch 1, Loss: 0.7364450693130493, Final Score Train: 0.8029757142066956, Final Score Test: 0.7416903972625732 (gap -0.061285316944122314) macro F1 Train: 0.6499248124614168, macro F1 Test: 0.5578987149286138, 1-TPR Gap Train: 0.9560266137123108, 1-TPR Gap Test: 0.9254820942878723\n",
      "Epoch 10, Loss: 0.7159979343414307, Final Score Train: 0.8030576109886169, Final Score Test: 0.7410898804664612 (gap -0.06196773052215576) macro F1 Train: 0.6501003227035008, macro F1 Test: 0.5567701822447798, 1-TPR Gap Train: 0.95601487159729, 1-TPR Gap Test: 0.9254095554351807\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7409304976463318 gap -0.062124431133270264 || Macro F1: 0.5559762969463277 1-TPR_gap: 0.9258847236633301\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_25647\n",
      "Epoch 1, Loss: 0.6408193111419678, Final Score Train: 0.8031237125396729, Final Score Test: 0.7411488890647888 (gap -0.06197482347488403) macro F1 Train: 0.6502294928042485, macro F1 Test: 0.5567088889040899, 1-TPR Gap Train: 0.9560178518295288, 1-TPR Gap Test: 0.9255889058113098\n",
      "Epoch 10, Loss: 0.6564795970916748, Final Score Train: 0.8032481074333191, Final Score Test: 0.740572452545166 (gap -0.06267565488815308) macro F1 Train: 0.6504990633611604, macro F1 Test: 0.5560783083350314, 1-TPR Gap Train: 0.9559971690177917, 1-TPR Gap Test: 0.9250666499137878\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7407488822937012 gap -0.0625234842300415 || Macro F1: 0.5563588334287092 1-TPR_gap: 0.9251389503479004\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_51248\n",
      "Epoch 1, Loss: 0.5741192102432251, Final Score Train: 0.8033105134963989, Final Score Test: 0.741149365901947 (gap -0.062161147594451904) macro F1 Train: 0.6506335919307702, macro F1 Test: 0.556787729379255, 1-TPR Gap Train: 0.9559873938560486, 1-TPR Gap Test: 0.9255110025405884\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.7411199808120728 gap -0.062261343002319336 || Macro F1: 0.5567289830916269 1-TPR_gap: 0.9255110025405884\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.01_batch_size_102449\n",
      "Epoch 1, Loss: 0.6268364787101746, Final Score Train: 0.8033813238143921, Final Score Test: 0.7416901588439941 (gap -0.06169116497039795) macro F1 Train: 0.6507801313823716, macro F1 Test: 0.5571825483911416, 1-TPR Gap Train: 0.9559824466705322, 1-TPR Gap Test: 0.9261978268623352\n",
      "Epoch 10, Loss: 0.5929868817329407, Final Score Train: 0.8035537004470825, Final Score Test: 0.7418253421783447 (gap -0.06172835826873779) macro F1 Train: 0.6511249633049575, macro F1 Test: 0.5577620519895319, 1-TPR Gap Train: 0.9559824466705322, 1-TPR Gap Test: 0.9258885979652405\n",
      "Arrêt précoce après 17 époques\n",
      "Final Evaluation Score: 0.7413936853408813 gap -0.06225228309631348 || Macro F1: 0.5574021895078954 1-TPR_gap: 0.9253852367401123\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_5650\n",
      "Epoch 1, Loss: 0.8071430921554565, Final Score Train: 0.8035532236099243, Final Score Test: 0.7413601875305176 (gap -0.06219303607940674) macro F1 Train: 0.6510875427321681, macro F1 Test: 0.5575279917748289, 1-TPR Gap Train: 0.95601886510849, 1-TPR Gap Test: 0.9251923561096191\n",
      "Epoch 10, Loss: 0.6332421898841858, Final Score Train: 0.8030956983566284, Final Score Test: 0.7404409050941467 (gap -0.06265479326248169) macro F1 Train: 0.6501044916954302, macro F1 Test: 0.5559197597230551, 1-TPR Gap Train: 0.9560869336128235, 1-TPR Gap Test: 0.924962043762207\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7406601309776306 gap -0.062394797801971436 || Macro F1: 0.5569802501667663 1-TPR_gap: 0.924340009689331\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_12851\n",
      "Epoch 1, Loss: 0.6766750812530518, Final Score Train: 0.8030948638916016, Final Score Test: 0.7405929565429688 (gap -0.06250190734863281) macro F1 Train: 0.6502150887502841, macro F1 Test: 0.5561221141716364, 1-TPR Gap Train: 0.9559746384620667, 1-TPR Gap Test: 0.9250638484954834\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.7407567501068115 gap -0.062407732009887695 || Macro F1: 0.5571937922673685 1-TPR_gap: 0.9243197441101074\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_25652\n",
      "Epoch 1, Loss: 0.5724500417709351, Final Score Train: 0.8032964468002319, Final Score Test: 0.740668773651123 (gap -0.06262767314910889) macro F1 Train: 0.6506057888612358, macro F1 Test: 0.5571472165494411, 1-TPR Gap Train: 0.9559870958328247, 1-TPR Gap Test: 0.9241903424263\n",
      "Arrêt précoce après 7 époques\n",
      "Final Evaluation Score: 0.7407430410385132 gap -0.06271028518676758 || Macro F1: 0.5572957942630036 1-TPR_gap: 0.9241903424263\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_51253\n",
      "Epoch 1, Loss: 0.6200684309005737, Final Score Train: 0.8034507036209106, Final Score Test: 0.7408616542816162 (gap -0.06258904933929443) macro F1 Train: 0.650914345962993, macro F1 Test: 0.5575328935550397, 1-TPR Gap Train: 0.9559870958328247, 1-TPR Gap Test: 0.9241903424263\n",
      "Epoch 10, Loss: 0.6288981437683105, Final Score Train: 0.8034640550613403, Final Score Test: 0.7409284114837646 (gap -0.06253564357757568) macro F1 Train: 0.650941060671452, macro F1 Test: 0.557666439023231, 1-TPR Gap Train: 0.9559870958328247, 1-TPR Gap Test: 0.9241903424263\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7409172058105469 gap -0.06254744529724121 || Macro F1: 0.5576441135506501 1-TPR_gap: 0.9241903424263\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.001_batch_size_102454\n",
      "Epoch 1, Loss: 0.6149839162826538, Final Score Train: 0.8035120964050293, Final Score Test: 0.7408337593078613 (gap -0.06267833709716797) macro F1 Train: 0.6510371337864697, macro F1 Test: 0.5574771268260947, 1-TPR Gap Train: 0.9559870958328247, 1-TPR Gap Test: 0.9241903424263\n",
      "Epoch 10, Loss: 0.5929068326950073, Final Score Train: 0.8036149144172668, Final Score Test: 0.7416222095489502 (gap -0.06199270486831665) macro F1 Train: 0.6512427045481436, macro F1 Test: 0.5580939364334812, 1-TPR Gap Train: 0.9559870958328247, 1-TPR Gap Test: 0.9251503944396973\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7416222095489502 gap -0.06204104423522949 || Macro F1: 0.5580939364334812 1-TPR_gap: 0.9251503944396973\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_5655\n",
      "Epoch 1, Loss: 0.7353899478912354, Final Score Train: 0.8036056756973267, Final Score Test: 0.7413256168365479 (gap -0.06228005886077881) macro F1 Train: 0.6511879192374215, macro F1 Test: 0.5578144200909496, 1-TPR Gap Train: 0.9560234546661377, 1-TPR Gap Test: 0.9248368740081787\n",
      "Arrêt précoce après 7 époques\n",
      "Final Evaluation Score: 0.7402935028076172 gap -0.06322181224822998 || Macro F1: 0.5555938028610127 1-TPR_gap: 0.9249932765960693\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_12856\n",
      "Epoch 1, Loss: 0.658998966217041, Final Score Train: 0.803569495677948, Final Score Test: 0.7407048940658569 (gap -0.06286460161209106) macro F1 Train: 0.6510400955705645, macro F1 Test: 0.5560440559472678, 1-TPR Gap Train: 0.9560989141464233, 1-TPR Gap Test: 0.9253657460212708\n",
      "Arrêt précoce après 8 époques\n",
      "Final Evaluation Score: 0.7406182289123535 gap -0.06271195411682129 || Macro F1: 0.5557616982443043 1-TPR_gap: 0.9254747629165649\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_25657\n",
      "Epoch 1, Loss: 0.636002779006958, Final Score Train: 0.8034878373146057, Final Score Test: 0.7405879497528076 (gap -0.0628998875617981) macro F1 Train: 0.6509131231138896, macro F1 Test: 0.5562147705852738, 1-TPR Gap Train: 0.9560625553131104, 1-TPR Gap Test: 0.9249610900878906\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.740304708480835 gap -0.06318312883377075 || Macro F1: 0.5556323495089636 1-TPR_gap: 0.9249770045280457\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_51258\n",
      "Epoch 1, Loss: 0.6536517143249512, Final Score Train: 0.8034878373146057, Final Score Test: 0.7406719923019409 (gap -0.0628158450126648) macro F1 Train: 0.6509131231138896, macro F1 Test: 0.556294671047748, 1-TPR Gap Train: 0.9560625553131104, 1-TPR Gap Test: 0.9250493049621582\n",
      "Epoch 10, Loss: 0.7046627998352051, Final Score Train: 0.8036108016967773, Final Score Test: 0.7404723167419434 (gap -0.06313848495483398) macro F1 Train: 0.6511590881674278, macro F1 Test: 0.5563398783048555, 1-TPR Gap Train: 0.9560625553131104, 1-TPR Gap Test: 0.9246047139167786\n",
      "Arrêt précoce après 12 époques\n",
      "Final Evaluation Score: 0.7406517863273621 gap -0.063007652759552 || Macro F1: 0.556567550073061 1-TPR_gap: 0.9247360229492188\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_Momentum_lr_0.0001_batch_size_102459\n",
      "Epoch 1, Loss: 0.6050999164581299, Final Score Train: 0.8036588430404663, Final Score Test: 0.7408881187438965 (gap -0.06277072429656982) macro F1 Train: 0.6512551163059295, macro F1 Test: 0.5570402616663159, 1-TPR Gap Train: 0.9560625553131104, 1-TPR Gap Test: 0.9247360229492188\n",
      "Epoch 10, Loss: 0.619949996471405, Final Score Train: 0.8038105368614197, Final Score Test: 0.7415480613708496 (gap -0.06226247549057007) macro F1 Train: 0.651558503865509, macro F1 Test: 0.5577858146453997, 1-TPR Gap Train: 0.9560625553131104, 1-TPR Gap Test: 0.9253103733062744\n",
      "Arrêt précoce après 14 époques\n",
      "Final Evaluation Score: 0.7414132356643677 gap -0.062397301197052 || Macro F1: 0.5577908207115361 1-TPR_gap: 0.9250356554985046\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_5660\n",
      "Epoch 1, Loss: 0.8753472566604614, Final Score Train: 0.8038234114646912, Final Score Test: 0.7415618896484375 (gap -0.06226152181625366) macro F1 Train: 0.6515468683746665, macro F1 Test: 0.5578665661933284, 1-TPR Gap Train: 0.9560999274253845, 1-TPR Gap Test: 0.9252572059631348\n",
      "Epoch 10, Loss: 0.677222490310669, Final Score Train: 0.803246021270752, Final Score Test: 0.7411267161369324 (gap -0.06211930513381958) macro F1 Train: 0.6504817265490498, macro F1 Test: 0.5577292085721876, 1-TPR Gap Train: 0.9560103416442871, 1-TPR Gap Test: 0.9245242476463318\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.7411797642707825 gap -0.06171923875808716 || Macro F1: 0.5562983751003613 1-TPR_gap: 0.9260611534118652\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_12861\n",
      "Epoch 1, Loss: 0.7308581471443176, Final Score Train: 0.8029782772064209, Final Score Test: 0.7400102615356445 (gap -0.06296801567077637) macro F1 Train: 0.6497989839314523, macro F1 Test: 0.5548687070968331, 1-TPR Gap Train: 0.9561575055122375, 1-TPR Gap Test: 0.9251517653465271\n",
      "Arrêt précoce après 9 époques\n",
      "Final Evaluation Score: 0.7400813698768616 gap -0.063412606716156 || Macro F1: 0.555846792227528 1-TPR_gap: 0.9243159294128418\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_25662\n",
      "Epoch 1, Loss: 0.6608606576919556, Final Score Train: 0.8035141825675964, Final Score Test: 0.7403546571731567 (gap -0.0631595253944397) macro F1 Train: 0.6508836175277011, macro F1 Test: 0.5563297934086511, 1-TPR Gap Train: 0.9561447501182556, 1-TPR Gap Test: 0.9243794679641724\n",
      "Epoch 10, Loss: 0.6741868257522583, Final Score Train: 0.8036940097808838, Final Score Test: 0.7406812906265259 (gap -0.06301271915435791) macro F1 Train: 0.6512374270965363, macro F1 Test: 0.5574616023163109, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9239009022712708\n",
      "Arrêt précoce après 19 époques\n",
      "Final Evaluation Score: 0.7405675649642944 gap -0.06316357851028442 || Macro F1: 0.5569992021344193 1-TPR_gap: 0.9241358637809753\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_51263\n",
      "Epoch 1, Loss: 0.6027971506118774, Final Score Train: 0.8037366271018982, Final Score Test: 0.740766704082489 (gap -0.06296992301940918) macro F1 Train: 0.6513226244998485, macro F1 Test: 0.5576325068283754, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9239009022712708\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7408080101013184 gap -0.0629231333732605 || Macro F1: 0.5573367387447009 1-TPR_gap: 0.9242792725563049\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.01_batch_size_102464\n",
      "Epoch 1, Loss: 0.5935922861099243, Final Score Train: 0.8037311434745789, Final Score Test: 0.7410728931427002 (gap -0.06265825033187866) macro F1 Train: 0.6513116485352979, macro F1 Test: 0.557866430259663, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9242792725563049\n",
      "Epoch 10, Loss: 0.6055006980895996, Final Score Train: 0.8037366271018982, Final Score Test: 0.7411317825317383 (gap -0.06260484457015991) macro F1 Train: 0.6513226244998485, macro F1 Test: 0.5580037071257258, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9242598414421082\n",
      "Epoch 20, Loss: 0.6227104663848877, Final Score Train: 0.8038002252578735, Final Score Test: 0.7410473823547363 (gap -0.06275284290313721) macro F1 Train: 0.6514498632270724, macro F1 Test: 0.5578154230316227, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9242792725563049\n",
      "Epoch 30, Loss: 0.621353030204773, Final Score Train: 0.8039594292640686, Final Score Test: 0.7415956258773804 (gap -0.06236380338668823) macro F1 Train: 0.651768201884043, macro F1 Test: 0.558931486167303, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9242598414421082\n",
      "Epoch 40, Loss: 0.6366703510284424, Final Score Train: 0.8039594292640686, Final Score Test: 0.7414225935935974 (gap -0.06253683567047119) macro F1 Train: 0.651768201884043, macro F1 Test: 0.5588008587579086, 1-TPR Gap Train: 0.9561506509780884, 1-TPR Gap Test: 0.9240443110466003\n",
      "Arrêt précoce après 43 époques\n",
      "Final Evaluation Score: 0.7414646148681641 gap -0.06254053115844727 || Macro F1: 0.5588849996707016 1-TPR_gap: 0.9240443110466003\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_5665\n",
      "Epoch 1, Loss: 0.8403655886650085, Final Score Train: 0.8039146065711975, Final Score Test: 0.7411177158355713 (gap -0.06279689073562622) macro F1 Train: 0.6516844850977852, macro F1 Test: 0.5576723654957474, 1-TPR Gap Train: 0.9561447501182556, 1-TPR Gap Test: 0.9245629906654358\n",
      "Arrêt précoce après 10 époques\n",
      "Final Evaluation Score: 0.7422130107879639 gap -0.06124579906463623 || Macro F1: 0.5582562667910543 1-TPR_gap: 0.926169753074646\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_12866\n",
      "Epoch 1, Loss: 0.5575939416885376, Final Score Train: 0.8038216233253479, Final Score Test: 0.741265058517456 (gap -0.06255656480789185) macro F1 Train: 0.65151133461639, macro F1 Test: 0.5580548483170732, 1-TPR Gap Train: 0.9561319351196289, 1-TPR Gap Test: 0.9244753122329712\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7410755157470703 gap -0.06274610757827759 || Macro F1: 0.5573619373137653 1-TPR_gap: 0.9247891306877136\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_25667\n",
      "Epoch 1, Loss: 0.6345540285110474, Final Score Train: 0.8038475513458252, Final Score Test: 0.7409488558769226 (gap -0.06289869546890259) macro F1 Train: 0.6515513391706413, macro F1 Test: 0.5573364273326379, 1-TPR Gap Train: 0.9561437964439392, 1-TPR Gap Test: 0.9245612621307373\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7410340309143066 gap -0.06286084651947021 || Macro F1: 0.5574334159760956 1-TPR_gap: 0.924634575843811\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_51268\n",
      "Epoch 1, Loss: 0.6367210745811462, Final Score Train: 0.8038948774337769, Final Score Test: 0.741152286529541 (gap -0.06274259090423584) macro F1 Train: 0.6516148183569345, macro F1 Test: 0.5576699443448198, 1-TPR Gap Train: 0.9561749696731567, 1-TPR Gap Test: 0.924634575843811\n",
      "Epoch 10, Loss: 0.5889403820037842, Final Score Train: 0.8040456175804138, Final Score Test: 0.741783618927002 (gap -0.062261998653411865) macro F1 Train: 0.6519162792483213, macro F1 Test: 0.5587837783227481, 1-TPR Gap Train: 0.9561749696731567, 1-TPR Gap Test: 0.9247835278511047\n",
      "Arrêt précoce après 16 époques\n",
      "Final Evaluation Score: 0.7415215969085693 gap -0.06252455711364746 || Macro F1: 0.558385452966997 1-TPR_gap: 0.9246578216552734\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.001_batch_size_102469\n",
      "Epoch 1, Loss: 0.6243029832839966, Final Score Train: 0.8040456175804138, Final Score Test: 0.7416080236434937 (gap -0.062437593936920166) macro F1 Train: 0.6519162792483213, macro F1 Test: 0.5585582988748702, 1-TPR Gap Train: 0.9561749696731567, 1-TPR Gap Test: 0.9246578216552734\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7416515350341797 gap -0.06248676776885986 || Macro F1: 0.558645295301601 1-TPR_gap: 0.9246578216552734\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_5670\n",
      "Epoch 1, Loss: 0.7817178964614868, Final Score Train: 0.8040807247161865, Final Score Test: 0.7410391569137573 (gap -0.0630415678024292) macro F1 Train: 0.6519550460915954, macro F1 Test: 0.5579910393519824, 1-TPR Gap Train: 0.9562064409255981, 1-TPR Gap Test: 0.9240872859954834\n",
      "Epoch 10, Loss: 0.6784443855285645, Final Score Train: 0.8040450811386108, Final Score Test: 0.7417004108428955 (gap -0.06234467029571533) macro F1 Train: 0.6519152590954322, macro F1 Test: 0.5584389676453041, 1-TPR Gap Train: 0.9561749696731567, 1-TPR Gap Test: 0.9249618053436279\n",
      "Arrêt précoce après 11 époques\n",
      "Final Evaluation Score: 0.7417513132095337 gap -0.06227850914001465 || Macro F1: 0.5588477288176924 1-TPR_gap: 0.9246549010276794\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_12871\n",
      "Epoch 1, Loss: 0.7519584894180298, Final Score Train: 0.8040850162506104, Final Score Test: 0.741761326789856 (gap -0.062323689460754395) macro F1 Train: 0.6519561672194947, macro F1 Test: 0.5589797270107058, 1-TPR Gap Train: 0.9562138319015503, 1-TPR Gap Test: 0.9245428442955017\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7416709065437317 gap -0.062385380268096924 || Macro F1: 0.5585242689356301 1-TPR_gap: 0.9248175621032715\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_25672\n",
      "Epoch 1, Loss: 0.6518054008483887, Final Score Train: 0.8041035532951355, Final Score Test: 0.7414811849594116 (gap -0.06262236833572388) macro F1 Train: 0.6520168971106607, macro F1 Test: 0.5581197613563607, 1-TPR Gap Train: 0.9561902284622192, 1-TPR Gap Test: 0.9248425364494324\n",
      "Epoch 10, Loss: 0.5901780128479004, Final Score Train: 0.8042249083518982, Final Score Test: 0.7418155670166016 (gap -0.06240934133529663) macro F1 Train: 0.652247733554682, macro F1 Test: 0.5589805689263682, 1-TPR Gap Train: 0.9562020897865295, 1-TPR Gap Test: 0.9246505498886108\n",
      "Arrêt précoce après 18 époques\n",
      "Final Evaluation Score: 0.741689920425415 gap -0.06248664855957031 || Macro F1: 0.5585816537106967 1-TPR_gap: 0.9247981905937195\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_51273\n",
      "Epoch 1, Loss: 0.5758530497550964, Final Score Train: 0.8041791915893555, Final Score Test: 0.7416452169418335 (gap -0.06253397464752197) macro F1 Train: 0.6521562528120842, macro F1 Test: 0.5584728737216416, 1-TPR Gap Train: 0.9562020897865295, 1-TPR Gap Test: 0.9248175621032715\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7414988279342651 gap -0.06277245283126831 || Macro F1: 0.5572609586961242 1-TPR_gap: 0.9257367253303528\n",
      "\n",
      "\n",
      "Starting to train model NN-28-28_NAG_lr_0.0001_batch_size_102474\n",
      "Epoch 1, Loss: 0.6071901321411133, Final Score Train: 0.8042712807655334, Final Score Test: 0.7414988279342651 (gap -0.06277245283126831) macro F1 Train: 0.6523404987064663, macro F1 Test: 0.5572609586961242, 1-TPR Gap Train: 0.9562020897865295, 1-TPR Gap Test: 0.9257367253303528\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.7415978908538818 gap -0.06267940998077393 || Macro F1: 0.557302912611066 1-TPR_gap: 0.9258929491043091\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!! AJOUTER PATIENCE !!!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),  # Additional layer for complexity\n",
    "    nn.Softmax(dim=1))  # LogSoftmax for multi-class classification\n",
    "\n",
    "optimizer_dict = {'Adam': optim.Adam(model.parameters(), lr=learning_rate),#, weight_decay=0.0000),\n",
    "                    'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, initial_accumulator_value=0, eps=1e-10),  #, weight_decay=0.0000)\n",
    "                    'SGD': optim.SGD(model.parameters(), lr=learning_rate),   #,weight_decay=0.0001),\n",
    "                    'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9), #, weight_decay=0.0001),\n",
    "                    'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True), #,weight_decay=0.0001)\n",
    "                    }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score_train','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-28-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending, final_score_train= train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train,final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score_train</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_560</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>tensor(0.6970)</td>\n",
       "      <td>tensor(0.6803)</td>\n",
       "      <td>0.431378</td>\n",
       "      <td>tensor(0.9292)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_1281</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7317)</td>\n",
       "      <td>tensor(0.7058)</td>\n",
       "      <td>0.471969</td>\n",
       "      <td>tensor(0.9396)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_2562</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>256</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7486)</td>\n",
       "      <td>tensor(0.7163)</td>\n",
       "      <td>0.496323</td>\n",
       "      <td>tensor(0.9363)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_5123</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>tensor(0.7567)</td>\n",
       "      <td>tensor(0.7195)</td>\n",
       "      <td>0.496419</td>\n",
       "      <td>tensor(0.9425)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN-28-28_Adam_lr_0.01_batch_size_10244</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1024</td>\n",
       "      <td>12</td>\n",
       "      <td>tensor(0.7707)</td>\n",
       "      <td>tensor(0.7270)</td>\n",
       "      <td>0.508407</td>\n",
       "      <td>tensor(0.9457)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model optimizer    lr  batch_size  \\\n",
       "0    NN-28-28_Adam_lr_0.01_batch_size_560      Adam  0.01          56   \n",
       "1   NN-28-28_Adam_lr_0.01_batch_size_1281      Adam  0.01         128   \n",
       "2   NN-28-28_Adam_lr_0.01_batch_size_2562      Adam  0.01         256   \n",
       "3   NN-28-28_Adam_lr_0.01_batch_size_5123      Adam  0.01         512   \n",
       "4  NN-28-28_Adam_lr_0.01_batch_size_10244      Adam  0.01        1024   \n",
       "\n",
       "   early_ending final_score_train     final_score  macro_f1   macro_tpr_gap  \n",
       "0            10    tensor(0.6970)  tensor(0.6803)  0.431378  tensor(0.9292)  \n",
       "1            12    tensor(0.7317)  tensor(0.7058)  0.471969  tensor(0.9396)  \n",
       "2            12    tensor(0.7486)  tensor(0.7163)  0.496323  tensor(0.9363)  \n",
       "3             7    tensor(0.7567)  tensor(0.7195)  0.496419  tensor(0.9425)  \n",
       "4            12    tensor(0.7707)  tensor(0.7270)  0.508407  tensor(0.9457)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open('RESULTS_NN-28-28_12-03-2024_decay.pkl', 'wb') as f:\n",
    "   pickle.dump(Res, f)\n",
    "\n",
    "path_pkl = 'pkl_files/'\n",
    "Res = pd.read_pickle('RESULTS_NN-28-28_12-03-2024_decay.pkl')\n",
    "   \n",
    "Res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>early_ending</th>\n",
       "      <th>final_score_train</th>\n",
       "      <th>final_score</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_tpr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, optimizer, lr, batch_size, early_ending, final_score_train, final_score, macro_f1, macro_tpr_gap]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 leanring rate x 7 batch size = 49 combinaisons par optimizer\n",
    "# 5 optimizer x 49 combinaison = 245\n",
    "Res.iloc[97:144,:].sort_values(by='batch_size').head(49)  #'batch_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       model optimizer      lr  batch_size  \\\n",
      "30      NN-28-28_SGD_lr_0.01_batch_size_5630       SGD  0.0100          56   \n",
      "31     NN-28-28_SGD_lr_0.01_batch_size_12831       SGD  0.0100         128   \n",
      "32     NN-28-28_SGD_lr_0.01_batch_size_25632       SGD  0.0100         256   \n",
      "33     NN-28-28_SGD_lr_0.01_batch_size_51233       SGD  0.0100         512   \n",
      "34    NN-28-28_SGD_lr_0.01_batch_size_102434       SGD  0.0100        1024   \n",
      "35     NN-28-28_SGD_lr_0.001_batch_size_5635       SGD  0.0010          56   \n",
      "36    NN-28-28_SGD_lr_0.001_batch_size_12836       SGD  0.0010         128   \n",
      "37    NN-28-28_SGD_lr_0.001_batch_size_25637       SGD  0.0010         256   \n",
      "38    NN-28-28_SGD_lr_0.001_batch_size_51238       SGD  0.0010         512   \n",
      "39   NN-28-28_SGD_lr_0.001_batch_size_102439       SGD  0.0010        1024   \n",
      "40    NN-28-28_SGD_lr_0.0001_batch_size_5640       SGD  0.0001          56   \n",
      "41   NN-28-28_SGD_lr_0.0001_batch_size_12841       SGD  0.0001         128   \n",
      "42   NN-28-28_SGD_lr_0.0001_batch_size_25642       SGD  0.0001         256   \n",
      "43   NN-28-28_SGD_lr_0.0001_batch_size_51243       SGD  0.0001         512   \n",
      "44  NN-28-28_SGD_lr_0.0001_batch_size_102444       SGD  0.0001        1024   \n",
      "\n",
      "    early_ending final_score_train     final_score  macro_f1   macro_tpr_gap  \n",
      "30            31    tensor(0.8032)  tensor(0.7419)  0.558415  tensor(0.9254)  \n",
      "31             6    tensor(0.8033)  tensor(0.7421)  0.558876  tensor(0.9254)  \n",
      "32             6    tensor(0.8034)  tensor(0.7418)  0.557836  tensor(0.9258)  \n",
      "33             6    tensor(0.8034)  tensor(0.7418)  0.557508  tensor(0.9262)  \n",
      "34            13    tensor(0.8034)  tensor(0.7417)  0.557450  tensor(0.9260)  \n",
      "35            18    tensor(0.8033)  tensor(0.7421)  0.558601  tensor(0.9256)  \n",
      "36             6    tensor(0.8034)  tensor(0.7419)  0.558383  tensor(0.9254)  \n",
      "37             6    tensor(0.8035)  tensor(0.7417)  0.557372  tensor(0.9260)  \n",
      "38             6    tensor(0.8035)  tensor(0.7418)  0.557477  tensor(0.9260)  \n",
      "39            13    tensor(0.8035)  tensor(0.7418)  0.557477  tensor(0.9260)  \n",
      "40            19    tensor(0.8034)  tensor(0.7421)  0.558911  tensor(0.9253)  \n",
      "41             6    tensor(0.8034)  tensor(0.7419)  0.557900  tensor(0.9259)  \n",
      "42             6    tensor(0.8034)  tensor(0.7417)  0.557372  tensor(0.9260)  \n",
      "43             6    tensor(0.8035)  tensor(0.7417)  0.557372  tensor(0.9260)  \n",
      "44            17    tensor(0.8035)  tensor(0.7418)  0.557500  tensor(0.9260)  \n"
     ]
    }
   ],
   "source": [
    "print(Res[Res['optimizer']==list(optimizer_dict.keys())[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_56_0\n",
      "Epoch 1, Loss: 0.9892361760139465, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_128_1\n",
      "Epoch 1, Loss: 0.9884281754493713, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_256_2\n",
      "Epoch 1, Loss: 0.9859962463378906, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_512_3\n",
      "Epoch 1, Loss: 0.9828978776931763, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.01_batch_size_1024_4\n",
      "Epoch 1, Loss: 0.9799492359161377, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_56_5\n",
      "Epoch 1, Loss: 0.9903466701507568, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_128_6\n",
      "Epoch 1, Loss: 0.9895843267440796, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_256_7\n",
      "Epoch 1, Loss: 0.9820587635040283, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_512_8\n",
      "Epoch 1, Loss: 0.9862840175628662, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.001_batch_size_1024_9\n",
      "Epoch 1, Loss: 0.9830951690673828, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.0001_batch_size_56_10\n",
      "Epoch 1, Loss: 0.9912843704223633, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.0001_batch_size_128_11\n",
      "Epoch 1, Loss: 0.9880565404891968, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.0001_batch_size_256_12\n",
      "Epoch 1, Loss: 0.9817121028900146, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.0001_batch_size_512_13\n",
      "Epoch 1, Loss: 0.9819608926773071, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adam_lr_0.0001_batch_size_1024_14\n",
      "Epoch 1, Loss: 0.9837605357170105, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_56_15\n",
      "Epoch 1, Loss: 0.9918316602706909, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_128_16\n",
      "Epoch 1, Loss: 0.9899646043777466, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_256_17\n",
      "Epoch 1, Loss: 0.9861330986022949, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_512_18\n",
      "Epoch 1, Loss: 0.9854204654693604, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.01_batch_size_1024_19\n",
      "Epoch 1, Loss: 0.9799747467041016, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.001_batch_size_56_20\n",
      "Epoch 1, Loss: 0.9921746850013733, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.001_batch_size_128_21\n",
      "Epoch 1, Loss: 0.9893889427185059, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.001_batch_size_256_22\n",
      "Epoch 1, Loss: 0.9875879883766174, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.001_batch_size_512_23\n",
      "Epoch 1, Loss: 0.9848350286483765, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.001_batch_size_1024_24\n",
      "Epoch 1, Loss: 0.9837436079978943, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.0001_batch_size_56_25\n",
      "Epoch 1, Loss: 0.9923732280731201, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.0001_batch_size_128_26\n",
      "Epoch 1, Loss: 0.989048957824707, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.0001_batch_size_256_27\n",
      "Epoch 1, Loss: 0.9861723780632019, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.0001_batch_size_512_28\n",
      "Epoch 1, Loss: 0.9758281707763672, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Adagrad_lr_0.0001_batch_size_1024_29\n",
      "Epoch 1, Loss: 0.9780877828598022, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_56_30\n",
      "Epoch 1, Loss: 0.9906747341156006, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_128_31\n",
      "Epoch 1, Loss: 0.9895732402801514, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_256_32\n",
      "Epoch 1, Loss: 0.9843598008155823, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_512_33\n",
      "Epoch 1, Loss: 0.9875304698944092, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.01_batch_size_1024_34\n",
      "Epoch 1, Loss: 0.9805771112442017, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.001_batch_size_56_35\n",
      "Epoch 1, Loss: 0.9915656447410583, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.001_batch_size_128_36\n",
      "Epoch 1, Loss: 0.9879195690155029, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.001_batch_size_256_37\n",
      "Epoch 1, Loss: 0.9857820272445679, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.001_batch_size_512_38\n",
      "Epoch 1, Loss: 0.9877598881721497, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.001_batch_size_1024_39\n",
      "Epoch 1, Loss: 0.9820141792297363, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.0001_batch_size_56_40\n",
      "Epoch 1, Loss: 0.9900191426277161, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.0001_batch_size_128_41\n",
      "Epoch 1, Loss: 0.9895063638687134, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.0001_batch_size_256_42\n",
      "Epoch 1, Loss: 0.9757034778594971, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.0001_batch_size_512_43\n",
      "Epoch 1, Loss: 0.9883922338485718, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_SGD_lr_0.0001_batch_size_1024_44\n",
      "Epoch 1, Loss: 0.9835923314094543, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_56_45\n",
      "Epoch 1, Loss: 0.9898541569709778, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_128_46\n",
      "Epoch 1, Loss: 0.9906012415885925, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_256_47\n",
      "Epoch 1, Loss: 0.986717939376831, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_512_48\n",
      "Epoch 1, Loss: 0.9891722202301025, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.01_batch_size_1024_49\n",
      "Epoch 1, Loss: 0.9706296920776367, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_56_50\n",
      "Epoch 1, Loss: 0.9925752878189087, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_128_51\n",
      "Epoch 1, Loss: 0.9884440302848816, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_256_52\n",
      "Epoch 1, Loss: 0.9722869396209717, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_512_53\n",
      "Epoch 1, Loss: 0.9804865121841431, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.001_batch_size_1024_54\n",
      "Epoch 1, Loss: 0.9704859256744385, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_56_55\n",
      "Epoch 1, Loss: 0.9900306463241577, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_128_56\n",
      "Epoch 1, Loss: 0.9739511013031006, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_256_57\n",
      "Epoch 1, Loss: 0.9772665500640869, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_512_58\n",
      "Epoch 1, Loss: 0.980048418045044, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_Momentum_lr_0.0001_batch_size_1024_59\n",
      "Epoch 1, Loss: 0.9755661487579346, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_56_60\n",
      "Epoch 1, Loss: 0.99234938621521, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_128_61\n",
      "Epoch 1, Loss: 0.9883649349212646, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_256_62\n",
      "Epoch 1, Loss: 0.9869030714035034, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_512_63\n",
      "Epoch 1, Loss: 0.9843043088912964, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.01_batch_size_1024_64\n",
      "Epoch 1, Loss: 0.97481769323349, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_56_65\n",
      "Epoch 1, Loss: 0.9907623529434204, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_128_66\n",
      "Epoch 1, Loss: 0.9886221289634705, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_256_67\n",
      "Epoch 1, Loss: 0.9868824481964111, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_512_68\n",
      "Epoch 1, Loss: 0.9858796000480652, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.001_batch_size_1024_69\n",
      "Epoch 1, Loss: 0.9773842692375183, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_56_70\n",
      "Epoch 1, Loss: 0.9896029233932495, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_128_71\n",
      "Epoch 1, Loss: 0.9901460409164429, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_256_72\n",
      "Epoch 1, Loss: 0.9767405986785889, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_512_73\n",
      "Epoch 1, Loss: 0.9823052883148193, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n",
      "\n",
      "\n",
      "Starting to train model NN-2048-256-28_NAG_lr_0.0001_batch_size_1024_74\n",
      "Epoch 1, Loss: 0.9826124906539917, Final Score Train: 0.5014733672142029, Final Score Test: 0.5000121593475342 (gap -0.0014612078666687012) macro F1 Train: 0.004430882483300406, macro F1 Test: 0.005871472210363275, 1-TPR Gap Train: 0.9985159039497375, 1-TPR Gap Test: 0.9941529035568237\n",
      "Arrêt précoce après 6 époques\n",
      "Final Evaluation Score: 0.5000121593475342 gap -0.0014612078666687012 || Macro F1: 0.005871472210363275 1-TPR_gap: 0.9941529035568237\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "#          BOUCLE HYPERPARAMETRES\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(768, 2048),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 256),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(256, 28),  # Additional layer for complexity\n",
    "    nn.Softmax(dim=1)  # LogSoftmax for multi-class classification\n",
    "    )\n",
    "\n",
    "optimizer_dict = {'Adam': optim.Adam(model.parameters(), lr=learning_rate),#, weight_decay=0.0000),\n",
    "                    'Adagrad': optim.Adagrad(model.parameters(), lr=learning_rate, lr_decay=0, initial_accumulator_value=0, eps=1e-10),  #, weight_decay=0.0000)\n",
    "                    'SGD': optim.SGD(model.parameters(), lr=learning_rate),   #,weight_decay=0.0001),\n",
    "                    'Momentum' : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9), #, weight_decay=0.0001),\n",
    "                    'NAG': optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True), #,weight_decay=0.0001)\n",
    "                    }\n",
    "lr_list = [ 0.01, 0.001, 0.0001]\n",
    "batch_size_list = [56,128,256,512,1024]\n",
    "num_epochs = 10000 \n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "Res_2=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score','gap','final_score','macro_f1','macro_tpr_gap'])\n",
    "i=0\n",
    "for opt_name, optimizer in optimizer_dict.items():\n",
    "    for learning_rate in lr_list:\n",
    "        for batch_size in batch_size_list:\n",
    "            name = 'NN-2048-256-28_'+opt_name+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)+'_'+str(i)\n",
    "            print('\\n\\nStarting to train model', name)\n",
    "            model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap , early_ending , final_score_train = train_NN_with_custom_loss(model_2, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "            Res_2.loc[i]=[name,opt_name,learning_rate,batch_size,early_ending,final_score_train, final_score_train - final_score, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "            save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pkl = ''\n",
    "\n",
    "with open(path_pkl + 'RESULTS_NN-2048-256-28_12-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(Res_2, f)\n",
    "\n",
    "#path_pkl = 'pkl_files/'\n",
    "#train = pd.read_pickle(path_pkl + 'train_pp.pkl')\n",
    "   \n",
    "   Res_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.57      0.63        81\n",
      "           1       0.67      0.50      0.57       127\n",
      "           2       0.83      0.86      0.84       458\n",
      "           3       0.38      0.22      0.28        36\n",
      "           4       0.00      0.00      0.00        48\n",
      "           5       0.85      0.81      0.83        72\n",
      "           6       0.89      0.74      0.81       178\n",
      "           7       0.83      0.72      0.77        54\n",
      "           8       0.54      0.72      0.62        18\n",
      "           9       0.76      0.77      0.77        91\n",
      "          10       0.61      0.50      0.55        22\n",
      "          11       0.67      0.72      0.69       286\n",
      "          12       0.84      0.73      0.78       110\n",
      "          13       0.79      0.73      0.76       258\n",
      "          14       0.80      0.72      0.76       112\n",
      "          15       0.00      0.00      0.00        19\n",
      "          16       0.52      0.42      0.47        33\n",
      "          17       0.00      0.00      0.00        26\n",
      "          18       0.84      0.82      0.83       383\n",
      "          19       0.79      0.77      0.78       611\n",
      "          20       0.58      0.66      0.62        98\n",
      "          21       0.83      0.86      0.84      1636\n",
      "          22       0.68      0.62      0.65       264\n",
      "          23       0.00      0.00      0.00        16\n",
      "          24       0.56      0.61      0.58        89\n",
      "          25       0.62      0.58      0.60       183\n",
      "          26       0.59      0.57      0.58       227\n",
      "          27       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.74      5550\n",
      "   macro avg       0.58      0.54      0.56      5550\n",
      "weighted avg       0.75      0.74      0.75      5550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classification_report\n",
    "\n",
    "with torch.no_grad():  # We do not need gradient computation for prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    Y_pred_probs = model(X_test_tensor)\n",
    "    Y_pred = torch.argmax(Y_pred_probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "# Convert Y_pred to a DataFrame\n",
    "Y_pred_df = pd.DataFrame(Y_pred.numpy(), columns=['Predicted'])\n",
    "\n",
    "# Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "print(classification_report(Y_test, Y_pred_df['Predicted']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZING TRAINING FUNCTION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVEC MINI BATCH\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        # Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

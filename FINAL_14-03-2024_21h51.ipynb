{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSE DU PROBLEME**\n",
    "---\n",
    "\n",
    "Problème de classification (28 classes) avec évaluation \"sur mesure\" (composite) qui tient compte de la performance et de l'équité du modèle<br><br>. \n",
    "\n",
    "Jeu de donnée de 25.000 personnes, après voir lancé la baseline, j'ai entrainé un **adversarial NN**, qui permettait de gagner un peu d'équité (passage de 80% à 85% sur le TRP gap) mais a entrainé un éffondrement de l'accuracy (de 65% à 35%). après avoir cherché à personaliser la fonction de perte (avec difficulté pour entrainer le modele), je suis repassée à une approche plus progressive, sur la régression logistique\n",
    "\n",
    "**regression logisitique (baseline) => 3 problemes**<br>\n",
    "1. ***l'échantillonage*** : Le \"train_test_split\" ne tient pas compte de S, nous ne savons pas si l'attribut protégé (H/F) est bien réparti entre X_train et X_test, cela n'est pas optimum pour l'apprentissage car certaines classes contiennent peu de points. Il peut aussi etre plus pertinent de fixer les hyperparametre grace à X_train, et d'entrainer le modele sur tout l'échantillon (avec les memes hyperparametres) avant de prédire X_test_true et de soumettre<br>\n",
    "=> nous réaliserons des train_test_split en tenant compte de X et Y grace à Y56 = Y + 28*S <br>\n",
    "=> nous ferons des soumissions double (modele appris sur X_train et aussi X) pour le datachallenge, pour améliorer les qualités prédicitive du modèles <br><br>\n",
    "2. la fonction d'évaluation est une ***moyenne de scores calculé par classes***, chaque classe a donc le meme poids dans le score final. Or, l'apprentissage est fait pour optimser l'accuracy de l'échantillon (chaque élément à le meme poids). Comme la distribution de l'échatillon est très mal équilibrée entre les classes (93 à 8.285 personnes), le modèle ne peut apprendre correctement<br>\n",
    "=> nous essaierons pour la forme del a data augmentation, mais sans conviction compte tenu de l'embedding sémantique fait avec BERT<br>\n",
    "=> nous utiliserons une correction de la fonction de pertes pour y inclure le poids de chaque classe (Y et Y56)<br>\n",
    "=> nous essaierons du contrastive learning pour obtenir un espace de réprésentation adéquat<br><br>\n",
    "2. ***l'apprentissage n'est pas optimal (alignement et hyperparametres à améliorer)***. Il manque la régularization et du cross validation. L'apprentissage ,'est pas aligné avec \"score final\": le modele fait pour optimiser l'accuracy (la précision), or la fonction d'évaluation est une moyenne entre le F1 score (moyenne harmonique de la précision et du rappel) et une métrique de fairness. Le F1 score pénalise les écarts importants entre la précision et le rappel. Le score<br>\n",
    "=> nous adapterons le fonctions de pertes et/ou l'architecture des modèles intégrer au mieux notrescore final\n",
    "=> nous ajouterons systematiquement des termes de régularisations et de cross validation (stratified k-fold cross validation si possibl)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHODOLOGIE**\n",
    "---\n",
    "\n",
    "**Nous allons procéder en 4 temps** :\n",
    "\n",
    "**1/ optimiser la régression logisitique**<br>\n",
    "<br>\n",
    "ajouter double prédiction sur S et nonS<br><br>\n",
    "\n",
    "**2/ débiaiser l'échantillon**<br> \n",
    "\n",
    "\n",
    "**3/ personaliser la foncition de perts**\n",
    "\n",
    "**4/ explorer modeles précis à la frontiere**<br>\n",
    "KNN\n",
    "SVM linéaire et gaussien\n",
    "1 versus all sur classe mal prédites\n",
    "\n",
    "**5/ regression linéaire des modèles**\n",
    "échantilloner X par rapport à Y et S** traiter le probleme 1 et 2, en créant un label à 56 classes (2 x 28) pour distinguer chaque label i en fonction de s'il est protégé ou non. Nous augmenterons la taille de l'échantillon (avec 56 labels) pour créer X_XL et Y_XL pour avoir le meme nombre d'items pour chacune des 56 classes et un jeu avec le meme nombre d'instance par classe, permettant d'aligner les scores moyens des classes avec la moyenne du nouveau sample \"XL\" décuplé (passage de 27.749 lignes à 255.304 lignes ). Nous programmerons la régression logisitique avec pytorch pour accélérer les calculs.\n",
    "\n",
    "2/ nous **ajusterons ensuite la fonction de pertes**, pour qu'elle tienne compte au mieux de la fonction d'évaluation, qui est composée de 2 éléments : le F1 score et le 1-equal ooprtunity gap<br>\n",
    "- le 'macro f score' (F1 score) n'est pas dérivable (à valider), nous l'approcherons donc par accuracy et le recall<br>\n",
    "- true positive gap : 1 - TRP_GAP est calculé comme \n",
    "A noter que le F1 score n'est pas dérivable. However, we can approximate the F1 score in a differentiable manner by using the ***Sørensen–Dice coefficient***, which is closely related to the F1 score and is differentiable. The Sørensen–Dice coefficient is defined as \\(2 * \\frac{precision * recall}{precision + recall}\\), which is equivalent to the F1 score formula. For a differentiable approximation, we can use soft versions of precision and recall.\n",
    "<br><br>\n",
    "\n",
    "3/ Nous explorerons **différents modèles de classification** K-means, SVM, random forest tree<br><br>\n",
    "\n",
    "4/ Avant de mener cette approche j'avais essayer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Import, loading data, eval functions**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import make_scorer #, f1_score\n",
    "\n",
    "#from Data_challenge_fairness_2024.evaluator import *\n",
    "from evaluator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_model = 'FINAL_models/'  # must finish by '/'\n",
    "path_Y_pred_true = 'FINAL_Y_pred_true/'  # must finish by '/'\n",
    "\n",
    "# with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: model = pickle.load(f)\n",
    "\n",
    "def get_final_score(Y_pred, Y, S):\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "    return final_score\n",
    "    \n",
    "    \n",
    "def get_scores(Y_pred, Y, S):\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "    macro_f1 = eval_scores['macro_fscore']\n",
    "    inv_macro_gap = 1-eval_scores['TPR_GAP']\n",
    "\n",
    "    #print results\n",
    "    print('final score :',final_score)\n",
    "    print('macro_f1    :',eval_scores['macro_fscore'])\n",
    "    print('inv_macro_gap',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, macro_f1, inv_macro_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Load pickle file and convert to numpy array\n",
    "#####################################################\n",
    "\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    "\n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "Y56= Y + 28*S\n",
    "#X, X_test,Y,S, S_test = dat[1]\n",
    "\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n",
    "\n",
    "path_model = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOGISTIC REGRESSION MODELS**\n",
    "---\n",
    "<br>\n",
    "0_Baseline : logistic regression (on X_train)<br><br>\n",
    "1_Optimised : optimized logistic regression (on X + hyperparameters+ stratified k-fold cross validation)<br><br>\n",
    "2_Stratified K fold Logistic Regression\n",
    "3_Orthogonal debiasing of X on model 2 (Optimised Logisitic regression with custom loss function) <br><br>\n",
    "4_Orthogonal debiasing of X on MAN/WOMAN embedding difference  on model 2 (Optimised Logisitic regression with custom loss function) <br><br>\n",
    "5_Custom loss fonctin on Optimised Logisitic regression (56 classes with Y56= Y + 28*S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = pd.DataFrame(columns=['Model','Accuracy','final score','Fscore macro','1 - TPR gap'])\n",
    "RESULTS.loc[0]=['baseline (Reglog28)',accuracy_0,final_score_0, eval_scores_0['macro_fscore'],1-eval_scores_0['TPR_GAP']]\n",
    "RESULTS.loc[2]=['RegLog28 + YxS',accuracy_1,final_score_1, eval_scores_1['macro_fscore'],1-eval_scores_1['TPR_GAP']]\n",
    "RESULTS.loc[4]=['RegLog28 + XL',accuracy_2,final_score_2, eval_scores_2['macro_fscore'],1-eval_scores_2['TPR_GAP']]\n",
    "RESULTS.loc[5]=['RegLog28 + YxS + XL',accuracy_3,final_score_3, eval_scores_3['macro_fscore'],1-eval_scores_3['TPR_GAP']]\n",
    "RESULTS.loc[6]=['RegLog56 + YxS + XL',accuracy_4,final_score_4, eval_scores_4['macro_fscore'],1-eval_scores_4['TPR_GAP']]\n",
    "RESULTS.loc[3]=['RegLog56 + YxS',accuracy_5,final_score_5, eval_scores_5['macro_fscore'],1-eval_scores_5['TPR_GAP']]\n",
    "RESULTS.loc[1]=['baseline seed=1 (Reglog28)',accuracy_0_1,final_score_0_1, eval_scores_0_1['macro_fscore'],1-eval_scores_0_1['TPR_GAP']]\n",
    "RESULTS = RESULTS.sort_index()\n",
    "RESULTS\n",
    "\n",
    "with open('RESULTS_10-03-2024.pkl', 'wb') as f:\n",
    "   pickle.dump(RESULTS, f)\n",
    "\n",
    "#RESULTS =  pd.read_pickle('RESULTS_10-03-2024.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOGISTIC REGRESSION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7638438438438439\n",
      "final score : 0.7365033273594812\n",
      "macro_f1    : 0.669337208333607\n",
      "inv_macro_gap 0.8036694463853555\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# 0. LOGISTIC REGRESSION - BASELINE \n",
    "###############################################\n",
    "\n",
    "name ='0_Reglog_baseline' # changer clf_i\n",
    "\n",
    "# Refresh training data\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "# training (or load)logistic model\n",
    "#clf_0 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y_train)\n",
    "with open(path_model + name + '.pkl', 'rb') as f: clf_0 = pickle.load(f)\n",
    "model = clf_0 \n",
    "  \n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "Y_pred_true = model.predict(X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.24653D+04    |proj g|=  4.11128D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.05318D+04    |proj g|=  8.07961D+02\n",
      "\n",
      "At iterate  100    f=  1.73620D+04    |proj g|=  6.71832D+02\n",
      "\n",
      "At iterate  150    f=  1.61758D+04    |proj g|=  1.03107D+02\n",
      "\n",
      "At iterate  200    f=  1.57705D+04    |proj g|=  1.24845D+02\n",
      "\n",
      "At iterate  250    f=  1.56370D+04    |proj g|=  3.30421D+01\n",
      "\n",
      "At iterate  300    f=  1.55878D+04    |proj g|=  2.25705D+01\n",
      "\n",
      "At iterate  350    f=  1.55716D+04    |proj g|=  3.01638D+01\n",
      "\n",
      "At iterate  400    f=  1.55661D+04    |proj g|=  1.65593D+01\n",
      "\n",
      "At iterate  450    f=  1.55641D+04    |proj g|=  1.12923D+01\n",
      "\n",
      "At iterate  500    f=  1.55632D+04    |proj g|=  3.90256D+00\n",
      "\n",
      "At iterate  550    f=  1.55628D+04    |proj g|=  6.05409D+00\n",
      "\n",
      "At iterate  600    f=  1.55624D+04    |proj g|=  4.51413D+00\n",
      "\n",
      "At iterate  650    f=  1.55619D+04    |proj g|=  1.26648D+01\n",
      "\n",
      "At iterate  700    f=  1.55610D+04    |proj g|=  7.07335D+00\n",
      "\n",
      "At iterate  750    f=  1.55595D+04    |proj g|=  1.62580D+01\n",
      "\n",
      "At iterate  800    f=  1.55577D+04    |proj g|=  4.04886D+00\n",
      "\n",
      "At iterate  850    f=  1.55561D+04    |proj g|=  8.06942D+00\n",
      "\n",
      "At iterate  900    f=  1.55553D+04    |proj g|=  3.19503D+00\n",
      "\n",
      "At iterate  950    f=  1.55548D+04    |proj g|=  7.35077D+00\n",
      "\n",
      "At iterate 1000    f=  1.55546D+04    |proj g|=  5.86208D+00\n",
      "\n",
      "At iterate 1050    f=  1.55544D+04    |proj g|=  7.19974D+00\n",
      "\n",
      "At iterate 1100    f=  1.55543D+04    |proj g|=  1.86093D+00\n",
      "\n",
      "At iterate 1150    f=  1.55542D+04    |proj g|=  6.22605D+00\n",
      "\n",
      "At iterate 1200    f=  1.55540D+04    |proj g|=  1.72893D+00\n",
      "\n",
      "At iterate 1250    f=  1.55538D+04    |proj g|=  2.47744D+00\n",
      "\n",
      "At iterate 1300    f=  1.55534D+04    |proj g|=  1.54724D+01\n",
      "\n",
      "At iterate 1350    f=  1.55529D+04    |proj g|=  5.64675D+00\n",
      "\n",
      "At iterate 1400    f=  1.55524D+04    |proj g|=  7.40508D+00\n",
      "\n",
      "At iterate 1450    f=  1.55520D+04    |proj g|=  1.60392D+00\n",
      "\n",
      "At iterate 1500    f=  1.55519D+04    |proj g|=  3.46264D+00\n",
      "\n",
      "At iterate 1550    f=  1.55518D+04    |proj g|=  1.11544D+00\n",
      "\n",
      "At iterate 1600    f=  1.55518D+04    |proj g|=  7.54015D-01\n",
      "\n",
      "At iterate 1650    f=  1.55518D+04    |proj g|=  6.12268D-01\n",
      "\n",
      "At iterate 1700    f=  1.55518D+04    |proj g|=  1.06131D+00\n",
      "\n",
      "At iterate 1750    f=  1.55517D+04    |proj g|=  9.33577D-01\n",
      "\n",
      "At iterate 1800    f=  1.55517D+04    |proj g|=  6.66243D-01\n",
      "\n",
      "At iterate 1850    f=  1.55516D+04    |proj g|=  1.86799D+00\n",
      "\n",
      "At iterate 1900    f=  1.55516D+04    |proj g|=  1.80377D+00\n",
      "\n",
      "At iterate 1950    f=  1.55515D+04    |proj g|=  9.75567D-01\n",
      "\n",
      "At iterate 2000    f=  1.55515D+04    |proj g|=  4.66221D-01\n",
      "\n",
      "At iterate 2050    f=  1.55514D+04    |proj g|=  2.32000D+00\n",
      "\n",
      "At iterate 2100    f=  1.55514D+04    |proj g|=  1.20032D+00\n",
      "\n",
      "At iterate 2150    f=  1.55514D+04    |proj g|=  4.91983D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532   2176   2292      1     0     0   2.146D-01   1.555D+04\n",
      "  F =   15551.423405080426     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Accuracy on transformed test data: 0.856096096096096\n",
      "final score : 0.8207123104282983\n",
      "macro_f1    : 0.8407599159632768\n",
      "inv_macro_gap 0.8006647048933198\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 1. LOGISTIC REGRESSION - BASELINE OPTIMISED\n",
    "# (no validation set - training on all X)\n",
    "##################################################\n",
    "\n",
    "name ='1_Reglog_optimised' # changer clf_i\n",
    "\n",
    "# Refresh training data\n",
    "# TRAINED ON ALL X\n",
    "\n",
    "# training (or load)logistic model\n",
    "# added regularisation 'l2' with coeff 0.2 (C) and early stopping with tol = 0.0001\n",
    "# clf_1 = LogisticRegression(random_state=42, max_iter=5000,verbose=1,penalty='l2', C=0.2, tol=0.0001).fit(X, Y)\n",
    "with open(path_model + name + '.pkl', 'rb') as f: clf_1 = pickle.load(f)\n",
    "model = clf_1\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "Y_pred_true = model.predict(X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.856096096096096\n",
      "final score : 0.8207123104282983\n",
      "macro_f1    : 0.8407599159632768\n",
      "inv_macro_gap 0.8006647048933198\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 2. LOGISTIC REGRESSION - STRATIFIED K-FOLD\n",
    "# on optimized Log. Reg. regularisatio, early stoppping\n",
    "# (no validation set - training on all X)\n",
    "##################################################\n",
    "\n",
    "name ='2_Reglog_StratKFold' # changer clf_i\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Refresh training data\n",
    "# TRAINED ON ALL X\n",
    "\n",
    "# training (or load)logistic model\n",
    "# added regularisation 'l2' with coeff 0.2 (C) and early stopping with tol = 0.0001\n",
    "clf_2 = LogisticRegression(random_state=42, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores = cross_val_score(clf_2, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "clf_2.fit(X, Y)\n",
    "model = clf_2\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_1 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "Y_pred_true = model.predict(X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.8545345345345345\n",
      "final score : 0.8290222283482765\n",
      "macro_f1    : 0.8381637333117026\n",
      "inv_macro_gap 0.8198807233848505\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 3. DEBIASING WITH ORTHOGONAL PROJECTION + OPYIMISED MODEL \n",
    "# (using the vector of the average difference of embedding between groups \n",
    "# to debiase representation of X, by operating an orthogonal projection)\n",
    "##########################################################################\n",
    "\n",
    "name ='3_ReglogState0_Optimized_orthogonal' # changer clf_i\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF ORTHOGONAL PROJECTION \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Comparison of average embeddingof sensitive group and non-sensitive group\n",
    "\n",
    "#calculate average embedding for group sensitive (S=1) and non sensitive (S=0)\n",
    "X_sensitive = X[S==1]\n",
    "X_sensitive_mean=X_sensitive.mean()\n",
    "\n",
    "X_non_sensitive = X[S!=1]\n",
    "X_non_sensitive_mean=X_non_sensitive.mean()\n",
    "\n",
    "bias = X_sensitive_mean-X_non_sensitive_mean\n",
    "\n",
    "# function to debiase data\n",
    "def remove_bias(dataset, bias):\n",
    "    # Compute the dot product of each row of the dataset with diff\n",
    "    dot_products = np.dot(dataset, bias)\n",
    "    \n",
    "    # Compute the magnitude of bias\n",
    "    bias_magnitude_squared = np.dot(bias, bias)\n",
    "    \n",
    "    # Compute the projection of each row onto diff\n",
    "    projection = np.outer(dot_products / bias_magnitude_squared, bias)\n",
    "    \n",
    "    # Subtract the projection from the dataset\n",
    "    debiased_dataset = dataset - projection\n",
    "    \n",
    "    return debiased_dataset\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "clf_3 = LogisticRegression(random_state=0, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001).fit(debiased_X, Y)\n",
    "model = clf_3\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_3 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y_pred_true = model.predict(modified_X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 3. DEBIASING WITH ORTHOGONAL PROJECTION + MODEL  (stratified K-fold) \n",
    "# (using the vector of the average difference of embedding between groups \n",
    "# to debiase representation of X, by operating an orthogonal projection)\n",
    "##########################################################################\n",
    "\n",
    "name ='3_Reglogstate0_StratKFold_orthogonal' # changer clf_i\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF ORTHOGONAL PROJECTION \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Comparison of average embeddingof sensitive group and non-sensitive group\n",
    "\n",
    "#calculate average embedding for group sensitive (S=1) and non sensitive (S=0)\n",
    "X_sensitive = X[S==1]\n",
    "X_sensitive_mean=X_sensitive.mean()\n",
    "\n",
    "X_non_sensitive = X[S!=1]\n",
    "X_non_sensitive_mean=X_non_sensitive.mean()\n",
    "\n",
    "bias = X_sensitive_mean-X_non_sensitive_mean\n",
    "\n",
    "# function to debiase data\n",
    "def remove_bias(dataset, bias):\n",
    "    # Compute the dot product of each row of the dataset with diff\n",
    "    dot_products = np.dot(dataset, bias)\n",
    "    \n",
    "    # Compute the magnitude of bias\n",
    "    bias_magnitude_squared = np.dot(bias, bias)\n",
    "    \n",
    "    # Compute the projection of each row onto diff\n",
    "    projection = np.outer(dot_products / bias_magnitude_squared, bias)\n",
    "    \n",
    "    # Subtract the projection from the dataset\n",
    "    debiased_dataset = dataset - projection\n",
    "    \n",
    "    return debiased_dataset\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "clf_3 = LogisticRegression(random_state=0, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores = cross_val_score(clf_3, debiased_X, Y,  cv=cv, n_jobs=-1)\n",
    "clf_3.fit(debiased_X, Y)\n",
    "model = clf_3\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_3 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y_pred_true = model.predict(modified_X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_model + '1_LogReg_optimised.pkl', 'rb') as f: recup = pickle.load(f)\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y_pred_true_recup = model.predict(modified_X_test_true)\n",
    "results_recup=pd.DataFrame(Y_pred_true_recup, columns= ['score'])\n",
    "results_recup.to_csv(path_Y_pred_true + \"Data_Challenge_recpu.csv\", header = None, index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''##########################################################################\n",
    "# 4. BERT (WOMAN-MAN) DEBIASING WITH ORTHOGONAL PROJECTION + MODEL 2 (k-fold)\n",
    "# (using the vector of the difference of embedding between WOMAN and MAN\n",
    "# to debiase representation of X, by operating an orthogonal projection)\n",
    "# !!!! uncertainty about model of BERT used for the training !!!!\n",
    "##########################################################################\n",
    "\n",
    "name ='4_Reglog_StratKFold_orthogonalBERT' # changer clf_i\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# GET MAN AND WOMAN EMBEDDING IN BERT\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import tokenizers as tk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer et modèle de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize \"man\" and \"woman\"\n",
    "tokens_man = tokenizer.encode(\"man\", add_special_tokens=True)  # Ajoute les tokens spéciaux\n",
    "tokens_woman = tokenizer.encode(\"woman\", add_special_tokens=True)\n",
    "\n",
    "print(tokens_man)\n",
    "print(tokens_woman)\n",
    "\n",
    "# transforme en tensor de batch (meme si un seul mot)\n",
    "\n",
    "input_ids_man = tf.constant(tokens_man)[None, :]  # Ajoute une dimension de batch\n",
    "input_ids_woman = tf.constant(tokens_woman)[None, :]  # Ajoute une dimension de batch\n",
    "\n",
    "# Obtenir les embeddings\n",
    "outputs_man = model(input_ids_man)\n",
    "outputs_woman = model(input_ids_woman)\n",
    "\n",
    "# Les embeddings du dernier layer pour le premier token ('[CLS]' par défaut)\n",
    "embedding_man = outputs_man.last_hidden_state[0][0]\n",
    "embedding_woman = outputs_woman.last_hidden_state[0][0]\n",
    "bias_BERT = embedding_woman - embedding_man\n",
    "bias_BERT = np.array (bias_BERT)\n",
    "print(embedding_man.shape)  # Doit être (768,)\n",
    "print(embedding_woman.shape)  # Doit être (768,)\n",
    "print(bias_BERT.shape)  # Doit être (768,)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X_BERT = remove_bias(X, bias_BERT)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "clf_4 = LogisticRegression(random_state=42, max_iter=5000,verbose=1,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "#custom_scorer = make_scorer(final_score_S)\n",
    "scores = cross_val_score(clf_4, debiased_X_BERT, Y, cv=cv, n_jobs=-1)\n",
    "clf_4.fit(debiased_X_BERT, Y)\n",
    "model = clf_4\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_3 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias_BERT)  # debiasing\n",
    "Y_pred_true = model.predict(modified_X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' PROBLEME SUR FINAL SCORE QUI DEPEND DE S, OR SICKIT LEARN NE GERE\\nPAS S DANS LA CROSS VALIDATION, LA LOSS FUNCTION PERSONALISEE NE PEUT \\nETRE CALCULEE  =======> CODAGE SUR PYTORCH\\n\\n##########################################################################\\n# 5. CUSTOM LOSS FUNCTION (Using stratified K + custom_scorer)\\n# + Debiasing with orthogonal projection + Optimised Logisitic Regression\\n##########################################################################\\n\\nname =\\'5_Reglog_StratKFold1_Orthogonal_CustomLoss\\' # changer clf_i\\n\\n#---------------------------------------------------------------------\\n# Debiasing X (Projection of X orthogonally to bias) \\n#---------------------------------------------------------------------\\n\\n# projection in new representation space\\ndebiased_X = remove_bias(X, bias)\\n\\n#------------------------------------------------------------------------\\n# CREATION OF CUSTOM LOSS FUNCTION (FINAL SCORE) \\n# -----------------------------------------------------------------------\\n\\n#It is not possible to split S with the stratified k-fold implementation \\n# in sickit learn. We will use Y56 = Y + 28*S to train logistic model\\n# and derive from Y56 the value of Y = Y56 % 28 and S = Y56//28\\n\\ndef final_score_S(Y,Y_pred):\\n    # wrapper function to include S\\n    # Note order : custom scorer expects (y_true, y_pred) as inputs\\n    \\n    return get_final_score(Y_pred,Y,S)\\n\\ncustom_scorer = make_scorer(final_score_S)\\n\\n#---------------------------------------------------------------------\\n# TRAINING MODEL AND PREDICTING\\n#---------------------------------------------------------------------\\n\\nclf_5 = LogisticRegression(random_state=0, max_iter=5000,verbose=0,penalty=\\'l2\\', C=0.2, tol=0.0001)\\n\\n# Préparer la procédure de validation croisée k-fold stratifiée\\ncv = StratifiedKFold(n_splits=1)\\nscores = cross_val_score(clf_3, debiased_X, Y, scoring=custom_scorer ,cv=cv, n_jobs=-1)\\nclf_5.fit(debiased_X, Y56)\\nmodel = clf_5\\n# with open(path_model + name + \\'.pkl\\', \\'rb\\') as f: clf_5 = pickle.load(f)\\n\\n# predicting and assessing\\nY56_pred = model.predict(X_test)\\n#S_pred = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\\nY_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\\naccuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\\n\\n# predict X_test_true and save\\nmodified_X_test_true = remove_info(X_test_true, bias)  # debiasing\\nY56_pred_true = model.predict(modified_X_test_true)\\nY_pred_true = Y56_pred_true % 28  # reste (original Y)   ex 33% 28 = classe 5\\nresults=pd.DataFrame(Y_pred_true, columns= [\\'score\\'])\\nresults.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\\n\\n# save model\\nwith open(path_model + name + \\'.pkl\\', \\'wb\\') as f: pickle.dump(model, f)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' PROBLEME SUR FINAL SCORE QUI DEPEND DE S, OR SICKIT LEARN NE GERE\n",
    "PAS S DANS LA CROSS VALIDATION, LA LOSS FUNCTION PERSONALISEE NE PEUT \n",
    "ETRE CALCULEE  =======> CODAGE SUR PYTORCH\n",
    "\n",
    "##########################################################################\n",
    "# 5. CUSTOM LOSS FUNCTION (Using stratified K + custom_scorer)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "name ='5_Reglog_StratKFold1_Orthogonal_CustomLoss' # changer clf_i\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF CUSTOM LOSS FUNCTION (FINAL SCORE) \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "#It is not possible to split S with the stratified k-fold implementation \n",
    "# in sickit learn. We will use Y56 = Y + 28*S to train logistic model\n",
    "# and derive from Y56 the value of Y = Y56 % 28 and S = Y56//28\n",
    "\n",
    "def final_score_S(Y,Y_pred):\n",
    "    # wrapper function to include S\n",
    "    # Note order : custom scorer expects (y_true, y_pred) as inputs\n",
    "    \n",
    "    return get_final_score(Y_pred,Y,S)\n",
    "\n",
    "custom_scorer = make_scorer(final_score_S)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "clf_5 = LogisticRegression(random_state=0, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=1)\n",
    "scores = cross_val_score(clf_3, debiased_X, Y, scoring=custom_scorer ,cv=cv, n_jobs=-1)\n",
    "clf_5.fit(debiased_X, Y56)\n",
    "model = clf_5\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_5 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y56_pred = model.predict(X_test)\n",
    "#S_pred = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y56_pred_true = model.predict(modified_X_test_true)\n",
    "Y_pred_true = Y56_pred_true % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaele/myenv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06522D+04\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06381D+04\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06118D+04\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06269D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06161D+04\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06534D+04\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06381D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06464D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.96015D+04    |proj g|=  4.23754D+02\n",
      "\n",
      "At iterate   50    f=  3.00049D+04    |proj g|=  3.68947D+03\n",
      "\n",
      "At iterate   50    f=  2.98276D+04    |proj g|=  3.31223D+02\n",
      "\n",
      "At iterate   50    f=  2.93594D+04    |proj g|=  1.22474D+03\n",
      "\n",
      "At iterate   50    f=  2.94112D+04    |proj g|=  5.68829D+02\n",
      "\n",
      "At iterate   50    f=  2.94696D+04    |proj g|=  1.12163D+03\n",
      "\n",
      "At iterate   50    f=  3.03922D+04    |proj g|=  6.12752D+02\n",
      "\n",
      "At iterate   50    f=  2.97034D+04    |proj g|=  2.36665D+03\n",
      "\n",
      "At iterate  100    f=  2.33063D+04    |proj g|=  2.02884D+03\n",
      "\n",
      "At iterate  100    f=  2.34883D+04    |proj g|=  3.96808D+02\n",
      "\n",
      "At iterate  100    f=  2.33991D+04    |proj g|=  7.55888D+02\n",
      "\n",
      "At iterate  100    f=  2.31575D+04    |proj g|=  2.04184D+02\n",
      "\n",
      "At iterate  100    f=  2.31886D+04    |proj g|=  5.45021D+02\n",
      "\n",
      "At iterate  100    f=  2.33333D+04    |proj g|=  1.20937D+03\n",
      "\n",
      "At iterate  100    f=  2.31890D+04    |proj g|=  1.10662D+03\n",
      "\n",
      "At iterate  100    f=  2.32847D+04    |proj g|=  2.13262D+02\n",
      "\n",
      "At iterate  150    f=  2.13872D+04    |proj g|=  4.98616D+02\n",
      "\n",
      "At iterate  150    f=  2.13729D+04    |proj g|=  5.20534D+02\n",
      "\n",
      "At iterate  150    f=  2.14297D+04    |proj g|=  2.72775D+02\n",
      "\n",
      "At iterate  150    f=  2.12190D+04    |proj g|=  3.24485D+02\n",
      "\n",
      "At iterate  150    f=  2.13853D+04    |proj g|=  1.82197D+02\n",
      "\n",
      "At iterate  150    f=  2.13139D+04    |proj g|=  1.17598D+02\n",
      "\n",
      "At iterate  150    f=  2.12830D+04    |proj g|=  2.03261D+02\n",
      "\n",
      "At iterate  150    f=  2.13045D+04    |proj g|=  3.26161D+02\n",
      "\n",
      "At iterate  200    f=  2.06801D+04    |proj g|=  1.42939D+02\n",
      "\n",
      "At iterate  200    f=  2.06751D+04    |proj g|=  2.90860D+02\n",
      "\n",
      "At iterate  200    f=  2.06975D+04    |proj g|=  9.29624D+01\n",
      "\n",
      "At iterate  200    f=  2.05860D+04    |proj g|=  2.34382D+02\n",
      "\n",
      "At iterate  200    f=  2.06967D+04    |proj g|=  1.36270D+02\n",
      "\n",
      "At iterate  200    f=  2.06544D+04    |proj g|=  8.26399D+01\n",
      "\n",
      "At iterate  200    f=  2.06267D+04    |proj g|=  1.18163D+02\n",
      "\n",
      "At iterate  200    f=  2.06659D+04    |proj g|=  4.87774D+01\n",
      "\n",
      "At iterate  250    f=  2.04686D+04    |proj g|=  9.71996D+01\n",
      "\n",
      "At iterate  250    f=  2.04540D+04    |proj g|=  7.91235D+01\n",
      "\n",
      "At iterate  250    f=  2.04626D+04    |proj g|=  4.10598D+01\n",
      "\n",
      "At iterate  250    f=  2.03743D+04    |proj g|=  4.84658D+01\n",
      "\n",
      "At iterate  250    f=  2.04111D+04    |proj g|=  1.45346D+02\n",
      "\n",
      "At iterate  250    f=  2.04407D+04    |proj g|=  4.06020D+01\n",
      "\n",
      "At iterate  250    f=  2.04500D+04    |proj g|=  6.20538D+01\n",
      "\n",
      "At iterate  250    f=  2.04224D+04    |proj g|=  2.46891D+02\n",
      "\n",
      "At iterate  300    f=  2.03916D+04    |proj g|=  6.03684D+01\n",
      "\n",
      "At iterate  300    f=  2.03296D+04    |proj g|=  3.27092D+01\n",
      "\n",
      "At iterate  300    f=  2.03782D+04    |proj g|=  1.52793D+02\n",
      "\n",
      "At iterate  300    f=  2.03741D+04    |proj g|=  6.13464D+01\n",
      "\n",
      "At iterate  300    f=  2.03727D+04    |proj g|=  2.03650D+01\n",
      "\n",
      "At iterate  300    f=  2.03068D+04    |proj g|=  1.09507D+02\n",
      "\n",
      "At iterate  300    f=  2.03520D+04    |proj g|=  5.91176D+01\n",
      "\n",
      "At iterate  300    f=  2.03575D+04    |proj g|=  4.01514D+01\n",
      "\n",
      "At iterate  350    f=  2.03661D+04    |proj g|=  1.64962D+01\n",
      "\n",
      "At iterate  350    f=  2.03009D+04    |proj g|=  2.08812D+01\n",
      "\n",
      "At iterate  350    f=  2.03490D+04    |proj g|=  4.03851D+01\n",
      "\n",
      "At iterate  350    f=  2.03259D+04    |proj g|=  1.64717D+01\n",
      "\n",
      "At iterate  350    f=  2.03466D+04    |proj g|=  1.54739D+01\n",
      "\n",
      "At iterate  350    f=  2.03480D+04    |proj g|=  1.94933D+01\n",
      "\n",
      "At iterate  350    f=  2.02728D+04    |proj g|=  2.55412D+01\n",
      "\n",
      "At iterate  350    f=  2.03304D+04    |proj g|=  7.84811D+01\n",
      "\n",
      "At iterate  400    f=  2.03574D+04    |proj g|=  8.24386D+00\n",
      "\n",
      "At iterate  400    f=  2.02913D+04    |proj g|=  1.68383D+01\n",
      "\n",
      "At iterate  400    f=  2.03149D+04    |proj g|=  5.72098D+01\n",
      "\n",
      "At iterate  400    f=  2.03414D+04    |proj g|=  2.57795D+01\n",
      "\n",
      "At iterate  400    f=  2.03379D+04    |proj g|=  1.29938D+01\n",
      "\n",
      "At iterate  400    f=  2.02635D+04    |proj g|=  1.24961D+01\n",
      "\n",
      "At iterate  400    f=  2.03388D+04    |proj g|=  3.90378D+01\n",
      "\n",
      "At iterate  400    f=  2.03204D+04    |proj g|=  3.73629D+01\n",
      "\n",
      "At iterate  450    f=  2.03544D+04    |proj g|=  6.14858D+00\n",
      "\n",
      "At iterate  450    f=  2.02881D+04    |proj g|=  2.18571D+01\n",
      "\n",
      "At iterate  450    f=  2.03116D+04    |proj g|=  9.53012D+00\n",
      "\n",
      "At iterate  450    f=  2.03349D+04    |proj g|=  4.50323D+00\n",
      "\n",
      "At iterate  450    f=  2.03384D+04    |proj g|=  4.67629D+00\n",
      "\n",
      "At iterate  450    f=  2.02608D+04    |proj g|=  1.10401D+01\n",
      "\n",
      "At iterate  450    f=  2.03356D+04    |proj g|=  9.25815D+00\n",
      "\n",
      "At iterate  450    f=  2.03167D+04    |proj g|=  2.59884D+01\n",
      "\n",
      "At iterate  500    f=  2.02869D+04    |proj g|=  3.47760D+00\n",
      "\n",
      "At iterate  500    f=  2.03531D+04    |proj g|=  4.14946D+00\n",
      "\n",
      "At iterate  500    f=  2.03104D+04    |proj g|=  4.26813D+00\n",
      "\n",
      "At iterate  500    f=  2.03338D+04    |proj g|=  3.77207D+00\n",
      "\n",
      "At iterate  500    f=  2.03373D+04    |proj g|=  6.49278D+00\n",
      "\n",
      "At iterate  500    f=  2.02596D+04    |proj g|=  7.32795D+00\n",
      "\n",
      "At iterate  500    f=  2.03345D+04    |proj g|=  2.86077D+00\n",
      "\n",
      "At iterate  500    f=  2.03153D+04    |proj g|=  1.79558D+01\n",
      "\n",
      "At iterate  550    f=  2.02863D+04    |proj g|=  6.54396D+00\n",
      "\n",
      "At iterate  550    f=  2.03526D+04    |proj g|=  6.31812D+00\n",
      "\n",
      "At iterate  550    f=  2.02591D+04    |proj g|=  2.57450D+00\n",
      "\n",
      "At iterate  550    f=  2.03334D+04    |proj g|=  3.27686D+00\n",
      "\n",
      "At iterate  550    f=  2.03098D+04    |proj g|=  3.66002D+00\n",
      "\n",
      "At iterate  550    f=  2.03368D+04    |proj g|=  4.36918D+00\n",
      "\n",
      "At iterate  550    f=  2.03339D+04    |proj g|=  1.53773D+01\n",
      "\n",
      "At iterate  550    f=  2.03149D+04    |proj g|=  1.89242D+00\n",
      "\n",
      "At iterate  600    f=  2.02860D+04    |proj g|=  3.31240D+00\n",
      "\n",
      "At iterate  600    f=  2.03523D+04    |proj g|=  1.90958D+00\n",
      "\n",
      "At iterate  600    f=  2.02588D+04    |proj g|=  4.01567D+00\n",
      "\n",
      "At iterate  600    f=  2.03330D+04    |proj g|=  1.97761D+00\n",
      "\n",
      "At iterate  600    f=  2.03095D+04    |proj g|=  2.94784D+00\n",
      "\n",
      "At iterate  600    f=  2.03365D+04    |proj g|=  4.01201D+00\n",
      "\n",
      "At iterate  600    f=  2.03336D+04    |proj g|=  8.44430D+00\n",
      "\n",
      "At iterate  600    f=  2.03146D+04    |proj g|=  3.70731D+00\n",
      "\n",
      "At iterate  650    f=  2.03520D+04    |proj g|=  2.55812D+00\n",
      "\n",
      "At iterate  650    f=  2.02856D+04    |proj g|=  3.81417D+00\n",
      "\n",
      "At iterate  650    f=  2.02585D+04    |proj g|=  5.82510D+00\n",
      "\n",
      "At iterate  650    f=  2.03327D+04    |proj g|=  8.05127D+00\n",
      "\n",
      "At iterate  650    f=  2.03362D+04    |proj g|=  2.69857D+00\n",
      "\n",
      "At iterate  650    f=  2.03091D+04    |proj g|=  1.02096D+01\n",
      "\n",
      "At iterate  650    f=  2.03333D+04    |proj g|=  1.81500D+00\n",
      "\n",
      "At iterate  650    f=  2.03143D+04    |proj g|=  5.31340D+00\n",
      "\n",
      "At iterate  700    f=  2.03515D+04    |proj g|=  6.27851D+00\n",
      "\n",
      "At iterate  700    f=  2.02850D+04    |proj g|=  9.48403D+00\n",
      "\n",
      "At iterate  700    f=  2.03321D+04    |proj g|=  3.71903D+00\n",
      "\n",
      "At iterate  700    f=  2.02580D+04    |proj g|=  2.29541D+01\n",
      "\n",
      "At iterate  700    f=  2.03357D+04    |proj g|=  9.77901D+00\n",
      "\n",
      "At iterate  700    f=  2.03329D+04    |proj g|=  9.14741D+00\n",
      "\n",
      "At iterate  700    f=  2.03084D+04    |proj g|=  7.71069D+00\n",
      "\n",
      "At iterate  700    f=  2.03138D+04    |proj g|=  9.95080D+00\n",
      "\n",
      "At iterate  750    f=  2.03504D+04    |proj g|=  6.52289D+00\n",
      "\n",
      "At iterate  750    f=  2.02838D+04    |proj g|=  5.70789D+00\n",
      "\n",
      "At iterate  750    f=  2.03309D+04    |proj g|=  6.11053D+00\n",
      "\n",
      "At iterate  750    f=  2.02569D+04    |proj g|=  1.80945D+01\n",
      "\n",
      "At iterate  750    f=  2.03347D+04    |proj g|=  2.41396D+01\n",
      "\n",
      "At iterate  750    f=  2.03319D+04    |proj g|=  1.02893D+01\n",
      "\n",
      "At iterate  750    f=  2.03069D+04    |proj g|=  5.83837D+00\n",
      "\n",
      "At iterate  750    f=  2.03130D+04    |proj g|=  2.65130D+00\n",
      "\n",
      "At iterate  800    f=  2.02815D+04    |proj g|=  1.27138D+01\n",
      "\n",
      "At iterate  800    f=  2.03487D+04    |proj g|=  2.67435D+01\n",
      "\n",
      "At iterate  800    f=  2.03289D+04    |proj g|=  1.24722D+01\n",
      "\n",
      "At iterate  800    f=  2.02550D+04    |proj g|=  2.17075D+01\n",
      "\n",
      "At iterate  800    f=  2.03301D+04    |proj g|=  1.13318D+01\n",
      "\n",
      "At iterate  800    f=  2.03329D+04    |proj g|=  6.68136D+00\n",
      "\n",
      "At iterate  800    f=  2.03043D+04    |proj g|=  4.71495D+01\n",
      "\n",
      "At iterate  800    f=  2.03111D+04    |proj g|=  8.28692D+00\n",
      "\n",
      "At iterate  850    f=  2.02784D+04    |proj g|=  1.07101D+01\n",
      "\n",
      "At iterate  850    f=  2.03467D+04    |proj g|=  8.31609D+00\n",
      "\n",
      "At iterate  850    f=  2.03260D+04    |proj g|=  9.56235D+00\n",
      "\n",
      "At iterate  850    f=  2.02529D+04    |proj g|=  6.77891D+00\n",
      "\n",
      "At iterate  850    f=  2.03272D+04    |proj g|=  1.10816D+01\n",
      "\n",
      "At iterate  850    f=  2.03087D+04    |proj g|=  4.85220D+01\n",
      "\n",
      "At iterate  850    f=  2.03010D+04    |proj g|=  2.07828D+01\n",
      "\n",
      "At iterate  850    f=  2.03298D+04    |proj g|=  8.49564D+00\n",
      "\n",
      "At iterate  900    f=  2.03238D+04    |proj g|=  6.53237D+00\n",
      "\n",
      "At iterate  900    f=  2.02762D+04    |proj g|=  8.41554D+00\n",
      "\n",
      "At iterate  900    f=  2.02506D+04    |proj g|=  4.49915D+00\n",
      "\n",
      "At iterate  900    f=  2.03447D+04    |proj g|=  1.92698D+01\n",
      "\n",
      "At iterate  900    f=  2.03065D+04    |proj g|=  1.47357D+01\n",
      "\n",
      "At iterate  900    f=  2.02986D+04    |proj g|=  4.21623D+00\n",
      "\n",
      "At iterate  900    f=  2.03250D+04    |proj g|=  5.92188D+00\n",
      "\n",
      "At iterate  900    f=  2.03277D+04    |proj g|=  5.70549D+00\n",
      "\n",
      "At iterate  950    f=  2.02494D+04    |proj g|=  2.25434D+01\n",
      "\n",
      "At iterate  950    f=  2.03226D+04    |proj g|=  6.76423D+00\n",
      "\n",
      "At iterate  950    f=  2.02752D+04    |proj g|=  1.95851D+01\n",
      "\n",
      "At iterate  950    f=  2.03053D+04    |proj g|=  3.22509D+00\n",
      "\n",
      "At iterate  950    f=  2.03434D+04    |proj g|=  5.61261D+00\n",
      "\n",
      "At iterate  950    f=  2.02972D+04    |proj g|=  1.32370D+01\n",
      "\n",
      "At iterate  950    f=  2.03239D+04    |proj g|=  5.74376D+00\n",
      "\n",
      "At iterate  950    f=  2.03270D+04    |proj g|=  6.63647D+00\n",
      "\n",
      "At iterate 1000    f=  2.03222D+04    |proj g|=  2.29251D+00\n",
      "\n",
      "At iterate 1000    f=  2.02488D+04    |proj g|=  3.24652D+00\n",
      "\n",
      "At iterate 1000    f=  2.02748D+04    |proj g|=  3.00843D+00\n",
      "\n",
      "At iterate 1000    f=  2.03048D+04    |proj g|=  1.12021D+01\n",
      "\n",
      "At iterate 1000    f=  2.02966D+04    |proj g|=  3.79978D+00\n",
      "\n",
      "At iterate 1000    f=  2.03429D+04    |proj g|=  4.51920D+00\n",
      "\n",
      "At iterate 1000    f=  2.03234D+04    |proj g|=  6.25009D+00\n",
      "\n",
      "At iterate 1000    f=  2.03266D+04    |proj g|=  8.83127D+00\n",
      "\n",
      "At iterate 1050    f=  2.02487D+04    |proj g|=  3.35659D+00\n",
      "\n",
      "At iterate 1050    f=  2.03220D+04    |proj g|=  1.09993D+00\n",
      "\n",
      "At iterate 1050    f=  2.03046D+04    |proj g|=  3.94074D+00\n",
      "\n",
      "At iterate 1050    f=  2.02746D+04    |proj g|=  4.30258D+00\n",
      "\n",
      "At iterate 1050    f=  2.02965D+04    |proj g|=  4.20480D+00\n",
      "\n",
      "At iterate 1050    f=  2.03426D+04    |proj g|=  1.86604D+00\n",
      "\n",
      "At iterate 1050    f=  2.03232D+04    |proj g|=  2.24609D+00\n",
      "\n",
      "At iterate 1050    f=  2.03265D+04    |proj g|=  1.34678D+00\n",
      "\n",
      "At iterate 1100    f=  2.03220D+04    |proj g|=  2.27690D+00\n",
      "\n",
      "At iterate 1100    f=  2.02486D+04    |proj g|=  1.21786D+00\n",
      "\n",
      "At iterate 1100    f=  2.03045D+04    |proj g|=  3.92111D+00\n",
      "\n",
      "At iterate 1100    f=  2.02745D+04    |proj g|=  3.20588D+00\n",
      "\n",
      "At iterate 1100    f=  2.02964D+04    |proj g|=  8.59184D-01\n",
      "\n",
      "At iterate 1100    f=  2.03424D+04    |proj g|=  1.19062D+00\n",
      "\n",
      "At iterate 1100    f=  2.03264D+04    |proj g|=  1.00979D+00\n",
      "\n",
      "At iterate 1100    f=  2.03231D+04    |proj g|=  5.44482D+00\n",
      "\n",
      "At iterate 1150    f=  2.03219D+04    |proj g|=  1.72319D+00\n",
      "\n",
      "At iterate 1150    f=  2.02485D+04    |proj g|=  2.87800D+00\n",
      "\n",
      "At iterate 1150    f=  2.03044D+04    |proj g|=  1.69280D+00\n",
      "\n",
      "At iterate 1150    f=  2.02745D+04    |proj g|=  3.43756D+00\n",
      "\n",
      "At iterate 1150    f=  2.02963D+04    |proj g|=  1.33870D+00\n",
      "\n",
      "At iterate 1150    f=  2.03422D+04    |proj g|=  2.30182D+00\n",
      "\n",
      "At iterate 1150    f=  2.03263D+04    |proj g|=  1.39659D+00\n",
      "\n",
      "At iterate 1150    f=  2.03230D+04    |proj g|=  2.61670D+00\n",
      "\n",
      "At iterate 1200    f=  2.03219D+04    |proj g|=  1.99275D+00\n",
      "\n",
      "At iterate 1200    f=  2.02484D+04    |proj g|=  8.91552D-01\n",
      "\n",
      "At iterate 1200    f=  2.03043D+04    |proj g|=  9.18495D+00\n",
      "\n",
      "At iterate 1200    f=  2.02744D+04    |proj g|=  2.24163D+00\n",
      "\n",
      "At iterate 1200    f=  2.02963D+04    |proj g|=  1.70503D+00\n",
      "\n",
      "At iterate 1200    f=  2.03421D+04    |proj g|=  4.51253D+00\n",
      "\n",
      "At iterate 1200    f=  2.03263D+04    |proj g|=  3.01351D+00\n",
      "\n",
      "At iterate 1200    f=  2.03229D+04    |proj g|=  2.11754D+00\n",
      "\n",
      "At iterate 1250    f=  2.03218D+04    |proj g|=  7.60417D+00\n",
      "\n",
      "At iterate 1250    f=  2.02483D+04    |proj g|=  5.68805D+00\n",
      "\n",
      "At iterate 1250    f=  2.02742D+04    |proj g|=  5.54891D+00\n",
      "\n",
      "At iterate 1250    f=  2.03042D+04    |proj g|=  2.22860D+00\n",
      "\n",
      "At iterate 1250    f=  2.02962D+04    |proj g|=  2.43089D+00\n",
      "\n",
      "At iterate 1250    f=  2.03420D+04    |proj g|=  1.44101D+00\n",
      "\n",
      "At iterate 1250    f=  2.03262D+04    |proj g|=  4.12671D+00\n",
      "\n",
      "At iterate 1250    f=  2.03229D+04    |proj g|=  1.15862D+00\n",
      "\n",
      "At iterate 1300    f=  2.03216D+04    |proj g|=  3.34579D+00\n",
      "\n",
      "At iterate 1300    f=  2.02482D+04    |proj g|=  3.44512D+00\n",
      "\n",
      "At iterate 1300    f=  2.02738D+04    |proj g|=  6.74191D+00\n",
      "\n",
      "At iterate 1300    f=  2.03418D+04    |proj g|=  1.28862D+01\n",
      "\n",
      "At iterate 1300    f=  2.03041D+04    |proj g|=  1.10461D+01\n",
      "\n",
      "At iterate 1300    f=  2.02960D+04    |proj g|=  4.36286D+00\n",
      "\n",
      "At iterate 1300    f=  2.03260D+04    |proj g|=  5.02263D+00\n",
      "\n",
      "At iterate 1300    f=  2.03227D+04    |proj g|=  2.63956D+00\n",
      "\n",
      "At iterate 1350    f=  2.03210D+04    |proj g|=  1.04080D+01\n",
      "\n",
      "At iterate 1350    f=  2.02479D+04    |proj g|=  6.55615D+00\n",
      "\n",
      "At iterate 1350    f=  2.02734D+04    |proj g|=  5.72693D+00\n",
      "\n",
      "At iterate 1350    f=  2.03415D+04    |proj g|=  6.35237D+00\n",
      "\n",
      "At iterate 1350    f=  2.03039D+04    |proj g|=  8.10094D+00\n",
      "\n",
      "At iterate 1350    f=  2.02957D+04    |proj g|=  1.97707D+00\n",
      "\n",
      "At iterate 1350    f=  2.03256D+04    |proj g|=  2.07864D+00\n",
      "\n",
      "At iterate 1350    f=  2.03225D+04    |proj g|=  5.09595D+00\n",
      "\n",
      "At iterate 1400    f=  2.03203D+04    |proj g|=  2.82894D+00\n",
      "\n",
      "At iterate 1400    f=  2.02475D+04    |proj g|=  2.86055D+00\n",
      "\n",
      "At iterate 1400    f=  2.02729D+04    |proj g|=  2.10901D+00\n",
      "\n",
      "At iterate 1400    f=  2.03411D+04    |proj g|=  3.22040D+00\n",
      "\n",
      "At iterate 1400    f=  2.03036D+04    |proj g|=  6.88120D+00\n",
      "\n",
      "At iterate 1400    f=  2.02952D+04    |proj g|=  3.49070D+00\n",
      "\n",
      "At iterate 1400    f=  2.03222D+04    |proj g|=  6.49297D+00\n",
      "\n",
      "At iterate 1400    f=  2.03249D+04    |proj g|=  1.18349D+01\n",
      "\n",
      "At iterate 1450    f=  2.03198D+04    |proj g|=  3.01133D+00\n",
      "\n",
      "At iterate 1450    f=  2.02471D+04    |proj g|=  2.82684D+00\n",
      "\n",
      "At iterate 1450    f=  2.02726D+04    |proj g|=  1.53898D+00\n",
      "\n",
      "At iterate 1450    f=  2.03408D+04    |proj g|=  7.78505D+00\n",
      "\n",
      "At iterate 1450    f=  2.03033D+04    |proj g|=  3.68033D+00\n",
      "\n",
      "At iterate 1450    f=  2.02948D+04    |proj g|=  2.08630D+00\n",
      "\n",
      "At iterate 1450    f=  2.03244D+04    |proj g|=  2.95603D+00\n",
      "\n",
      "At iterate 1450    f=  2.03218D+04    |proj g|=  9.43397D+00\n",
      "\n",
      "At iterate 1500    f=  2.03196D+04    |proj g|=  2.21056D+00\n",
      "\n",
      "At iterate 1500    f=  2.02468D+04    |proj g|=  1.77576D+00\n",
      "\n",
      "At iterate 1500    f=  2.02725D+04    |proj g|=  2.31832D+00\n",
      "\n",
      "At iterate 1500    f=  2.03030D+04    |proj g|=  2.98675D+00\n",
      "\n",
      "At iterate 1500    f=  2.03405D+04    |proj g|=  2.33109D+00\n",
      "\n",
      "At iterate 1500    f=  2.02946D+04    |proj g|=  1.80970D+00\n",
      "\n",
      "At iterate 1550    f=  2.03194D+04    |proj g|=  1.14984D+00\n",
      "\n",
      "At iterate 1500    f=  2.03242D+04    |proj g|=  1.17169D+00\n",
      "\n",
      "At iterate 1500    f=  2.03214D+04    |proj g|=  4.44499D+00\n",
      "\n",
      "At iterate 1550    f=  2.02467D+04    |proj g|=  2.77470D+00\n",
      "\n",
      "At iterate 1550    f=  2.03028D+04    |proj g|=  2.73135D+00\n",
      "\n",
      "At iterate 1550    f=  2.02944D+04    |proj g|=  1.07117D+00\n",
      "\n",
      "At iterate 1550    f=  2.02724D+04    |proj g|=  1.88831D+00\n",
      "\n",
      "At iterate 1550    f=  2.03404D+04    |proj g|=  2.65534D+00\n",
      "\n",
      "At iterate 1550    f=  2.03241D+04    |proj g|=  3.14152D+00\n",
      "\n",
      "At iterate 1600    f=  2.03194D+04    |proj g|=  1.45199D+00\n",
      "\n",
      "At iterate 1600    f=  2.02466D+04    |proj g|=  1.23492D+00\n",
      "\n",
      "At iterate 1550    f=  2.03212D+04    |proj g|=  1.17940D+01\n",
      "\n",
      "At iterate 1600    f=  2.02944D+04    |proj g|=  5.13786D-01\n",
      "\n",
      "At iterate 1600    f=  2.03026D+04    |proj g|=  4.95544D+00\n",
      "\n",
      "At iterate 1600    f=  2.02724D+04    |proj g|=  9.15896D-01\n",
      "\n",
      "At iterate 1600    f=  2.03403D+04    |proj g|=  7.34238D-01\n",
      "\n",
      "At iterate 1600    f=  2.03241D+04    |proj g|=  2.55196D+00\n",
      "\n",
      "At iterate 1650    f=  2.03194D+04    |proj g|=  3.31285D-01\n",
      "\n",
      "At iterate 1650    f=  2.02466D+04    |proj g|=  6.05769D-01\n",
      "\n",
      "At iterate 1600    f=  2.03211D+04    |proj g|=  1.59061D+00\n",
      "\n",
      "At iterate 1650    f=  2.02944D+04    |proj g|=  3.60531D-01\n",
      "\n",
      "At iterate 1650    f=  2.03025D+04    |proj g|=  3.69602D+00\n",
      "\n",
      "At iterate 1650    f=  2.02724D+04    |proj g|=  7.33315D-01\n",
      "\n",
      "At iterate 1650    f=  2.03403D+04    |proj g|=  3.51435D+00\n",
      "\n",
      "At iterate 1650    f=  2.03241D+04    |proj g|=  1.14014D+00\n",
      "\n",
      "At iterate 1700    f=  2.03194D+04    |proj g|=  6.87300D-01\n",
      "\n",
      "At iterate 1700    f=  2.02466D+04    |proj g|=  1.09735D+00\n",
      "\n",
      "At iterate 1650    f=  2.03210D+04    |proj g|=  1.74518D+00\n",
      "\n",
      "At iterate 1700    f=  2.02943D+04    |proj g|=  7.73621D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   1729   1823      1     0     0   2.966D-01   2.032D+04\n",
      "  F =   20319.379292093399     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00529D+05    |proj g|=  2.06742D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1700    f=  2.03024D+04    |proj g|=  1.53727D+00\n",
      "\n",
      "At iterate 1700    f=  2.02723D+04    |proj g|=  5.79839D+00\n",
      "\n",
      "At iterate 1700    f=  2.03240D+04    |proj g|=  1.02865D+00\n",
      "\n",
      "At iterate 1700    f=  2.03402D+04    |proj g|=  6.44789D-01\n",
      "\n",
      "At iterate 1750    f=  2.02466D+04    |proj g|=  1.05729D+00\n",
      "\n",
      "At iterate 1750    f=  2.02943D+04    |proj g|=  1.11329D+00\n",
      "\n",
      "At iterate   50    f=  2.97012D+04    |proj g|=  3.05223D+02\n",
      "\n",
      "At iterate 1700    f=  2.03210D+04    |proj g|=  1.75409D+00\n",
      "\n",
      "At iterate 1750    f=  2.03240D+04    |proj g|=  4.07429D-01\n",
      "\n",
      "At iterate 1750    f=  2.03023D+04    |proj g|=  1.77250D+00\n",
      "\n",
      "At iterate 1800    f=  2.02465D+04    |proj g|=  1.35379D+00\n",
      "\n",
      "At iterate 1750    f=  2.02723D+04    |proj g|=  1.33830D+00\n",
      "\n",
      "At iterate 1750    f=  2.03402D+04    |proj g|=  7.74652D-01\n",
      "\n",
      "At iterate  100    f=  2.33094D+04    |proj g|=  2.01986D+03\n",
      "\n",
      "At iterate 1800    f=  2.02943D+04    |proj g|=  9.80940D-01\n",
      "\n",
      "At iterate 1750    f=  2.03210D+04    |proj g|=  9.75117D-01\n",
      "\n",
      "At iterate 1800    f=  2.03023D+04    |proj g|=  7.57674D-01\n",
      "\n",
      "At iterate 1800    f=  2.03240D+04    |proj g|=  2.66699D+00\n",
      "\n",
      "At iterate 1850    f=  2.02465D+04    |proj g|=  7.45895D-01\n",
      "\n",
      "At iterate 1800    f=  2.03402D+04    |proj g|=  1.88831D+00\n",
      "\n",
      "At iterate 1800    f=  2.02722D+04    |proj g|=  1.06549D+00\n",
      "\n",
      "At iterate 1850    f=  2.02942D+04    |proj g|=  3.86620D+00\n",
      "\n",
      "At iterate  150    f=  2.13189D+04    |proj g|=  8.27342D+02\n",
      "\n",
      "At iterate 1800    f=  2.03209D+04    |proj g|=  1.12963D+00\n",
      "\n",
      "At iterate 1850    f=  2.03023D+04    |proj g|=  7.89902D-01\n",
      "\n",
      "At iterate 1850    f=  2.03240D+04    |proj g|=  2.21316D+00\n",
      "\n",
      "At iterate 1900    f=  2.02464D+04    |proj g|=  3.18159D+00\n",
      "\n",
      "At iterate 1850    f=  2.03401D+04    |proj g|=  1.03017D+00\n",
      "\n",
      "At iterate 1850    f=  2.02722D+04    |proj g|=  1.90327D+00\n",
      "\n",
      "At iterate 1900    f=  2.02941D+04    |proj g|=  1.72784D+00\n",
      "\n",
      "At iterate  200    f=  2.06806D+04    |proj g|=  1.73097D+02\n",
      "\n",
      "At iterate 1850    f=  2.03209D+04    |proj g|=  7.87686D-01\n",
      "\n",
      "At iterate 1900    f=  2.03239D+04    |proj g|=  1.98031D+00\n",
      "\n",
      "At iterate 1900    f=  2.03023D+04    |proj g|=  1.78968D+00\n",
      "\n",
      "At iterate 1950    f=  2.02463D+04    |proj g|=  7.76785D-01\n",
      "\n",
      "At iterate 1900    f=  2.03400D+04    |proj g|=  3.66523D+00\n",
      "\n",
      "At iterate 1900    f=  2.02721D+04    |proj g|=  1.72365D+00\n",
      "\n",
      "At iterate  250    f=  2.04559D+04    |proj g|=  5.70777D+01\n",
      "\n",
      "At iterate 1950    f=  2.02940D+04    |proj g|=  9.53100D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   1929   2057      1     0     0   3.606D+00   2.030D+04\n",
      "  F =   20302.251120555400     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 1900    f=  2.03209D+04    |proj g|=  1.49247D+00\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        43064     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.00533D+05    |proj g|=  2.06300D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1950    f=  2.03239D+04    |proj g|=  4.41164D+00\n",
      "\n",
      "At iterate 2000    f=  2.02463D+04    |proj g|=  2.18766D+00\n",
      "\n",
      "At iterate 1950    f=  2.03398D+04    |proj g|=  1.96170D+00\n",
      "\n",
      "At iterate 1950    f=  2.02721D+04    |proj g|=  1.06405D+00\n",
      "\n",
      "At iterate  300    f=  2.03706D+04    |proj g|=  4.17668D+01\n",
      "\n",
      "At iterate 2000    f=  2.02939D+04    |proj g|=  1.49481D+00\n",
      "\n",
      "At iterate   50    f=  2.97676D+04    |proj g|=  2.56310D+03\n",
      "\n",
      "At iterate 1950    f=  2.03209D+04    |proj g|=  2.21508D+00\n",
      "\n",
      "At iterate 2000    f=  2.03239D+04    |proj g|=  1.46411D+00\n",
      "\n",
      "At iterate 2050    f=  2.02463D+04    |proj g|=  5.35193D-01\n",
      "\n",
      "At iterate 2000    f=  2.03398D+04    |proj g|=  1.90226D+00\n",
      "\n",
      "At iterate 2000    f=  2.02720D+04    |proj g|=  1.92140D+00\n",
      "\n",
      "At iterate  350    f=  2.03370D+04    |proj g|=  2.10126D+01\n",
      "\n",
      "At iterate 2050    f=  2.02939D+04    |proj g|=  1.12505D+00\n",
      "\n",
      "At iterate  100    f=  2.32992D+04    |proj g|=  4.03493D+02\n",
      "\n",
      "At iterate 2000    f=  2.03209D+04    |proj g|=  4.93690D-01\n",
      "\n",
      "At iterate 2050    f=  2.03238D+04    |proj g|=  1.44995D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2097   2245      1     0     0   3.131D-01   2.025D+04\n",
      "  F =   20246.239599959656     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 2050    f=  2.03397D+04    |proj g|=  1.98255D+00\n",
      "\n",
      "At iterate 2050    f=  2.02719D+04    |proj g|=  1.07187D+00\n",
      "\n",
      "At iterate  400    f=  2.03242D+04    |proj g|=  2.10705D+01\n",
      "\n",
      "At iterate  150    f=  2.13025D+04    |proj g|=  2.28207D+02\n",
      "\n",
      "At iterate 2100    f=  2.02939D+04    |proj g|=  1.86770D+00\n",
      "\n",
      "At iterate 2050    f=  2.03208D+04    |proj g|=  1.02081D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2098   2246      1     0     0   1.234D+00   2.032D+04\n",
      "  F =   20323.839038904229     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 2100    f=  2.03397D+04    |proj g|=  5.11098D+00\n",
      "\n",
      "At iterate 2100    f=  2.02718D+04    |proj g|=  6.92338D-01\n",
      "\n",
      "At iterate  450    f=  2.03205D+04    |proj g|=  4.85919D+00\n",
      "\n",
      "At iterate  200    f=  2.06811D+04    |proj g|=  5.41541D+01\n",
      "\n",
      "At iterate 2150    f=  2.02939D+04    |proj g|=  4.15721D-01\n",
      "\n",
      "At iterate 2100    f=  2.03208D+04    |proj g|=  1.64950D+00\n",
      "\n",
      "At iterate 2150    f=  2.03397D+04    |proj g|=  1.71956D+00\n",
      "\n",
      "At iterate 2150    f=  2.02718D+04    |proj g|=  6.60428D-01\n",
      "\n",
      "At iterate  500    f=  2.03192D+04    |proj g|=  1.11814D+01\n",
      "\n",
      "At iterate  250    f=  2.04713D+04    |proj g|=  1.07362D+02\n",
      "\n",
      "At iterate 2200    f=  2.02939D+04    |proj g|=  6.83292D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2203   2363      1     0     0   1.589D+00   2.029D+04\n",
      "  F =   20293.852754705174     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 2150    f=  2.03208D+04    |proj g|=  1.01823D+00\n",
      "\n",
      "At iterate 2200    f=  2.03397D+04    |proj g|=  1.95380D+00\n",
      "\n",
      "At iterate 2200    f=  2.02718D+04    |proj g|=  4.32182D-01\n",
      "\n",
      "At iterate  550    f=  2.03187D+04    |proj g|=  6.80901D+00\n",
      "\n",
      "At iterate  300    f=  2.03986D+04    |proj g|=  5.03441D+01\n",
      "\n",
      "At iterate 2200    f=  2.03208D+04    |proj g|=  1.42631D+00\n",
      "\n",
      "At iterate 2250    f=  2.03397D+04    |proj g|=  5.58649D-01\n",
      "\n",
      "At iterate 2250    f=  2.02718D+04    |proj g|=  3.77621D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2250   2401      1     0     0   3.776D-01   2.027D+04\n",
      "  F =   20271.755280054007     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate  600    f=  2.03184D+04    |proj g|=  2.05319D+00\n",
      "\n",
      "At iterate  350    f=  2.03723D+04    |proj g|=  6.05304D+01\n",
      "\n",
      "At iterate 2250    f=  2.03208D+04    |proj g|=  6.03620D-01\n",
      "\n",
      "At iterate 2300    f=  2.03396D+04    |proj g|=  2.11510D+00\n",
      "\n",
      "At iterate  650    f=  2.03181D+04    |proj g|=  4.33250D+00\n",
      "\n",
      "At iterate  400    f=  2.03627D+04    |proj g|=  2.11201D+01\n",
      "\n",
      "At iterate 2300    f=  2.03208D+04    |proj g|=  5.82525D-01\n",
      "\n",
      "At iterate 2350    f=  2.03396D+04    |proj g|=  2.37654D+00\n",
      "\n",
      "At iterate  700    f=  2.03177D+04    |proj g|=  4.27824D+00\n",
      "\n",
      "At iterate  450    f=  2.03596D+04    |proj g|=  1.76233D+01\n",
      "\n",
      "At iterate 2350    f=  2.03208D+04    |proj g|=  4.69687D-01\n",
      "\n",
      "At iterate 2400    f=  2.03396D+04    |proj g|=  2.71528D+00\n",
      "\n",
      "At iterate  750    f=  2.03172D+04    |proj g|=  3.06633D+00\n",
      "\n",
      "At iterate  500    f=  2.03586D+04    |proj g|=  1.68530D+01\n",
      "\n",
      "At iterate 2400    f=  2.03207D+04    |proj g|=  8.44636D-01\n",
      "\n",
      "At iterate 2450    f=  2.03396D+04    |proj g|=  1.06811D+00\n",
      "\n",
      "At iterate  800    f=  2.03161D+04    |proj g|=  4.39623D+00\n",
      "\n",
      "At iterate  550    f=  2.03581D+04    |proj g|=  3.94616D+00\n",
      "\n",
      "At iterate 2450    f=  2.03207D+04    |proj g|=  1.07174D+00\n",
      "\n",
      "At iterate 2500    f=  2.03395D+04    |proj g|=  1.24030D+00\n",
      "\n",
      "At iterate  850    f=  2.03142D+04    |proj g|=  1.65895D+01\n",
      "\n",
      "At iterate  600    f=  2.03578D+04    |proj g|=  1.94092D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2492   2664      1     0     0   3.145D+00   2.032D+04\n",
      "  F =   20320.714565038681     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 2550    f=  2.03395D+04    |proj g|=  2.11136D+00\n",
      "\n",
      "At iterate  900    f=  2.03117D+04    |proj g|=  6.01648D+00\n",
      "\n",
      "At iterate  650    f=  2.03574D+04    |proj g|=  6.26938D+00\n",
      "\n",
      "At iterate 2600    f=  2.03395D+04    |proj g|=  1.26378D+00\n",
      "\n",
      "At iterate  950    f=  2.03099D+04    |proj g|=  7.79904D+00\n",
      "\n",
      "At iterate  700    f=  2.03566D+04    |proj g|=  3.15809D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2630   2795      1     0     0   3.273D-01   2.034D+04\n",
      "  F =   20339.490524483947     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 1000    f=  2.03090D+04    |proj g|=  1.50098D+01\n",
      "\n",
      "At iterate  750    f=  2.03553D+04    |proj g|=  4.48435D+00\n",
      "\n",
      "At iterate 1050    f=  2.03086D+04    |proj g|=  5.64700D+00\n",
      "\n",
      "At iterate  800    f=  2.03533D+04    |proj g|=  8.52488D+00\n",
      "\n",
      "At iterate 1100    f=  2.03085D+04    |proj g|=  1.27287D+00\n",
      "\n",
      "At iterate  850    f=  2.03511D+04    |proj g|=  9.10194D+00\n",
      "\n",
      "At iterate 1150    f=  2.03084D+04    |proj g|=  1.35774D+00\n",
      "\n",
      "At iterate  900    f=  2.03493D+04    |proj g|=  2.00268D+01\n",
      "\n",
      "At iterate 1200    f=  2.03083D+04    |proj g|=  8.51015D-01\n",
      "\n",
      "At iterate  950    f=  2.03483D+04    |proj g|=  1.45220D+01\n",
      "\n",
      "At iterate 1250    f=  2.03083D+04    |proj g|=  1.37084D+00\n",
      "\n",
      "At iterate 1000    f=  2.03478D+04    |proj g|=  7.31082D+00\n",
      "\n",
      "At iterate 1300    f=  2.03082D+04    |proj g|=  1.35499D+00\n",
      "\n",
      "At iterate 1050    f=  2.03476D+04    |proj g|=  1.51425D+00\n",
      "\n",
      "At iterate 1350    f=  2.03081D+04    |proj g|=  2.23404D+00\n",
      "\n",
      "At iterate 1100    f=  2.03474D+04    |proj g|=  9.76720D-01\n",
      "\n",
      "At iterate 1400    f=  2.03078D+04    |proj g|=  3.80861D+00\n",
      "\n",
      "At iterate 1150    f=  2.03474D+04    |proj g|=  2.42167D+00\n",
      "\n",
      "At iterate 1450    f=  2.03073D+04    |proj g|=  2.90262D+00\n",
      "\n",
      "At iterate 1200    f=  2.03473D+04    |proj g|=  3.15050D+00\n",
      "\n",
      "At iterate 1500    f=  2.03070D+04    |proj g|=  3.68686D+00\n",
      "\n",
      "At iterate 1250    f=  2.03472D+04    |proj g|=  1.97856D+00\n",
      "\n",
      "At iterate 1550    f=  2.03069D+04    |proj g|=  3.36743D+00\n",
      "\n",
      "At iterate 1300    f=  2.03470D+04    |proj g|=  1.31390D+00\n",
      "\n",
      "At iterate 1600    f=  2.03068D+04    |proj g|=  3.15492D+00\n",
      "\n",
      "At iterate 1350    f=  2.03467D+04    |proj g|=  2.39767D+00\n",
      "\n",
      "At iterate 1650    f=  2.03068D+04    |proj g|=  8.38306D-01\n",
      "\n",
      "At iterate 1400    f=  2.03464D+04    |proj g|=  1.01912D+01\n",
      "\n",
      "At iterate 1700    f=  2.03068D+04    |proj g|=  1.01617D+00\n",
      "\n",
      "At iterate 1450    f=  2.03459D+04    |proj g|=  2.97542D+00\n",
      "\n",
      "At iterate 1750    f=  2.03067D+04    |proj g|=  4.16446D+00\n",
      "\n",
      "At iterate 1500    f=  2.03456D+04    |proj g|=  1.68713D+00\n",
      "\n",
      "At iterate 1800    f=  2.03067D+04    |proj g|=  5.98577D+00\n",
      "\n",
      "At iterate 1550    f=  2.03454D+04    |proj g|=  2.69780D+00\n",
      "\n",
      "At iterate 1850    f=  2.03067D+04    |proj g|=  1.52195D+00\n",
      "\n",
      "At iterate 1600    f=  2.03453D+04    |proj g|=  5.26490D+00\n",
      "\n",
      "At iterate 1900    f=  2.03067D+04    |proj g|=  2.79593D+00\n",
      "\n",
      "At iterate 1650    f=  2.03453D+04    |proj g|=  6.56212D-01\n",
      "\n",
      "At iterate 1950    f=  2.03066D+04    |proj g|=  1.50907D+00\n",
      "\n",
      "At iterate 1700    f=  2.03452D+04    |proj g|=  2.25991D+00\n",
      "\n",
      "At iterate 2000    f=  2.03066D+04    |proj g|=  1.08098D+00\n",
      "\n",
      "At iterate 1750    f=  2.03452D+04    |proj g|=  1.63351D+00\n",
      "\n",
      "At iterate 2050    f=  2.03065D+04    |proj g|=  1.21323D+00\n",
      "\n",
      "At iterate 1800    f=  2.03451D+04    |proj g|=  2.15281D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2095   2243      1     0     0   1.668D+00   2.031D+04\n",
      "  F =   20306.532875681383     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "\n",
      "At iterate 1850    f=  2.03450D+04    |proj g|=  1.74608D+00\n",
      "\n",
      "At iterate 1900    f=  2.03449D+04    |proj g|=  2.60769D+00\n",
      "\n",
      "At iterate 1950    f=  2.03447D+04    |proj g|=  2.74453D+00\n",
      "\n",
      "At iterate 2000    f=  2.03446D+04    |proj g|=  1.15019D+00\n",
      "\n",
      "At iterate 2050    f=  2.03445D+04    |proj g|=  1.66611D+00\n",
      "\n",
      "At iterate 2100    f=  2.03444D+04    |proj g|=  1.45345D+00\n",
      "\n",
      "At iterate 2150    f=  2.03444D+04    |proj g|=  4.46674D+00\n",
      "\n",
      "At iterate 2200    f=  2.03444D+04    |proj g|=  4.82760D-01\n",
      "\n",
      "At iterate 2250    f=  2.03444D+04    |proj g|=  6.10974D-01\n",
      "\n",
      "At iterate 2300    f=  2.03443D+04    |proj g|=  1.78794D+00\n",
      "\n",
      "At iterate 2350    f=  2.03443D+04    |proj g|=  2.61139D+00\n",
      "\n",
      "At iterate 2400    f=  2.03442D+04    |proj g|=  2.83124D+00\n",
      "\n",
      "At iterate 2450    f=  2.03441D+04    |proj g|=  1.75493D+00\n",
      "\n",
      "At iterate 2500    f=  2.03441D+04    |proj g|=  1.06470D+00\n",
      "\n",
      "At iterate 2550    f=  2.03440D+04    |proj g|=  1.39436D+00\n",
      "\n",
      "At iterate 2600    f=  2.03440D+04    |proj g|=  4.15120D-01\n",
      "\n",
      "At iterate 2650    f=  2.03440D+04    |proj g|=  1.17779D+00\n",
      "\n",
      "At iterate 2700    f=  2.03440D+04    |proj g|=  4.24299D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "43064   2700   2899      1     0     0   4.243D-01   2.034D+04\n",
      "  F =   20343.964946425567     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Accuracy on transformed test data: 0.8546546546546546\n",
      "final score : 0.8302681123276323\n",
      "macro_f1    : 0.8440635817317848\n",
      "inv_macro_gap 0.81647264292348\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 4. CUSTOM LOSS FUNCTION WITH 56 CLASS CLASSIFIER (USING Y56 = Y + S*28)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "name ='4_Reglog56_StratKFold_Orthogonal_CustomLoss' # changer clf_i\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF CUSTOM LOSS FUNCTION (FINAL SCORE) \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "#It is not possible to split S with the stratified k-fold implementation \n",
    "# in sickit learn. We will use Y56 = Y + 28*S to train logistic model\n",
    "# and derive from Y56 the value of Y = Y56 % 28 and S = Y56//28\n",
    "\n",
    "def final_score_S(Y56,Y_pred):\n",
    "    '''custom scorer expects (y_true, y_pred) as inputs\n",
    "    Inputs : Y56 and Y pred\n",
    "    Outputs : final score on 28 classes with Y56 trick'''\n",
    "    S = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "    Y = Y56 % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "    return get_final_score(Y_pred,Y,S)\n",
    "\n",
    "custom_scorer = make_scorer(final_score_S)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "clf_4 = LogisticRegression(random_state=42, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores = cross_val_score(clf_4, debiased_X, Y56, scoring=custom_scorer ,cv=cv, n_jobs=-1)\n",
    "clf_4.fit(debiased_X, Y56)\n",
    "model = clf_4\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_5 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y56_pred = model.predict(X_test)\n",
    "#S_pred = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y56_pred_true = model.predict(modified_X_test_true)\n",
    "Y_pred_true = Y56_pred_true % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 5. APPROCHE DUALE : MODEL HOMME + MODEL FEMME\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# PREPARATION DES DONNEES\n",
    "#-----------------------------------------------------\n",
    "\n",
    "X_F , X_H = X[S==1] , X[S==0]\n",
    "Y_F , Y_H = Y[S==1] , Y[S==0]\n",
    "S_F , S_H = S[S==1] , S[S==0]\n",
    "print(X_F.shape,X_H.shape)\n",
    "print(Y_F.shape,Y_H.shape)\n",
    "print(S_F.shape,S_H.shape)\n",
    "\n",
    "\n",
    "# Prepare Logistic Regression models\n",
    "model_X = LogisticRegression(random_state=0, max_iter=5000,penalty='l2', C=0.2, tol=0.0001).fit(X,Y)\n",
    "model_F = LogisticRegression(random_state=0, max_iter=5000,penalty='l2', C=0.2, tol=0.0001)\n",
    "model_H = LogisticRegression(random_state=0, max_iter=5000, penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "'''# Keep the original index for later use\n",
    "X['original_index'] = X.index\n",
    "\n",
    "# Step 1: Keep original index by resetting it if not already in a suitable format\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "Y = Y.reset_index(drop=True)\n",
    "S = S.reset_index(drop=True)\n",
    "\n",
    "# Create a combined Y and S DataFrame for stratification\n",
    "YS = pd.DataFrame({'Y': Y, 'S': S})'''\n",
    "\n",
    "# Split the data while stratifying based on Y and S\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=0, stratify=YS)\n",
    "print('train',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test',X_test.shape,Y_test.shape,S_test.shape)\n",
    "\n",
    "# Create XS and XnotS subsets for training and test sets\n",
    "XS_train = X_train[S == True]\n",
    "YS_train = Y_train[S == True]\n",
    "\n",
    "XnotS_train = X_train[S == False]\n",
    "YnotS_train = Y_train[S == False]\n",
    "\n",
    "XS_test = X_test[S == True]\n",
    "YS_test = Y_test[S == True]\n",
    "\n",
    "XnotS_test = X_test[S == False ]\n",
    "XnotS_test = X_test[S == False ]\n",
    "\n",
    "XS_train.head(5)\n",
    "\n",
    "\n",
    "print(Y_train[S == True].shape)\n",
    "Y_train[S == True].head()\n",
    "\n",
    "\n",
    "\n",
    "# Train the models\n",
    "model_X.fit(X_train, Y_train)\n",
    "model_XS.fit(XS_train, Y_train[S==1])\n",
    "model_XnotS.fit(XnotS_train, Y_train[S==0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 6 - ADPAPTING LOSS FUNCTION IN LOGISTIC REGRESSION (PYTORCH)**\n",
    "---\n",
    "1. Functions for Custom loss function (re-written in pytorch)\n",
    "2. Functions for Stratified K-fold and training batches\n",
    "3. Running Model 6\n",
    "4. Optimising Model 6 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 6. CUSTOM LOSS FUNCTION IN PYTORCH (CLASSIFIER ON 28 ORIGINAL CLASSES)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"SENSITIVE FRONTIER\" MODELS <br>(USED ON DEBIASED DATASET)**\n",
    "---\n",
    "K nearest neighboors<br>\n",
    "SVM linear<br>\n",
    "SVM gaussian kernel<br>\n",
    "One versus all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#   SCORE A BLANC\n",
    "#####################################################\n",
    "\n",
    "\n",
    "n=Y.shape[0]\n",
    "Y_pred =np.ones(n)*1\n",
    "accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score)\n",
    "print('macro_fscore',eval_scores['macro_fscore'])\n",
    "print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test score \"prediction uniforme\"\n",
    "\n",
    "n=Y.shape[0]\n",
    "\n",
    "Scores_U=pd.DataFrame(columns=['N','N_f','N_h','accuracy','final_score','macro_f1','macro_gap'])\n",
    "\n",
    "print(Scores_U)\n",
    "for i in range(28):\n",
    "    #Test value for all i values\n",
    "    test=pd.DataFrame(np.ones(11893,dtype=int)*i)\n",
    "    test.to_csv(\"all/Data_Challenge_all_\"+str(i)+\".csv\", header = None, index = None)\n",
    "    \n",
    "    Y_pred=pd.DataFrame(np.ones(n,dtype=int)*i)\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    macro_f1 = eval_scores['macro_fscore']\n",
    "    macro_gap = eval_scores['TPR_GAP']\n",
    "    final_score = (macro_f1 +1- macro_gap)/2\n",
    "    # check number of occurence\n",
    "    N = (Y==i).sum()\n",
    "    N_f = (Y56 == i + 28).sum()\n",
    "    N_h = (Y56 == i).sum()\n",
    "    Scores_U.loc[i]= [N, N_f, N_h, accuracy,final_score,macro_f1,macro_gap]\n",
    "\n",
    "Scores_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = Scores_U['N']\n",
    "score = Scores_U['final_score']\n",
    "\n",
    "\n",
    "# Créer et entraîner le modèle de régression linéaire\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reverse = LinearRegression()\n",
    "reverse.fit(np.array(score).reshape(-1, 1),np.array(size).reshape(-1, 1))\n",
    "\n",
    "# Load the scores for X_true\n",
    "scores_true = pd. read_csv('all/distribution_Y_true.txt',header = None,index_col=0)\n",
    "dist_true_pred = reverse.predict(scores_true) #/ 11893 * 27749\n",
    "\n",
    "# Afficher la distribution de X_test_true\n",
    "plt.scatter(size,score)\n",
    "plt.scatter(dist_true_pred,scores_true)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Scores_U['final_score'],scores_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(28),size,label='X')\n",
    "plt.plot(range(28),np.round(dist_true_pred),'X_test_true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# train_test_split with Y56 (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame({'label':Y,'S':S})\n",
    "\n",
    "# Sum counts of 1s and 0s to get the total count for each label\n",
    "result = dist.groupby('label')['S'].value_counts().unstack(fill_value=0)\n",
    "result.columns = ['S', 'not_S']\n",
    "result['total_count'] = result.sum(axis=1)\n",
    "\n",
    "# Calculate totals for each column\n",
    "totals = result.sum(axis=0)\n",
    "result.loc[30] = totals\n",
    "\n",
    "# Calculate total count percentages for each count\n",
    "result['%_S_label'] = round((result['S'] / result['total_count']) * 100)\n",
    "result['%_not_S_label'] = round((result['not_S'] / result['total_count']) * 100)\n",
    "result['%_total'] = np.round(result['total_count']/len(Y)*100,2 ) # % of total count\n",
    "result['%_S_total'] = np.round(result['S']/(S==1).sum()*100,2 ) # % of total count\n",
    "result['%_not_S_total'] = np.round(result['not_S']/(S!=1).sum()*100,2 ) # % of total count\n",
    "result['diff_%_S_total']=result['%_S_total']-result['%_total']\n",
    "result['|']='|'\n",
    "#Reorder table\n",
    "result = result.reindex(columns=['total_count', '%_total','|','S','%_S_total','diff_%_S_total','|','not_S', '%_not_S_total','|','%_S_label', '%_not_S_label'])\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the '%_S' column\n",
    "labels = pd.to_numeric(result.index)\n",
    "data = result['%_total']\n",
    "data_S = result['%_S_total']\n",
    "data_not_S = result['%_not_S_total']\n",
    "\n",
    "# Create a bar plot\n",
    "plt.plot(labels[:-1], data[:-1])\n",
    "plt.plot(labels[:-1],data_S[:-1], label='Sensitive')\n",
    "plt.plot(labels[:-1],data_not_S[:-1], label='Not_sensitive')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage of total')\n",
    "plt.title('Percentage of total for sample, S and non-S')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sorted = result.iloc[:-1].sort_values(by='diff_%_S_total', ascending=False)\n",
    "result_sorted['original_label'] = result_sorted.index\n",
    "result_sorted=result_sorted.reset_index(drop=True)\n",
    "#result_sorted.reset_index(drop=True, inplace=True)\n",
    "result_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the '%_S' column\n",
    "labels_sorted = result_sorted['original_label']\n",
    "#print(labels_sorted)\n",
    "data = result_sorted['%_total']\n",
    "data_S = result_sorted['%_S_total']\n",
    "data_not_S = result_sorted['%_not_S_total']\n",
    "\n",
    "# Create a bar plot\n",
    "plt.plot(data)\n",
    "plt.plot(data_S, label='Sensitive')\n",
    "plt.plot(data_not_S, label='Not_sensitive')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage of total')\n",
    "#plt.xticks(labels_sorted)\n",
    "plt.xticks(ticks=range(len(labels_sorted)), labels=labels_sorted, rotation=90)  # Rotate if there are many labels\n",
    "\n",
    "plt.title('Percentage of total for sample, S and non-S')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL ORTHOGONAL DEBIASING**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Fonction de similarité cosinus\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Pré-générer les embeddings pour chaque token dans le vocabulaire\n",
    "# ATTENTION : Cette étape est très coûteuse !\n",
    "vocab_embeddings = {}\n",
    "for word, token_id in tokenizer.vocab.items():\n",
    "    input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(word)]).unsqueeze(0)  # Ajouter batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    vocab_embeddings[word] = outputs.last_hidden_state[0, 0, :].numpy()  # Utiliser l'embedding du token\n",
    "\n",
    "# Supposons que X soit votre matrice d'embeddings (n, 768)\n",
    "# X = ...\n",
    "\n",
    "# Trouver le mot le plus proche pour les 10 premiers embeddings\n",
    "for i in range(10):\n",
    "    embedding = X[i]\n",
    "    similarities = {word: cosine_similarity(embedding, word_emb) for word, word_emb in vocab_embeddings.items()}\n",
    "    closest_word = max(similarities, key=similarities.get)\n",
    "    print(f\"Ligne {i}: Mot le plus proche = {closest_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**baseline Logistic regression on raw data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\", clf_1.coef_.shape)\n",
    "print(\"Intercept:\", clf_1.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_scores_1)\n",
    "print(confusion_matrices_eval_1.keys)\n",
    "\n",
    "#show confusion matrix for Accurarcy key 0\n",
    "pd.DataFrame(confusion_matrices_eval_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true_1 = clf_1.predict(X_test_true)\n",
    "results_1=pd.DataFrame(y_test_true_1, columns= ['score'])\n",
    "\n",
    "results_1.to_csv(\"Data_Challenge_MDI_341_1.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECOND METHOD - ADVERSARIAL NN**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh training data\n",
    "# X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(256, activation='relu')(input_layer)\n",
    "main_task_output = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(256, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_2 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              loss_weights=[1, -0.1],\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# X normal:isation\n",
    "# none in method 2\n",
    "\n",
    "# Train the model\n",
    "model_2.fit(X_train, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_2 = Model(inputs=model_2.input, outputs=main_task_hidden)\n",
    "X_train_transformed_2 = feature_extractor_2.predict(X_train)\n",
    "X_test_transformed_2 = feature_extractor_2.predict(X_test)\n",
    "X_test_true_transformed_2 = feature_extractor_2.predict(X_test_true)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "clf_2 = LogisticRegression(max_iter=5000)  # Increase max_iter if needed for convergence\n",
    "history_new_2 = clf_2.fit(X_train_transformed_2, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_2 = clf_2.predict(X_test_transformed_2)\n",
    "accuracy_2= accuracy_score(Y_test, Y_pred_2)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_2}\")\n",
    "\n",
    "# Step 4 : Predict with gloabl score\n",
    "eval_scores_2, confusion_matrices_eval_2 = gap_eval_scores(Y_pred_2, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_2 = (eval_scores_2['macro_fscore']+ (1-eval_scores_2['TPR_GAP']))/2\n",
    "print('\\nfinal',final_score_2)\n",
    "print('macro_fscore',eval_scores_2['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores_2['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the provided test data with you classifier\n",
    "y_test = clf_2.predict(X_test_true_transformed_2)\n",
    "results=pd.DataFrame(y_test, columns= ['score'])\n",
    "\n",
    "results.to_csv(\"Data_Challenge_MDI_341_2.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIRD METHOD**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# get X_train an\n",
    "# X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform your training data\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "X_test_true_standardized = scaler.transform(X_test_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(256, activation='relu')(input_layer)\n",
    "main_task_hidden = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(256, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_3 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train_standardized.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# Train the model\n",
    "model_3.fit(X_train_standardized, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_3 = Model(inputs=model_3.input, outputs=main_task_hidden)\n",
    "X_train_transformed_3 = feature_extractor_3.predict(X_train_standardized)\n",
    "X_test_transformed_3 = feature_extractor_3.predict(X_test_standardized)\n",
    "X_test_true_transformed_3 = feature_extractor_3.predict(X_test_true_standardized)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "new_classifier_3 = LogisticRegression(max_iter=10000)  # Increase max_iter if needed for convergence\n",
    "history_new_3 = new_classifier_3.fit(X_train_transformed_3, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_3 = new_classifier_3.predict(X_test_transformed_3)\n",
    "accuracy_3= accuracy_score(Y_test, Y_pred_3)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate final score\n",
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred_3, Y_test, S_test, metrics=['TPR'])\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "print('macro_fscore',eval_scores['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores['TPR_GAP'])\n",
    "print('final score (average)',final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "X_test_true_transformed_3 = feature_extractor_3.predict(X_test_true)\n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true = clf.predict(X_test_true_transformed_3)\n",
    "results_3=pd.DataFrame(y_test_true, columns= ['score'])\n",
    "\n",
    "results_3.to_csv(\"Data_Challenge_MDI_341_3.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4th METHOD**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(512, activation='relu')(input_layer)\n",
    "main_task_hidden = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(512, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_4 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train_standardized.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# Train the model\n",
    "model_4.fit(X_train_standardized, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_4 = Model(inputs=model_4.input, outputs=main_task_hidden)\n",
    "X_train_transformed_4 = feature_extractor_4.predict(X_train_standardized)\n",
    "X_test_transformed_4 = feature_extractor_4.predict(X_test_standardized)\n",
    "X_test_true_transformed_4 = feature_extractor_4.predict(X_test_true_standardized)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "new_classifier_4 = LogisticRegression(max_iter=10000)  # Increase max_iter if needed for convergence\n",
    "history_4 = new_classifier_4.fit(X_train_transformed_4, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_4 = new_classifier_4.predict(X_test_transformed_4)\n",
    "accuracy_4 = accuracy_score(Y_test, Y_pred_4)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred_4, Y_test, S_test, metrics=['TPR'])\n",
    "#eval_scores#eval_scores['macro_fscore']\n",
    "#eval_scores['TPR_GAP']\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "X_test_true_transformed_4 = feature_extractor_4.predict(X_test_true)\n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true = clf.predict(X_test_true_transformed_4)\n",
    "results_4=pd.DataFrame(y_test_true, columns= ['score'])\n",
    "\n",
    "results_4.to_csv(\"Data_Challenge_MDI_341_4.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constrastive learning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from Data_challenge_fairness_2024.evaluator import *\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#\n",
    "#   CONTRASTIVE LEARNING FUNCTIONS (PAIRS)\n",
    "#\n",
    "#   1/ generate_random_sample_pairs()\n",
    "#   2/ get_pair_sample(df,df_sample_pairs_indices)\n",
    "#\n",
    "######################################################################\n",
    "\n",
    "# Function to generate a random sample a pairs (K positive pairs (if possible) and K negatives pairs)\n",
    "\n",
    "def generate_random_sample_pairs(df,K=6):\n",
    "    '''GENERATES A SAMPLE OF RANDOM POSITIVE AND NEGATIVE PAIRS FOR CONTRASTIVE LEARNING\n",
    "\n",
    "    Input : \n",
    "    - df : a pandas DataFrame with a columns 'label'\n",
    "    - K : a integer representing number of positive pairs and netagive pairs to be generated for each label \n",
    "    a maximum of 2*K pairs will be generated.\n",
    "    Note : maximum number of positive pairs generated for a label = n_label * (n_label - 1) / 2 \n",
    "    with n_label the number of lines for this label.\n",
    "    if label has 1 line => 0 pair, 2 lines =>1 pair, 3 lines => 3 pairs, 4 lines => 6 pairs, 5 lines => 10 pairs, 6 lines => 15 pairs, etc...\n",
    "\n",
    "    Ouput : a dataframe with 2 columns 'pair' and 'value'\n",
    "    - pair : a set (unordered) of 2 indices\n",
    "    - value : 1 if positive pair (with same label) / 0 if negative pair (with different labels)\n",
    "    '''\n",
    "    labels=df['label'].unique()\n",
    "    labels.sort()\n",
    "    \n",
    "    sample_positive_pairs=[] # To store positive pairs generated\n",
    "    sample_negative_pairs=[] # To store negative pairs generated\n",
    "    sample_pairs = []  # To store all generated pairs\n",
    "    #print('\\nlabel n°',l,'-', label, 'number of lines:',n_label_indices,'\\n',label_indices)\n",
    "\n",
    "    for l,label in enumerate(labels):\n",
    "        print('label',label)\n",
    "        check=(l,label)\n",
    "        if l%1000==0:print(\"generating pair for label:\",l)\n",
    "       \n",
    "        # GENERATION OF POSITIVE PAIRS\n",
    "        #-----------------------------\n",
    "        \n",
    "        # save and shuffle indexes of products with same labels\n",
    "        label_indices = df[df['label'] == label].index.tolist()\n",
    "        n_label_indices = len(label_indices)  # Number of same-label indices\n",
    "        max_positive_pairs = n_label_indices * (n_label_indices - 1) / 2 # maximum number of possible positive pair (n! /(n-2!*2!))\n",
    "    \n",
    "        # initialise list of positive pairs for this label\n",
    "        positive_pairs = [] # pairs of indices with same label\n",
    "        n_positive_pairs= 0 # Number of positive pairs generated for label\n",
    "          \n",
    "        # Loop for positive pairs (same labels) if more than 1 data\n",
    "        if n_label_indices > 1:\n",
    "            while n_positive_pairs < min(K, max_positive_pairs):\n",
    "                temp=label_indices.copy()\n",
    "                while (n_positive_pairs < min(K, max_positive_pairs)) and (len(temp)>=2) :\n",
    "                    draw_pair = random.sample(temp , 2)\n",
    "                    pair=set(draw_pair)\n",
    "                    temp.remove(draw_pair[0]) \n",
    "                    temp.remove(draw_pair[1])\n",
    "                    if pair not in positive_pairs:\n",
    "                        positive_pairs.append(pair)\n",
    "                        n_positive_pairs=len(positive_pairs)\n",
    "\n",
    "            #print('end of positive while loop',n_positive_pairs,'pairs out of ',max_positive_pairs,'possible pairs\\n',positive_pairs)\n",
    " \n",
    "        # GENERATION OF NEGATIVE PAIRS\n",
    "        #----------------------------- \n",
    "\n",
    "        # save indexes of products with different labels\n",
    "        different_label_indices = df[df['label'] != label].index.tolist()\n",
    "        n_different_indices=len(different_label_indices)\n",
    "        \n",
    "        # initialise list of negative pairs for this label\n",
    "        negative_pairs = [] # pairs with different label\n",
    "        n_negative_pairs=0 # Number of negative pairs generated for label\n",
    "    \n",
    "        # Loop for negative pairs (different label)\n",
    "        while n_negative_pairs < min (K,n_different_indices):  \n",
    "            temp_positive=label_indices.copy()\n",
    "            temp_negative=different_label_indices.copy()\n",
    "            while (n_negative_pairs < min(K, n_different_indices)) and (len(temp_positive)>0) and (len(temp_negative)>0) :\n",
    "                positive = random.choice(temp_positive)\n",
    "                negative = random.choice(temp_negative)\n",
    "                pair = {positive,negative}\n",
    "                if (pair not in negative_pairs) and (pair not in sample_negative_pairs):\n",
    "                    negative_pairs.append(pair)\n",
    "                    n_negative_pairs=len(negative_pairs)\n",
    "                    temp_positive.remove(positive)\n",
    "                    temp_negative.remove(negative)\n",
    "        #print('end of negative while loop',n_negative_pairs,'pairs out of ',n_different_indices,'possible pairs\\n',negative_pairs)\n",
    "    \n",
    "    \n",
    "        sample_positive_pairs.extend(positive_pairs)\n",
    "        sample_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    sample_positive=pd.DataFrame({'pair':sample_positive_pairs,'value':1})\n",
    "    sample_negative=pd.DataFrame({'pair':sample_negative_pairs,'value':0})\n",
    "    sample_pairs_indices = pd.concat([sample_positive,sample_negative], ignore_index=True)\n",
    "    print('number of pairs in sample:',len(sample_pairs_indices),'(',len(sample_pairs_indices[sample_pairs_indices['value']==1]),'positives and',len(sample_pairs_indices[sample_pairs_indices['value']==0]),'negatives)')\n",
    "\n",
    "    return sample_pairs_indices\n",
    "\n",
    "\n",
    "def generate_optimized_sample_pairs(df,K=6): \n",
    "    '''GENERATES A SAMPLE OF RANDOM POSITIVE AND NEGATIVE PAIRS FOR CONTRASTIVE LEARNING\n",
    "            (positive pairs are far from each other, negative pairs are close)\n",
    "\n",
    "    Input : \n",
    "    - df : a pandas DataFrame with a columns 'label'\n",
    "    - K : a integer representing number of positive pairs and netagive pairs to be generated for each label \n",
    "    a maximum of 2*K pairs will be generated.\n",
    "    Note : maximum number of positive pairs generated for a label = n_label * (n_label - 1) / 2 \n",
    "    with n_label the number of lines for this label.\n",
    "    if label has 1 line => 0 pair, 2 lines =>1 pair, 3 lines => 3 pairs, 4 lines => 6 pairs, 5 lines => 10 pairs, 6 lines => 15 pairs, etc...\n",
    "\n",
    "    Ouput : a dataframe with 2 columns 'pair' and 'value'\n",
    "    - pair : a set (unordered) of 2 indices\n",
    "    - value : 1 if positive pair (with same label) / 0 if negative pair (with different labels)\n",
    "  '''\n",
    "    \n",
    "def get_pair_sample(df,df_sample_pairs_indices):\n",
    "    '''returns a dataset containing a list of 2 embeddings for each pair\n",
    "    '''\n",
    "    # create pandas dataframe for sample\n",
    "    columns = [f'p1_{i}' for i in range(300)]+[f'p2_{i}' for i in range(300)]+['label']\n",
    "    sample = pd.DataFrame(index=df_sample_pairs_indices.index,columns=columns)\n",
    "    \n",
    "    # extract embedding for each pair\n",
    "    for i in range(df_sample_pairs_indices.shape[0]):\n",
    "        if i%25000==0: print(i)\n",
    "        \n",
    "        # Get indices of 1st and 2nd pair\n",
    "        p1 , p2 = df_sample_pairs_indices.iloc[i,0]\n",
    "        \n",
    "        # import embedding and label (O or 1) \n",
    "        if len(df.loc[p1,'average_embedding'])==300 :\n",
    "            sample.iloc[i,0:300]=df.loc[p1,'average_embedding']\n",
    "        if len(df.loc[p2,'average_embedding'])==300 :\n",
    "            sample.iloc[i,300:600]=df.loc[p2,'average_embedding']\n",
    "        sample.loc[i,'label']=df_sample_pairs_indices.iloc[i,1]\n",
    "    \n",
    "    # drop lines containing at least 1 NaN value (meaning that one of the embeddings was empty)\n",
    "    print('number of pairs of indices provided:',df_sample_pairs_indices.shape[0])\n",
    "    #sample_cleaned = sample.dropna()\n",
    "    #print('shape of sample generated:',sample_cleaned[0])\n",
    "    #print(f'{sample.shape[0]-sample_cleaned.shape[0]} lines dropped (because of NaN values)')\n",
    "    return sample#_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Load pickle file and convert to numpy array\n",
    "#####################################################\n",
    "\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    " \n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "Y56= Y + 28*S\n",
    "#X, X_test,Y,S, S_test = dat[1]\n",
    "\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (22199, 768) (22199,) (22199,)\n",
      "test: (5550, 768) (5550,) (5550,)\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# train_test_split with Y56 (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# génération des paires pour le constractive learning\n",
    "##############################################################\n",
    "path_pkl='pkl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0\n",
      "generating pair for label: 0\n",
      "label 1\n",
      "label 2\n",
      "label 3\n",
      "label 4\n",
      "label 5\n",
      "label 6\n",
      "label 7\n",
      "label 8\n",
      "label 9\n",
      "label 10\n",
      "label 11\n",
      "label 12\n",
      "label 13\n",
      "label 14\n",
      "label 15\n",
      "label 16\n",
      "label 17\n",
      "label 18\n",
      "label 19\n",
      "label 20\n",
      "label 21\n",
      "label 22\n",
      "label 23\n",
      "label 24\n",
      "label 25\n",
      "label 26\n",
      "label 27\n",
      "number of pairs in sample: 219736 ( 107736 positives and 112000 negatives)\n",
      "label 0\n",
      "generating pair for label: 0\n",
      "label 1\n",
      "label 2\n",
      "label 3\n",
      "label 4\n",
      "label 5\n",
      "label 6\n",
      "label 7\n",
      "label 8\n",
      "label 9\n",
      "label 10\n",
      "label 11\n",
      "label 12\n",
      "label 13\n",
      "label 14\n",
      "label 15\n",
      "label 16\n",
      "label 17\n",
      "label 18\n",
      "label 19\n",
      "label 20\n",
      "label 21\n",
      "label 22\n",
      "label 23\n",
      "label 24\n",
      "label 25\n",
      "label 26\n",
      "label 27\n",
      "number of pairs in sample: 50249 ( 22249 positives and 28000 negatives)\n"
     ]
    }
   ],
   "source": [
    "# Pair generation (K=4000) with Y (28 dimension)\n",
    "\n",
    "K=4000\n",
    "Y_train_df=pd.DataFrame({'label':Y_train})\n",
    "pairs_train_4000 = generate_random_sample_pairs(Y_train_df,K)\n",
    "\n",
    "K_test=1000 # ratio 20 / 80 %\n",
    "Y_test_df=pd.DataFrame({'label':Y_test})\n",
    "pairs_test_1000 = generate_random_sample_pairs(Y_test_df,K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0\n",
      "generating pair for label: 0\n",
      "label 1\n",
      "label 2\n",
      "label 3\n",
      "label 4\n",
      "label 5\n",
      "label 6\n",
      "label 7\n",
      "label 8\n",
      "label 9\n",
      "label 10\n",
      "label 11\n",
      "label 12\n",
      "label 13\n",
      "label 14\n",
      "label 15\n"
     ]
    }
   ],
   "source": [
    "# Pair generation (K=8000) with Y (28 dimension)\n",
    "# peut etre trop grand...\n",
    "\n",
    "K=8000\n",
    "Y_train_df=pd.DataFrame({'label':Y_train})\n",
    "pairs_train_8000 = generate_random_sample_pairs(Y_train_df,K)\n",
    "\n",
    "K_test=2000 # ratio 20 / 80 %\n",
    "Y_test_df=pd.DataFrame({'label':Y_test})\n",
    "pairs_test_2000 = generate_random_sample_pairs(Y_test_df,K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair generation with Y56 (56 dimension = Y x S = 28 x 2)\n",
    "\n",
    "K = 200\n",
    "Y56_train_df=pd.DataFrame({'label',Y56_train})\n",
    "pairs56_train_200 = generate_random_sample_pairs(Y56_train_df,K)\n",
    "\n",
    "K_test = 50\n",
    "Y56_test_df=pd.DataFrame({'label',Y56_test})\n",
    "pairs56_test_50 = generate_random_sample_pairs(Y56_test_df,K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair generation with Y56 (56 dimension = Y x S = 28 x 2)\n",
    "\n",
    "K = 400\n",
    "Y56_train_df=pd.DataFrame({'label',Y56_train})\n",
    "pairs56_train_400 = generate_random_sample_pairs(Y56_train_df,K)\n",
    "\n",
    "K_test = 100\n",
    "Y56_test_df=pd.DataFrame({'label',Y56_test})\n",
    "pairs56_test_100 = generate_random_sample_pairs(Y56_test_df,K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE FILES\n",
    "\n",
    "#with Y56 (Y = 28 labels)\n",
    "\n",
    "with open(path_pkl+'sample_pair_indices_train_8000.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs_train_8000, f)\n",
    "with open(path_pkl+'sample_pair_indices_test_2000.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs_test_2000, f)\n",
    "    \n",
    "with open(path_pkl+'sample_pair_indices_train_4000.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs_train_4000, f)\n",
    "with open(path_pkl+'sample_pair_indices_test_1000.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs_test_1000, f)\n",
    "\n",
    "#with Y56 (Y = 28 labels x S = 2 labels)\n",
    "\n",
    "with open(path_pkl+'sample_pair56_indices_test_400.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs56_train_400, f)\n",
    "with open(path_pkl+'sample_pair56_indices_test_100.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs56_test_100, f)\n",
    "    \n",
    "    \n",
    "with open(path_pkl+'sample_pair56_indices_test_200.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs56_train_200, f)\n",
    "with open(path_pkl+'sample_pair56_indices_test_50.pkl', 'wb') as f:\n",
    "    pickle.dump(pairs56_test_50, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m path_pkl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpkl/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m sample_pairs_indices_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_pickle(path_pkl\u001b[38;5;241m+\u001b[39mpairs[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m sample_pairs_indices_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(path_pkl\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m+\u001b[39mpairs[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample_pairs_indices_train\u001b[38;5;241m.\u001b[39mshape,sample_pairs_indices_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "#        GENERATE SAMPLES (size = 1537 = 2x768 + 1)\n",
    "######################################################\n",
    "\n",
    "#------------------------------------------------------\n",
    "#-      RE-LOAD SAMPLE PARIS INDICES \n",
    "#            (if necessary)\n",
    "#------------------------------------------------------\n",
    "\n",
    "Pairs =[['sample_pair_indices_train_8000.pkl','sample_pair_indices_test_2000.pkl'],\n",
    "        ['sample_pair_indices_train_4000.pkl','sample_pair_indices_test_1000.pkl'],\n",
    "        ['sample_pair56_indices_train_400.pkl','sample_pair56_indices_test_100.pkl'],\n",
    "        ['sample_pair56_indices_train_200.pkl','sample_pair56_indices_test_50.pkl'],\n",
    "        ]\n",
    "\n",
    "    \n",
    "path_pkl = 'pkl/'\n",
    "i = 0\n",
    "sample_pairs_indices_train = pd.read_pickle(path_pkl+pairs[i][0])\n",
    "sample_pairs_indices_test = pd.read_pickle(path_pkl++pairs[i][0])\n",
    "print(sample_pairs_indices_train.shape,sample_pairs_indices_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "#-      GENERATE SAMPLES\n",
    "#------------------------------------------------------\n",
    "# \n",
    "# # Generate the sample of pairs train\n",
    "# sample dim (n,1537) with 1537 = 1st embedding (dim=768) + 2nd embedding  (dim=768) + label (dim=1)\n",
    "pair_sample_train=get_pair_sample(train,sample_pairs_indices_train)#.iloc[:10000,:])\n",
    "print(pair_sample_train.shape)\n",
    "\n",
    "# Generate the sample of pairs test\n",
    "pair_sample_test=get_pair_sample(test,sample_pairs_indices_test)#.iloc[:10000,:])\n",
    "print(pair_sample_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "# REMOVE NaN VALUES (when 'tokens_to_embed'=[] i.e. no words were embedded )\n",
    "\n",
    "# train\n",
    "pair_sample_train=pair_sample_train.dropna()\n",
    "initial=sample_pairs_indices_train.shape[0]\n",
    "final=pair_sample_train.shape[0]\n",
    "print('size of pair_sample:',pair_sample_train.shape,f'\\n{initial-final} lines dropped due to NaN values')\n",
    "\n",
    "# test\n",
    "pair_sample_test=pair_sample_test.dropna()\n",
    "initial_test=sample_pairs_indices_test.shape[0]\n",
    "final_test=pair_sample_test.shape[0]\n",
    "print('size of pair_sample (test):',pair_sample_test.shape,f'\\n{initial_test-final_test} lines dropped due to NaN values')\n",
    "\n",
    "#------------------------------------------------------\n",
    "#-      SAVE SAMPLES\n",
    "#------------------------------------------------------\n",
    "\n",
    "# SAVE PAIR SAMPLE\n",
    "with open(path_pkl+'pair_sample_train.pkl', 'wb') as f:\n",
    "    pickle.dump(pair_sample_train, f)\n",
    "    \n",
    "# SAVE PAIR SAMPLE\n",
    "with open(path_pkl+'pair_sample_test.pkl', 'wb') as f:\n",
    "    pickle.dump(pair_sample_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "#   3. PREDICTIONS\n",
    "###################################################\n",
    "\n",
    "#--------------------------------\n",
    "# TO RELOAD DATA\n",
    "#---------------------------------\n",
    "\n",
    "path_pkl = 'pkl/'\n",
    "\n",
    "# train sample of pairs\n",
    "train = pd.read_pickle(path_pkl+'pair_sample_train.pkl')\n",
    "print('train shape',train.shape)\n",
    "\n",
    "X_train = train.iloc[:,:600]\n",
    "y_train = train.iloc[:,600]\n",
    "y_train = y_train.astype('int')\n",
    "\n",
    "# test sample of pairs\n",
    "test = pd.read_pickle(path_pkl+'pair_sample_test.pkl')\n",
    "print('test shape',test.shape)\n",
    "\n",
    "X_test = test.iloc[:,:600]\n",
    "y_test = test.iloc[:,600]\n",
    "y_test = y_test.astype('int')\n",
    "\n",
    "#-----------------------------------------------\n",
    "#SCALING\n",
    "#-----------------------------------------------\n",
    "#from sklearn import preprocessing\n",
    "#scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#-----------------------------------------------\n",
    "#TRAIN LOGISITIC REGRESSION\n",
    "#-----------------------------------------------\n",
    "\n",
    "# Create and train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# EVALUATE\n",
    "#---------------------------------\n",
    "\n",
    "# Calculate base rate\n",
    "# N pairs / N positive\n",
    "# target rate >> N_positive / N_total\n",
    "\n",
    "# Calculate accuracy of train\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "print('train accuracy',accuracy_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (test):\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

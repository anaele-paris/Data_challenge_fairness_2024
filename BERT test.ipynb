{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guigu\\Anaconda\\envs\\l_Oreal\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guigu\\Anaconda\\envs\\l_Oreal\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! pip install transformers tensorflow\n",
    "# ! pip install tf-keras\n",
    "# ! pip install sacremoses \n",
    "# ! pip install sentencepiece\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import tokenizers as tk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT multilangual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guigu\\Anaconda\\envs\\l_Oreal\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Charger le tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Charger le modèle\n",
    "model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [1:28:17<00:00, 24.08s/it] \n"
     ]
    }
   ],
   "source": [
    "# test sur 10% des données\n",
    "# récupérer les données sample_pair_indices_train.pkl\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_parquet(r\"C:\\Users\\guigu\\Desktop\\Telecom\\Projet Fil Rouge\\data\\df_train.parquet\")\n",
    "\n",
    "train = train.sample(frac=0.1, random_state=1)\n",
    "\n",
    "#  Prétraiter le texte\n",
    "train['text'] = train['title'] + ' ' + train['description'] + ' ' + train['brand'] + ' ' + train['category']\n",
    "train = train.dropna(subset=['text'])\n",
    "\n",
    "# Initialiser une liste pour stocker les embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "# Diviser les données en lots et les traiter un par un\n",
    "batch_size = 32  # Définir une taille de lot qui convient à votre machine\n",
    "for i in tqdm(range(0, len(train), batch_size)):\n",
    "    batch = train.iloc[i:i+batch_size]\n",
    "    input_tokens = tokenizer(batch['text'].tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)  # Réduire la longueur max si nécessaire\n",
    "    outputs = model(input_tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Utiliser l'embedding CLS\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Concaténer tous les embeddings de lots en une seule matrice\n",
    "embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./embeddings/BERT/embeddings_multilangual.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22451888809693513\n"
     ]
    }
   ],
   "source": [
    "# régression logistique \n",
    "\n",
    "y = train['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pre = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pre)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guigu\\Anaconda\\envs\\l_Oreal\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, TFCamembertModel\n",
    "\n",
    "# Charger le tokenizer et le modèle CamemBERT \"base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = TFCamembertModel.from_pretrained('camembert-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [31:04<00:00,  8.47s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_parquet(r\"C:\\Users\\guigu\\Desktop\\Telecom\\Projet Fil Rouge\\data\\df_train.parquet\")\n",
    "\n",
    "train = train.sample(frac=0.1, random_state=1)\n",
    "\n",
    "#  Prétraiter le texte\n",
    "train['text'] = train['title'] + ' ' + train['description'] + ' ' + train['brand'] + ' ' + train['category']\n",
    "train = train.dropna(subset=['text'])\n",
    "# Initialiser une liste pour stocker les embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "# Diviser les données en lots et les traiter un par un\n",
    "batch_size = 32  # Définir une taille de lot qui convient à votre machine\n",
    "for i in tqdm(range(0, len(train), batch_size)):\n",
    "    batch = train.iloc[i:i+batch_size]\n",
    "    input_tokens = tokenizer(batch['text'].tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)  # Réduire la longueur max si nécessaire\n",
    "    outputs = model(input_tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Utiliser l'embedding CLS\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Concaténer tous les embeddings de lots en une seule matrice\n",
    "embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./embeddings/BERT/embeddings_camembert.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.14611546685673557\n"
     ]
    }
   ],
   "source": [
    "# régression logistique \n",
    "\n",
    "y = train['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pre = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pre)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT XML-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, TFXLMRobertaModel\n",
    "\n",
    "# Charger le tokenizer et le modèle XLM-RoBERTa \"base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [29:16<00:00,  7.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialiser une liste pour stocker les embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "# Diviser les données en lots et les traiter un par un\n",
    "batch_size = 32  # Définir une taille de lot qui convient à votre machine\n",
    "for i in tqdm(range(0, len(train), batch_size)):\n",
    "    batch = train.iloc[i:i+batch_size]\n",
    "    input_tokens = tokenizer(batch['text'].tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)  # Réduire la longueur max si nécessaire\n",
    "    outputs = model(input_tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Utiliser l'embedding CLS\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Concaténer tous les embeddings de lots en une seule matrice\n",
    "embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./embeddings/BERT/embeddings_roberta.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10192444761225944\n"
     ]
    }
   ],
   "source": [
    "# régression logistique \n",
    "\n",
    "y = train['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pre = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pre)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAUBERT large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFFlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing TFFlaubertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFFlaubertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFFlaubertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFFlaubertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaubertTokenizer, TFFlaubertModel\n",
    "\n",
    "# Charger le tokenizer FlauBERT\n",
    "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_large_cased')\n",
    "\n",
    "# Charger le modèle FlauBERT en convertissant les poids de PyTorch vers TensorFlow\n",
    "model = TFFlaubertModel.from_pretrained('flaubert/flaubert_large_cased', from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [2:04:52<00:00, 34.05s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Initialiser une liste pour stocker les embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "# Diviser les données en lots et les traiter un par un\n",
    "batch_size = 32  # Définir une taille de lot qui convient à votre machine\n",
    "for i in tqdm(range(0, len(train), batch_size)):\n",
    "    batch = train.iloc[i:i+batch_size]\n",
    "    input_tokens = tokenizer(batch['text'].tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)  # Réduire la longueur max si nécessaire\n",
    "    outputs = model(input_tokens)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # Utiliser l'embedding CLS\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Concaténer tous les embeddings de lots en une seule matrice\n",
    "embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./embeddings/BERT/embeddings_flaubert.txt', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22380612972202424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guigu\\Anaconda\\envs\\l_Oreal\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# régression logistique \n",
    "\n",
    "y = train['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pre = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pre)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pre))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l_Oreal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

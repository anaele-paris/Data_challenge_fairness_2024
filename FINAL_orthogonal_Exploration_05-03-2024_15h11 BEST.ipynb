{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from Data_challenge_fairness_2024.evaluator import *\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Load pickle file and convert to numpy array\n",
    "#####################################################\n",
    "\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    "\n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test']\n",
    "\n",
    "Y56= Y + 28*S\n",
    "#X, X_test,Y,S, S_test = dat[1]\n",
    "\n",
    "print(X.shape,Y.shape,S.shape,X_test_true.shape,S_test_true.shape)\n",
    "\n",
    "path_model = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELS**\n",
    "---\n",
    "<br>\n",
    "0_Baseline : logistic regression (on X_train)<br>\n",
    "1_Baseline optimised : optimized logistic regression (on X + hyperparameters)<br>\n",
    "2_Orthogonal : optimised logisitic regression (on orthogonal X)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'FINAL_models/'  # must finish by '/'\n",
    "path_Y_pred_true = 'FINAL_Y_pred_true/'  # must finish by '/'\n",
    "\n",
    "# with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: model = pickle.load(f)\n",
    "\n",
    "def get_scores(Y_pred, Y, S):\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "    macro_f1 = eval_scores['macro_fscore']\n",
    "    inv_macro_gap = 1-eval_scores['TPR_GAP']\n",
    "\n",
    "    #print results\n",
    "    print('final score :',final_score)\n",
    "    print('macro_f1    :',eval_scores['macro_fscore'])\n",
    "    print('inv_macro_gap',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, macro_f1, inv_macro_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOGISTIC REGRESSION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 0.7638438438438439\n",
      "final score : 0.7365033273594812\n",
      "macro_f1    : 0.669337208333607\n",
      "inv_macro_gap 0.8036694463853555\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# 0. LOGISTIC REGRESSION - BASELINE \n",
    "###############################################\n",
    "\n",
    "name ='0_Reglog_baseline' # changer clf_i\n",
    "\n",
    "# Refresh training data\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "# training (or load)logistic model\n",
    "#clf_0 = LogisticRegression(random_state=0, max_iter=5000,verbose=1).fit(X_train, Y_train)\n",
    "with open(path_model + name + '.pkl', 'rb') as f: clf_0 = pickle.load(f)\n",
    "model = clf_0 \n",
    "  \n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "Y_pred_true = model.predict(X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.24653D+04    |proj g|=  4.11128D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.05318D+04    |proj g|=  8.07961D+02\n",
      "\n",
      "At iterate  100    f=  1.73620D+04    |proj g|=  6.71832D+02\n",
      "\n",
      "At iterate  150    f=  1.61758D+04    |proj g|=  1.03107D+02\n",
      "\n",
      "At iterate  200    f=  1.57705D+04    |proj g|=  1.24845D+02\n",
      "\n",
      "At iterate  250    f=  1.56370D+04    |proj g|=  3.30421D+01\n",
      "\n",
      "At iterate  300    f=  1.55878D+04    |proj g|=  2.25705D+01\n",
      "\n",
      "At iterate  350    f=  1.55716D+04    |proj g|=  3.01638D+01\n",
      "\n",
      "At iterate  400    f=  1.55661D+04    |proj g|=  1.65593D+01\n",
      "\n",
      "At iterate  450    f=  1.55641D+04    |proj g|=  1.12923D+01\n",
      "\n",
      "At iterate  500    f=  1.55632D+04    |proj g|=  3.90256D+00\n",
      "\n",
      "At iterate  550    f=  1.55628D+04    |proj g|=  6.05409D+00\n",
      "\n",
      "At iterate  600    f=  1.55624D+04    |proj g|=  4.51413D+00\n",
      "\n",
      "At iterate  650    f=  1.55619D+04    |proj g|=  1.26648D+01\n",
      "\n",
      "At iterate  700    f=  1.55610D+04    |proj g|=  7.07335D+00\n",
      "\n",
      "At iterate  750    f=  1.55595D+04    |proj g|=  1.62580D+01\n",
      "\n",
      "At iterate  800    f=  1.55577D+04    |proj g|=  4.04886D+00\n",
      "\n",
      "At iterate  850    f=  1.55561D+04    |proj g|=  8.06942D+00\n",
      "\n",
      "At iterate  900    f=  1.55553D+04    |proj g|=  3.19503D+00\n",
      "\n",
      "At iterate  950    f=  1.55548D+04    |proj g|=  7.35077D+00\n",
      "\n",
      "At iterate 1000    f=  1.55546D+04    |proj g|=  5.86208D+00\n",
      "\n",
      "At iterate 1050    f=  1.55544D+04    |proj g|=  7.19974D+00\n",
      "\n",
      "At iterate 1100    f=  1.55543D+04    |proj g|=  1.86093D+00\n",
      "\n",
      "At iterate 1150    f=  1.55542D+04    |proj g|=  6.22605D+00\n",
      "\n",
      "At iterate 1200    f=  1.55540D+04    |proj g|=  1.72893D+00\n",
      "\n",
      "At iterate 1250    f=  1.55538D+04    |proj g|=  2.47744D+00\n",
      "\n",
      "At iterate 1300    f=  1.55534D+04    |proj g|=  1.54724D+01\n",
      "\n",
      "At iterate 1350    f=  1.55529D+04    |proj g|=  5.64675D+00\n",
      "\n",
      "At iterate 1400    f=  1.55524D+04    |proj g|=  7.40508D+00\n",
      "\n",
      "At iterate 1450    f=  1.55520D+04    |proj g|=  1.60392D+00\n",
      "\n",
      "At iterate 1500    f=  1.55519D+04    |proj g|=  3.46264D+00\n",
      "\n",
      "At iterate 1550    f=  1.55518D+04    |proj g|=  1.11544D+00\n",
      "\n",
      "At iterate 1600    f=  1.55518D+04    |proj g|=  7.54015D-01\n",
      "\n",
      "At iterate 1650    f=  1.55518D+04    |proj g|=  6.12268D-01\n",
      "\n",
      "At iterate 1700    f=  1.55518D+04    |proj g|=  1.06131D+00\n",
      "\n",
      "At iterate 1750    f=  1.55517D+04    |proj g|=  9.33577D-01\n",
      "\n",
      "At iterate 1800    f=  1.55517D+04    |proj g|=  6.66243D-01\n",
      "\n",
      "At iterate 1850    f=  1.55516D+04    |proj g|=  1.86799D+00\n",
      "\n",
      "At iterate 1900    f=  1.55516D+04    |proj g|=  1.80377D+00\n",
      "\n",
      "At iterate 1950    f=  1.55515D+04    |proj g|=  9.75567D-01\n",
      "\n",
      "At iterate 2000    f=  1.55515D+04    |proj g|=  4.66221D-01\n",
      "\n",
      "At iterate 2050    f=  1.55514D+04    |proj g|=  2.32000D+00\n",
      "\n",
      "At iterate 2100    f=  1.55514D+04    |proj g|=  1.20032D+00\n",
      "\n",
      "At iterate 2150    f=  1.55514D+04    |proj g|=  4.91983D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "21532   2176   2292      1     0     0   2.146D-01   1.555D+04\n",
      "  F =   15551.423405080426     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Accuracy on transformed test data: 0.856096096096096\n",
      "final score : 0.8207123104282983\n",
      "macro_f1    : 0.8407599159632768\n",
      "inv_macro_gap 0.8006647048933198\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# 1. LOGISTIC REGRESSION - BASELINE OPTIMISED\n",
    "# (no validation set - training on all X)\n",
    "##################################################\n",
    "\n",
    "name ='1_Reglog_optimised' # changer clf_i\n",
    "\n",
    "# Refresh training data\n",
    "# TRAINED ON ALL X\n",
    "\n",
    "# training (or load)logistic model\n",
    "# added regularisation 'l2' with coeff 0.2 (C) and early stopping with tol = 0.0001\n",
    "clf_1 = LogisticRegression(random_state=42, max_iter=5000,verbose=1,penalty='l2', C=0.2, tol=0.0001).fit(X, Y)\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_1 = pickle.load(f)\n",
    "model = clf_1\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "Y_pred_true = model.predict(X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        21532     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  9.24653D+04    |proj g|=  4.11537D+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.09922D+04    |proj g|=  1.42201D+03\n",
      "\n",
      "At iterate  100    f=  1.75913D+04    |proj g|=  4.34926D+02\n",
      "\n",
      "At iterate  150    f=  1.64928D+04    |proj g|=  1.25458D+02\n",
      "\n",
      "At iterate  200    f=  1.60922D+04    |proj g|=  1.01404D+02\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 3. MODEL 2 + ORTHOGONAL PROJECTION \n",
    "# (using the vector of the average difference of embedding between groups \n",
    "# to debiase representation of X, by operating an orthogonal projection\n",
    "##########################################################################\n",
    "\n",
    "name ='2_LogReg_optimised_orthogonal' # changer clf_i\n",
    "\n",
    "# CREATION OF ORTHOGONAL PROJECTION \n",
    "# ---------------------------------\n",
    "# Comparison of average embedding\n",
    "\n",
    "#calculate average embedding for group sensitive (S=1) and non sensitive (S=0)\n",
    "X_sensitive = X[S==1]\n",
    "X_sensitive_mean=X_sensitive.mean()\n",
    "\n",
    "X_non_sensitive = X[S!=1]\n",
    "X_non_sensitive_mean=X_non_sensitive.mean()\n",
    "\n",
    "diff_mean = X_sensitive_mean-X_non_sensitive_mean\n",
    "\n",
    "# function to debiase data\n",
    "def remove_info(dataset, diff):\n",
    "    # Compute the dot product of each row of the dataset with diff\n",
    "    dot_products = np.dot(dataset, diff)\n",
    "    \n",
    "    # Compute the magnitude of diff\n",
    "    diff_magnitude_squared = np.dot(diff, diff)\n",
    "    \n",
    "    # Compute the projection of each row onto diff\n",
    "    projection = np.outer(dot_products / diff_magnitude_squared, diff)\n",
    "    \n",
    "    # Subtract the projection from the dataset\n",
    "    modified_dataset = dataset - projection\n",
    "    \n",
    "    return modified_dataset\n",
    "\n",
    "# Debiasing X (Projection of X orthogonally to diff) \n",
    "# projection in new representation space\n",
    "modified_X = remove_info(X, diff_mean)\n",
    "\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "clf_2 = LogisticRegression(random_state=42, max_iter=5000,verbose=1,penalty='l2', C=0.2, tol=0.0001).fit(modified_X, Y)\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_2 = pickle.load(f)\n",
    "model = clf_2\n",
    "\n",
    "# predicting and assessing\n",
    "Y_pred = model.predict(X_test)\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, diff_mean)  # debiasing\n",
    "Y_pred_true = model.predict(modified_X_test_true)\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# ORTHOGONAL PROJECTION - MAN-WOMEN\n",
    "###############################################\n",
    "# \n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import tokenizers as tk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Charger le modèle\n",
    "model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize \"man\" and \"woman\"\n",
    "tokens_man = tokenizer.encode(\"man\", add_special_tokens=True)  # Ajoute les tokens spéciaux\n",
    "tokens_woman = tokenizer.encode(\"woman\", add_special_tokens=True)\n",
    "\n",
    "print(tokens_man)\n",
    "print(tokens_woman)\n",
    "\n",
    "# transforme en tensor de batch (meme si un seul mot)\n",
    "\n",
    "input_ids_man = tf.constant(tokens_man)[None, :]  # Ajoute une dimension de batch\n",
    "input_ids_woman = tf.constant(tokens_woman)[None, :]  # Ajoute une dimension de batch\n",
    "\n",
    "# Obtenir les embeddings\n",
    "outputs_man = model(input_ids_man)\n",
    "outputs_woman = model(input_ids_woman)\n",
    "\n",
    "# Les embeddings du dernier layer pour le premier token ('[CLS]' par défaut)\n",
    "embedding_man = outputs_man.last_hidden_state[0][0]\n",
    "embedding_woman = outputs_woman.last_hidden_state[0][0]\n",
    "\n",
    "print(embedding_man.shape)  # Doit être (768,)\n",
    "print(embedding_woman.shape)  # Doit être (768,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_sensitive_mean[:10])\n",
    "#print(X_debiaised_sensitive_mean[:10])\n",
    "#print(X_non_sensitive_mean[:10])\n",
    "\n",
    "# Create DataFrame\n",
    "compare = pd.DataFrame({'X_sensitive': X_sensitive_mean, 'Difference S/non S': diff_mean, 'X_debiaised': X_debiaised_sensitive_mean,'X_non_sensitive':X_non_sensitive_mean})\n",
    "compare\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#   SCORE A BLANC\n",
    "#####################################################\n",
    "\n",
    "\n",
    "n=Y.shape[0]\n",
    "Y_pred =np.ones(n)*1\n",
    "accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "#print results\n",
    "print('final score',final_score)\n",
    "print('macro_fscore',eval_scores['macro_fscore'])\n",
    "print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test score \"prediction uniforme\"\n",
    "\n",
    "n=Y.shape[0]\n",
    "\n",
    "Scores_U=pd.DataFrame(columns=['N','N_f','N_h','accuracy','final_score','macro_f1','macro_gap'])\n",
    "\n",
    "print(Scores_U)\n",
    "for i in range(28):\n",
    "    #Test value for all i values\n",
    "    test=pd.DataFrame(np.ones(11893,dtype=int)*i)\n",
    "    test.to_csv(\"all/Data_Challenge_all_\"+str(i)+\".csv\", header = None, index = None)\n",
    "    \n",
    "    Y_pred=pd.DataFrame(np.ones(n,dtype=int)*i)\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    macro_f1 = eval_scores['macro_fscore']\n",
    "    macro_gap = eval_scores['TPR_GAP']\n",
    "    final_score = (macro_f1 +1- macro_gap)/2\n",
    "    # check number of occurence\n",
    "    N = (Y==i).sum()\n",
    "    N_f = (Y56 == i + 28).sum()\n",
    "    N_h = (Y56 == i).sum()\n",
    "    Scores_U.loc[i]= [N, N_f, N_h, accuracy,final_score,macro_f1,macro_gap]\n",
    "\n",
    "Scores_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = Scores_U['N']\n",
    "score = Scores_U['final_score']\n",
    "\n",
    "\n",
    "# Créer et entraîner le modèle de régression linéaire\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reverse = LinearRegression()\n",
    "reverse.fit(np.array(score).reshape(-1, 1),np.array(size).reshape(-1, 1))\n",
    "\n",
    "# Load the scores for X_true\n",
    "scores_true = pd. read_csv('all/distribution_Y_true.txt',header = None,index_col=0)\n",
    "dist_true_pred = reverse.predict(scores_true) #/ 11893 * 27749\n",
    "\n",
    "# Afficher la distribution de X_test_true\n",
    "plt.scatter(size,score)\n",
    "plt.scatter(dist_true_pred,scores_true)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Scores_U['final_score'],scores_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(28),size,label='X')\n",
    "plt.plot(range(28),np.round(dist_true_pred),'X_test_true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (L2 norm recommended for embeddings)\n",
    "#X = normalize(X, norm='l2')\n",
    "#X_test_true = normalize(X_test_true, norm='l2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# train_test_split with Y56 (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(X, Y56, test_size=0.2, random_state=42)\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame({'label':Y,'S':S})\n",
    "\n",
    "# Sum counts of 1s and 0s to get the total count for each label\n",
    "result = dist.groupby('label')['S'].value_counts().unstack(fill_value=0)\n",
    "result.columns = ['S', 'not_S']\n",
    "result['total_count'] = result.sum(axis=1)\n",
    "\n",
    "# Calculate totals for each column\n",
    "totals = result.sum(axis=0)\n",
    "result.loc[30] = totals\n",
    "\n",
    "# Calculate total count percentages for each count\n",
    "result['%_S_label'] = round((result['S'] / result['total_count']) * 100)\n",
    "result['%_not_S_label'] = round((result['not_S'] / result['total_count']) * 100)\n",
    "result['%_total'] = np.round(result['total_count']/len(Y)*100,2 ) # % of total count\n",
    "result['%_S_total'] = np.round(result['S']/(S==1).sum()*100,2 ) # % of total count\n",
    "result['%_not_S_total'] = np.round(result['not_S']/(S!=1).sum()*100,2 ) # % of total count\n",
    "result['diff_%_S_total']=result['%_S_total']-result['%_total']\n",
    "result['|']='|'\n",
    "#Reorder table\n",
    "result = result.reindex(columns=['total_count', '%_total','|','S','%_S_total','diff_%_S_total','|','not_S', '%_not_S_total','|','%_S_label', '%_not_S_label'])\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the '%_S' column\n",
    "labels = pd.to_numeric(result.index)\n",
    "data = result['%_total']\n",
    "data_S = result['%_S_total']\n",
    "data_not_S = result['%_not_S_total']\n",
    "\n",
    "# Create a bar plot\n",
    "plt.plot(labels[:-1], data[:-1])\n",
    "plt.plot(labels[:-1],data_S[:-1], label='Sensitive')\n",
    "plt.plot(labels[:-1],data_not_S[:-1], label='Not_sensitive')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage of total')\n",
    "plt.title('Percentage of total for sample, S and non-S')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sorted = result.iloc[:-1].sort_values(by='diff_%_S_total', ascending=False)\n",
    "result_sorted['original_label'] = result_sorted.index\n",
    "result_sorted=result_sorted.reset_index(drop=True)\n",
    "#result_sorted.reset_index(drop=True, inplace=True)\n",
    "result_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from the '%_S' column\n",
    "labels_sorted = result_sorted['original_label']\n",
    "#print(labels_sorted)\n",
    "data = result_sorted['%_total']\n",
    "data_S = result_sorted['%_S_total']\n",
    "data_not_S = result_sorted['%_not_S_total']\n",
    "\n",
    "# Create a bar plot\n",
    "plt.plot(data)\n",
    "plt.plot(data_S, label='Sensitive')\n",
    "plt.plot(data_not_S, label='Not_sensitive')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage of total')\n",
    "#plt.xticks(labels_sorted)\n",
    "plt.xticks(ticks=range(len(labels_sorted)), labels=labels_sorted, rotation=90)  # Rotate if there are many labels\n",
    "\n",
    "plt.title('Percentage of total for sample, S and non-S')\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL ORTHOGONAL DEBIASING**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Fonction de similarité cosinus\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Pré-générer les embeddings pour chaque token dans le vocabulaire\n",
    "# ATTENTION : Cette étape est très coûteuse !\n",
    "vocab_embeddings = {}\n",
    "for word, token_id in tokenizer.vocab.items():\n",
    "    input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(word)]).unsqueeze(0)  # Ajouter batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    vocab_embeddings[word] = outputs.last_hidden_state[0, 0, :].numpy()  # Utiliser l'embedding du token\n",
    "\n",
    "# Supposons que X soit votre matrice d'embeddings (n, 768)\n",
    "# X = ...\n",
    "\n",
    "# Trouver le mot le plus proche pour les 10 premiers embeddings\n",
    "for i in range(10):\n",
    "    embedding = X[i]\n",
    "    similarities = {word: cosine_similarity(embedding, word_emb) for word, word_emb in vocab_embeddings.items()}\n",
    "    closest_word = max(similarities, key=similarities.get)\n",
    "    print(f\"Ligne {i}: Mot le plus proche = {closest_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**baseline Logistic regression on raw data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients:\", clf_1.coef_.shape)\n",
    "print(\"Intercept:\", clf_1.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_scores_1)\n",
    "print(confusion_matrices_eval_1.keys)\n",
    "\n",
    "#show confusion matrix for Accurarcy key 0\n",
    "pd.DataFrame(confusion_matrices_eval_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true_1 = clf_1.predict(X_test_true)\n",
    "results_1=pd.DataFrame(y_test_true_1, columns= ['score'])\n",
    "\n",
    "results_1.to_csv(\"Data_Challenge_MDI_341_1.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECOND METHOD - ADVERSARIAL NN**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh training data\n",
    "# X_train, X_test, Y_train, Y_test, S_train, S_test = X_train_, X_test_, Y_train_, Y_test_, S_train_, S_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(256, activation='relu')(input_layer)\n",
    "main_task_output = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(256, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_2 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              loss_weights=[1, -0.1],\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# X normal:isation\n",
    "# none in method 2\n",
    "\n",
    "# Train the model\n",
    "model_2.fit(X_train, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_2 = Model(inputs=model_2.input, outputs=main_task_hidden)\n",
    "X_train_transformed_2 = feature_extractor_2.predict(X_train)\n",
    "X_test_transformed_2 = feature_extractor_2.predict(X_test)\n",
    "X_test_true_transformed_2 = feature_extractor_2.predict(X_test_true)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "clf_2 = LogisticRegression(max_iter=5000)  # Increase max_iter if needed for convergence\n",
    "history_new_2 = clf_2.fit(X_train_transformed_2, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_2 = clf_2.predict(X_test_transformed_2)\n",
    "accuracy_2= accuracy_score(Y_test, Y_pred_2)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_2}\")\n",
    "\n",
    "# Step 4 : Predict with gloabl score\n",
    "eval_scores_2, confusion_matrices_eval_2 = gap_eval_scores(Y_pred_2, Y_test, S_test, metrics=['TPR'])\n",
    "final_score_2 = (eval_scores_2['macro_fscore']+ (1-eval_scores_2['TPR_GAP']))/2\n",
    "print('\\nfinal',final_score_2)\n",
    "print('macro_fscore',eval_scores_2['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores_2['TPR_GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the provided test data with you classifier\n",
    "y_test = clf_2.predict(X_test_true_transformed_2)\n",
    "results=pd.DataFrame(y_test, columns= ['score'])\n",
    "\n",
    "results.to_csv(\"Data_Challenge_MDI_341_2.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIRD METHOD**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# get X_train an\n",
    "# X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform your training data\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "X_test_true_standardized = scaler.transform(X_test_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(256, activation='relu')(input_layer)\n",
    "main_task_hidden = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(256, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_3 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train_standardized.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# Train the model\n",
    "model_3.fit(X_train_standardized, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_3 = Model(inputs=model_3.input, outputs=main_task_hidden)\n",
    "X_train_transformed_3 = feature_extractor_3.predict(X_train_standardized)\n",
    "X_test_transformed_3 = feature_extractor_3.predict(X_test_standardized)\n",
    "X_test_true_transformed_3 = feature_extractor_3.predict(X_test_true_standardized)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "new_classifier_3 = LogisticRegression(max_iter=10000)  # Increase max_iter if needed for convergence\n",
    "history_new_3 = new_classifier_3.fit(X_train_transformed_3, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_3 = new_classifier_3.predict(X_test_transformed_3)\n",
    "accuracy_3= accuracy_score(Y_test, Y_pred_3)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate final score\n",
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred_3, Y_test, S_test, metrics=['TPR'])\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "print('macro_fscore',eval_scores['macro_fscore'])\n",
    "print('1-eval_scores',1-eval_scores['TPR_GAP'])\n",
    "print('final score (average)',final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "X_test_true_transformed_3 = feature_extractor_3.predict(X_test_true)\n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true = clf.predict(X_test_true_transformed_3)\n",
    "results_3=pd.DataFrame(y_test_true, columns= ['score'])\n",
    "\n",
    "results_3.to_csv(\"Data_Challenge_MDI_341_3.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4th METHOD**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy , CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train is your input embeddings and S is your sensitive attribute\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(768,))\n",
    "\n",
    "# Main task classifier layers\n",
    "main_task_hidden = Dense(512, activation='relu')(input_layer)\n",
    "main_task_hidden = Dropout(0.5)(main_task_hidden)\n",
    "main_task_output = Dense(28, activation='softmax', name='main_task_output')(main_task_hidden)\n",
    "\n",
    "# Adversarial component layers\n",
    "adversary_hidden = Dense(512, activation='relu')(main_task_hidden)\n",
    "adversary_hidden = Dropout(0.5)(adversary_hidden)\n",
    "adversarial_output = Dense(1, activation='sigmoid', name='adversarial_output')(adversary_hidden)\n",
    "\n",
    "# Model\n",
    "model_4 = Model(inputs=input_layer, outputs=[main_task_output, adversarial_output])\n",
    "\n",
    "# Optimizers\n",
    "#main_task_optimizer = Adam(learning_rate=0.001)\n",
    "#adversarial_optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Loss functions\n",
    "main_task_loss = CategoricalCrossentropy()\n",
    "adversarial_loss = BinaryCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model_4.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss={'main_task_output': main_task_loss, 'adversarial_output': adversarial_loss},\n",
    "              metrics={'main_task_output': ['accuracy'], 'adversarial_output': [AUC()]})\n",
    "\n",
    "# Prepare the labels for the main task and the adversarial task\n",
    "Y_main_task = to_categorical(Y_train, num_classes=28)#Y_train # Your main task labels\n",
    "Y_adversary = S_train    # Your sensitive attribute labels\n",
    "\n",
    "# check size on input .output\n",
    "print(X_train_standardized.shape,Y_main_task.shape,Y_adversary.shape)\n",
    "\n",
    "# Train the model\n",
    "model_4.fit(X_train_standardized, {'main_task_output': Y_main_task, 'adversarial_output': Y_adversary}, epochs=10)\n",
    "\n",
    "# After training, you can use the output of `main_task_hidden` as your new unbiased representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X_train and X_test\n",
    "feature_extractor_4 = Model(inputs=model_4.input, outputs=main_task_hidden)\n",
    "X_train_transformed_4 = feature_extractor_4.predict(X_train_standardized)\n",
    "X_test_transformed_4 = feature_extractor_4.predict(X_test_standardized)\n",
    "X_test_true_transformed_4 = feature_extractor_4.predict(X_test_true_standardized)\n",
    "\n",
    "# Step 2: Train a new classifier on the transformed training data\n",
    "new_classifier_4 = LogisticRegression(max_iter=10000)  # Increase max_iter if needed for convergence\n",
    "history_4 = new_classifier_4.fit(X_train_transformed_4, Y_train)  # Y_train are your original training labels\n",
    "\n",
    "# Step 3: Predict on the transformed test data and evaluate\n",
    "Y_pred_4 = new_classifier_4.predict(X_test_transformed_4)\n",
    "accuracy_4 = accuracy_score(Y_test, Y_pred_4)  # Y_test are your original test labels\n",
    "print(f\"Accuracy on transformed test data: {accuracy_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred_4, Y_test, S_test, metrics=['TPR'])\n",
    "#eval_scores#eval_scores['macro_fscore']\n",
    "#eval_scores['TPR_GAP']\n",
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"true\" test data\n",
    "X_test_true = dat['X_test']\n",
    "S_test_true = dat['S_test'] \n",
    "\n",
    "X_test_true_transformed_4 = feature_extractor_4.predict(X_test_true)\n",
    "\n",
    "# Classify the provided test data with you classifier\n",
    "y_test_true = clf.predict(X_test_true_transformed_4)\n",
    "results_4=pd.DataFrame(y_test_true, columns= ['score'])\n",
    "\n",
    "results_4.to_csv(\"Data_Challenge_MDI_341_4.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
